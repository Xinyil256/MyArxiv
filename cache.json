{"2024-03-25T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2403.10963v2","updated":"2024-03-25T12:37:16Z","published":"2024-03-16T16:17:47Z","title":"Pointer-Generator Networks for Low-Resource Machine Translation: Don't\n  Copy That!","summary":"  While Transformer-based neural machine translation (NMT) is very effective in\nhigh-resource settings, many languages lack the necessary large parallel\ncorpora to benefit from it. In the context of low-resource (LR) MT between two\nclosely-related languages, a natural intuition is to seek benefits from\nstructural \"shortcuts\", such as copying subwords from the source to the target,\ngiven that such language pairs often share a considerable number of identical\nwords, cognates, and borrowings. We test Pointer-Generator Networks for this\npurpose for six language pairs over a variety of resource ranges, and find weak\nimprovements for most settings. However, analysis shows that the model does not\nshow greater improvements for closely-related vs. more distant language pairs,\nor for lower resource ranges, and that the models do not exhibit the expected\nusage of the mechanism for shared subwords. Our discussion of the reasons for\nthis behaviour highlights several general challenges for LR NMT, such as modern\ntokenization strategies, noisy real-world conditions, and linguistic\ncomplexities. We call for better scrutiny of linguistically motivated\nimprovements to NMT given the blackbox nature of Transformer models, as well as\nfor a focus on the above problems in the field.\n","authors":["Niyati Bafna","Philipp Koehn","David Yarowsky"],"pdf_url":"https://arxiv.org/pdf/2403.10963v2.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2403.16702v1","updated":"2024-03-25T12:34:33Z","published":"2024-03-25T12:34:33Z","title":"ProCQA: A Large-scale Community-based Programming Question Answering\n  Dataset for Code Search","summary":"  Retrieval-based code question answering seeks to match user queries in\nnatural language to relevant code snippets. Previous approaches typically rely\non pretraining models using crafted bi-modal and uni-modal datasets to align\ntext and code representations. In this paper, we introduce ProCQA, a\nlarge-scale programming question answering dataset extracted from the\nStackOverflow community, offering naturally structured mixed-modal QA pairs. To\nvalidate its effectiveness, we propose a modality-agnostic contrastive\npre-training approach to improve the alignment of text and code representations\nof current code language models. Compared to previous models that primarily\nemploy bimodal and unimodal pairs extracted from CodeSearchNet for\npre-training, our model exhibits significant performance improvements across a\nwide range of code retrieval benchmarks.\n","authors":["Zehan Li","Jianfei Zhang","Chuantao Yin","Yuanxin Ouyang","Wenge Rong"],"pdf_url":"https://arxiv.org/pdf/2403.16702v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.16685v1","updated":"2024-03-25T12:21:38Z","published":"2024-03-25T12:21:38Z","title":"ToXCL: A Unified Framework for Toxic Speech Detection and Explanation","summary":"  The proliferation of online toxic speech is a pertinent problem posing\nthreats to demographic groups. While explicit toxic speech contains offensive\nlexical signals, implicit one consists of coded or indirect language.\nTherefore, it is crucial for models not only to detect implicit toxic speech\nbut also to explain its toxicity. This draws a unique need for unified\nframeworks that can effectively detect and explain implicit toxic speech. Prior\nworks mainly formulated the task of toxic speech detection and explanation as a\ntext generation problem. Nonetheless, models trained using this strategy can be\nprone to suffer from the consequent error propagation problem. Moreover, our\nexperiments reveal that the detection results of such models are much lower\nthan those that focus only on the detection task. To bridge these gaps, we\nintroduce ToXCL, a unified framework for the detection and explanation of\nimplicit toxic speech. Our model consists of three modules: a (i) Target Group\nGenerator to generate the targeted demographic group(s) of a given post; an\n(ii) Encoder-Decoder Model in which the encoder focuses on detecting implicit\ntoxic speech and is boosted by a (iii) Teacher Classifier via knowledge\ndistillation, and the decoder generates the necessary explanation. ToXCL\nachieves new state-of-the-art effectiveness, and outperforms baselines\nsignificantly.\n","authors":["Nhat M. Hoang","Xuan Long Do","Duc Anh Do","Duc Anh Vu","Luu Anh Tuan"],"pdf_url":"https://arxiv.org/pdf/2403.16685v1.pdf","comment":"Accepted at NAACL 2024 (Main Conference)"},{"id":"http://arxiv.org/abs/2403.16668v1","updated":"2024-03-25T12:07:21Z","published":"2024-03-25T12:07:21Z","title":"Who is bragging more online? A large scale analysis of bragging in\n  social media","summary":"  Bragging is the act of uttering statements that are likely to be positively\nviewed by others and it is extensively employed in human communication with the\naim to build a positive self-image of oneself. Social media is a natural\nplatform for users to employ bragging in order to gain admiration, respect,\nattention and followers from their audiences. Yet, little is known about the\nscale of bragging online and its characteristics. This paper employs\ncomputational sociolinguistics methods to conduct the first large scale study\nof bragging behavior on Twitter (U.S.) by focusing on its overall prevalence,\ntemporal dynamics and impact of demographic factors. Our study shows that the\nprevalence of bragging decreases over time within the same population of users.\nIn addition, younger, more educated and popular users in the U.S. are more\nlikely to brag. Finally, we conduct an extensive linguistics analysis to unveil\nspecific bragging themes associated with different user traits.\n","authors":["Mali Jin","Daniel Preoţiuc-Pietro","A. Seza Doğruöz","Nikolaos Aletras"],"pdf_url":"https://arxiv.org/pdf/2403.16668v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.02930v2","updated":"2024-03-25T12:07:13Z","published":"2024-03-05T12:48:29Z","title":"A Second Look on BASS -- Boosting Abstractive Summarization with Unified\n  Semantic Graphs -- A Replication Study","summary":"  We present a detailed replication study of the BASS framework, an abstractive\nsummarization system based on the notion of Unified Semantic Graphs. Our\ninvestigation includes challenges in replicating key components and an ablation\nstudy to systematically isolate error sources rooted in replicating novel\ncomponents. Our findings reveal discrepancies in performance compared to the\noriginal work. We highlight the significance of paying careful attention even\nto reasonably omitted details for replicating advanced frameworks like BASS,\nand emphasize key practices for writing replicable papers.\n","authors":["Osman Alperen Koraş","Jörg Schlötterer","Christin Seifert"],"pdf_url":"https://arxiv.org/pdf/2403.02930v2.pdf","comment":"This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Advances in Information Retrieval, 46th European Conference on\n  Information Retrieval, ECIR 2024. 16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.16662v1","updated":"2024-03-25T11:56:29Z","published":"2024-03-25T11:56:29Z","title":"RU22Fact: Optimizing Evidence for Multilingual Explainable Fact-Checking\n  on Russia-Ukraine Conflict","summary":"  Fact-checking is the task of verifying the factuality of a given claim by\nexamining the available evidence. High-quality evidence plays a vital role in\nenhancing fact-checking systems and facilitating the generation of explanations\nthat are understandable to humans. However, the provision of both sufficient\nand relevant evidence for explainable fact-checking systems poses a challenge.\nTo tackle this challenge, we propose a method based on a Large Language Model\nto automatically retrieve and summarize evidence from the Web. Furthermore, we\nconstruct RU22Fact, a novel multilingual explainable fact-checking dataset on\nthe Russia-Ukraine conflict in 2022 of 16K samples, each containing real-world\nclaims, optimized evidence, and referenced explanation. To establish a baseline\nfor our dataset, we also develop an end-to-end explainable fact-checking system\nto verify claims and generate explanations. Experimental results demonstrate\nthe prospect of optimized evidence in increasing fact-checking performance and\nalso indicate the possibility of further progress in the end-to-end claim\nverification and explanation generation tasks.\n","authors":["Yirong Zeng","Xiao Ding","Yi Zhao","Xiangyu Li","Jie Zhang","Chao Yao","Ting Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2403.16662v1.pdf","comment":"12 pages, 3 figures, accepted by lrec-coling2024"},{"id":"http://arxiv.org/abs/2402.10685v2","updated":"2024-03-25T11:50:32Z","published":"2024-02-16T13:39:34Z","title":"LongHeads: Multi-Head Attention is Secretly a Long Context Processor","summary":"  Large language models (LLMs) have achieved impressive performance in numerous\ndomains but often struggle to process lengthy inputs effectively and\nefficiently due to limited length generalization and attention's quadratic\ncomputational demands. Many sought to mitigate this by restricting the\nattention window within the pre-trained length. However, these methods\nintroduce new issues such as ignoring the middle context and requiring\nadditional training. To address these problems, we propose LongHeads, a\ntraining-free framework that enhances LLM's long context ability by unlocking\nmulti-head attention's untapped potential. Instead of allowing each head to\nattend to the full sentence, which struggles with generalizing to longer\nsequences due to out-of-distribution (OOD) issues, we allow each head to\nprocess in-distribution length by selecting and attending to important context\nchunks. To this end, we propose a chunk selection strategy that relies on the\ninherent correlation between the query and the key representations, efficiently\ndistributing context chunks to different heads. In this way, each head ensures\nit can effectively process attended tokens within the trained length, while\ndifferent heads in different layers can collectively process longer contexts.\nLongHeads works efficiently in linear time, fits seamlessly with many LLMs that\nuse relative positional encoding. LongHeads achieves 100% accuracy at the 128k\nlength on passkey retrieval task, verifying LongHeads's efficacy in extending\nthe usable context window for existing models. We release our code at\nhttps://github.com/LuLuLuyi/LongHeads .\n","authors":["Yi Lu","Xin Zhou","Wei He","Jun Zhao","Tao Ji","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2402.10685v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16655v1","updated":"2024-03-25T11:45:21Z","published":"2024-03-25T11:45:21Z","title":"Grammatical vs Spelling Error Correction: An Investigation into the\n  Responsiveness of Transformer-based Language Models using BART and MarianMT","summary":"  Text continues to remain a relevant form of representation for information.\nText documents are created either in digital native platforms or through the\nconversion of other media files such as images and speech. While the digital\nnative text is invariably obtained through physical or virtual keyboards,\ntechnologies such as OCR and speech recognition are utilized to transform the\nimages and speech signals into text content. All these variety of mechanisms of\ntext generation also introduce errors into the captured text.\n  This project aims at analyzing different kinds of error that occurs in text\ndocuments. The work employs two of the advanced deep neural network-based\nlanguage models, namely, BART and MarianMT, to rectify the anomalies present in\nthe text. Transfer learning of these models with available dataset is performed\nto finetune their capacity for error correction. A comparative study is\nconducted to investigate the effectiveness of these models in handling each of\nthe defined error categories. It is observed that while both models can bring\ndown the erroneous sentences by 20+%, BART can handle spelling errors far\nbetter (24.6%) than grammatical errors (8.8%).\n","authors":["Rohit Raju","Peeta Basa Pati","SA Gandheesh","Gayatri Sanjana Sannala","Suriya KS"],"pdf_url":"https://arxiv.org/pdf/2403.16655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16630v1","updated":"2024-03-25T11:20:23Z","published":"2024-03-25T11:20:23Z","title":"A comparative analysis of embedding models for patent similarity","summary":"  This paper makes two contributions to the field of text-based patent\nsimilarity. First, it compares the performance of different kinds of\npatent-specific pretrained embedding models, namely static word embeddings\n(such as word2vec and doc2vec models) and contextual word embeddings (such as\ntransformers based models), on the task of patent similarity calculation.\nSecond, it compares specifically the performance of Sentence Transformers\n(SBERT) architectures with different training phases on the patent similarity\ntask. To assess the models' performance, we use information about patent\ninterferences, a phenomenon in which two or more patent claims belonging to\ndifferent patent applications are proven to be overlapping by patent examiners.\nTherefore, we use these interferences cases as a proxy for maximum similarity\nbetween two patents, treating them as ground-truth to evaluate the performance\nof the different embedding models. Our results point out that, first, Patent\nSBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer\narchitecture proposed in this research, outperforms the current\nstate-of-the-art in patent similarity. Second, they show that, in some cases,\nlarge static models performances are still comparable to contextual ones when\ntrained on extensive data; thus, we believe that the superiority in the\nperformance of contextual embeddings may not be related to the actual\narchitecture but rather to the way the training phase is performed.\n","authors":["Grazia Sveva Ascione","Valerio Sterzi"],"pdf_url":"https://arxiv.org/pdf/2403.16630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12263v2","updated":"2024-03-25T10:52:14Z","published":"2023-09-21T17:13:21Z","title":"On the Relationship between Skill Neurons and Robustness in Prompt\n  Tuning","summary":"  Prompt Tuning is a popular parameter-efficient finetuning method for\npre-trained large language models (PLMs). Based on experiments with RoBERTa, it\nhas been suggested that Prompt Tuning activates specific neurons in the\ntransformer's feed-forward networks, that are highly predictive and selective\nfor the given task. In this paper, we study the robustness of Prompt Tuning in\nrelation to these \"skill neurons\", using RoBERTa and T5. We show that prompts\ntuned for a specific task are transferable to tasks of the same type but are\nnot very robust to adversarial data. While prompts tuned for RoBERTa yield\nbelow-chance performance on adversarial data, prompts tuned for T5 are slightly\nmore robust and retain above-chance performance in two out of three cases. At\nthe same time, we replicate the finding that skill neurons exist in RoBERTa and\nfurther show that skill neurons also exist in T5. Interestingly, the skill\nneurons of T5 determined on non-adversarial data are also among the most\npredictive neurons on the adversarial data, which is not the case for RoBERTa.\nWe conclude that higher adversarial robustness may be related to a model's\nability to consistently activate the relevant skill neurons on adversarial\ndata.\n","authors":["Leon Ackermann","Xenia Ohmer"],"pdf_url":"https://arxiv.org/pdf/2309.12263v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16614v1","updated":"2024-03-25T10:44:38Z","published":"2024-03-25T10:44:38Z","title":"Semantically Enriched Cross-Lingual Sentence Embeddings for\n  Crisis-related Social Media Texts","summary":"  Tasks such as semantic search and clustering on crisis-related social media\ntexts enhance our comprehension of crisis discourse, aiding decision-making and\ntargeted interventions. Pre-trained language models have advanced performance\nin crisis informatics, but their contextual embeddings lack semantic\nmeaningfulness. Although the CrisisTransformers family includes a sentence\nencoder to address the semanticity issue, it remains monolingual, processing\nonly English texts. Furthermore, employing separate models for different\nlanguages leads to embeddings in distinct vector spaces, introducing challenges\nwhen comparing semantic similarities between multi-lingual texts. Therefore, we\npropose multi-lingual sentence encoders (CT-XLMR-SE and CT-mBERT-SE) that embed\ncrisis-related social media texts for over 50 languages, such that texts with\nsimilar meanings are in close proximity within the same vector space,\nirrespective of language diversity. Results in sentence encoding and sentence\nmatching tasks are promising, suggesting these models could serve as robust\nbaselines when embedding multi-lingual crisis-related social media texts. The\nmodels are publicly available at: https://huggingface.co/crisistransformers.\n","authors":["Rabindra Lamsal","Maria Rodriguez Read","Shanika Karunasekera"],"pdf_url":"https://arxiv.org/pdf/2403.16614v1.pdf","comment":"Accepted to ISCRAM 2024"},{"id":"http://arxiv.org/abs/2403.16609v1","updated":"2024-03-25T10:39:18Z","published":"2024-03-25T10:39:18Z","title":"Conversational Grounding: Annotation and Analysis of Grounding Acts and\n  Grounding Units","summary":"  Successful conversations often rest on common understanding, where all\nparties are on the same page about the information being shared. This process,\nknown as conversational grounding, is crucial for building trustworthy dialog\nsystems that can accurately keep track of and recall the shared information.\nThe proficiencies of an agent in grounding the conveyed information\nsignificantly contribute to building a reliable dialog system. Despite recent\nadvancements in dialog systems, there exists a noticeable deficit in their\ngrounding capabilities. Traum provided a framework for conversational grounding\nintroducing Grounding Acts and Grounding Units, but substantial progress,\nespecially in the realm of Large Language Models, remains lacking. To bridge\nthis gap, we present the annotation of two dialog corpora employing Grounding\nActs, Grounding Units, and a measure of their degree of grounding. We discuss\nour key findings during the annotation and also provide a baseline model to\ntest the performance of current Language Models in categorizing the grounding\nacts of the dialogs. Our work aims to provide a useful resource for further\nresearch in making conversations with machines better understood and more\nreliable in natural day-to-day collaborative dialogs.\n","authors":["Biswesh Mohapatra","Seemab Hassan","Laurent Romary","Justine Cassell"],"pdf_url":"https://arxiv.org/pdf/2403.16609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16592v1","updated":"2024-03-25T10:09:03Z","published":"2024-03-25T10:09:03Z","title":"TrustAI at SemEval-2024 Task 8: A Comprehensive Analysis of Multi-domain\n  Machine Generated Text Detection Techniques","summary":"  The Large Language Models (LLMs) exhibit remarkable ability to generate\nfluent content across a wide spectrum of user queries. However, this capability\nhas raised concerns regarding misinformation and personal information leakage.\nIn this paper, we present our methods for the SemEval2024 Task8, aiming to\ndetect machine-generated text across various domains in both mono-lingual and\nmulti-lingual contexts. Our study comprehensively analyzes various methods to\ndetect machine-generated text, including statistical, neural, and pre-trained\nmodel approaches. We also detail our experimental setup and perform a in-depth\nerror analysis to evaluate the effectiveness of these methods. Our methods\nobtain an accuracy of 86.9\\% on the test set of subtask-A mono and 83.7\\% for\nsubtask-B. Furthermore, we also highlight the challenges and essential factors\nfor consideration in future studies.\n","authors":["Ashok Urlana","Aditya Saibewar","Bala Mallikarjunarao Garlapati","Charaka Vinayak Kumar","Ajeet Kumar Singh","Srinivasa Rao Chalamala"],"pdf_url":"https://arxiv.org/pdf/2403.16592v1.pdf","comment":"8 pages, 1 Figure"},{"id":"http://arxiv.org/abs/2403.16584v1","updated":"2024-03-25T09:51:54Z","published":"2024-03-25T09:51:54Z","title":"Can Large Language Models (or Humans) Distill Text?","summary":"  We investigate the potential of large language models (LLMs) to distill text:\nto remove the textual traces of an undesired forbidden variable. We employ a\nrange of LLMs with varying architectures and training approaches to distill\ntext by identifying and removing information about the target variable while\npreserving other relevant signals. Our findings shed light on the strengths and\nlimitations of LLMs in addressing the distillation and provide insights into\nthe strategies for leveraging these models in computational social science\ninvestigations involving text data. In particular, we show that in the strong\ntest of removing sentiment, the statistical association between the processed\ntext and sentiment is still clearly detectable to machine learning classifiers\npost-LLM-distillation. Furthermore, we find that human annotators also struggle\nto distill sentiment while preserving other semantic content. This suggests\nthere may be limited separability between concept variables in some text\ncontexts, highlighting limitations of methods relying on text-level\ntransformations and also raising questions about the robustness of distillation\nmethods that achieve statistical independence in representation space if this\nis difficult for human coders operating on raw text to attain.\n","authors":["Nicolas Audinet de Pieuchon","Adel Daoud","Connor Thomas Jerzak","Moa Johansson","Richard Johansson"],"pdf_url":"https://arxiv.org/pdf/2403.16584v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15248v2","updated":"2024-03-25T09:36:54Z","published":"2024-02-23T10:27:42Z","title":"Chitchat as Interference: Adding User Backstories to Task-Oriented\n  Dialogues","summary":"  During task-oriented dialogues (TODs), human users naturally introduce\nchitchat that is beyond the immediate scope of the task, interfering with the\nflow of the conversation. To address this issue without the need for expensive\nmanual data creation, we use few-shot prompting with Llama-2-70B to enhance the\nMultiWOZ dataset with user backstories, a typical example of chitchat\ninterference in TODs. We assess the impact of this addition by testing two\nmodels: one trained solely on TODs and another trained on TODs with a\npreliminary chitchat interaction. Our analysis demonstrates that our enhanced\ndataset poses a challenge for these systems. Moreover, we demonstrate that our\ndataset can be effectively used for training purposes, enabling a system to\nconsistently acknowledge the user's backstory while also successfully moving\nthe task forward in the same turn, as confirmed by human evaluation. These\nfindings highlight the benefits of generating novel chitchat-TOD scenarios to\ntest TOD systems more thoroughly and improve their resilience to natural user\ninterferences\n","authors":["Armand Stricker","Patrick Paroubek"],"pdf_url":"https://arxiv.org/pdf/2402.15248v2.pdf","comment":"Accepted @ LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.16571v1","updated":"2024-03-25T09:36:51Z","published":"2024-03-25T09:36:51Z","title":"NSINA: A News Corpus for Sinhala","summary":"  The introduction of large language models (LLMs) has advanced natural\nlanguage processing (NLP), but their effectiveness is largely dependent on\npre-training resources. This is especially evident in low-resource languages,\nsuch as Sinhala, which face two primary challenges: the lack of substantial\ntraining data and limited benchmarking datasets. In response, this study\nintroduces NSINA, a comprehensive news corpus of over 500,000 articles from\npopular Sinhala news websites, along with three NLP tasks: news media\nidentification, news category prediction, and news headline generation. The\nrelease of NSINA aims to provide a solution to challenges in adapting LLMs to\nSinhala, offering valuable resources and benchmarks for improving NLP in the\nSinhala language. NSINA is the largest news corpus for Sinhala, available up to\ndate.\n","authors":["Hansi Hettiarachchi","Damith Premasiri","Lasitha Uyangodage","Tharindu Ranasinghe"],"pdf_url":"https://arxiv.org/pdf/2403.16571v1.pdf","comment":"Accepted to LREC-COLING 2024 (The 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation)"},{"id":"http://arxiv.org/abs/2403.16554v1","updated":"2024-03-25T09:04:14Z","published":"2024-03-25T09:04:14Z","title":"PE: A Poincare Explanation Method for Fast Text Hierarchy Generation","summary":"  The black-box nature of deep learning models in NLP hinders their widespread\napplication. The research focus has shifted to Hierarchical Attribution (HA)\nfor its ability to model feature interactions. Recent works model\nnon-contiguous combinations with a time-costly greedy search in Eculidean\nspaces, neglecting underlying linguistic information in feature\nrepresentations. In this work, we introduce a novel method, namely Poincar\\'e\nExplanation (PE), for modeling feature interactions using hyperbolic spaces in\nan $O(n^2logn)$ time complexity. Inspired by Poincar\\'e model, we propose a\nframework to project the embeddings into hyperbolic spaces, which exhibit\nbetter inductive biases for syntax and semantic hierarchical structures.\nEventually, we prove that the hierarchical clustering process in the projected\nspace could be viewed as building a minimum spanning tree and propose a time\nefficient algorithm. Experimental results demonstrate the effectiveness of our\napproach.\n","authors":["Qian Chen","Xiaofeng He","Hongzhao Li","Hongyu Yi"],"pdf_url":"https://arxiv.org/pdf/2403.16554v1.pdf","comment":"12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2310.19531v6","updated":"2024-03-25T08:46:58Z","published":"2023-10-30T13:33:21Z","title":"MiLe Loss: a New Loss for Mitigating the Bias of Learning Difficulties\n  in Generative Language Models","summary":"  Generative language models are usually pretrained on large text corpus via\npredicting the next token (i.e., sub-word/word/phrase) given the previous ones.\nRecent works have demonstrated the impressive performance of large generative\nlanguage models on downstream tasks. However, existing generative language\nmodels generally neglect an inherent challenge in text corpus during training,\ni.e., the imbalance between frequent tokens and infrequent ones. It can lead a\nlanguage model to be dominated by common and easy-to-learn tokens, thereby\noverlooking the infrequent and difficult-to-learn ones. To alleviate that, we\npropose a MiLe Loss function for mitigating the bias of learning difficulties\nwith tokens. During training, it can dynamically assess the learning difficulty\nof a to-be-learned token, according to the information entropy of the\ncorresponding predicted probability distribution over the vocabulary. Then it\nscales the training loss adaptively, trying to lead the model to focus more on\nthe difficult-to-learn tokens. On the Pile dataset, we train generative\nlanguage models at different scales of 468M, 1.2B, and 6.7B parameters.\nExperiments reveal that models incorporating the proposed MiLe Loss can gain\nconsistent performance improvement on downstream benchmarks.\n","authors":["Zhenpeng Su","Xing Wu","Xue Bai","Zijia Lin","Hui Chen","Guiguang Ding","Wei Zhou","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2310.19531v6.pdf","comment":"This paper has been accepted by NAACL 2024"},{"id":"http://arxiv.org/abs/2403.01479v3","updated":"2024-03-25T08:46:15Z","published":"2024-03-03T11:13:44Z","title":"Align-to-Distill: Trainable Attention Alignment for Knowledge\n  Distillation in Neural Machine Translation","summary":"  The advent of scalable deep models and large datasets has improved the\nperformance of Neural Machine Translation. Knowledge Distillation (KD) enhances\nefficiency by transferring knowledge from a teacher model to a more compact\nstudent model. However, KD approaches to Transformer architecture often rely on\nheuristics, particularly when deciding which teacher layers to distill from. In\nthis paper, we introduce the 'Align-to-Distill' (A2D) strategy, designed to\naddress the feature mapping problem by adaptively aligning student attention\nheads with their teacher counterparts during training. The Attention Alignment\nModule in A2D performs a dense head-by-head comparison between student and\nteacher attention heads across layers, turning the combinatorial mapping\nheuristics into a learning problem. Our experiments show the efficacy of A2D,\ndemonstrating gains of up to +3.61 and +0.63 BLEU points for WMT-2022 De->Dsb\nand WMT-2014 En->De, respectively, compared to Transformer baselines.\n","authors":["Heegon Jin","Seonil Son","Jemin Park","Youngseok Kim","Hyungjong Noh","Yeonsoo Lee"],"pdf_url":"https://arxiv.org/pdf/2403.01479v3.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2307.06708v2","updated":"2024-03-25T08:44:53Z","published":"2023-07-13T12:06:48Z","title":"To share or not to share: What risks would laypeople accept to give\n  sensitive data to differentially-private NLP systems?","summary":"  Although the NLP community has adopted central differential privacy as a\ngo-to framework for privacy-preserving model training or data sharing, the\nchoice and interpretation of the key parameter, privacy budget $\\varepsilon$\nthat governs the strength of privacy protection, remains largely arbitrary. We\nargue that determining the $\\varepsilon$ value should not be solely in the\nhands of researchers or system developers, but must also take into account the\nactual people who share their potentially sensitive data. In other words: Would\nyou share your instant messages for $\\varepsilon$ of 10? We address this\nresearch gap by designing, implementing, and conducting a behavioral experiment\n(311 lay participants) to study the behavior of people in uncertain\ndecision-making situations with respect to privacy-threatening situations.\nFraming the risk perception in terms of two realistic NLP scenarios and using a\nvignette behavioral study help us determine what $\\varepsilon$ thresholds would\nlead lay people to be willing to share sensitive textual data - to our\nknowledge, the first study of its kind.\n","authors":["Christopher Weiss","Frauke Kreuter","Ivan Habernal"],"pdf_url":"https://arxiv.org/pdf/2307.06708v2.pdf","comment":"Accepted at LREC-COLING 2024; final camera-ready version"},{"id":"http://arxiv.org/abs/2403.16543v1","updated":"2024-03-25T08:36:06Z","published":"2024-03-25T08:36:06Z","title":"Efficient Information Extraction in Few-Shot Relation Classification\n  through Contrastive Representation Learning","summary":"  Differentiating relationships between entity pairs with limited labeled\ninstances poses a significant challenge in few-shot relation classification.\nRepresentations of textual data extract rich information spanning the domain,\nentities, and relations. In this paper, we introduce a novel approach to\nenhance information extraction combining multiple sentence representations and\ncontrastive learning. While representations in relation classification are\ncommonly extracted using entity marker tokens, we argue that substantial\ninformation within the internal model representations remains untapped. To\naddress this, we propose aligning multiple sentence representations, such as\nthe [CLS] token, the [MASK] token used in prompting, and entity marker tokens.\nOur method employs contrastive learning to extract complementary discriminative\ninformation from these individual representations. This is particularly\nrelevant in low-resource settings where information is scarce. Leveraging\nmultiple sentence representations is especially effective in distilling\ndiscriminative information for relation classification when additional\ninformation, like relation descriptions, are not available. We validate the\nadaptability of our approach, maintaining robust performance in scenarios that\ninclude relation descriptions, and showcasing its flexibility to adapt to\ndifferent resource constraints.\n","authors":["Philipp Borchert","Jochen De Weerdt","Marie-Francine Moens"],"pdf_url":"https://arxiv.org/pdf/2403.16543v1.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2309.08503v2","updated":"2024-03-25T08:33:37Z","published":"2023-09-15T16:05:48Z","title":"HealthFC: Verifying Health Claims with Evidence-Based Medical\n  Fact-Checking","summary":"  In the digital age, seeking health advice on the Internet has become a common\npractice. At the same time, determining the trustworthiness of online medical\ncontent is increasingly challenging. Fact-checking has emerged as an approach\nto assess the veracity of factual claims using evidence from credible knowledge\nsources. To help advance automated Natural Language Processing (NLP) solutions\nfor this task, in this paper we introduce a novel dataset HealthFC. It consists\nof 750 health-related claims in German and English, labeled for veracity by\nmedical experts and backed with evidence from systematic reviews and clinical\ntrials. We provide an analysis of the dataset, highlighting its characteristics\nand challenges. The dataset can be used for NLP tasks related to automated\nfact-checking, such as evidence retrieval, claim verification, or explanation\ngeneration. For testing purposes, we provide baseline systems based on\ndifferent approaches, examine their performance, and discuss the findings. We\nshow that the dataset is a challenging test bed with a high potential for\nfuture use.\n","authors":["Juraj Vladika","Phillip Schneider","Florian Matthes"],"pdf_url":"https://arxiv.org/pdf/2309.08503v2.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2401.11504v2","updated":"2024-03-25T08:16:06Z","published":"2024-01-21T14:28:41Z","title":"With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation","summary":"  Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference.\n","authors":["Y. Wang","D. Ma","D. Cai"],"pdf_url":"https://arxiv.org/pdf/2401.11504v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16527v1","updated":"2024-03-25T08:11:02Z","published":"2024-03-25T08:11:02Z","title":"Hallucination Detection in Foundation Models for Decision-Making: A\n  Flexible Definition and Review of the State of the Art","summary":"  Autonomous systems are soon to be ubiquitous, from manufacturing autonomy to\nagricultural field robots, and from health care assistants to the entertainment\nindustry. The majority of these systems are developed with modular\nsub-components for decision-making, planning, and control that may be\nhand-engineered or learning-based. While these existing approaches have been\nshown to perform well under the situations they were specifically designed for,\nthey can perform especially poorly in rare, out-of-distribution scenarios that\nwill undoubtedly arise at test-time. The rise of foundation models trained on\nmultiple tasks with impressively large datasets from a variety of fields has\nled researchers to believe that these models may provide common sense reasoning\nthat existing planners are missing. Researchers posit that this common sense\nreasoning will bridge the gap between algorithm development and deployment to\nout-of-distribution tasks, like how humans adapt to unexpected scenarios. Large\nlanguage models have already penetrated the robotics and autonomous systems\ndomains as researchers are scrambling to showcase their potential use cases in\ndeployment. While this application direction is very promising empirically,\nfoundation models are known to hallucinate and generate decisions that may\nsound reasonable, but are in fact poor. We argue there is a need to step back\nand simultaneously design systems that can quantify the certainty of a model's\ndecision, and detect when it may be hallucinating. In this work, we discuss the\ncurrent use cases of foundation models for decision-making tasks, provide a\ngeneral definition for hallucinations with examples, discuss existing\napproaches to hallucination detection and mitigation with a focus on decision\nproblems, and explore areas for further research in this exciting field.\n","authors":["Neeloy Chakraborty","Melkior Ornik","Katherine Driggs-Campbell"],"pdf_url":"https://arxiv.org/pdf/2403.16527v1.pdf","comment":"31 pages, 2 tables"},{"id":"http://arxiv.org/abs/2403.16516v1","updated":"2024-03-25T08:00:43Z","published":"2024-03-25T08:00:43Z","title":"Visually Guided Generative Text-Layout Pre-training for Document\n  Intelligence","summary":"  Prior study shows that pre-training techniques can boost the performance of\nvisual document understanding (VDU), which typically requires models to gain\nabilities to perceive and reason both document texts and layouts (e.g.,\nlocations of texts and table-cells). To this end, we propose visually guided\ngenerative text-layout pre-training, named ViTLP. Given a document image, the\nmodel optimizes hierarchical language and layout modeling objectives to\ngenerate the interleaved text and layout sequence. In addition, to address the\nlimitation of processing long documents by Transformers, we introduce a\nstraightforward yet effective multi-segment generative pre-training scheme,\nfacilitating ViTLP to process word-intensive documents of any length. ViTLP can\nfunction as a native OCR model to localize and recognize texts of document\nimages. Besides, ViTLP can be effectively applied to various downstream VDU\ntasks. Extensive experiments show that ViTLP achieves competitive performance\nover existing baselines on benchmark VDU tasks, including information\nextraction, document classification, and document question answering.\n","authors":["Zhiming Mao","Haoli Bai","Lu Hou","Jiansheng Wei","Xin Jiang","Qun Liu","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2403.16516v1.pdf","comment":"Accepted to NAACL 2024 main conference. The first version of this\n  paper was submitted to OpenReview\n  (https://openreview.net/forum?id=ARtBIBAmNR) in June 2023"},{"id":"http://arxiv.org/abs/2403.16512v1","updated":"2024-03-25T07:55:29Z","published":"2024-03-25T07:55:29Z","title":"LLMs Are Few-Shot In-Context Low-Resource Language Learners","summary":"  In-context learning (ICL) empowers large language models (LLMs) to perform\ndiverse tasks in underrepresented languages using only short in-context\ninformation, offering a crucial avenue for narrowing the gap between\nhigh-resource and low-resource languages. Nonetheless, there is only a handful\nof works explored ICL for low-resource languages with most of them focusing on\nrelatively high-resource languages, such as French and Spanish. In this work,\nwe extensively study ICL and its cross-lingual variation (X-ICL) on 25\nlow-resource and 7 relatively higher-resource languages. Our study not only\nassesses the effectiveness of ICL with LLMs in low-resource languages but also\nidentifies the shortcomings of in-context label alignment, and introduces a\nmore effective alternative: query alignment. Moreover, we provide valuable\ninsights into various facets of ICL for low-resource languages. Our study\nconcludes the significance of few-shot in-context information on enhancing the\nlow-resource understanding quality of LLMs through semantically relevant\ninformation by closing the language gap in the target language and aligning the\nsemantics between the targeted low-resource and the high-resource language that\nthe model is proficient in. Our work highlights the importance of advancing ICL\nresearch, particularly for low-resource languages.\n","authors":["Samuel Cahyawijaya","Holy Lovenia","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2403.16512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16504v1","updated":"2024-03-25T07:38:40Z","published":"2024-03-25T07:38:40Z","title":"LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent\n  Classification","summary":"  Following the significant achievements of large language models (LLMs),\nresearchers have employed in-context learning for text classification tasks.\nHowever, these studies focused on monolingual, single-turn classification\ntasks. In this paper, we introduce LARA (Linguistic-Adaptive\nRetrieval-Augmented Language Models), designed to enhance accuracy in\nmulti-turn classification tasks across six languages, accommodating numerous\nintents in chatbot interactions. Multi-turn intent classification is notably\nchallenging due to the complexity and evolving nature of conversational\ncontexts. LARA tackles these issues by combining a fine-tuned smaller model\nwith a retrieval-augmented mechanism, integrated within the architecture of\nLLMs. This integration allows LARA to dynamically utilize past dialogues and\nrelevant intents, thereby improving the understanding of the context.\nFurthermore, our adaptive retrieval techniques bolster the cross-lingual\ncapabilities of LLMs without extensive retraining and fine-tune. Comprehensive\nexperiments demonstrate that LARA achieves state-of-the-art performance on\nmulti-turn intent classification tasks, enhancing the average accuracy by 3.67%\ncompared to existing methods.\n","authors":["Liu Junhua","Tan Yong Keat","Fu Bin"],"pdf_url":"https://arxiv.org/pdf/2403.16504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16483v1","updated":"2024-03-25T07:08:13Z","published":"2024-03-25T07:08:13Z","title":"Automatic Construction of a Large-Scale Corpus for Geoparsing Using\n  Wikipedia Hyperlinks","summary":"  Geoparsing is the task of estimating the latitude and longitude (coordinates)\nof location expressions in texts. Geoparsing must deal with the ambiguity of\nthe expressions that indicate multiple locations with the same notation. For\nevaluating geoparsing systems, several corpora have been proposed in previous\nwork. However, these corpora are small-scale and suffer from the coverage of\nlocation expressions on general domains. In this paper, we propose Wikipedia\nHyperlink-based Location Linking (WHLL), a novel method to construct a\nlarge-scale corpus for geoparsing from Wikipedia articles. WHLL leverages\nhyperlinks in Wikipedia to annotate multiple location expressions with\ncoordinates. With this method, we constructed the WHLL corpus, a new\nlarge-scale corpus for geoparsing. The WHLL corpus consists of 1.3M articles,\neach containing about 7.8 unique location expressions. 45.6% of location\nexpressions are ambiguous and refer to more than one location with the same\nnotation. In each article, location expressions of the article title and those\nhyperlinks to other articles are assigned with coordinates. By utilizing\nhyperlinks, we can accurately assign location expressions with coordinates even\nwith ambiguous location expressions in the texts. Experimental results show\nthat there remains room for improvement by disambiguating location expressions.\n","authors":["Keyaki Ohno","Hirotaka Kameko","Keisuke Shirai","Taichi Nishimura","Shinsuke Mori"],"pdf_url":"https://arxiv.org/pdf/2403.16483v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2306.04357v4","updated":"2024-03-25T06:54:10Z","published":"2023-06-07T11:40:07Z","title":"Dial-MAE: ConTextual Masked Auto-Encoder for Retrieval-based Dialogue\n  Systems","summary":"  Dialogue response selection aims to select an appropriate response from\nseveral candidates based on a given user and system utterance history. Most\nexisting works primarily focus on post-training and fine-tuning tailored for\ncross-encoders. However, there are no post-training methods tailored for dense\nencoders in dialogue response selection. We argue that when the current\nlanguage model, based on dense dialogue systems (such as BERT), is employed as\na dense encoder, it separately encodes dialogue context and response, leading\nto a struggle to achieve the alignment of both representations. Thus, we\npropose Dial-MAE (Dialogue Contextual Masking Auto-Encoder), a straightforward\nyet effective post-training technique tailored for dense encoders in dialogue\nresponse selection. Dial-MAE uses an asymmetric encoder-decoder architecture to\ncompress the dialogue semantics into dense vectors, which achieves better\nalignment between the features of the dialogue context and response. Our\nexperiments have demonstrated that Dial-MAE is highly effective, achieving\nstate-of-the-art performance on two commonly evaluated benchmarks.\n","authors":["Zhenpeng Su","Xing Wu","Wei Zhou","Guangyuan Ma","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2306.04357v4.pdf","comment":"This paper has been accepted by NAACL 2024"},{"id":"http://arxiv.org/abs/2309.13182v2","updated":"2024-03-25T06:49:16Z","published":"2023-09-22T21:15:28Z","title":"Effective Distillation of Table-based Reasoning Ability from LLMs","summary":"  Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of natural language processing tasks. However, their enormous\nparameter size and extremely high requirements for compute power pose\nchallenges for their practical deployment. Recent research has revealed that\nspecific capabilities of LLMs, such as numerical reasoning, can be transferred\nto smaller models through distillation. Some studies explore the potential of\nleveraging LLMs to perform table-based reasoning. However, there has been no\nprior work focusing on table reasoning skills in smaller models specifically\ntailored for scientific table-to-text generation tasks. In this paper, we\npropose a novel table-based reasoning distillation approach, with the aim of\ndistilling LLMs into tailored smaller models. Our experimental results have\nshown that a 220 million parameter model (Flan-T5-base) fine-tuned using\ndistilled data, not only achieves a significant improvement compared to\ntraditionally fine-tuned baselines, but also surpasses specific LLMs on a\nscientific table-to-text generation dataset. Our code is available at\nhttps://github.com/Bernard-Yang/DistillTableCoT.\n","authors":["Bohao Yang","Chen Tang","Kun Zhao","Chenghao Xiao","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2309.13182v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16463v1","updated":"2024-03-25T06:45:09Z","published":"2024-03-25T06:45:09Z","title":"Few-shot Named Entity Recognition via Superposition Concept\n  Discrimination","summary":"  Few-shot NER aims to identify entities of target types with only limited\nnumber of illustrative instances. Unfortunately, few-shot NER is severely\nchallenged by the intrinsic precise generalization problem, i.e., it is hard to\naccurately determine the desired target type due to the ambiguity stemming from\ninformation deficiency. In this paper, we propose Superposition Concept\nDiscriminator (SuperCD), which resolves the above challenge via an active\nlearning paradigm. Specifically, a concept extractor is first introduced to\nidentify superposition concepts from illustrative instances, with each concept\ncorresponding to a possible generalization boundary. Then a superposition\ninstance retriever is applied to retrieve corresponding instances of these\nsuperposition concepts from large-scale text corpus. Finally, annotators are\nasked to annotate the retrieved instances and these annotated instances\ntogether with original illustrative instances are used to learn FS-NER models.\nTo this end, we learn a universal concept extractor and superposition instance\nretriever using a large-scale openly available knowledge bases. Experiments\nshow that SuperCD can effectively identify superposition concepts from\nillustrative instances, retrieve superposition instances from large-scale\ncorpus, and significantly improve the few-shot NER performance with minimal\nadditional efforts.\n","authors":["Jiawei Chen","Hongyu Lin","Xianpei Han","Yaojie Lu","Shanshan Jiang","Bin Dong","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2403.16463v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.16447v1","updated":"2024-03-25T06:18:18Z","published":"2024-03-25T06:18:18Z","title":"A Study on How Attention Scores in the BERT Model are Aware of Lexical\n  Categories in Syntactic and Semantic Tasks on the GLUE Benchmark","summary":"  This study examines whether the attention scores between tokens in the BERT\nmodel significantly vary based on lexical categories during the fine-tuning\nprocess for downstream tasks. Drawing inspiration from the notion that in human\nlanguage processing, syntactic and semantic information is parsed differently,\nwe categorize tokens in sentences according to their lexical categories and\nfocus on changes in attention scores among these categories. Our hypothesis\nposits that in downstream tasks that prioritize semantic information, attention\nscores centered on content words are enhanced, while in cases emphasizing\nsyntactic information, attention scores centered on function words are\nintensified. Through experimentation conducted on six tasks from the GLUE\nbenchmark dataset, we substantiate our hypothesis regarding the fine-tuning\nprocess. Furthermore, our additional investigations reveal the presence of BERT\nlayers that consistently assign more bias to specific lexical categories,\nirrespective of the task, highlighting the existence of task-agnostic lexical\ncategory preferences.\n","authors":["Dongjun Jang","Sungjoo Byun","Hyopil Shin"],"pdf_url":"https://arxiv.org/pdf/2403.16447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16446v1","updated":"2024-03-25T06:17:54Z","published":"2024-03-25T06:17:54Z","title":"Towards Automatic Evaluation for LLMs' Clinical Capabilities: Metric,\n  Data, and Algorithm","summary":"  Large language models (LLMs) are gaining increasing interests to improve\nclinical efficiency for medical diagnosis, owing to their unprecedented\nperformance in modelling natural language. Ensuring the safe and reliable\nclinical applications, the evaluation of LLMs indeed becomes critical for\nbetter mitigating the potential risks, e.g., hallucinations. However, current\nevaluation methods heavily rely on labor-intensive human participation to\nachieve human-preferred judgements. To overcome this challenge, we propose an\nautomatic evaluation paradigm tailored to assess the LLMs' capabilities in\ndelivering clinical services, e.g., disease diagnosis and treatment. The\nevaluation paradigm contains three basic elements: metric, data, and algorithm.\nSpecifically, inspired by professional clinical practice pathways, we formulate\na LLM-specific clinical pathway (LCP) to define the clinical capabilities that\na doctor agent should possess. Then, Standardized Patients (SPs) from the\nmedical education are introduced as the guideline for collecting medical data\nfor evaluation, which can well ensure the completeness of the evaluation\nprocedure. Leveraging these steps, we develop a multi-agent framework to\nsimulate the interactive environment between SPs and a doctor agent, which is\nequipped with a Retrieval-Augmented Evaluation (RAE) to determine whether the\nbehaviors of a doctor agent are in accordance with LCP. The above paradigm can\nbe extended to any similar clinical scenarios to automatically evaluate the\nLLMs' medical capabilities. Applying such paradigm, we construct an evaluation\nbenchmark in the field of urology, including a LCP, a SPs dataset, and an\nautomated RAE. Extensive experiments are conducted to demonstrate the\neffectiveness of the proposed approach, providing more insights for LLMs' safe\nand reliable deployments in clinical practice.\n","authors":["Lei Liu","Xiaoyan Yang","Fangzhou Li","Chenfei Chi","Yue Shen","Shiwei Lyu Ming Zhang","Xiaowei Ma","Xiangguo Lyu","Liya Ma","Zhiqiang Zhang","Wei Xue","Yiran Huang","Jinjie Gu"],"pdf_url":"https://arxiv.org/pdf/2403.16446v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16444v1","updated":"2024-03-25T06:15:21Z","published":"2024-03-25T06:15:21Z","title":"KIT-19: A Comprehensive Korean Instruction Toolkit on 19 Tasks for\n  Fine-Tuning Korean Large Language Models","summary":"  Instruction Tuning on Large Language Models is an essential process for model\nto function well and achieve high performance in specific tasks. Accordingly,\nin mainstream languages such as English, instruction-based datasets are being\nconstructed and made publicly available. In the case of Korean, publicly\navailable models and datasets all rely on using the output of ChatGPT or\ntranslating datasets built in English. In this paper, We introduce\n\\textit{KIT-19} as an instruction dataset for the development of LLM in Korean.\n\\textit{KIT-19} is a dataset created in an instruction format, comprising 19\nexisting open-source datasets for Korean NLP tasks. In this paper, we train a\nKorean Pretrained LLM using \\textit{KIT-19} to demonstrate its effectiveness.\nThe experimental results show that the model trained on \\textit{KIT-19}\nsignificantly outperforms existing Korean LLMs. Based on the its quality and\nempirical results, this paper proposes that \\textit{KIT-19} has the potential\nto make a substantial contribution to the future improvement of Korean LLMs'\nperformance.\n","authors":["Dongjun Jang","Sungjoo Byun","Hyemi Jo","Hyopil Shin"],"pdf_url":"https://arxiv.org/pdf/2403.16444v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16443v1","updated":"2024-03-25T06:09:55Z","published":"2024-03-25T06:09:55Z","title":"CodeS: Natural Language to Code Repository via Multi-Layer Sketch","summary":"  The impressive performance of large language models (LLMs) on code-related\ntasks has shown the potential of fully automated software development. In light\nof this, we introduce a new software engineering task, namely Natural Language\nto code Repository (NL2Repo). This task aims to generate an entire code\nrepository from its natural language requirements. To address this task, we\npropose a simple yet effective framework CodeS, which decomposes NL2Repo into\nmultiple sub-tasks by a multi-layer sketch. Specifically, CodeS includes three\nmodules: RepoSketcher, FileSketcher, and SketchFiller. RepoSketcher first\ngenerates a repository's directory structure for given requirements;\nFileSketcher then generates a file sketch for each file in the generated\nstructure; SketchFiller finally fills in the details for each function in the\ngenerated file sketch. To rigorously assess CodeS on the NL2Repo task, we carry\nout evaluations through both automated benchmarking and manual feedback\nanalysis. For benchmark-based evaluation, we craft a repository-oriented\nbenchmark, SketchEval, and design an evaluation metric, SketchBLEU. For\nfeedback-based evaluation, we develop a VSCode plugin for CodeS and engage 30\nparticipants in conducting empirical studies. Extensive experiments prove the\neffectiveness and practicality of CodeS on the NL2Repo task.\n","authors":["Daoguang Zan","Ailun Yu","Wei Liu","Dong Chen","Bo Shen","Wei Li","Yafen Yao","Yongshun Gong","Xiaolin Chen","Bei Guan","Zhiguang Yang","Yongji Wang","Qianxiang Wang","Lizhen Cui"],"pdf_url":"https://arxiv.org/pdf/2403.16443v1.pdf","comment":"https://github.com/NL2Code/CodeS"},{"id":"http://arxiv.org/abs/2403.16442v1","updated":"2024-03-25T06:05:50Z","published":"2024-03-25T06:05:50Z","title":"If CLIP Could Talk: Understanding Vision-Language Model Representations\n  Through Their Preferred Concept Descriptions","summary":"  Recent works often assume that Vision-Language Model (VLM) representations\nare based on visual attributes like shape. However, it is unclear to what\nextent VLMs prioritize this information to represent concepts. We propose\nExtract and Explore (EX2), a novel approach to characterize important textual\nfeatures for VLMs. EX2 uses reinforcement learning to align a large language\nmodel with VLM preferences and generates descriptions that incorporate the\nimportant features for the VLM. Then, we inspect the descriptions to identify\nthe features that contribute to VLM representations. We find that spurious\ndescriptions have a major role in VLM representations despite providing no\nhelpful information, e.g., Click to enlarge photo of CONCEPT. More importantly,\namong informative descriptions, VLMs rely significantly on non-visual\nattributes like habitat to represent visual concepts. Also, our analysis\nreveals that different VLMs prioritize different attributes in their\nrepresentations. Overall, we show that VLMs do not simply match images to scene\ndescriptions and that non-visual or even spurious descriptions significantly\ninfluence their representations.\n","authors":["Reza Esfandiarpoor","Cristina Menghini","Stephen H. Bach"],"pdf_url":"https://arxiv.org/pdf/2403.16442v1.pdf","comment":"Code: https://github.com/BatsResearch/ex2"},{"id":"http://arxiv.org/abs/2310.14566v5","updated":"2024-03-25T06:05:24Z","published":"2023-10-23T04:49:09Z","title":"HallusionBench: An Advanced Diagnostic Suite for Entangled Language\n  Hallucination and Visual Illusion in Large Vision-Language Models","summary":"  We introduce HallusionBench, a comprehensive benchmark designed for the\nevaluation of image-context reasoning. This benchmark presents significant\nchallenges to advanced large visual-language models (LVLMs), such as\nGPT-4V(Vision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, by emphasizing\nnuanced understanding and interpretation of visual data. The benchmark\ncomprises 346 images paired with 1129 questions, all meticulously crafted by\nhuman experts. We introduce a novel structure for these visual questions\ndesigned to establish control groups. This structure enables us to conduct a\nquantitative analysis of the models' response tendencies, logical consistency,\nand various failure modes. In our evaluation on HallusionBench, we benchmarked\n15 different models, highlighting a 31.42% question-pair accuracy achieved by\nthe state-of-the-art GPT-4V. Notably, all other evaluated models achieve\naccuracy below 16%. Moreover, our analysis not only highlights the observed\nfailure modes, including language hallucination and visual illusion, but also\ndeepens an understanding of these pitfalls. Our comprehensive case studies\nwithin HallusionBench shed light on the challenges of hallucination and\nillusion in LVLMs. Based on these insights, we suggest potential pathways for\ntheir future improvement. The benchmark and codebase can be accessed at\nhttps://github.com/tianyi-lab/HallusionBench.\n","authors":["Tianrui Guan","Fuxiao Liu","Xiyang Wu","Ruiqi Xian","Zongxia Li","Xiaoyu Liu","Xijun Wang","Lichang Chen","Furong Huang","Yaser Yacoob","Dinesh Manocha","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.14566v5.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2311.08298v2","updated":"2024-03-25T06:01:49Z","published":"2023-11-14T16:43:29Z","title":"A Survey of Confidence Estimation and Calibration in Large Language\n  Models","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks in various domains. Despite their impressive performance,\nthey can be unreliable due to factual errors in their generations. Assessing\ntheir confidence and calibrating them across different tasks can help mitigate\nrisks and enable LLMs to produce better generations. There has been a lot of\nrecent research aiming to address this, but there has been no comprehensive\noverview to organize it and outline the main lessons learned. The present\nsurvey aims to bridge this gap. In particular, we outline the challenges and we\nsummarize recent technical advancements for LLM confidence estimation and\ncalibration. We further discuss their applications and suggest promising\ndirections for future work.\n","authors":["Jiahui Geng","Fengyu Cai","Yuxia Wang","Heinz Koeppl","Preslav Nakov","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2311.08298v2.pdf","comment":"16 pages, 1 page, 1 table"},{"id":"http://arxiv.org/abs/2403.16437v1","updated":"2024-03-25T05:37:16Z","published":"2024-03-25T05:37:16Z","title":"Evaluating Large Language Models with Runtime Behavior of Program\n  Execution","summary":"  Large language models for code (i.e., code LLMs) have shown strong code\nunderstanding and generation capabilities. To evaluate the capabilities of code\nLLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval\nand ClassEval). Code reasoning is one of the most essential abilities of code\nLLMs, but existing benchmarks for code reasoning are not sufficient. Typically,\nthey focus on predicting the input and output of a program, ignoring the\nevaluation of the intermediate behavior during program execution, as well as\nthe logical consistency (e.g., the model should not give the correct output if\nthe prediction of execution path is wrong) when performing the reasoning. To\naddress these problems, in this paper, we propose a framework, namely REval,\nfor evaluating code reasoning abilities and consistency of code LLMs with\nprogram execution. We utilize existing code benchmarks and adapt them to new\nbenchmarks within our framework. A large-scale empirical study is conducted and\nmost LLMs show unsatisfactory performance on both Runtime Behavior Reasoning\n(i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation\n(i.e., an average IC score of 10.3). Evaluation results of current code LLMs\nreflect the urgent need for the community to strengthen the code reasoning\ncapability of code LLMs.\n","authors":["Junkai Chen","Zhiyuan Pan","Xing Hu","Zhenhao Li","Ge Li","Xin Xia"],"pdf_url":"https://arxiv.org/pdf/2403.16437v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06199v4","updated":"2024-03-25T05:36:56Z","published":"2024-03-10T12:43:27Z","title":"Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small\n  Language Models","summary":"  Multimodal Large Language Models (MLLMs) have showcased impressive skills in\ntasks related to visual understanding and reasoning. Yet, their widespread\napplication faces obstacles due to the high computational demands during both\nthe training and inference phases, restricting their use to a limited audience\nwithin the research and user communities. In this paper, we investigate the\ndesign aspects of Multimodal Small Language Models (MSLMs) and propose an\nefficient multimodal assistant named Mipha, which is designed to create synergy\namong various aspects: visual representation, language models, and optimization\nstrategies. We show that without increasing the volume of training data, our\nMipha-3B outperforms the state-of-the-art large MLLMs, especially\nLLaVA-1.5-13B, on multiple benchmarks. Through detailed discussion, we provide\ninsights and guidelines for developing strong MSLMs that rival the capabilities\nof MLLMs. Our code is available at https://github.com/zhuyiche/llava-phi.\n","authors":["Minjie Zhu","Yichen Zhu","Xin Liu","Ning Liu","Zhiyuan Xu","Chaomin Shen","Yaxin Peng","Zhicai Ou","Feifei Feng","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2403.06199v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14643v2","updated":"2024-03-25T05:35:12Z","published":"2024-02-21T16:44:35Z","title":"Exploring ChatGPT and its Impact on Society","summary":"  Artificial intelligence has been around for a while, but suddenly it has\nreceived more attention than ever before. Thanks to innovations from companies\nlike Google, Microsoft, Meta, and other major brands in technology. OpenAI,\nthough, has triggered the button with its ground-breaking invention ChatGPT.\nChatGPT is a Large Language Model (LLM) based on Transformer architecture that\nhas the ability to generate human-like responses in a conversational context.\nIt uses deep learning algorithms to generate natural language responses to\ninput text. Its large number of parameters, contextual generation, and\nopen-domain training make it a versatile and effective tool for a wide range of\napplications, from chatbots to customer service to language translation. It has\nthe potential to revolutionize various industries and transform the way we\ninteract with technology. However, the use of ChatGPT has also raised several\nconcerns, including ethical, social, and employment challenges, which must be\ncarefully considered to ensure the responsible use of this technology. The\narticle provides an overview of ChatGPT, delving into its architecture and\ntraining process. It highlights the potential impacts of ChatGPT on the\nsociety. In this paper, we suggest some approaches involving technology,\nregulation, education, and ethics in an effort to maximize ChatGPT's benefits\nwhile minimizing its negative impacts. This study is expected to contribute to\na greater understanding of ChatGPT and aid in predicting the potential changes\nit may bring about.\n","authors":["Md. Asraful Haque","Shuai Li"],"pdf_url":"https://arxiv.org/pdf/2403.14643v2.pdf","comment":"13 Pages"},{"id":"http://arxiv.org/abs/2403.16435v1","updated":"2024-03-25T05:31:22Z","published":"2024-03-25T05:31:22Z","title":"InstUPR : Instruction-based Unsupervised Passage Reranking with Large\n  Language Models","summary":"  This paper introduces InstUPR, an unsupervised passage reranking method based\non large language models (LLMs). Different from existing approaches that rely\non extensive training with query-document pairs or retrieval-specific\ninstructions, our method leverages the instruction-following capabilities of\ninstruction-tuned LLMs for passage reranking without any additional\nfine-tuning. To achieve this, we introduce a soft score aggregation technique\nand employ pairwise reranking for unsupervised passage reranking. Experiments\non the BEIR benchmark demonstrate that InstUPR outperforms unsupervised\nbaselines as well as an instruction-tuned reranker, highlighting its\neffectiveness and superiority. Source code to reproduce all experiments is\nopen-sourced at https://github.com/MiuLab/InstUPR\n","authors":["Chao-Wei Huang","Yun-Nung Chen"],"pdf_url":"https://arxiv.org/pdf/2403.16435v1.pdf","comment":"Preprint. This manuscript was originally written and submitted in\n  June 2023"},{"id":"http://arxiv.org/abs/2403.16432v1","updated":"2024-03-25T05:27:35Z","published":"2024-03-25T05:27:35Z","title":"$\\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on\n  Prompt-based Language Models","summary":"  Prompt-based learning is a new language model training paradigm that adapts\nthe Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes\nthe performance benchmarks across various natural language processing (NLP)\ntasks. Instead of using a fixed prompt template to fine-tune the model, some\nresearch demonstrates the effectiveness of searching for the prompt via\noptimization. Such prompt optimization process of prompt-based learning on PLMs\nalso gives insight into generating adversarial prompts to mislead the model,\nraising concerns about the adversarial vulnerability of this paradigm. Recent\nstudies have shown that universal adversarial triggers (UATs) can be generated\nto alter not only the predictions of the target PLMs but also the prediction of\ncorresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based\nlearning paradigm. However, UATs found in previous works are often unreadable\ntokens or characters and can be easily distinguished from natural texts with\nadaptive defenses. In this work, we consider the naturalness of the UATs and\ndevelop $\\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs\nby a gradient-based beam search algorithm that not only effectively attacks the\ntarget PLMs and PFMs but also maintains the naturalness among the trigger\ntokens. Extensive results demonstrate the effectiveness of\n$\\textit{LinkPrompt}$, as well as the transferability of UATs generated by\n\\textit{LinkPrompt} to open-sourced Large Language Model (LLM) Llama2 and\nAPI-accessed LLM GPT-3.5-turbo.\n","authors":["Yue Xu","Wenjie Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16432v1.pdf","comment":"Accepted to the main conference of NAACL2024"},{"id":"http://arxiv.org/abs/2310.01777v2","updated":"2024-03-25T04:04:05Z","published":"2023-10-03T03:56:26Z","title":"SEA: Sparse Linear Attention with Estimated Attention Mask","summary":"  The transformer architecture has driven breakthroughs in recent years on\ntasks which require modeling pairwise relationships between sequential\nelements, as is the case in natural language understanding. However, long\nseqeuences pose a problem due to the quadratic complexity of the attention\noperation. Previous research has aimed to lower the complexity by sparsifying\nor linearly approximating the attention matrix. Yet, these approaches cannot\nstraightforwardly distill knowledge from a teacher's attention matrix and often\nrequire complete retraining from scratch. Furthermore, previous sparse and\nlinear approaches lose interpretability if they cannot produce full attention\nmatrices. To address these challenges, we propose SEA: Sparse linear attention\nwith an Estimated Attention mask. SEA estimates the attention matrix with\nlinear complexity via kernel-based linear attention, then subsequently creates\na sparse attention matrix with a top-k selection to perform a sparse attention\noperation. For language modeling tasks (Wikitext2), previous linear and sparse\nattention methods show roughly two-fold worse perplexity scores over the\nquadratic OPT-1.3B baseline, while SEA achieves better perplexity than\nOPT-1.3B, using roughly half the memory of OPT-1.3B, providing interpretable\nattention matrix. We believe that our work will have a large practical impact,\nas it opens the possibility of running large transformers on resource-limited\ndevices with less memory.\n","authors":["Heejun Lee","Jina Kim","Jeffrey Willette","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2310.01777v2.pdf","comment":"9 main pages"},{"id":"http://arxiv.org/abs/2308.14115v2","updated":"2024-03-25T03:54:48Z","published":"2023-08-27T14:14:28Z","title":"Situated Natural Language Explanations","summary":"  Natural language is among the most accessible tools for explaining decisions\nto humans, and large pretrained language models (PLMs) have demonstrated\nimpressive abilities to generate coherent natural language explanations (NLE).\nThe existing NLE research perspectives do not take the audience into account.\nAn NLE can have high textual quality, but it might not accommodate audiences'\nneeds and preference. To address this limitation, we propose an alternative\nperspective, \\textit{situated} NLE. On the evaluation side, we set up automated\nevaluation scores. These scores describe the properties of NLEs in lexical,\nsemantic, and pragmatic categories. On the generation side, we identify three\nprompt engineering techniques and assess their applicability on the situations.\nSituated NLE provides a perspective and facilitates further research on the\ngeneration and evaluation of explanations.\n","authors":["Zining Zhu","Haoming Jiang","Jingfeng Yang","Sreyashi Nag","Chao Zhang","Jie Huang","Yifan Gao","Frank Rudzicz","Bing Yin"],"pdf_url":"https://arxiv.org/pdf/2308.14115v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16396v1","updated":"2024-03-25T03:19:20Z","published":"2024-03-25T03:19:20Z","title":"Is There a One-Model-Fits-All Approach to Information Extraction?\n  Revisiting Task Definition Biases","summary":"  Definition bias is a negative phenomenon that can mislead models. Definition\nbias in information extraction appears not only across datasets from different\ndomains but also within datasets sharing the same domain. We identify two types\nof definition bias in IE: bias among information extraction datasets and bias\nbetween information extraction datasets and instruction tuning datasets. To\nsystematically investigate definition bias, we conduct three probing\nexperiments to quantitatively analyze it and discover the limitations of\nunified information extraction and large language models in solving definition\nbias. To mitigate definition bias in information extraction, we propose a\nmulti-stage framework consisting of definition bias measurement, bias-aware\nfine-tuning, and task-specific bias mitigation. Experimental results\ndemonstrate the effectiveness of our framework in addressing definition bias.\nResources of this paper can be found at\nhttps://github.com/EZ-hwh/definition-bias\n","authors":["Wenhao Huang","Qianyu He","Zhixu Li","Jiaqing Liang","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.16396v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.16394v1","updated":"2024-03-25T03:18:39Z","published":"2024-03-25T03:18:39Z","title":"Skews in the Phenomenon Space Hinder Generalization in Text-to-Image\n  Generation","summary":"  The literature on text-to-image generation is plagued by issues of faithfully\ncomposing entities with relations. But there lacks a formal understanding of\nhow entity-relation compositions can be effectively learned. Moreover, the\nunderlying phenomenon space that meaningfully reflects the problem structure is\nnot well-defined, leading to an arms race for larger quantities of data in the\nhope that generalization emerges out of large-scale pretraining. We hypothesize\nthat the underlying phenomenological coverage has not been proportionally\nscaled up, leading to a skew of the presented phenomenon which harms\ngeneralization. We introduce statistical metrics that quantify both the\nlinguistic and visual skew of a dataset for relational learning, and show that\ngeneralization failures of text-to-image generation are a direct result of\nincomplete or unbalanced phenomenological coverage. We first perform\nexperiments in a synthetic domain and demonstrate that systematically\ncontrolled metrics are strongly predictive of generalization performance. Then\nwe move to natural images and show that simple distribution perturbations in\nlight of our theories boost generalization without enlarging the absolute data\nsize. This work informs an important direction towards quality-enhancing the\ndata diversity or balance orthogonal to scaling up the absolute size. Our\ndiscussions point out important open questions on 1) Evaluation of generated\nentity-relation compositions, and 2) Better models for reasoning with abstract\nrelations.\n","authors":["Yingshan Chang","Yasi Zhang","Zhiyuan Fang","Yingnian Wu","Yonatan Bisk","Feng Gao"],"pdf_url":"https://arxiv.org/pdf/2403.16394v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15365v2","updated":"2024-03-25T03:06:08Z","published":"2024-03-22T17:33:11Z","title":"A Transfer Attack to Image Watermarks","summary":"  Watermark has been widely deployed by industry to detect AI-generated images.\nThe robustness of such watermark-based detector against evasion attacks in the\nwhite-box and black-box settings is well understood in the literature. However,\nthe robustness in the no-box setting is much less understood. In particular,\nmultiple studies claimed that image watermark is robust in such setting. In\nthis work, we propose a new transfer evasion attack to image watermark in the\nno-box setting. Our transfer attack adds a perturbation to a watermarked image\nto evade multiple surrogate watermarking models trained by the attacker itself,\nand the perturbed watermarked image also evades the target watermarking model.\nOur major contribution is to show that, both theoretically and empirically,\nwatermark-based AI-generated image detector is not robust to evasion attacks\neven if the attacker does not have access to the watermarking model nor the\ndetection API.\n","authors":["Yuepeng Hu","Zhengyuan Jiang","Moyang Guo","Neil Gong"],"pdf_url":"https://arxiv.org/pdf/2403.15365v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16385v1","updated":"2024-03-25T03:02:27Z","published":"2024-03-25T03:02:27Z","title":"Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators\n  for Reasoning-Based Chart VQA","summary":"  Understanding data visualizations like charts and plots requires reasoning\nabout both visual elements and numerics. Although strong in extractive\nquestions, current chart visual question answering (chart VQA) models suffer on\ncomplex reasoning questions. In this work, we address the lack of reasoning\nability by data augmentation. We leverage Large Language Models (LLMs), which\nhave shown to have strong reasoning ability, as an automatic data annotator\nthat generates question-answer annotations for chart images. The key innovation\nin our method lies in the Synthesize Step-by-Step strategy: our LLM-based data\ngenerator learns to decompose the complex question into step-by-step\nsub-questions (rationales), which are then used to derive the final answer\nusing external tools, i.e. Python. This step-wise generation procedure is\ntrained on synthetic data generated using a template-based QA generation\npipeline. Experimental results highlight the significance of the proposed\nstep-by-step generation. By training with the LLM-augmented data (LAMENDA), we\nsignificantly enhance the chart VQA models, achieving the state-of-the-art\naccuracy on the ChartQA and PlotQA datasets. In particular, our approach\nimproves the accuracy of the previous state-of-the-art approach from 38% to 54%\non the human-written questions in the ChartQA dataset, which needs strong\nreasoning. We hope our work underscores the potential of synthetic data and\nencourages further exploration of data augmentation using LLMs for\nreasoning-heavy tasks.\n","authors":["Li Zhuowan","Jasani Bhavan","Tang Peng","Ghadar Shabnam"],"pdf_url":"https://arxiv.org/pdf/2403.16385v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2308.11432v4","updated":"2024-03-25T02:56:58Z","published":"2023-08-22T13:30:37Z","title":"A Survey on Large Language Model based Autonomous Agents","summary":"  Autonomous agents have long been a prominent research focus in both academic\nand industry communities. Previous research in this field often focuses on\ntraining agents with limited knowledge within isolated environments, which\ndiverges significantly from human learning processes, and thus makes the agents\nhard to achieve human-like decisions. Recently, through the acquisition of vast\namounts of web knowledge, large language models (LLMs) have demonstrated\nremarkable potential in achieving human-level intelligence. This has sparked an\nupsurge in studies investigating LLM-based autonomous agents. In this paper, we\npresent a comprehensive survey of these studies, delivering a systematic review\nof the field of LLM-based autonomous agents from a holistic perspective. More\nspecifically, we first discuss the construction of LLM-based autonomous agents,\nfor which we propose a unified framework that encompasses a majority of the\nprevious work. Then, we present a comprehensive overview of the diverse\napplications of LLM-based autonomous agents in the fields of social science,\nnatural science, and engineering. Finally, we delve into the evaluation\nstrategies commonly used for LLM-based autonomous agents. Based on the previous\nstudies, we also present several challenges and future directions in this\nfield. To keep track of this field and continuously update our survey, we\nmaintain a repository of relevant references at\nhttps://github.com/Paitesanshi/LLM-Agent-Survey.\n","authors":["Lei Wang","Chen Ma","Xueyang Feng","Zeyu Zhang","Hao Yang","Jingsen Zhang","Zhiyuan Chen","Jiakai Tang","Xu Chen","Yankai Lin","Wayne Xin Zhao","Zhewei Wei","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2308.11432v4.pdf","comment":"35 pages, 5 figures, 3 tables, has been accepted by frontiers of\n  computer science (FCS), doi={10.1007/s11704-024-40231-1}"},{"id":"http://arxiv.org/abs/2402.10670v2","updated":"2024-03-25T02:52:43Z","published":"2024-02-16T13:21:33Z","title":"OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via\n  Vision-Language Foundation Models","summary":"  Object navigation (ObjectNav) requires an agent to navigate through unseen\nenvironments to find queried objects. Many previous methods attempted to solve\nthis task by relying on supervised or reinforcement learning, where they are\ntrained on limited household datasets with close-set objects. However, two key\nchallenges are unsolved: understanding free-form natural language instructions\nthat demand open-set objects, and generalizing to new environments in a\nzero-shot manner. Aiming to solve the two challenges, in this paper, we propose\nOpenFMNav, an Open-set Foundation Model based framework for zero-shot object\nNavigation. We first unleash the reasoning abilities of large language models\n(LLMs) to extract proposed objects from natural language instructions that meet\nthe user's demand. We then leverage the generalizability of large vision\nlanguage models (VLMs) to actively discover and detect candidate objects from\nthe scene, building a Versatile Semantic Score Map (VSSM). Then, by conducting\ncommon sense reasoning on VSSM, our method can perform effective\nlanguage-guided exploration and exploitation of the scene and finally reach the\ngoal. By leveraging the reasoning and generalizing abilities of foundation\nmodels, our method can understand free-form human instructions and perform\neffective open-set zero-shot navigation in diverse environments. Extensive\nexperiments on the HM3D ObjectNav benchmark show that our method surpasses all\nthe strong baselines on all metrics, proving our method's effectiveness.\nFurthermore, we perform real robot demonstrations to validate our method's\nopen-set-ness and generalizability to real-world environments.\n","authors":["Yuxuan Kuang","Hai Lin","Meng Jiang"],"pdf_url":"https://arxiv.org/pdf/2402.10670v2.pdf","comment":"NAACL 2024 Findings"},{"id":"http://arxiv.org/abs/2403.16345v1","updated":"2024-03-25T00:43:44Z","published":"2024-03-25T00:43:44Z","title":"Enhanced Facet Generation with LLM Editing","summary":"  In information retrieval, facet identification of a user query is an\nimportant task. If a search service can recognize the facets of a user's query,\nit has the potential to offer users a much broader range of search results.\nPrevious studies can enhance facet prediction by leveraging retrieved documents\nand related queries obtained through a search engine. However, there are\nchallenges in extending it to other applications when a search engine operates\nas part of the model. First, search engines are constantly updated. Therefore,\nadditional information may change during training and test, which may reduce\nperformance. The second challenge is that public search engines cannot search\nfor internal documents. Therefore, a separate search system needs to be built\nto incorporate documents from private domains within the company. We propose\ntwo strategies that focus on a framework that can predict facets by taking only\nqueries as input without a search engine. The first strategy is multi-task\nlearning to predict SERP. By leveraging SERP as a target instead of a source,\nthe proposed model deeply understands queries without relying on external\nmodules. The second strategy is to enhance the facets by combining Large\nLanguage Model (LLM) and the small model. Overall performance improves when\nsmall model and LLM are combined rather than facet generation individually.\n","authors":["Joosung Lee","Jinhong Kim"],"pdf_url":"https://arxiv.org/pdf/2403.16345v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.09738v4","updated":"2024-03-25T23:53:01Z","published":"2024-03-13T18:16:21Z","title":"Evaluating Large Language Models as Generative User Simulators for\n  Conversational Recommendation","summary":"  Synthetic users are cost-effective proxies for real users in the evaluation\nof conversational recommender systems. Large language models show promise in\nsimulating human-like behavior, raising the question of their ability to\nrepresent a diverse population of users. We introduce a new protocol to measure\nthe degree to which language models can accurately emulate human behavior in\nconversational recommendation. This protocol is comprised of five tasks, each\ndesigned to evaluate a key property that a synthetic user should exhibit:\nchoosing which items to talk about, expressing binary preferences, expressing\nopen-ended preferences, requesting recommendations, and giving feedback.\nThrough evaluation of baseline simulators, we demonstrate these tasks\neffectively reveal deviations of language models from human behavior, and offer\ninsights on how to reduce the deviations with model selection and prompting\nstrategies.\n","authors":["Se-eun Yoon","Zhankui He","Jessica Maria Echterhoff","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2403.09738v4.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.02472v4","updated":"2024-03-25T23:09:58Z","published":"2024-03-04T20:34:58Z","title":"OffLanDat: A Community Based Implicit Offensive Language Dataset\n  Generated by Large Language Model Through Prompt Engineering","summary":"  The widespread presence of offensive languages on social media has resulted\nin adverse effects on societal well-being. As a result, it has become very\nimportant to address this issue with high priority. Offensive languages exist\nin both explicit and implicit forms, with the latter being more challenging to\ndetect. Current research in this domain encounters several challenges. Firstly,\nthe existing datasets primarily rely on the collection of texts containing\nexplicit offensive keywords, making it challenging to capture implicitly\noffensive contents that are devoid of these keywords. Secondly, usual\nmethodologies tend to focus solely on textual analysis, neglecting the valuable\ninsights that community information can provide. In this research paper, we\nintroduce a novel dataset OffLanDat, a community based implicit offensive\nlanguage dataset generated by ChatGPT containing data for 38 different target\ngroups. Despite limitations in generating offensive texts using ChatGPT due to\nethical constraints, we present a prompt-based approach that effectively\ngenerates implicit offensive languages. To ensure data quality, we evaluate our\ndata with human. Additionally, we employ a prompt-based Zero-Shot method with\nChatGPT and compare the detection results between human annotation and ChatGPT\nannotation. We utilize existing state-of-the-art models to see how effective\nthey are in detecting such languages. We will make our code and dataset public\nfor other researchers.\n","authors":["Amit Das","Mostafa Rahgouy","Dongji Feng","Zheng Zhang","Tathagata Bhattacharya","Nilanjana Raychawdhary","Mary Sandage","Lauramarie Pope","Gerry Dozier","Cheryl Seals"],"pdf_url":"https://arxiv.org/pdf/2403.02472v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.18040v2","updated":"2024-03-25T23:03:58Z","published":"2024-01-31T18:03:39Z","title":"Enhancing End-to-End Multi-Task Dialogue Systems: A Study on Intrinsic\n  Motivation Reinforcement Learning Algorithms for Improved Training and\n  Adaptability","summary":"  End-to-end multi-task dialogue systems are usually designed with separate\nmodules for the dialogue pipeline. Among these, the policy module is essential\nfor deciding what to do in response to user input. This policy is trained by\nreinforcement learning algorithms by taking advantage of an environment in\nwhich an agent receives feedback in the form of a reward signal. The current\ndialogue systems, however, only provide meagre and simplistic rewards.\nInvestigating intrinsic motivation reinforcement learning algorithms is the\ngoal of this study. Through this, the agent can quickly accelerate training and\nimprove its capacity to judge the quality of its actions by teaching it an\ninternal incentive system. In particular, we adapt techniques for random\nnetwork distillation and curiosity-driven reinforcement learning to measure the\nfrequency of state visits and encourage exploration by using semantic\nsimilarity between utterances. Experimental results on MultiWOZ, a\nheterogeneous dataset, show that intrinsic motivation-based debate systems\noutperform policies that depend on extrinsic incentives. By adopting random\nnetwork distillation, for example, which is trained using semantic similarity\nbetween user-system dialogues, an astounding average success rate of 73% is\nachieved. This is a significant improvement over the baseline Proximal Policy\nOptimization (PPO), which has an average success rate of 60%. In addition,\nperformance indicators such as booking rates and completion rates show a 10%\nrise over the baseline. Furthermore, these intrinsic incentive models help\nimprove the system's policy's resilience in an increasing amount of domains.\nThis implies that they could be useful in scaling up to settings that cover a\nwider range of domains.\n","authors":["Navin Kamuni","Hardik Shah","Sathishkumar Chintala","Naveen Kunchakuri","Sujatha Alla Old Dominion"],"pdf_url":"https://arxiv.org/pdf/2401.18040v2.pdf","comment":"6 pages, 1 figure, 18th IEEE International Conference on Semantic\n  Computing"},{"id":"http://arxiv.org/abs/2403.17254v1","updated":"2024-03-25T23:02:33Z","published":"2024-03-25T23:02:33Z","title":"A Hybrid Approach To Aspect Based Sentiment Analysis Using Transfer\n  Learning","summary":"  Aspect-Based Sentiment Analysis (ABSA) aims to identify terms or multiword\nexpressions (MWEs) on which sentiments are expressed and the sentiment\npolarities associated with them. The development of supervised models has been\nat the forefront of research in this area. However, training these models\nrequires the availability of manually annotated datasets which is both\nexpensive and time-consuming. Furthermore, the available annotated datasets are\ntailored to a specific domain, language, and text type. In this work, we\naddress this notable challenge in current state-of-the-art ABSA research. We\npropose a hybrid approach for Aspect Based Sentiment Analysis using transfer\nlearning. The approach focuses on generating weakly-supervised annotations by\nexploiting the strengths of both large language models (LLM) and traditional\nsyntactic dependencies. We utilise syntactic dependency structures of sentences\nto complement the annotations generated by LLMs, as they may overlook\ndomain-specific aspect terms. Extensive experimentation on multiple datasets is\nperformed to demonstrate the efficacy of our hybrid method for the tasks of\naspect term extraction and aspect sentiment classification.\n  Keywords: Aspect Based Sentiment Analysis, Syntactic Parsing, large language\nmodel (LLM)\n","authors":["Gaurav Negi","Rajdeep Sarkar","Omnia Zayed","Paul Buitelaar"],"pdf_url":"https://arxiv.org/pdf/2403.17254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17246v1","updated":"2024-03-25T22:47:13Z","published":"2024-03-25T22:47:13Z","title":"TwoStep: Multi-agent Task Planning using Classical Planners and Large\n  Language Models","summary":"  Classical planning formulations like the Planning Domain Definition Language\n(PDDL) admit action sequences guaranteed to achieve a goal state given an\ninitial state if any are possible. However, reasoning problems defined in PDDL\ndo not capture temporal aspects of action taking, for example that two agents\nin the domain can execute an action simultaneously if postconditions of each do\nnot interfere with preconditions of the other. A human expert can decompose a\ngoal into largely independent constituent parts and assign each agent to one of\nthese subgoals to take advantage of simultaneous actions for faster execution\nof plan steps, each using only single agent planning. By contrast, large\nlanguage models (LLMs) used for directly inferring plan steps do not guarantee\nexecution success, but do leverage commonsense reasoning to assemble action\nsequences. We combine the strengths of classical planning and LLMs by\napproximating human intuitions for two-agent planning goal decomposition. We\ndemonstrate that LLM-based goal decomposition leads to faster planning times\nthan solving multi-agent PDDL problems directly while simultaneously achieving\nfewer plan execution steps than a single agent plan alone and preserving\nexecution success. Additionally, we find that LLM-based approximations of\nsubgoals can achieve similar multi-agent execution steps than those specified\nby human experts. Website and resources at https://glamor-usc.github.io/twostep\n","authors":["Ishika Singh","David Traum","Jesse Thomason"],"pdf_url":"https://arxiv.org/pdf/2403.17246v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2403.17245v1","updated":"2024-03-25T22:46:16Z","published":"2024-03-25T22:46:16Z","title":"SPLICE: A Singleton-Enhanced PipeLIne for Coreference REsolution","summary":"  Singleton mentions, i.e.~entities mentioned only once in a text, are\nimportant to how humans understand discourse from a theoretical perspective.\nHowever previous attempts to incorporate their detection in end-to-end neural\ncoreference resolution for English have been hampered by the lack of singleton\nmention spans in the OntoNotes benchmark. This paper addresses this limitation\nby combining predicted mentions from existing nested NER systems and features\nderived from OntoNotes syntax trees. With this approach, we create a near\napproximation of the OntoNotes dataset with all singleton mentions, achieving\n~94% recall on a sample of gold singletons. We then propose a two-step neural\nmention and coreference resolution system, named SPLICE, and compare its\nperformance to the end-to-end approach in two scenarios: the OntoNotes test set\nand the out-of-domain (OOD) OntoGUM corpus. Results indicate that reconstructed\nsingleton training yields results comparable to end-to-end systems for\nOntoNotes, while improving OOD stability (+1.1 avg. F1). We conduct error\nanalysis for mention detection and delve into its impact on coreference\nclustering, revealing that precision improvements deliver more substantial\nbenefits than increases in recall for resolving coreference chains.\n","authors":["Yilun Zhu","Siyao Peng","Sameer Pradhan","Amir Zeldes"],"pdf_url":"https://arxiv.org/pdf/2403.17245v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17240v1","updated":"2024-03-25T22:42:19Z","published":"2024-03-25T22:42:19Z","title":"The Role of $n$-gram Smoothing in the Age of Neural Networks","summary":"  For nearly three decades, language models derived from the $n$-gram\nassumption held the state of the art on the task. The key to their success lay\nin the application of various smoothing techniques that served to combat\noverfitting. However, when neural language models toppled $n$-gram models as\nthe best performers, $n$-gram smoothing techniques became less relevant.\nIndeed, it would hardly be an understatement to suggest that the line of\ninquiry into $n$-gram smoothing techniques became dormant. This paper re-opens\nthe role classical $n$-gram smoothing techniques may play in the age of neural\nlanguage models. First, we draw a formal equivalence between label smoothing, a\npopular regularization technique for neural language models, and add-$\\lambda$\nsmoothing. Second, we derive a generalized framework for converting \\emph{any}\n$n$-gram smoothing technique into a regularizer compatible with neural language\nmodels. Our empirical results find that our novel regularizers are comparable\nto and, indeed, sometimes outperform label smoothing on language modeling and\nmachine translation.\n","authors":["Luca Malagutti","Andrius Buinovskij","Anej Svete","Clara Meister","Afra Amini","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2403.17240v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17220v1","updated":"2024-03-25T21:48:36Z","published":"2024-03-25T21:48:36Z","title":"Making Sentence Embeddings Robust to User-Generated Content","summary":"  NLP models have been known to perform poorly on user-generated content (UGC),\nmainly because it presents a lot of lexical variations and deviates from the\nstandard texts on which most of these models were trained. In this work, we\nfocus on the robustness of LASER, a sentence embedding model, to UGC data. We\nevaluate this robustness by LASER's ability to represent non-standard sentences\nand their standard counterparts close to each other in the embedding space.\nInspired by previous works extending LASER to other languages and modalities,\nwe propose RoLASER, a robust English encoder trained using a teacher-student\napproach to reduce the distances between the representations of standard and\nUGC sentences. We show that with training only on standard and synthetic\nUGC-like data, RoLASER significantly improves LASER's robustness to both\nnatural and artificial UGC data by achieving up to 2x and 11x better scores. We\nalso perform a fine-grained analysis on artificial UGC data and find that our\nmodel greatly outperforms LASER on its most challenging UGC phenomena such as\nkeyboard typos and social media abbreviations. Evaluation on downstream tasks\nshows that RoLASER performs comparably to or better than LASER on standard\ndata, while consistently outperforming it on UGC data.\n","authors":["Lydia Nishimwe","Benoît Sagot","Rachel Bawden"],"pdf_url":"https://arxiv.org/pdf/2403.17220v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17216v1","updated":"2024-03-25T21:46:35Z","published":"2024-03-25T21:46:35Z","title":"Ontology Completion with Natural Language Inference and Concept\n  Embeddings: An Analysis","summary":"  We consider the problem of finding plausible knowledge that is missing from a\ngiven ontology, as a generalisation of the well-studied taxonomy expansion\ntask. One line of work treats this task as a Natural Language Inference (NLI)\nproblem, thus relying on the knowledge captured by language models to identify\nthe missing knowledge. Another line of work uses concept embeddings to identify\nwhat different concepts have in common, taking inspiration from cognitive\nmodels for category based induction. These two approaches are intuitively\ncomplementary, but their effectiveness has not yet been compared. In this\npaper, we introduce a benchmark for evaluating ontology completion methods and\nthoroughly analyse the strengths and weaknesses of both approaches. We find\nthat both approaches are indeed complementary, with hybrid strategies achieving\nthe best overall results. We also find that the task is highly challenging for\nLarge Language Models, even after fine-tuning.\n","authors":["Na Li","Thomas Bailleux","Zied Bouraoui","Steven Schockaert"],"pdf_url":"https://arxiv.org/pdf/2403.17216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10966v6","updated":"2024-03-25T21:30:19Z","published":"2023-09-19T23:39:07Z","title":"MBR and QE Finetuning: Training-time Distillation of the Best and Most\n  Expensive Decoding Methods","summary":"  Recent research in decoding methods for Natural Language Generation (NLG)\ntasks has shown that MAP decoding is not optimal, because model probabilities\ndo not always align with human preferences. Stronger decoding methods,\nincluding Quality Estimation (QE) reranking and Minimum Bayes' Risk (MBR)\ndecoding, have since been proposed to mitigate the model-perplexity-vs-quality\nmismatch. While these decoding methods achieve state-of-the-art performance,\nthey are prohibitively expensive to compute. In this work, we propose MBR\nfinetuning and QE finetuning which distill the quality gains from these\ndecoding methods at training time, while using an efficient decoding algorithm\nat inference time. Using the canonical NLG task of Neural Machine Translation\n(NMT), we show that even with self-training, these finetuning methods\nsignificantly outperform the base model. Moreover, when using an external LLM\nas a teacher model, these finetuning methods outperform finetuning on\nhuman-generated references. These findings suggest new ways to leverage\nmonolingual data to achieve improvements in model quality that are on par with,\nor even exceed, improvements from human-curated data, while maintaining maximum\nefficiency during decoding.\n","authors":["Mara Finkelstein","Subhajit Naskar","Mehdi Mirzazadeh","Apurva Shah","Markus Freitag"],"pdf_url":"https://arxiv.org/pdf/2309.10966v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17199v1","updated":"2024-03-25T21:19:50Z","published":"2024-03-25T21:19:50Z","title":"Extracting Social Support and Social Isolation Information from Clinical\n  Psychiatry Notes: Comparing a Rule-based NLP System and a Large Language\n  Model","summary":"  Background: Social support (SS) and social isolation (SI) are social\ndeterminants of health (SDOH) associated with psychiatric outcomes. In\nelectronic health records (EHRs), individual-level SS/SI is typically\ndocumented as narrative clinical notes rather than structured coded data.\nNatural language processing (NLP) algorithms can automate the otherwise\nlabor-intensive process of data extraction.\n  Data and Methods: Psychiatric encounter notes from Mount Sinai Health System\n(MSHS, n=300) and Weill Cornell Medicine (WCM, n=225) were annotated and\nestablished a gold standard corpus. A rule-based system (RBS) involving\nlexicons and a large language model (LLM) using FLAN-T5-XL were developed to\nidentify mentions of SS and SI and their subcategories (e.g., social network,\ninstrumental support, and loneliness).\n  Results: For extracting SS/SI, the RBS obtained higher macro-averaged\nf-scores than the LLM at both MSHS (0.89 vs. 0.65) and WCM (0.85 vs. 0.82). For\nextracting subcategories, the RBS also outperformed the LLM at both MSHS (0.90\nvs. 0.62) and WCM (0.82 vs. 0.81).\n  Discussion and Conclusion: Unexpectedly, the RBS outperformed the LLMs across\nall metrics. Intensive review demonstrates that this finding is due to the\ndivergent approach taken by the RBS and LLM. The RBS were designed and refined\nto follow the same specific rules as the gold standard annotations. Conversely,\nthe LLM were more inclusive with categorization and conformed to common\nEnglish-language understanding. Both approaches offer advantages and are made\navailable open-source for future testing.\n","authors":["Braja Gopal Patra","Lauren A. Lepow","Praneet Kasi Reddy Jagadeesh Kumar","Veer Vekaria","Mohit Manoj Sharma","Prakash Adekkanattu","Brian Fennessy","Gavin Hynes","Isotta Landi","Jorge A. Sanchez-Ruiz","Euijung Ryu","Joanna M. Biernacka","Girish N. Nadkarni","Ardesheer Talati","Myrna Weissman","Mark Olfson","J. John Mann","Alexander W. Charney","Jyotishman Pathak"],"pdf_url":"https://arxiv.org/pdf/2403.17199v1.pdf","comment":"2 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.17196v1","updated":"2024-03-25T21:17:14Z","published":"2024-03-25T21:17:14Z","title":"GPT-4 Understands Discourse at Least as Well as Humans Do","summary":"  We test whether a leading AI system GPT-4 understands discourse as well as\nhumans do, using a standardized test of discourse comprehension. Participants\nare presented with brief stories and then answer eight yes/no questions probing\ntheir comprehension of the story. The questions are formatted to assess the\nseparate impacts of directness (stated vs. implied) and salience (main idea vs.\ndetails). GPT-4 performs slightly, but not statistically significantly, better\nthan humans given the very high level of human performance. Both GPT-4 and\nhumans exhibit a strong ability to make inferences about information that is\nnot explicitly stated in a story, a critical test of understanding.\n","authors":["Thomas Shultz","Jamie Wise","Ardavan Salehi Nobandegani"],"pdf_url":"https://arxiv.org/pdf/2403.17196v1.pdf","comment":"7 pages, 1 figure, 3 tables"},{"id":"http://arxiv.org/abs/2311.09602v2","updated":"2024-03-25T20:42:57Z","published":"2023-11-16T06:20:13Z","title":"Language Models (Mostly) Do Not Consider Emotion Triggers When\n  Predicting Emotion","summary":"  Situations and events evoke emotions in humans, but to what extent do they\ninform the prediction of emotion detection models? This work investigates how\nwell human-annotated emotion triggers correlate with features that models\ndeemed salient in their prediction of emotions. First, we introduce a novel\ndataset EmoTrigger, consisting of 900 social media posts sourced from three\ndifferent datasets; these were annotated by experts for emotion triggers with\nhigh agreement. Using EmoTrigger, we evaluate the ability of large language\nmodels (LLMs) to identify emotion triggers, and conduct a comparative analysis\nof the features considered important for these tasks between LLMs and\nfine-tuned models. Our analysis reveals that emotion triggers are largely not\nconsidered salient features for emotion prediction models, instead there is\nintricate interplay between various features and the task of emotion detection.\n","authors":["Smriti Singh","Cornelia Caragea","Junyi Jessy Li"],"pdf_url":"https://arxiv.org/pdf/2311.09602v2.pdf","comment":"NAACL 2024 Camera Ready"},{"id":"http://arxiv.org/abs/2403.17169v1","updated":"2024-03-25T20:36:03Z","published":"2024-03-25T20:36:03Z","title":"NUMTEMP: A real-world benchmark to verify claims with statistical and\n  temporal expressions","summary":"  Automated fact checking has gained immense interest to tackle the growing\nmisinformation in the digital era. Existing systems primarily focus on\nsynthetic claims on Wikipedia, and noteworthy progress has also been made on\nreal-world claims. In this work, we release Numtemp, a diverse, multi-domain\ndataset focused exclusively on numerical claims, encompassing temporal,\nstatistical and diverse aspects with fine-grained metadata and an evidence\ncollection without leakage. This addresses the challenge of verifying\nreal-world numerical claims, which are complex and often lack precise\ninformation, not addressed by existing works that mainly focus on synthetic\nclaims. We evaluate and quantify the limitations of existing solutions for the\ntask of verifying numerical claims. We also evaluate claim decomposition based\nmethods, numerical understanding based models and our best baselines achieves a\nmacro-F1 of 58.32. This demonstrates that Numtemp serves as a challenging\nevaluation set for numerical claim verification.\n","authors":["Venktesh V","Abhijit Anand","Avishek Anand","Vinay Setty"],"pdf_url":"https://arxiv.org/pdf/2403.17169v1.pdf","comment":"17 pages, 1 figure"},{"id":"http://arxiv.org/abs/2403.17158v1","updated":"2024-03-25T20:16:14Z","published":"2024-03-25T20:16:14Z","title":"Reflecting the Male Gaze: Quantifying Female Objectification in 19th and\n  20th Century Novels","summary":"  Inspired by the concept of the male gaze (Mulvey, 1975) in literature and\nmedia studies, this paper proposes a framework for analyzing gender bias in\nterms of female objectification: the extent to which a text portrays female\nindividuals as objects of visual pleasure. Our framework measures female\nobjectification along two axes. First, we compute an agency bias score that\nindicates whether male entities are more likely to appear in the text as\ngrammatical agents than female entities. Next, by analyzing the word embedding\nspace induced by a text (Caliskan et al., 2017), we compute an appearance bias\nscore that indicates whether female entities are more closely associated with\nappearance-related words than male entities. Applying our framework to 19th and\n20th century novels reveals evidence of female objectification in literature:\nwe find that novels written from a male perspective systematically objectify\nfemale characters, while novels written from a female perspective do not\nexhibit statistically significant objectification of any gender.\n","authors":["Kexin Luo","Yue Mao","Bei Zhang","Sophie Hao"],"pdf_url":"https://arxiv.org/pdf/2403.17158v1.pdf","comment":"To appear in LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2310.16937v2","updated":"2024-03-25T20:14:07Z","published":"2023-10-25T19:04:33Z","title":"Learning Transfers over Several Programming Languages","summary":"  Large language models (LLMs) have become remarkably good at improving\ndeveloper productivity for high-resource programming languages. These models\nuse two kinds of data: large amounts of unlabeled code samples for pre-training\nand relatively smaller amounts of labeled code samples for fine-tuning or\nin-context learning. Unfortunately, many programming languages are\nlow-resource, lacking labeled samples for most tasks and often even lacking\nunlabeled samples. Therefore, users of low-resource languages (e.g., legacy or\nnew languages) miss out on the benefits of LLMs. Cross-lingual transfer uses\ndata from a source language to improve model performance on a target language.\nIt has been well-studied for natural languages, but has received little\nattention for programming languages. This paper reports extensive experiments\non four tasks using a transformer-based LLM and 11 to 41 programming languages\nto explore the following questions. First, how well does cross-lingual transfer\nwork for a given task across different language pairs. Second, given a task and\ntarget language, how should one choose a source language. Third, which\ncharacteristics of a language pair are predictive of transfer performance, and\nhow does that depend on the given task. Our empirical study with 1,808\nexperiments reveals practical and scientific insights, such as Kotlin and\nJavaScript being the most transferable source languages and different tasks\nrelying on substantially different features. Overall, we find that learning\ntransfers well across several programming languages.\n","authors":["Razan Baltaji","Saurabh Pujar","Louis Mandel","Martin Hirzel","Luca Buratti","Lav Varshney"],"pdf_url":"https://arxiv.org/pdf/2310.16937v2.pdf","comment":"15 pages, 9 figures, 8 tables"},{"id":"http://arxiv.org/abs/2403.17155v1","updated":"2024-03-25T20:12:02Z","published":"2024-03-25T20:12:02Z","title":"Task-Agnostic Detector for Insertion-Based Backdoor Attacks","summary":"  Textual backdoor attacks pose significant security threats. Current detection\napproaches, typically relying on intermediate feature representation or\nreconstructing potential triggers, are task-specific and less effective beyond\nsentence classification, struggling with tasks like question answering and\nnamed entity recognition. We introduce TABDet (Task-Agnostic Backdoor\nDetector), a pioneering task-agnostic method for backdoor detection. TABDet\nleverages final layer logits combined with an efficient pooling technique,\nenabling unified logit representation across three prominent NLP tasks. TABDet\ncan jointly learn from diverse task-specific models, demonstrating superior\ndetection efficacy over traditional task-specific methods.\n","authors":["Weimin Lyu","Xiao Lin","Songzhu Zheng","Lu Pang","Haibin Ling","Susmit Jha","Chao Chen"],"pdf_url":"https://arxiv.org/pdf/2403.17155v1.pdf","comment":"Findings of NAACL 2024"},{"id":"http://arxiv.org/abs/2402.12243v4","updated":"2024-03-25T19:48:16Z","published":"2024-02-19T15:58:15Z","title":"Understanding the Effects of Noise in Text-to-SQL: An Examination of the\n  BIRD-Bench Benchmark","summary":"  Text-to-SQL, which involves translating natural language into Structured\nQuery Language (SQL), is crucial for enabling broad access to structured\ndatabases without expert knowledge. However, designing models for such tasks is\nchallenging due to numerous factors, including the presence of 'noise,' such as\nambiguous questions and syntactical errors. This study provides an in-depth\nanalysis of the distribution and types of noise in the widely used BIRD-Bench\nbenchmark and the impact of noise on models. While BIRD-Bench was created to\nmodel dirty and noisy database values, it was not created to contain noise and\nerrors in the questions and gold queries. We found that noise in questions and\ngold queries are prevalent in the dataset, with varying amounts across domains,\nand with an uneven distribution between noise types. The presence of incorrect\ngold SQL queries, which then generate incorrect gold answers, has a significant\nimpact on the benchmark's reliability. Surprisingly, when evaluating models on\ncorrected SQL queries, zero-shot baselines surpassed the performance of\nstate-of-the-art prompting methods. We conclude that informative noise labels\nand reliable benchmarks are crucial to developing new Text-to-SQL methods that\ncan handle varying types of noise. All datasets, annotations, and code are\navailable at\nhttps://github.com/niklaswretblad/the-effects-of-noise-in-text-to-SQL.\n","authors":["Niklas Wretblad","Fredrik Gordh Riseby","Rahul Biswas","Amin Ahmadi","Oskar Holmström"],"pdf_url":"https://arxiv.org/pdf/2402.12243v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17146v1","updated":"2024-03-25T19:44:06Z","published":"2024-03-25T19:44:06Z","title":"Outcome-Constrained Large Language Models for Countering Hate Speech","summary":"  Counterspeech that challenges or responds to hate speech has been seen as an\nalternative to mitigate the negative impact of hate speech and foster\nproductive online communications. Research endeavors have been directed to\nusing language models for the automatic generation of counterspeech to assist\nefforts in combating online hate. Existing research focuses on the generation\nof counterspeech with certain linguistic attributes, such as being polite,\ninformative, and intent-driven. However, it remains unclear what impact the\ncounterspeech might have in an online environment. We first explore methods\nthat utilize large language models (LLM) to generate counterspeech constrained\nby potential conversation outcomes. We build two conversation outcome\nclassifiers that predict the incivility level and the hater reentry behavior\nfollowing replies to hate with Reddit data, then propose four methods to\nincorporate the desired outcomes, i.e., low conversation incivility and\nnon-hateful hater reentry, into the text generation process, including Prompt\nwith Instructions, Prompt and Select, LLM finetune, and LLM transformer\nreinforcement learning (TRL). Evaluation results show effective strategies to\ngenerate outcome-constrained counterspeech and the linguistic characteristics\nof texts generated by different methods.\n","authors":["Lingzi Hong","Pengcheng Luo","Eduardo Blanco","Xiaoying Song"],"pdf_url":"https://arxiv.org/pdf/2403.17146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17143v1","updated":"2024-03-25T19:40:26Z","published":"2024-03-25T19:40:26Z","title":"Guided Distant Supervision for Multilingual Relation Extraction Data:\n  Adapting to a New Language","summary":"  Relation extraction is essential for extracting and understanding\nbiographical information in the context of digital humanities and related\nsubjects. There is a growing interest in the community to build datasets\ncapable of training machine learning models to extract relationships. However,\nannotating such datasets can be expensive and time-consuming, in addition to\nbeing limited to English. This paper applies guided distant supervision to\ncreate a large biographical relationship extraction dataset for German. Our\ndataset, composed of more than 80,000 instances for nine relationship types, is\nthe largest biographical German relationship extraction dataset. We also create\na manually annotated dataset with 2000 instances to evaluate the models and\nrelease it together with the dataset compiled using guided distant supervision.\nWe train several state-of-the-art machine learning models on the automatically\ncreated dataset and release them as well. Furthermore, we experiment with\nmultilingual and cross-lingual experiments that could benefit many low-resource\nlanguages.\n","authors":["Alistair Plum","Tharindu Ranasinghe","Christoph Purschke"],"pdf_url":"https://arxiv.org/pdf/2403.17143v1.pdf","comment":"Accepted to LREC-COLING 2024 (The 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation)"},{"id":"http://arxiv.org/abs/2403.17141v1","updated":"2024-03-25T19:28:10Z","published":"2024-03-25T19:28:10Z","title":"MetaAligner: Conditional Weak-to-Strong Correction for Generalizable\n  Multi-Objective Alignment of Language Models","summary":"  Recent advancements in large language models (LLMs) aim to tackle\nheterogeneous human expectations and values via multi-objective preference\nalignment. However, existing methods are parameter-adherent to the policy\nmodel, leading to two key limitations: (1) the high-cost repetition of their\nalignment algorithms for each new target model; (2) they cannot expand to\nunseen objectives due to their static alignment objectives. In this work, we\npropose Meta-Objective Aligner (MetaAligner), a model that performs conditional\nweak-to-strong correction for weak responses to approach strong responses.\nMetaAligner is the first policy-agnostic and generalizable method for\nmulti-objective preference alignment, which enables plug-and-play alignment by\ndecoupling parameter updates from the policy models and facilitates zero-shot\npreference alignment for unseen objectives via in-context learning.\nExperimental results show that MetaAligner achieves significant and balanced\nimprovements in multi-objective alignments on 11 policy models with up to 63x\nmore parameters, and outperforms previous alignment methods with down to 22.27x\nless computational resources. The model also accurately aligns with unseen\nobjectives, marking the first step towards generalizable multi-objective\npreference alignment.\n","authors":["Kailai Yang","Zhiwei Liu","Qianqian Xie","Tianlin Zhang","Nirui Song","Jimin Huang","Ziyan Kuang","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2403.17141v1.pdf","comment":"Work in progress, more general experimental results to come"},{"id":"http://arxiv.org/abs/2403.17135v1","updated":"2024-03-25T19:17:59Z","published":"2024-03-25T19:17:59Z","title":"Exploring the Generalization of Cancer Clinical Trial Eligibility\n  Classifiers Across Diseases","summary":"  Clinical trials are pivotal in medical research, and NLP can enhance their\nsuccess, with application in recruitment. This study aims to evaluate the\ngeneralizability of eligibility classification across a broad spectrum of\nclinical trials. Starting with phase 3 cancer trials, annotated with seven\neligibility exclusions, then to determine how well models can generalize to\nnon-cancer and non-phase 3 trials. To assess this, we have compiled eligibility\ncriteria data for five types of trials: (1) additional phase 3 cancer trials,\n(2) phase 1 and 2 cancer trials, (3) heart disease trials, (4) type 2 diabetes\ntrials, and (5) observational trials for any disease, comprising 2,490\nannotated eligibility criteria across seven exclusion types. Our results show\nthat models trained on the extensive cancer dataset can effectively handle\ncriteria commonly found in non-cancer trials, such as autoimmune diseases.\nHowever, they struggle with criteria disproportionately prevalent in cancer\ntrials, like prior malignancy. We also experiment with few-shot learning,\ndemonstrating that a limited number of disease-specific examples can partially\novercome this performance gap. We are releasing this new dataset of annotated\neligibility statements to promote the development of cross-disease\ngeneralization in clinical trial classification.\n","authors":["Yumeng Yang","Ashley Gilliam","Ethan B Ludmir","Kirk Roberts"],"pdf_url":"https://arxiv.org/pdf/2403.17135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17125v1","updated":"2024-03-25T19:07:32Z","published":"2024-03-25T19:07:32Z","title":"The Strong Pull of Prior Knowledge in Large Language Models and Its\n  Impact on Emotion Recognition","summary":"  In-context Learning (ICL) has emerged as a powerful paradigm for performing\nnatural language tasks with Large Language Models (LLM) without updating the\nmodels' parameters, in contrast to the traditional gradient-based finetuning.\nThe promise of ICL is that the LLM can adapt to perform the present task at a\ncompetitive or state-of-the-art level at a fraction of the cost. The ability of\nLLMs to perform tasks in this few-shot manner relies on their background\nknowledge of the task (or task priors). However, recent work has found that,\nunlike traditional learning, LLMs are unable to fully integrate information\nfrom demonstrations that contrast task priors. This can lead to performance\nsaturation at suboptimal levels, especially for subjective tasks such as\nemotion recognition, where the mapping from text to emotions can differ widely\ndue to variability in human annotations. In this work, we design experiments\nand propose measurements to explicitly quantify the consistency of proxies of\nLLM priors and their pull on the posteriors. We show that LLMs have strong yet\ninconsistent priors in emotion recognition that ossify their predictions. We\nalso find that the larger the model, the stronger these effects become. Our\nresults suggest that caution is needed when using ICL with larger LLMs for\naffect-centered tasks outside their pre-training domain and when interpreting\nICL results.\n","authors":["Georgios Chochlakis","Alexandros Potamianos","Kristina Lerman","Shrikanth Narayanan"],"pdf_url":"https://arxiv.org/pdf/2403.17125v1.pdf","comment":"30 pages, 27 figures"},{"id":"http://arxiv.org/abs/2403.17124v1","updated":"2024-03-25T19:04:59Z","published":"2024-03-25T19:04:59Z","title":"Grounding Language Plans in Demonstrations Through Counterfactual\n  Perturbations","summary":"  Grounding the common-sense reasoning of Large Language Models in physical\ndomains remains a pivotal yet unsolved problem for embodied AI. Whereas prior\nworks have focused on leveraging LLMs directly for planning in symbolic spaces,\nthis work uses LLMs to guide the search of task structures and constraints\nimplicit in multi-step demonstrations. Specifically, we borrow from\nmanipulation planning literature the concept of mode families, which group\nrobot configurations by specific motion constraints, to serve as an abstraction\nlayer between the high-level language representations of an LLM and the\nlow-level physical trajectories of a robot. By replaying a few human\ndemonstrations with synthetic perturbations, we generate coverage over the\ndemonstrations' state space with additional successful executions as well as\ncounterfactuals that fail the task. Our explanation-based learning framework\ntrains an end-to-end differentiable neural network to predict successful\ntrajectories from failures and as a by-product learns classifiers that ground\nlow-level states and images in mode families without dense labeling. The\nlearned grounding classifiers can further be used to translate language plans\ninto reactive policies in the physical domain in an interpretable manner. We\nshow our approach improves the interpretability and reactivity of imitation\nlearning through 2D navigation and simulated and real robot manipulation tasks.\nWebsite: https://sites.google.com/view/grounding-plans\n","authors":["Yanwei Wang","Tsun-Hsuan Wang","Jiayuan Mao","Michael Hagenow","Julie Shah"],"pdf_url":"https://arxiv.org/pdf/2403.17124v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.07645v2","updated":"2024-03-25T18:52:34Z","published":"2023-07-14T22:25:39Z","title":"Othering and low status framing of immigrant cuisines in US restaurant\n  reviews and large language models","summary":"  Identifying implicit attitudes toward food can mitigate social prejudice due\nto food's salience as a marker of ethnic identity. Stereotypes about food are\nrepresentational harms that may contribute to racialized discourse and\nnegatively impact economic outcomes for restaurants. Understanding the presence\nof representational harms in online corpora in particular is important, given\nthe increasing use of large language models (LLMs) for text generation and\ntheir tendency to reproduce attitudes in their training data. Through careful\nlinguistic analyses, we evaluate social theories about attitudes toward\nimmigrant cuisine in a large-scale study of framing differences in 2.1M English\nlanguage Yelp reviews. Controlling for factors such as restaurant price and\nneighborhood racial diversity, we find that immigrant cuisines are more likely\nto be othered using socially constructed frames of authenticity (e.g.,\n\"authentic,\" \"traditional\"), and that non-European cuisines (e.g., Indian,\nMexican) in particular are described as more exotic compared to European ones\n(e.g., French). We also find that non-European cuisines are more likely to be\ndescribed as cheap and dirty, even after controlling for price, and even among\nthe most expensive restaurants. Finally, we show that reviews generated by LLMs\nreproduce similar framing tendencies, pointing to the downstream retention of\nthese representational harms. Our results corroborate social theories of\ngastronomic stereotyping, revealing racialized evaluative processes and\nlinguistic strategies through which they manifest.\n","authors":["Yiwei Luo","Kristina Gligorić","Dan Jurafsky"],"pdf_url":"https://arxiv.org/pdf/2307.07645v2.pdf","comment":"ICWSM '24"},{"id":"http://arxiv.org/abs/2403.12151v2","updated":"2024-03-25T18:50:06Z","published":"2024-03-18T18:08:44Z","title":"Fusing Domain-Specific Content from Large Language Models into Knowledge\n  Graphs for Enhanced Zero Shot Object State Classification","summary":"  Domain-specific knowledge can significantly contribute to addressing a wide\nvariety of vision tasks. However, the generation of such knowledge entails\nconsiderable human labor and time costs. This study investigates the potential\nof Large Language Models (LLMs) in generating and providing domain-specific\ninformation through semantic embeddings. To achieve this, an LLM is integrated\ninto a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors\nin the context of the Vision-based Zero-shot Object State Classification task.\nWe thoroughly examine the behavior of the LLM through an extensive ablation\nstudy. Our findings reveal that the integration of LLM-based embeddings, in\ncombination with general-purpose pre-trained embeddings, leads to substantial\nperformance improvements. Drawing insights from this ablation study, we conduct\na comparative analysis against competing models, thereby highlighting the\nstate-of-the-art performance achieved by the proposed approach.\n","authors":["Filippos Gouidis","Katerina Papantoniou","Konstantinos Papoutsakis Theodore Patkos","Antonis Argyros","Dimitris Plexousakis"],"pdf_url":"https://arxiv.org/pdf/2403.12151v2.pdf","comment":"Accepted at the AAAI-MAKE 24"},{"id":"http://arxiv.org/abs/2310.13257v2","updated":"2024-03-25T18:48:40Z","published":"2023-10-20T03:33:36Z","title":"Visual Grounding Helps Learn Word Meanings in Low-Data Regimes","summary":"  Modern neural language models (LMs) are powerful tools for modeling human\nsentence production and comprehension, and their internal representations are\nremarkably well-aligned with representations of language in the human brain.\nBut to achieve these results, LMs must be trained in distinctly un-human-like\nways - requiring orders of magnitude more language data than children receive\nduring development, and without perceptual or social context. Do models trained\nmore naturalistically -- with grounded supervision -- exhibit more humanlike\nlanguage learning? We investigate this question in the context of word\nlearning, a key sub-task in language acquisition. We train a diverse set of LM\narchitectures, with and without auxiliary visual supervision, on datasets of\nvarying scales. We then evaluate these models' learning of syntactic\ncategories, lexical relations, semantic features, word similarity, and\nalignment with human neural representations. We find that visual supervision\ncan indeed improve the efficiency of word learning. However, these improvements\nare limited: they are present almost exclusively in the low-data regime, and\nsometimes canceled out by the inclusion of rich distributional signals from\ntext. The information conveyed by text and images is not redundant -- models\nmainly driven by visual information yield qualitatively different from those\nmainly driven by word co-occurrences. However, our results suggest that current\nmultimodal modeling approaches fail to effectively leverage visual information\nto build human-like word representations from human-scale data.\n","authors":["Chengxu Zhuang","Evelina Fedorenko","Jacob Andreas"],"pdf_url":"https://arxiv.org/pdf/2310.13257v2.pdf","comment":"Accepted by NAACL 2024"},{"id":"http://arxiv.org/abs/2403.17104v1","updated":"2024-03-25T18:41:47Z","published":"2024-03-25T18:41:47Z","title":"Attribute First, then Generate: Locally-attributable Grounded Text\n  Generation","summary":"  Recent efforts to address hallucinations in Large Language Models (LLMs) have\nfocused on attributed text generation, which supplements generated texts with\ncitations of supporting sources for post-generation fact-checking and\ncorrections. Yet, these citations often point to entire documents or\nparagraphs, burdening users with extensive verification work. In this paper, we\nintroduce a locally-attributable text generation approach, prioritizing concise\nattributions. Our method, named ``Attribute First, then Generate'', breaks down\nthe conventional end-to-end generation process into three intuitive steps:\ncontent selection, sentence planning, and sequential sentence generation. By\ninitially identifying relevant source segments (``select first'') and then\nconditioning the generation process on them (``then generate''), we ensure\nthese segments also act as the output's fine-grained attributions (``select''\nbecomes ``attribute''). Tested on Multi-document Summarization and Long-form\nQuestion-answering, our method not only yields more concise citations than the\nbaselines but also maintains - and in some cases enhances - both generation\nquality and attribution accuracy. Furthermore, it significantly reduces the\ntime required for fact verification by human assessors.\n","authors":["Aviv Slobodkin","Eran Hirsch","Arie Cattan","Tal Schuster","Ido Dagan"],"pdf_url":"https://arxiv.org/pdf/2403.17104v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16338v2","updated":"2024-03-25T18:18:40Z","published":"2023-10-25T03:40:50Z","title":"Generative Pre-training for Speech with Flow Matching","summary":"  Generative models have gained more and more attention in recent years for\ntheir remarkable success in tasks that required estimating and sampling data\ndistribution to generate high-fidelity synthetic data. In speech,\ntext-to-speech synthesis and neural vocoder are good examples where generative\nmodels have shined. While generative models have been applied to different\napplications in speech, there exists no general-purpose generative model that\nmodels speech directly. In this work, we take a step toward this direction by\nshowing a single pre-trained generative model can be adapted to different\ndownstream tasks with strong performance. Specifically, we pre-trained a\ngenerative model, named SpeechFlow, on 60k hours of untranscribed speech with\nFlow Matching and masked conditions. Experiment results show the pre-trained\ngenerative model can be fine-tuned with task-specific data to match or surpass\nexisting expert models on speech enhancement, separation, and synthesis. Our\nwork suggested a foundational model for generation tasks in speech can be built\nwith generative pre-training.\n","authors":["Alexander H. Liu","Matt Le","Apoorv Vyas","Bowen Shi","Andros Tjandra","Wei-Ning Hsu"],"pdf_url":"https://arxiv.org/pdf/2310.16338v2.pdf","comment":"ICLR 2024"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2403.02969v2","updated":"2024-03-25T12:45:03Z","published":"2024-03-05T13:45:46Z","title":"Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception","summary":"  Multimodal Large Language Model (MLLMs) leverages Large Language Models as a\ncognitive framework for diverse visual-language tasks. Recent efforts have been\nmade to equip MLLMs with visual perceiving and grounding capabilities. However,\nthere still remains a gap in providing fine-grained pixel-level perceptions and\nextending interactions beyond text-specific inputs. In this work, we propose\n{\\bf{AnyRef}}, a general MLLM model that can generate pixel-wise object\nperceptions and natural language descriptions from multi-modality references,\nsuch as texts, boxes, images, or audio. This innovation empowers users with\ngreater flexibility to engage with the model beyond textual and regional\nprompts, without modality-specific designs. Through our proposed refocusing\nmechanism, the generated grounding output is guided to better focus on the\nreferenced object, implicitly incorporating additional pixel-level supervision.\nThis simple modification utilizes attention scores generated during the\ninference of LLM, eliminating the need for extra computations while exhibiting\nperformance enhancements in both grounding masks and referring expressions.\nWith only publicly available training data, our model achieves state-of-the-art\nresults across multiple benchmarks, including diverse modality referring\nsegmentation and region-level referring expression generation.\n","authors":["Junwen He","Yifan Wang","Lijun Wang","Huchuan Lu","Jun-Yan He","Jin-Peng Lan","Bin Luo","Xuansong Xie"],"pdf_url":"https://arxiv.org/pdf/2403.02969v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.16697v1","updated":"2024-03-25T12:31:01Z","published":"2024-03-25T12:31:01Z","title":"DPStyler: Dynamic PromptStyler for Source-Free Domain Generalization","summary":"  Source-Free Domain Generalization (SFDG) aims to develop a model that works\nfor unseen target domains without relying on any source domain. Recent work,\nPromptStyler, employs text prompts to simulate different distribution shifts in\nthe joint vision-language space, allowing the model to generalize effectively\nto unseen domains without using any images. However, 1) PromptStyler's style\ngeneration strategy has limitations, as all style patterns are fixed after the\nfirst training phase. This leads to the training set in the second training\nphase being restricted to a limited set of styles. Additionally, 2) the frozen\ntext encoder in PromptStyler result in the encoder's output varying with the\nstyle of the input text prompts, making it difficult for the model to learn\ndomain-invariant features. In this paper, we introduce Dynamic PromptStyler\n(DPStyler), comprising Style Generation and Style Removal modules to address\nthese issues. The Style Generation module refreshes all styles at every\ntraining epoch, while the Style Removal module eliminates variations in the\nencoder's output features caused by input styles. Moreover, since the Style\nGeneration module, responsible for generating style word vectors using random\nsampling or style mixing, makes the model sensitive to input text prompts, we\nintroduce a model ensemble method to mitigate this sensitivity. Extensive\nexperiments demonstrate that our framework outperforms state-of-the-art methods\non benchmark datasets.\n","authors":["Yunlong Tang","Yuxuan Wan","Lei Qi","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2403.16697v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16695v1","updated":"2024-03-25T12:26:32Z","published":"2024-03-25T12:26:32Z","title":"Assessing the Performance of Deep Learning for Automated Gleason Grading\n  in Prostate Cancer","summary":"  Prostate cancer is a dominant health concern calling for advanced diagnostic\ntools. Utilizing digital pathology and artificial intelligence, this study\nexplores the potential of 11 deep neural network architectures for automated\nGleason grading in prostate carcinoma focusing on comparing traditional and\nrecent architectures. A standardized image classification pipeline, based on\nthe AUCMEDI framework, facilitated robust evaluation using an in-house dataset\nconsisting of 34,264 annotated tissue tiles. The results indicated varying\nsensitivity across architectures, with ConvNeXt demonstrating the strongest\nperformance. Notably, newer architectures achieved superior performance, even\nthough with challenges in differentiating closely related Gleason grades. The\nConvNeXt model was capable of learning a balance between complexity and\ngeneralizability. Overall, this study lays the groundwork for enhanced Gleason\ngrading systems, potentially improving diagnostic efficiency for prostate\ncancer.\n","authors":["Dominik Müller","Philip Meyer","Lukas Rentschler","Robin Manz","Daniel Hieber","Jonas Bäcker","Samantha Cramer","Christoph Wengenmayr","Bruno Märkl","Ralf Huss","Frank Kramer","Iñaki Soto-Rey","Johannes Raffler"],"pdf_url":"https://arxiv.org/pdf/2403.16695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16689v1","updated":"2024-03-25T12:23:39Z","published":"2024-03-25T12:23:39Z","title":"Synapse: Learning Preferential Concepts from Visual Demonstrations","summary":"  This paper addresses the problem of preference learning, which aims to learn\nuser-specific preferences (e.g., \"good parking spot\", \"convenient drop-off\nlocation\") from visual input. Despite its similarity to learning factual\nconcepts (e.g., \"red cube\"), preference learning is a fundamentally harder\nproblem due to its subjective nature and the paucity of person-specific\ntraining data. We address this problem using a new framework called Synapse,\nwhich is a neuro-symbolic approach designed to efficiently learn preferential\nconcepts from limited demonstrations. Synapse represents preferences as\nneuro-symbolic programs in a domain-specific language (DSL) that operates over\nimages, and leverages a novel combination of visual parsing, large language\nmodels, and program synthesis to learn programs representing individual\npreferences. We evaluate Synapse through extensive experimentation including a\nuser case study focusing on mobility-related concepts in mobile robotics and\nautonomous driving. Our evaluation demonstrates that Synapse significantly\noutperforms existing baselines as well as its own ablations. The code and other\ndetails can be found on the project website https://amrl.cs.utexas.edu/synapse .\n","authors":["Sadanand Modak","Noah Patton","Isil Dillig","Joydeep Biswas"],"pdf_url":"https://arxiv.org/pdf/2403.16689v1.pdf","comment":"23 pages, 7 figures; Preprint"},{"id":"http://arxiv.org/abs/2403.16678v1","updated":"2024-03-25T12:15:42Z","published":"2024-03-25T12:15:42Z","title":"DeepGleason: a System for Automated Gleason Grading of Prostate Cancer\n  using Deep Neural Networks","summary":"  Advances in digital pathology and artificial intelligence (AI) offer\npromising opportunities for clinical decision support and enhancing diagnostic\nworkflows. Previous studies already demonstrated AI's potential for automated\nGleason grading, but lack state-of-the-art methodology and model reusability.\nTo address this issue, we propose DeepGleason: an open-source deep neural\nnetwork based image classification system for automated Gleason grading using\nwhole-slide histopathology images from prostate tissue sections. Implemented\nwith the standardized AUCMEDI framework, our tool employs a tile-wise\nclassification approach utilizing fine-tuned image preprocessing techniques in\ncombination with a ConvNeXt architecture which was compared to various\nstate-of-the-art architectures. The neural network model was trained and\nvalidated on an in-house dataset of 34,264 annotated tiles from 369 prostate\ncarcinoma slides. We demonstrated that DeepGleason is capable of highly\naccurate and reliable Gleason grading with a macro-averaged F1-score of 0.806,\nAUC of 0.991, and Accuracy of 0.974. The internal architecture comparison\nrevealed that the ConvNeXt model was superior performance-wise on our dataset\nto established and other modern architectures like transformers. Furthermore,\nwe were able to outperform the current state-of-the-art in tile-wise\nfine-classification with a sensitivity and specificity of 0.94 and 0.98 for\nbenign vs malignant detection as well as of 0.91 and 0.75 for Gleason 3 vs\nGleason 4 & 5 classification, respectively. Our tool contributes to the wider\nadoption of AI-based Gleason grading within the research community and paves\nthe way for broader clinical application of deep learning models in digital\npathology. DeepGleason is open-source and publicly available for research\napplication in the following Git repository:\nhttps://github.com/frankkramer-lab/DeepGleason.\n","authors":["Dominik Müller","Philip Meyer","Lukas Rentschler","Robin Manz","Jonas Bäcker","Samantha Cramer","Christoph Wengenmayr","Bruno Märkl","Ralf Huss","Iñaki Soto-Rey","Johannes Raffler"],"pdf_url":"https://arxiv.org/pdf/2403.16678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16677v1","updated":"2024-03-25T12:14:48Z","published":"2024-03-25T12:14:48Z","title":"FOOL: Addressing the Downlink Bottleneck in Satellite Computing with\n  Neural Feature Compression","summary":"  Nanosatellite constellations equipped with sensors capturing large geographic\nregions provide unprecedented opportunities for Earth observation. As\nconstellation sizes increase, network contention poses a downlink bottleneck.\nOrbital Edge Computing (OEC) leverages limited onboard compute resources to\nreduce transfer costs by processing the raw captures at the source. However,\ncurrent solutions have limited practicability due to reliance on crude\nfiltering methods or over-prioritizing particular downstream tasks.\n  This work presents FOOL, an OEC-native and task-agnostic feature compression\nmethod that preserves prediction performance. FOOL partitions high-resolution\nsatellite imagery to maximize throughput. Further, it embeds context and\nleverages inter-tile dependencies to lower transfer costs with negligible\noverhead. While FOOL is a feature compressor, it can recover images with\ncompetitive scores on perceptual quality measures at lower bitrates. We\nextensively evaluate transfer cost reduction by including the peculiarity of\nintermittently available network connections in low earth orbit. Lastly, we\ntest the feasibility of our system for standardized nanosatellite form factors.\nWe demonstrate that FOOL permits downlinking over 100x the data volume without\nrelying on prior information on the downstream tasks.\n","authors":["Alireza Furutanpey","Qiyang Zhang","Philipp Raith","Tobias Pfandzelter","Shangguang Wang","Schahram Dustdar"],"pdf_url":"https://arxiv.org/pdf/2403.16677v1.pdf","comment":"18 pages, double column, 19 figures, 7 tables, Initial Submission to\n  IEEE Transactions on Mobile Computing"},{"id":"http://arxiv.org/abs/2403.16669v1","updated":"2024-03-25T12:07:24Z","published":"2024-03-25T12:07:24Z","title":"Domain Adaptive Detection of MAVs: A Benchmark and Noise Suppression\n  Network","summary":"  Visual detection of Micro Air Vehicles (MAVs) has attracted increasing\nattention in recent years due to its important application in various tasks.\nThe existing methods for MAV detection assume that the training set and testing\nset have the same distribution. As a result, when deployed in new domains, the\ndetectors would have a significant performance degradation due to domain\ndiscrepancy. In this paper, we study the problem of cross-domain MAV detection.\nThe contributions of this paper are threefold. 1) We propose a\nMulti-MAV-Multi-Domain (M3D) dataset consisting of both simulation and\nrealistic images. Compared to other existing datasets, the proposed one is more\ncomprehensive in the sense that it covers rich scenes, diverse MAV types, and\nvarious viewing angles. A new benchmark for cross-domain MAV detection is\nproposed based on the proposed dataset. 2) We propose a Noise Suppression\nNetwork (NSN) based on the framework of pseudo-labeling and a large-to-small\ntraining procedure. To reduce the challenging pseudo-label noises, two novel\nmodules are designed in this network. The first is a prior-based curriculum\nlearning module for allocating adaptive thresholds for pseudo labels with\ndifferent difficulties. The second is a masked copy-paste augmentation module\nfor pasting truly-labeled MAVs on unlabeled target images and thus decreasing\npseudo-label noises. 3) Extensive experimental results verify the superior\nperformance of the proposed method compared to the state-of-the-art ones. In\nparticular, it achieves mAP of 46.9%(+5.8%), 50.5%(+3.7%), and 61.5%(+11.3%) on\nthe tasks of simulation-to-real adaptation, cross-scene adaptation, and\ncross-camera adaptation, respectively.\n","authors":["Yin Zhang","Jinhong Deng","Peidong Liu","Wen Li","Shiyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.16669v1.pdf","comment":"17 pages, 11 figures. Accepted by IEEE Transactions on Automation\n  Science and Engineering"},{"id":"http://arxiv.org/abs/2311.16515v2","updated":"2024-03-25T12:01:59Z","published":"2023-11-25T14:24:49Z","title":"Word4Per: Zero-shot Composed Person Retrieval","summary":"  Searching for specific person has great social benefits and security value,\nand it often involves a combination of visual and textual information.\nConventional person retrieval methods, whether image-based or text-based,\nusually fall short in effectively harnessing both types of information, leading\nto the loss of accuracy. In this paper, a whole new task called Composed Person\nRetrieval (CPR) is proposed to jointly utilize both image and text information\nfor target person retrieval. However, the supervised CPR requires very costly\nmanual annotation dataset, while there are currently no available resources. To\nmitigate this issue, we firstly introduce the Zero-shot Composed Person\nRetrieval (ZS-CPR), which leverages existing domain-related data to resolve the\nCPR problem without expensive annotations. Secondly, to learn ZS-CPR model, we\npropose a two-stage learning framework, Word4Per, where a lightweight Textual\nInversion Network (TINet) and a text-based person retrieval model based on\nfine-tuned Contrastive Language-Image Pre-training (CLIP) network are learned\nwithout utilizing any CPR data. Thirdly, a finely annotated Image-Text Composed\nPerson Retrieval (ITCPR) dataset is built as the benchmark to assess the\nperformance of the proposed Word4Per framework. Extensive experiments under\nboth Rank-1 and mAP demonstrate the effectiveness of Word4Per for the ZS-CPR\ntask, surpassing the comparative methods by over 10\\%. The code and ITCPR\ndataset will be publicly available at\nhttps://github.com/Delong-liu-bupt/Word4Per.\n","authors":["Delong Liu","Haiwen Li","Zhicheng Zhao","Fei Su","Yuan Dong"],"pdf_url":"https://arxiv.org/pdf/2311.16515v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05305v2","updated":"2024-03-25T11:48:27Z","published":"2024-02-07T22:50:47Z","title":"Knowledge Distillation for Road Detection based on cross-model\n  Semi-Supervised Learning","summary":"  The advancement of knowledge distillation has played a crucial role in\nenabling the transfer of knowledge from larger teacher models to smaller and\nmore efficient student models, and is particularly beneficial for online and\nresource-constrained applications. The effectiveness of the student model\nheavily relies on the quality of the distilled knowledge received from the\nteacher. Given the accessibility of unlabelled remote sensing data,\nsemi-supervised learning has become a prevalent strategy for enhancing model\nperformance. However, relying solely on semi-supervised learning with smaller\nmodels may be insufficient due to their limited capacity for feature\nextraction. This limitation restricts their ability to exploit training data.\nTo address this issue, we propose an integrated approach that combines\nknowledge distillation and semi-supervised learning methods. This hybrid\napproach leverages the robust capabilities of large models to effectively\nutilise large unlabelled data whilst subsequently providing the small student\nmodel with rich and informative features for enhancement. The proposed\nsemi-supervised learning-based knowledge distillation (SSLKD) approach\ndemonstrates a notable improvement in the performance of the student model, in\nthe application of road segmentation, surpassing the effectiveness of\ntraditional semi-supervised learning methods.\n","authors":["Wanli Ma","Oktay Karakus","Paul L. Rosin"],"pdf_url":"https://arxiv.org/pdf/2402.05305v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06744v2","updated":"2024-03-25T11:35:55Z","published":"2023-10-10T16:14:20Z","title":"HiFi-123: Towards High-fidelity One Image to 3D Content Generation","summary":"  Recent advances in diffusion models have enabled 3D generation from a single\nimage. However, current methods often produce suboptimal results for novel\nviews, with blurred textures and deviations from the reference image, limiting\ntheir practical applications. In this paper, we introduce HiFi-123, a method\ndesigned for high-fidelity and multi-view consistent 3D generation. Our\ncontributions are twofold: First, we propose a Reference-Guided Novel View\nEnhancement (RGNV) technique that significantly improves the fidelity of\ndiffusion-based zero-shot novel view synthesis methods. Second, capitalizing on\nthe RGNV, we present a novel Reference-Guided State Distillation (RGSD) loss.\nWhen incorporated into the optimization-based image-to-3D pipeline, our method\nsignificantly improves 3D generation quality, achieving state-of-the-art\nperformance. Comprehensive evaluations demonstrate the effectiveness of our\napproach over existing methods, both qualitatively and quantitatively. Video\nresults are available on the project page.\n","authors":["Wangbo Yu","Li Yuan","Yan-Pei Cao","Xiangjun Gao","Xiaoyu Li","Wenbo Hu","Long Quan","Ying Shan","Yonghong Tian"],"pdf_url":"https://arxiv.org/pdf/2310.06744v2.pdf","comment":"Project Page: https://drexubery.github.io/HiFi-123/"},{"id":"http://arxiv.org/abs/2403.16646v1","updated":"2024-03-25T11:32:05Z","published":"2024-03-25T11:32:05Z","title":"Clustering Propagation for Universal Medical Image Segmentation","summary":"  Prominent solutions for medical image segmentation are typically tailored for\nautomatic or interactive setups, posing challenges in facilitating progress\nachieved in one task to another.$_{\\!}$ This$_{\\!}$ also$_{\\!}$\nnecessitates$_{\\!}$ separate$_{\\!}$ models for each task, duplicating both\ntraining time and parameters.$_{\\!}$ To$_{\\!}$ address$_{\\!}$ above$_{\\!}$\nissues,$_{\\!}$ we$_{\\!}$ introduce$_{\\!}$ S2VNet,$_{\\!}$ a$_{\\!}$\nuniversal$_{\\!}$ framework$_{\\!}$ that$_{\\!}$ leverages$_{\\!}$\nSlice-to-Volume$_{\\!}$ propagation$_{\\!}$ to$_{\\!}$ unify automatic/interactive\nsegmentation within a single model and one training session. Inspired by\nclustering-based segmentation techniques, S2VNet makes full use of the\nslice-wise structure of volumetric data by initializing cluster centers from\nthe cluster$_{\\!}$ results$_{\\!}$ of$_{\\!}$ previous$_{\\!}$ slice.$_{\\!}$ This\nenables knowledge acquired from prior slices to assist in the segmentation of\nthe current slice, further efficiently bridging the communication between\nremote slices using mere 2D networks. Moreover, such a framework readily\naccommodates interactive segmentation with no architectural change, simply by\ninitializing centroids from user inputs. S2VNet distinguishes itself by swift\ninference speeds and reduced memory consumption compared to prevailing 3D\nsolutions. It can also handle multi-class interactions with each of them\nserving to initialize different centroids. Experiments on three benchmarks\ndemonstrate S2VNet surpasses task-specified solutions on both\nautomatic/interactive setups.\n","authors":["Yuhang Ding","Liulei Li","Wenguan Wang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2403.16646v1.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.16643v1","updated":"2024-03-25T11:29:19Z","published":"2024-03-25T11:29:19Z","title":"Self-Adaptive Reality-Guided Diffusion for Artifact-Free\n  Super-Resolution","summary":"  Artifact-free super-resolution (SR) aims to translate low-resolution images\ninto their high-resolution counterparts with a strict integrity of the original\ncontent, eliminating any distortions or synthetic details. While traditional\ndiffusion-based SR techniques have demonstrated remarkable abilities to enhance\nimage detail, they are prone to artifact introduction during iterative\nprocedures. Such artifacts, ranging from trivial noise to unauthentic textures,\ndeviate from the true structure of the source image, thus challenging the\nintegrity of the super-resolution process. In this work, we propose\nSelf-Adaptive Reality-Guided Diffusion (SARGD), a training-free method that\ndelves into the latent space to effectively identify and mitigate the\npropagation of artifacts. Our SARGD begins by using an artifact detector to\nidentify implausible pixels, creating a binary mask that highlights artifacts.\nFollowing this, the Reality Guidance Refinement (RGR) process refines artifacts\nby integrating this mask with realistic latent representations, improving\nalignment with the original image. Nonetheless, initial realistic-latent\nrepresentations from lower-quality images result in over-smoothing in the final\noutput. To address this, we introduce a Self-Adaptive Guidance (SAG) mechanism.\nIt dynamically computes a reality score, enhancing the sharpness of the\nrealistic latent. These alternating mechanisms collectively achieve\nartifact-free super-resolution. Extensive experiments demonstrate the\nsuperiority of our method, delivering detailed artifact-free high-resolution\nimages while reducing sampling steps by 2X. We release our code at\nhttps://github.com/ProAirVerse/Self-Adaptive-Guidance-Diffusion.git.\n","authors":["Qingping Zheng","Ling Zheng","Yuanfan Guo","Ying Li","Songcen Xu","Jiankang Deng","Hang Xu"],"pdf_url":"https://arxiv.org/pdf/2403.16643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16640v1","updated":"2024-03-25T11:28:52Z","published":"2024-03-25T11:28:52Z","title":"Multi-Scale Texture Loss for CT denoising with GANs","summary":"  Generative Adversarial Networks (GANs) have proved as a powerful framework\nfor denoising applications in medical imaging. However, GAN-based denoising\nalgorithms still suffer from limitations in capturing complex relationships\nwithin the images. In this regard, the loss function plays a crucial role in\nguiding the image generation process, encompassing how much a synthetic image\ndiffers from a real image. To grasp highly complex and non-linear textural\nrelationships in the training process, this work presents a loss function that\nleverages the intrinsic multi-scale nature of the Gray-Level-Co-occurrence\nMatrix (GLCM). Although the recent advances in deep learning have demonstrated\nsuperior performance in classification and detection tasks, we hypothesize that\nits information content can be valuable when integrated into GANs' training. To\nthis end, we propose a differentiable implementation of the GLCM suited for\ngradient-based optimization. Our approach also introduces a self-attention\nlayer that dynamically aggregates the multi-scale texture information extracted\nfrom the images. We validate our approach by carrying out extensive experiments\nin the context of low-dose CT denoising, a challenging application that aims to\nenhance the quality of noisy CT scans. We utilize three publicly available\ndatasets, including one simulated and two real datasets. The results are\npromising as compared to other well-established loss functions, being also\nconsistent across three different GAN architectures. The code is available at:\nhttps://github.com/FrancescoDiFeola/DenoTextureLoss\n","authors":["Francesco Di Feola","Lorenzo Tronchin","Valerio Guarrasi","Paolo Soda"],"pdf_url":"https://arxiv.org/pdf/2403.16640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16638v1","updated":"2024-03-25T11:26:18Z","published":"2024-03-25T11:26:18Z","title":"AI-Generated Video Detection via Spatio-Temporal Anomaly Learning","summary":"  The advancement of generation models has led to the emergence of highly\nrealistic artificial intelligence (AI)-generated videos. Malicious users can\neasily create non-existent videos to spread false information. This letter\nproposes an effective AI-generated video detection (AIGVDet) scheme by\ncapturing the forensic traces with a two-branch spatio-temporal convolutional\nneural network (CNN). Specifically, two ResNet sub-detectors are learned\nseparately for identifying the anomalies in spatical and optical flow domains,\nrespectively. Results of such sub-detectors are fused to further enhance the\ndiscrimination ability. A large-scale generated video dataset (GVD) is\nconstructed as a benchmark for model training and evaluation. Extensive\nexperimental results verify the high generalization and robustness of our\nAIGVDet scheme. Code and dataset will be available at\nhttps://github.com/multimediaFor/AIGVDet.\n","authors":["Jianfa Bai","Man Lin","Gang Cao"],"pdf_url":"https://arxiv.org/pdf/2403.16638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16476v4","updated":"2024-03-25T11:24:45Z","published":"2023-12-27T08:50:01Z","title":"SVGDreamer: Text Guided SVG Generation with Diffusion Model","summary":"  Recently, text-guided scalable vector graphics (SVGs) synthesis has shown\npromise in domains such as iconography and sketch. However, existing\ntext-to-SVG generation methods lack editability and struggle with visual\nquality and result diversity. To address these limitations, we propose a novel\ntext-guided vector graphics synthesis method called SVGDreamer. SVGDreamer\nincorporates a semantic-driven image vectorization (SIVE) process that enables\nthe decomposition of synthesis into foreground objects and background, thereby\nenhancing editability. Specifically, the SIVE process introduce attention-based\nprimitive control and an attention-mask loss function for effective control and\nmanipulation of individual elements. Additionally, we propose a Vectorized\nParticle-based Score Distillation (VPSD) approach to tackle the challenges of\nshape over-smoothing, color over-saturation, limited diversity in results, and\nslow convergence in existing text-to-SVG generation methods. VPSD models SVGs\nas distributions of control points and colors to counteract over-smoothing and\nover-saturation. Furthermore, VPSD leverages a reward model to reweight vector\nparticles, which improves aesthetic appeal and accelerates convergence.\nExtensive experiments have been conducted to validate the effectiveness of\nSVGDreamer, demonstrating its superiority over baseline methods in terms of\neditability, visual quality, and diversity. The code and demo of SVGDreamer can\nbe found at https://ximinng.github.io/SVGDreamer-project/\n","authors":["Ximing Xing","Haitao Zhou","Chuang Wang","Jing Zhang","Dong Xu","Qian Yu"],"pdf_url":"https://arxiv.org/pdf/2312.16476v4.pdf","comment":"Accepted by CVPR 2024. project link:\n  https://ximinng.github.io/SVGDreamer-project/"},{"id":"http://arxiv.org/abs/2403.16635v1","updated":"2024-03-25T11:24:02Z","published":"2024-03-25T11:24:02Z","title":"V2X-PC: Vehicle-to-everything Collaborative Perception via Point Cluster","summary":"  The objective of the collaborative vehicle-to-everything perception task is\nto enhance the individual vehicle's perception capability through message\ncommunication among neighboring traffic agents. Previous methods focus on\nachieving optimal performance within bandwidth limitations and typically adopt\nBEV maps as the basic collaborative message units. However, we demonstrate that\ncollaboration with dense representations is plagued by object feature\ndestruction during message packing, inefficient message aggregation for\nlong-range collaboration, and implicit structure representation communication.\nTo tackle these issues, we introduce a brand new message unit, namely point\ncluster, designed to represent the scene sparsely with a combination of\nlow-level structure information and high-level semantic information. The point\ncluster inherently preserves object information while packing messages, with\nweak relevance to the collaboration range, and supports explicit structure\nmodeling. Building upon this representation, we propose a novel framework\nV2X-PC for collaborative perception. This framework includes a Point Cluster\nPacking (PCP) module to keep object feature and manage bandwidth through the\nmanipulation of cluster point numbers. As for effective message aggregation, we\npropose a Point Cluster Aggregation (PCA) module to match and merge point\nclusters associated with the same object. To further handle time latency and\npose errors encountered in real-world scenarios, we propose parameter-free\nsolutions that can adapt to different noisy levels without finetuning.\nExperiments on two widely recognized collaborative perception benchmarks\nshowcase the superior performance of our method compared to the previous\nstate-of-the-art approaches relying on BEV maps.\n","authors":["Si Liu","Zihan Ding","Jiahui Fu","Hongyu Li","Siheng Chen","Shifeng Zhang","Xu Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.16635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16627v1","updated":"2024-03-25T11:16:23Z","published":"2024-03-25T11:16:23Z","title":"SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions","summary":"  Recent advancements in diffusion models have positioned them at the forefront\nof image generation. Despite their superior performance, diffusion models are\nnot without drawbacks; they are characterized by complex architectures and\nsubstantial computational demands, resulting in significant latency due to\ntheir iterative sampling process. To mitigate these limitations, we introduce a\ndual approach involving model miniaturization and a reduction in sampling\nsteps, aimed at significantly decreasing model latency. Our methodology\nleverages knowledge distillation to streamline the U-Net and image decoder\narchitectures, and introduces an innovative one-step DM training technique that\nutilizes feature matching and score distillation. We present two models,\nSDXS-512 and SDXS-1024, achieving inference speeds of approximately 100 FPS\n(30x faster than SD v1.5) and 30 FP (60x faster than SDXL) on a single GPU,\nrespectively. Moreover, our training approach offers promising applications in\nimage-conditioned control, facilitating efficient image-to-image translation.\n","authors":["Yuda Song","Zehao Sun","Xuanwu Yin"],"pdf_url":"https://arxiv.org/pdf/2403.16627v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17744v2","updated":"2024-03-25T11:04:17Z","published":"2023-11-29T15:49:31Z","title":"Variational Bayes image restoration with compressive autoencoders","summary":"  Regularization of inverse problems is of paramount importance in\ncomputational imaging. The ability of neural networks to learn efficient image\nrepresentations has been recently exploited to design powerful data-driven\nregularizers. While state-of-the-art plug-and-play methods rely on an implicit\nregularization provided by neural denoisers, alternative Bayesian approaches\nconsider Maximum A Posteriori (MAP) estimation in the latent space of a\ngenerative model, thus with an explicit regularization. However,\nstate-of-the-art deep generative models require a huge amount of training data\ncompared to denoisers. Besides, their complexity hampers the optimization\ninvolved in latent MAP derivation. In this work, we first propose to use\ncompressive autoencoders instead. These networks, which can be seen as\nvariational autoencoders with a flexible latent prior, are smaller and easier\nto train than state-of-the-art generative models. As a second contribution, we\nintroduce the Variational Bayes Latent Estimation (VBLE) algorithm, which\nperforms latent estimation within the framework of variational inference.\nThanks to a simple yet efficient parameterization of the variational posterior,\nVBLE allows for fast and easy (approximate) posterior sampling. Experimental\nresults on image datasets BSD and FFHQ demonstrate that VBLE reaches similar\nperformance than state-of-the-art plug-and-play methods, while being able to\nquantify uncertainties faster than other existing posterior sampling\ntechniques.\n","authors":["Maud Biquard","Marie Chabert","Thomas Oberlin"],"pdf_url":"https://arxiv.org/pdf/2311.17744v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12198v2","updated":"2024-03-25T11:04:04Z","published":"2023-12-19T14:34:36Z","title":"Mask Grounding for Referring Image Segmentation","summary":"  Referring Image Segmentation (RIS) is a challenging task that requires an\nalgorithm to segment objects referred by free-form language expressions.\nDespite significant progress in recent years, most state-of-the-art (SOTA)\nmethods still suffer from considerable language-image modality gap at the pixel\nand word level. These methods generally 1) rely on sentence-level language\nfeatures for language-image alignment and 2) lack explicit training supervision\nfor fine-grained visual grounding. Consequently, they exhibit weak object-level\ncorrespondence between visual and language features. Without well-grounded\nfeatures, prior methods struggle to understand complex expressions that require\nstrong reasoning over relationships among multiple objects, especially when\ndealing with rarely used or ambiguous clauses. To tackle this challenge, we\nintroduce a novel Mask Grounding auxiliary task that significantly improves\nvisual grounding within language features, by explicitly teaching the model to\nlearn fine-grained correspondence between masked textual tokens and their\nmatching visual objects. Mask Grounding can be directly used on prior RIS\nmethods and consistently bring improvements. Furthermore, to holistically\naddress the modality gap, we also design a cross-modal alignment loss and an\naccompanying alignment module. These additions work synergistically with Mask\nGrounding. With all these techniques, our comprehensive approach culminates in\nMagNet (Mask-grounded Network), an architecture that significantly outperforms\nprior arts on three key benchmarks (RefCOCO, RefCOCO+ and G-Ref), demonstrating\nour method's effectiveness in addressing current limitations of RIS algorithms.\nOur code and pre-trained weights will be released.\n","authors":["Yong Xien Chng","Henry Zheng","Yizeng Han","Xuchong Qiu","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2312.12198v2.pdf","comment":"Accepted by CVPR2024; Project page:\n  https://yxchng.github.io/projects/mask-grounding"},{"id":"http://arxiv.org/abs/2403.16612v1","updated":"2024-03-25T10:42:48Z","published":"2024-03-25T10:42:48Z","title":"Calibrating Bayesian UNet++ for Sub-Seasonal Forecasting","summary":"  Seasonal forecasting is a crucial task when it comes to detecting the extreme\nheat and colds that occur due to climate change. Confidence in the predictions\nshould be reliable since a small increase in the temperatures in a year has a\nbig impact on the world. Calibration of the neural networks provides a way to\nensure our confidence in the predictions. However, calibrating regression\nmodels is an under-researched topic, especially in forecasters. We calibrate a\nUNet++ based architecture, which was shown to outperform physics-based models\nin temperature anomalies. We show that with a slight trade-off between\nprediction error and calibration error, it is possible to get more reliable and\nsharper forecasts. We believe that calibration should be an important part of\nsafety-critical machine learning applications such as weather forecasters.\n","authors":["Busra Asan","Abdullah Akgul","Alper Unal","Melih Kandemir","Gozde Unal"],"pdf_url":"https://arxiv.org/pdf/2403.16612v1.pdf","comment":"Accepted as a workshop paper at \"ICLR 2024 Tackling Climate Change\n  with Machine Learning\""},{"id":"http://arxiv.org/abs/2403.16607v1","updated":"2024-03-25T10:38:17Z","published":"2024-03-25T10:38:17Z","title":"Enhancing Industrial Transfer Learning with Style Filter: Cost Reduction\n  and Defect-Focus","summary":"  Addressing the challenge of data scarcity in industrial domains, transfer\nlearning emerges as a pivotal paradigm. This work introduces Style Filter, a\ntailored methodology for industrial contexts. By selectively filtering source\ndomain data before knowledge transfer, Style Filter reduces the quantity of\ndata while maintaining or even enhancing the performance of transfer learning\nstrategy. Offering label-free operation, minimal reliance on prior knowledge,\nindependence from specific models, and re-utilization, Style Filter is\nevaluated on authentic industrial datasets, highlighting its effectiveness when\nemployed before conventional transfer strategies in the deep learning domain.\nThe results underscore the effectiveness of Style Filter in real-world\nindustrial applications.\n","authors":["Chen Li","Ruijie Ma","Xiang Qian","Xiaohao Wang","Xinghui Li"],"pdf_url":"https://arxiv.org/pdf/2403.16607v1.pdf","comment":"17 pages, 11 figures,4 tables"},{"id":"http://arxiv.org/abs/2403.16605v1","updated":"2024-03-25T10:30:22Z","published":"2024-03-25T10:30:22Z","title":"SatSynth: Augmenting Image-Mask Pairs through Diffusion Models for\n  Aerial Semantic Segmentation","summary":"  In recent years, semantic segmentation has become a pivotal tool in\nprocessing and interpreting satellite imagery. Yet, a prevalent limitation of\nsupervised learning techniques remains the need for extensive manual\nannotations by experts. In this work, we explore the potential of generative\nimage diffusion to address the scarcity of annotated data in earth observation\ntasks. The main idea is to learn the joint data manifold of images and labels,\nleveraging recent advancements in denoising diffusion probabilistic models. To\nthe best of our knowledge, we are the first to generate both images and\ncorresponding masks for satellite segmentation. We find that the obtained pairs\nnot only display high quality in fine-scale features but also ensure a wide\nsampling diversity. Both aspects are crucial for earth observation data, where\nsemantic classes can vary severely in scale and occurrence frequency. We employ\nthe novel data instances for downstream segmentation, as a form of data\naugmentation. In our experiments, we provide comparisons to prior works based\non discriminative diffusion models or GANs. We demonstrate that integrating\ngenerated samples yields significant quantitative improvements for satellite\nsemantic segmentation -- both compared to baselines and when training only on\nthe original data.\n","authors":["Aysim Toker","Marvin Eisenberger","Daniel Cremers","Laura Leal-Taixé"],"pdf_url":"https://arxiv.org/pdf/2403.16605v1.pdf","comment":"Accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2403.16594v1","updated":"2024-03-25T10:13:52Z","published":"2024-03-25T10:13:52Z","title":"EDUE: Expert Disagreement-Guided One-Pass Uncertainty Estimation for\n  Medical Image Segmentation","summary":"  Deploying deep learning (DL) models in medical applications relies on\npredictive performance and other critical factors, such as conveying\ntrustworthy predictive uncertainty. Uncertainty estimation (UE) methods provide\npotential solutions for evaluating prediction reliability and improving the\nmodel confidence calibration. Despite increasing interest in UE, challenges\npersist, such as the need for explicit methods to capture aleatoric uncertainty\nand align uncertainty estimates with real-life disagreements among domain\nexperts. This paper proposes an Expert Disagreement-Guided Uncertainty\nEstimation (EDUE) for medical image segmentation. By leveraging variability in\nground-truth annotations from multiple raters, we guide the model during\ntraining and incorporate random sampling-based strategies to enhance\ncalibration confidence. Our method achieves 55% and 23% improvement in\ncorrelation on average with expert disagreements at the image and pixel levels,\nrespectively, better calibration, and competitive segmentation performance\ncompared to the state-of-the-art deep ensembles, requiring only a single\nforward pass.\n","authors":["Kudaibergen Abutalip","Numan Saeed","Ikboljon Sobirov","Vincent Andrearczyk","Adrien Depeursinge","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2403.16594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14828v2","updated":"2024-03-25T10:12:46Z","published":"2024-03-21T20:43:10Z","title":"Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing","summary":"  Fashion illustration is a crucial medium for designers to convey their\ncreative vision and transform design concepts into tangible representations\nthat showcase the interplay between clothing and the human body. In the context\nof fashion design, computer vision techniques have the potential to enhance and\nstreamline the design process. Departing from prior research primarily focused\non virtual try-on, this paper tackles the task of multimodal-conditioned\nfashion image editing. Our approach aims to generate human-centric fashion\nimages guided by multimodal prompts, including text, human body poses, garment\nsketches, and fabric textures. To address this problem, we propose extending\nlatent diffusion models to incorporate these multiple modalities and modifying\nthe structure of the denoising network, taking multimodal prompts as input. To\ncondition the proposed architecture on fabric textures, we employ textual\ninversion techniques and let diverse cross-attention layers of the denoising\nnetwork attend to textual and texture information, thus incorporating different\ngranularity conditioning details. Given the lack of datasets for the task, we\nextend two existing fashion datasets, Dress Code and VITON-HD, with multimodal\nannotations. Experimental evaluations demonstrate the effectiveness of our\nproposed approach in terms of realism and coherence concerning the provided\nmultimodal inputs.\n","authors":["Alberto Baldrati","Davide Morelli","Marcella Cornia","Marco Bertini","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2403.14828v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16582v1","updated":"2024-03-25T09:49:42Z","published":"2024-03-25T09:49:42Z","title":"In the Search for Optimal Multi-view Learning Models for Crop\n  Classification with Global Remote Sensing Data","summary":"  Crop classification is of critical importance due to its role in studying\ncrop pattern changes, resource management, and carbon sequestration. When\nemploying data-driven techniques for its prediction, utilizing various temporal\ndata sources is necessary. Deep learning models have proven to be effective for\nthis task by mapping time series data to high-level representation for\nprediction. However, they face substantial challenges when dealing with\nmultiple input patterns. The literature offers limited guidance for Multi-View\nLearning (MVL) scenarios, as it has primarily focused on exploring fusion\nstrategies with specific encoders and validating them in local regions. In\ncontrast, we investigate the impact of simultaneous selection of the fusion\nstrategy and the encoder architecture evaluated on a global-scale cropland and\ncrop-type classifications. We use a range of five fusion strategies (Input,\nFeature, Decision, Ensemble, Hybrid) and five temporal encoder architectures\n(LSTM, GRU, TempCNN, TAE, L-TAE) as possible MVL model configurations. The\nvalidation is on the CropHarvest dataset that provides optical, radar, and\nweather time series, and topographic information as input data. We found that\nin scenarios with a limited number of labeled samples, a unique configuration\nis insufficient for all the cases. Instead, a specialized combination,\nincluding encoder and fusion strategy, should be meticulously sought. To\nstreamline this search process, we suggest initially identifying the optimal\nencoder architecture tailored for a particular fusion strategy, and then\ndetermining the most suitable fusion strategy for the classification task. We\nprovide a technical framework for researchers exploring crop classification or\nrelated tasks through a MVL approach.\n","authors":["Francisco Mena","Diego Arenas","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2403.16582v1.pdf","comment":"submitted to journal"},{"id":"http://arxiv.org/abs/2403.16578v1","updated":"2024-03-25T09:43:56Z","published":"2024-03-25T09:43:56Z","title":"SegICL: A Universal In-context Learning Framework for Enhanced\n  Segmentation in Medical Imaging","summary":"  Medical image segmentation models adapting to new tasks in a training-free\nmanner through in-context learning is an exciting advancement. Universal\nsegmentation models aim to generalize across the diverse modality of medical\nimages, yet their effectiveness often diminishes when applied to\nout-of-distribution (OOD) data modalities and tasks, requiring intricate\nfine-tuning of model for optimal performance. For addressing this challenge, we\nintroduce SegICL, a novel approach leveraging In-Context Learning (ICL) for\nimage segmentation. Unlike existing methods, SegICL has the capability to\nemploy text-guided segmentation and conduct in-context learning with a small\nset of image-mask pairs, eliminating the need for training the model from\nscratch or fine-tuning for OOD tasks (including OOD modality and dataset).\nExtensive experimental validation of SegICL demonstrates a positive correlation\nbetween the number of prompt samples and segmentation performance on OOD\nmodalities and tasks. This indicates that SegICL effectively address new\nsegmentation tasks based on contextual information. Additionally, SegICL also\nexhibits comparable segmentation performance to mainstream models on OOD and\nin-distribution tasks. Our code will be released soon.\n","authors":["Lingdong Shen","Fangxin Shang","Yehui Yang","Xiaoshuang Huang","Shining Xiang"],"pdf_url":"https://arxiv.org/pdf/2403.16578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10615v2","updated":"2024-03-25T09:42:13Z","published":"2024-03-15T18:26:33Z","title":"LightIt: Illumination Modeling and Control for Diffusion Models","summary":"  We introduce LightIt, a method for explicit illumination control for image\ngeneration. Recent generative methods lack lighting control, which is crucial\nto numerous artistic aspects of image generation such as setting the overall\nmood or cinematic appearance. To overcome these limitations, we propose to\ncondition the generation on shading and normal maps. We model the lighting with\nsingle bounce shading, which includes cast shadows. We first train a shading\nestimation module to generate a dataset of real-world images and shading pairs.\nThen, we train a control network using the estimated shading and normals as\ninput. Our method demonstrates high-quality image generation and lighting\ncontrol in numerous scenes. Additionally, we use our generated dataset to train\nan identity-preserving relighting model, conditioned on an image and a target\nshading. Our method is the first that enables the generation of images with\ncontrollable, consistent lighting and performs on par with specialized\nrelighting state-of-the-art methods.\n","authors":["Peter Kocsis","Julien Philip","Kalyan Sunkavalli","Matthias Nießner","Yannick Hold-Geoffroy"],"pdf_url":"https://arxiv.org/pdf/2403.10615v2.pdf","comment":"Project page: https://peter-kocsis.github.io/LightIt/ Video:\n  https://youtu.be/cCfSBD5aPLI"},{"id":"http://arxiv.org/abs/2403.15353v2","updated":"2024-03-25T09:36:42Z","published":"2024-03-22T17:08:03Z","title":"Fully automated workflow for the design of patient-specific orthopaedic\n  implants: application to total knee arthroplasty","summary":"  Arthroplasty is commonly performed to treat joint osteoarthritis, reducing\npain and improving mobility. While arthroplasty has known several technical\nimprovements, a significant share of patients are still unsatisfied with their\nsurgery. Personalised arthroplasty improves surgical outcomes however current\nsolutions require delays, making it difficult to integrate in clinical routine.\nWe propose a fully automated workflow to design patient-specific implants,\npresented for total knee arthroplasty, the most widely performed arthroplasty\nin the world nowadays.\n  The proposed pipeline first uses artificial neural networks to segment the\nproximal and distal extremities of the femur and tibia. Then the full bones are\nreconstructed using augmented statistical shape models, combining shape and\nlandmarks information. Finally, 77 morphological parameters are computed to\ndesign patient-specific implants. The developed workflow has been trained using\n91 CT scans of lower limb and evaluated on 41 CT scans manually segmented, in\nterms of accuracy and execution time.\n  The workflow accuracy was $0.4\\pm0.2mm$ for the segmentation, $1.2\\pm0.4mm$\nfor the full bones reconstruction, and $2.8\\pm2.2mm$ for the anatomical\nlandmarks determination. The custom implants fitted the patients' anatomy with\n$0.6\\pm0.2mm$ accuracy. The whole process from segmentation to implants' design\nlasted about 5 minutes.\n  The proposed workflow allows for a fast and reliable personalisation of knee\nimplants, directly from the patient CT image without requiring any manual\nintervention. It establishes a patient-specific pre-operative planning for TKA\nin a very short time making it easily available for all patients. Combined with\nefficient implant manufacturing techniques, this solution could help answer the\ngrowing number of arthroplasties while reducing complications and improving the\npatients' satisfaction.\n","authors":["Aziliz Guezou-Philippe","Arnaud Clavé","Ehouarn Maguet","Ludivine Maintier","Charles Garraud","Jean-Rassaire Fouefack","Valérie Burdin","Eric Stindel","Guillaume Dardenne"],"pdf_url":"https://arxiv.org/pdf/2403.15353v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16569v1","updated":"2024-03-25T09:36:10Z","published":"2024-03-25T09:36:10Z","title":"Revealing Vulnerabilities of Neural Networks in Parameter Learning and\n  Defense Against Explanation-Aware Backdoors","summary":"  Explainable Artificial Intelligence (XAI) strategies play a crucial part in\nincreasing the understanding and trustworthiness of neural networks.\nNonetheless, these techniques could potentially generate misleading\nexplanations. Blinding attacks can drastically alter a machine learning\nalgorithm's prediction and explanation, providing misleading information by\nadding visually unnoticeable artifacts into the input, while maintaining the\nmodel's accuracy. It poses a serious challenge in ensuring the reliability of\nXAI methods. To ensure the reliability of XAI methods poses a real challenge,\nwe leverage statistical analysis to highlight the changes in CNN weights within\na CNN following blinding attacks. We introduce a method specifically designed\nto limit the effectiveness of such attacks during the evaluation phase,\navoiding the need for extra training. The method we suggest defences against\nmost modern explanation-aware adversarial attacks, achieving an approximate\ndecrease of ~99\\% in the Attack Success Rate (ASR) and a ~91\\% reduction in the\nMean Square Error (MSE) between the original explanation and the defended\n(post-attack) explanation across three unique types of attacks.\n","authors":["Md Abdul Kadir","GowthamKrishna Addluri","Daniel Sonntag"],"pdf_url":"https://arxiv.org/pdf/2403.16569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16558v1","updated":"2024-03-25T09:17:15Z","published":"2024-03-25T09:17:15Z","title":"Elysium: Exploring Object-level Perception in Videos via MLLM","summary":"  Multi-modal Large Language Models (MLLMs) have demonstrated their ability to\nperceive objects in still images, but their application in video-related tasks,\nsuch as object tracking, remains understudied. This lack of exploration is\nprimarily due to two key challenges. Firstly, extensive pretraining on\nlarge-scale video datasets is required to equip MLLMs with the capability to\nperceive objects across multiple frames and understand inter-frame\nrelationships. Secondly, processing a large number of frames within the context\nwindow of Large Language Models (LLMs) can impose a significant computational\nburden. To address the first challenge, we introduce ElysiumTrack-1M, a\nlarge-scale video dataset paired with novel tasks: Referring Single Object\nTracking (RSOT) and Video Referring Expression Generation (Video-REG).\nElysiumTrack-1M contains 1.27 million annotated video frames with corresponding\nobject boxes and descriptions. Leveraging this dataset, we conduct training of\nMLLMs and propose a token-compression model T-Selector to tackle the second\nchallenge. Our proposed approach, Elysium: Exploring Object-level Perception in\nVideos via MLLM, is an end-to-end trainable MLLM that makes the first attempt\nto conduct object-level tasks in videos without requiring any additional\nplug-in or expert models.\n","authors":["Han Wang","Yanjie Wang","Yongjie Ye","Yuxiang Nie","Can Huang"],"pdf_url":"https://arxiv.org/pdf/2403.16558v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16552v1","updated":"2024-03-25T08:57:27Z","published":"2024-03-25T08:57:27Z","title":"QKFormer: Hierarchical Spiking Transformer using Q-K Attention","summary":"  Spiking Transformers, which integrate Spiking Neural Networks (SNNs) with\nTransformer architectures, have attracted significant attention due to their\npotential for energy efficiency and high performance. However, existing models\nin this domain still suffer from suboptimal performance. We introduce several\ninnovations to improve the performance: i) We propose a novel spike-form Q-K\nattention mechanism, tailored for SNNs, which efficiently models the importance\nof token or channel dimensions through binary vectors with linear complexity.\nii) We incorporate the hierarchical structure, which significantly benefits the\nperformance of both the brain and artificial neural networks, into spiking\ntransformers to obtain multi-scale spiking representation. iii) We design a\nversatile and powerful patch embedding module with a deformed shortcut\nspecifically for spiking transformers. Together, we develop QKFormer, a\nhierarchical spiking transformer based on Q-K attention with direct training.\nQKFormer shows significantly superior performance over existing\nstate-of-the-art SNN models on various mainstream datasets. Notably, with\ncomparable size to Spikformer (66.34 M, 74.81%), QKFormer (64.96 M) achieves a\ngroundbreaking top-1 accuracy of 85.65% on ImageNet-1k, substantially\noutperforming Spikformer by 10.84%. To our best knowledge, this is the first\ntime that directly training SNNs have exceeded 85% accuracy on ImageNet-1K. The\ncode and models are publicly available at\nhttps://github.com/zhouchenlin2096/QKFormer\n","authors":["Chenlin Zhou","Han Zhang","Zhaokun Zhou","Liutao Yu","Liwei Huang","Xiaopeng Fan","Li Yuan","Zhengyu Ma","Huihui Zhou","Yonghong Tian"],"pdf_url":"https://arxiv.org/pdf/2403.16552v1.pdf","comment":"10 pages, code: https://github.com/zhouchenlin2096/QKFormer"},{"id":"http://arxiv.org/abs/2403.11854v2","updated":"2024-03-25T08:54:33Z","published":"2024-03-18T15:03:56Z","title":"denoiSplit: a method for joint image splitting and unsupervised\n  denoising","summary":"  In this work we present denoiSplit, a method to tackle a new analysis task,\ni.e. the challenge of joint semantic image splitting and unsupervised\ndenoising. This dual approach has important applications in fluorescence\nmicroscopy, where semantic image splitting has important applications but noise\ndoes generally hinder the downstream analysis of image content. Image splitting\ninvolves dissecting an image into its distinguishable semantic structures. We\nshow that the current state-of-the-art method for this task struggles in the\npresence of image noise, inadvertently also distributing the noise across the\npredicted outputs. The method we present here can deal with image noise by\nintegrating an unsupervised denoising sub-task. This integration results in\nimproved semantic image unmixing, even in the presence of notable and realistic\nlevels of imaging noise. A key innovation in denoiSplit is the use of\nspecifically formulated noise models and the suitable adjustment of\nKL-divergence loss for the high-dimensional hierarchical latent space we are\ntraining. We showcase the performance of denoiSplit across 4 tasks on\nreal-world microscopy images. Additionally, we perform qualitative and\nquantitative evaluations and compare results to existing benchmarks,\ndemonstrating the effectiveness of using denoiSplit: a single Variational\nSplitting Encoder-Decoder (VSE) Network using two suitable noise models to\njointly perform semantic splitting and denoising.\n","authors":["Ashesh Ashesh","Florian Jug"],"pdf_url":"https://arxiv.org/pdf/2403.11854v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02970v5","updated":"2024-03-25T08:50:42Z","published":"2023-04-06T09:54:06Z","title":"Unraveling Instance Associations: A Closer Look for Audio-Visual\n  Segmentation","summary":"  Audio-visual segmentation (AVS) is a challenging task that involves\naccurately segmenting sounding objects based on audio-visual cues. The\neffectiveness of audio-visual learning critically depends on achieving accurate\ncross-modal alignment between sound and visual objects. Successful audio-visual\nlearning requires two essential components: 1) a challenging dataset with\nhigh-quality pixel-level multi-class annotated images associated with audio\nfiles, and 2) a model that can establish strong links between audio information\nand its corresponding visual object. However, these requirements are only\npartially addressed by current methods, with training sets containing biased\naudio-visual data, and models that generalise poorly beyond this biased\ntraining set. In this work, we propose a new cost-effective strategy to build\nchallenging and relatively unbiased high-quality audio-visual segmentation\nbenchmarks. We also propose a new informative sample mining method for\naudio-visual supervised contrastive learning to leverage discriminative\ncontrastive samples to enforce cross-modal understanding. We show empirical\nresults that demonstrate the effectiveness of our benchmark. Furthermore,\nexperiments conducted on existing AVS datasets and on our new benchmark show\nthat our method achieves state-of-the-art (SOTA) segmentation accuracy.\n","authors":["Yuanhong Chen","Yuyuan Liu","Hu Wang","Fengbei Liu","Chong Wang","Helen Frazer","Gustavo Carneiro"],"pdf_url":"https://arxiv.org/pdf/2304.02970v5.pdf","comment":"Code is available at https://github.com/cyh-0/CAVP"},{"id":"http://arxiv.org/abs/2403.06904v2","updated":"2024-03-25T08:45:37Z","published":"2024-03-11T16:56:37Z","title":"FocusCLIP: Multimodal Subject-Level Guidance for Zero-Shot Transfer in\n  Human-Centric Tasks","summary":"  We propose FocusCLIP, integrating subject-level guidance--a specialized\nmechanism for target-specific supervision--into the CLIP framework for improved\nzero-shot transfer on human-centric tasks. Our novel contributions enhance CLIP\non both the vision and text sides. On the vision side, we incorporate ROI\nheatmaps emulating human visual attention mechanisms to emphasize\nsubject-relevant image regions. On the text side, we introduce human pose\ndescriptions to provide rich contextual information. For human-centric tasks,\nFocusCLIP is trained with images from the MPII Human Pose dataset. The proposed\napproach surpassed CLIP by an average of 8.61% across five previously unseen\ndatasets covering three human-centric tasks. FocusCLIP achieved an average\naccuracy of 33.65% compared to 25.04% by CLIP. We observed a 3.98% improvement\nin activity recognition, a 14.78% improvement in age classification, and a\n7.06% improvement in emotion recognition. Moreover, using our proposed\nsingle-shot LLM prompting strategy, we release a high-quality MPII Pose\nDescriptions dataset to encourage further research in multimodal learning for\nhuman-centric tasks. Furthermore, we also demonstrate the effectiveness of our\nsubject-level supervision on non-human-centric tasks. FocusCLIP shows a 2.47%\nimprovement over CLIP in zero-shot bird classification using the CUB dataset.\nOur findings emphasize the potential of integrating subject-level guidance with\ngeneral pretraining methods for enhanced downstream performance.\n","authors":["Muhammad Saif Ullah Khan","Muhammad Ferjad Naeem","Federico Tombari","Luc Van Gool","Didier Stricker","Muhammad Zeshan Afzal"],"pdf_url":"https://arxiv.org/pdf/2403.06904v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.00247v4","updated":"2024-03-25T08:34:15Z","published":"2023-08-01T03:00:36Z","title":"Unleashing the Power of Self-Supervised Image Denoising: A Comprehensive\n  Review","summary":"  The advent of deep learning has brought a revolutionary transformation to\nimage denoising techniques. However, the persistent challenge of acquiring\nnoise-clean pairs for supervised methods in real-world scenarios remains\nformidable, necessitating the exploration of more practical self-supervised\nimage denoising. This paper focuses on self-supervised image denoising methods\nthat offer effective solutions to address this challenge. Our comprehensive\nreview thoroughly analyzes the latest advancements in self-supervised image\ndenoising approaches, categorizing them into three distinct classes: General\nmethods, Blind Spot Network (BSN)-based methods, and Transformer-based methods.\nFor each class, we provide a concise theoretical analysis along with their\npractical applications. To assess the effectiveness of these methods, we\npresent both quantitative and qualitative experimental results on various\ndatasets, utilizing classical algorithms as benchmarks. Additionally, we\ncritically discuss the current limitations of these methods and propose\npromising directions for future research. By offering a detailed overview of\nrecent developments in self-supervised image denoising, this review serves as\nan invaluable resource for researchers and practitioners in the field,\nfacilitating a deeper understanding of this emerging domain and inspiring\nfurther advancements.\n","authors":["Dan Zhang","Fangfang Zhou","Felix Albu","Yuanzhou Wei","Xiao Yang","Yuan Gu","Qiang Li"],"pdf_url":"https://arxiv.org/pdf/2308.00247v4.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2403.16539v1","updated":"2024-03-25T08:31:14Z","published":"2024-03-25T08:31:14Z","title":"DOrA: 3D Visual Grounding with Order-Aware Referring","summary":"  3D visual grounding aims to identify the target object within a 3D point\ncloud scene referred to by a natural language description. While previous works\nattempt to exploit the verbo-visual relation with proposed cross-modal\ntransformers, unstructured natural utterances and scattered objects might lead\nto undesirable performances. In this paper, we introduce DOrA, a novel 3D\nvisual grounding framework with Order-Aware referring. DOrA is designed to\nleverage Large Language Models (LLMs) to parse language description, suggesting\na referential order of anchor objects. Such ordered anchor objects allow DOrA\nto update visual features and locate the target object during the grounding\nprocess. Experimental results on the NR3D and ScanRefer datasets demonstrate\nour superiority in both low-resource and full-data scenarios. In particular,\nDOrA surpasses current state-of-the-art frameworks by 9.3% and 7.8% grounding\naccuracy under 1% data and 10% data settings, respectively.\n","authors":["Tung-Yu Wu","Sheng-Yu Huang","Yu-Chiang Frank Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08262v4","updated":"2024-03-25T08:29:52Z","published":"2024-03-13T05:25:49Z","title":"BiTT: Bi-directional Texture Reconstruction of Interacting Two Hands\n  from a Single Image","summary":"  Creating personalized hand avatars is important to offer a realistic\nexperience to users on AR / VR platforms. While most prior studies focused on\nreconstructing 3D hand shapes, some recent work has tackled the reconstruction\nof hand textures on top of shapes. However, these methods are often limited to\ncapturing pixels on the visible side of a hand, requiring diverse views of the\nhand in a video or multiple images as input. In this paper, we propose a novel\nmethod, BiTT(Bi-directional Texture reconstruction of Two hands), which is the\nfirst end-to-end trainable method for relightable, pose-free texture\nreconstruction of two interacting hands taking only a single RGB image, by\nthree novel components: 1) bi-directional (left $\\leftrightarrow$ right)\ntexture reconstruction using the texture symmetry of left / right hands, 2)\nutilizing a texture parametric model for hand texture recovery, and 3) the\noverall coarse-to-fine stage pipeline for reconstructing personalized texture\nof two interacting hands. BiTT first estimates the scene light condition and\nalbedo image from an input image, then reconstructs the texture of both hands\nthrough the texture parametric model and bi-directional texture reconstructor.\nIn experiments using InterHand2.6M and RGB2Hands datasets, our method\nsignificantly outperforms state-of-the-art hand texture reconstruction methods\nquantitatively and qualitatively. The code is available at\nhttps://github.com/yunminjin2/BiTT\n","authors":["Minje Kim","Tae-Kyun Kim"],"pdf_url":"https://arxiv.org/pdf/2403.08262v4.pdf","comment":"Accepted by CVPR 2024, Project Page:\n  https://yunminjin2.github.io/projects/bitt/"},{"id":"http://arxiv.org/abs/2403.16536v1","updated":"2024-03-25T08:26:42Z","published":"2024-03-25T08:26:42Z","title":"VMRNN: Integrating Vision Mamba and LSTM for Efficient and Accurate\n  Spatiotemporal Forecasting","summary":"  Combining CNNs or ViTs, with RNNs for spatiotemporal forecasting, has yielded\nunparalleled results in predicting temporal and spatial dynamics. However,\nmodeling extensive global information remains a formidable challenge; CNNs are\nlimited by their narrow receptive fields, and ViTs struggle with the intensive\ncomputational demands of their attention mechanisms. The emergence of recent\nMamba-based architectures has been met with enthusiasm for their exceptional\nlong-sequence modeling capabilities, surpassing established vision models in\nefficiency and accuracy, which motivates us to develop an innovative\narchitecture tailored for spatiotemporal forecasting. In this paper, we propose\nthe VMRNN cell, a new recurrent unit that integrates the strengths of Vision\nMamba blocks with LSTM. We construct a network centered on VMRNN cells to\ntackle spatiotemporal prediction tasks effectively. Our extensive evaluations\nshow that our proposed approach secures competitive results on a variety of\ntasks while maintaining a smaller model size. Our code is available at\nhttps://github.com/yyyujintang/VMRNN-PyTorch.\n","authors":["Yujin Tang","Peijie Dong","Zhenheng Tang","Xiaowen Chu","Junwei Liang"],"pdf_url":"https://arxiv.org/pdf/2403.16536v1.pdf","comment":"11 pages, 7 figures. arXiv admin note: text overlap with\n  arXiv:2308.09891 by other authors"},{"id":"http://arxiv.org/abs/2403.16530v1","updated":"2024-03-25T08:16:06Z","published":"2024-03-25T08:16:06Z","title":"An Intermediate Fusion ViT Enables Efficient Text-Image Alignment in\n  Diffusion Models","summary":"  Diffusion models have been widely used for conditional data cross-modal\ngeneration tasks such as text-to-image and text-to-video. However,\nstate-of-the-art models still fail to align the generated visual concepts with\nhigh-level semantics in a language such as object count, spatial relationship,\netc. We approach this problem from a multimodal data fusion perspective and\ninvestigate how different fusion strategies can affect vision-language\nalignment. We discover that compared to the widely used early fusion of\nconditioning text in a pretrained image feature space, a specially designed\nintermediate fusion can: (i) boost text-to-image alignment with improved\ngeneration quality and (ii) improve training and inference efficiency by\nreducing low-rank text-to-image attention calculations. We perform experiments\nusing a text-to-image generation task on the MS-COCO dataset. We compare our\nintermediate fusion mechanism with the classic early fusion mechanism on two\ncommon conditioning methods on a U-shaped ViT backbone. Our intermediate fusion\nmodel achieves a higher CLIP Score and lower FID, with 20% reduced FLOPs, and\n50% increased training speed compared to a strong U-ViT baseline with an early\nfusion.\n","authors":["Zizhao Hu","Shaochong Jia","Mohammad Rostami"],"pdf_url":"https://arxiv.org/pdf/2403.16530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16528v1","updated":"2024-03-25T08:14:22Z","published":"2024-03-25T08:14:22Z","title":"Open-Set Recognition in the Age of Vision-Language Models","summary":"  Are vision-language models (VLMs) open-set models because they are trained on\ninternet-scale datasets? We answer this question with a clear no - VLMs\nintroduce closed-set assumptions via their finite query set, making them\nvulnerable to open-set conditions. We systematically evaluate VLMs for open-set\nrecognition and find they frequently misclassify objects not contained in their\nquery set, leading to alarmingly low precision when tuned for high recall and\nvice versa. We show that naively increasing the size of the query set to\ncontain more and more classes does not mitigate this problem, but instead\ncauses diminishing task performance and open-set performance. We establish a\nrevised definition of the open-set problem for the age of VLMs, define a new\nbenchmark and evaluation protocol to facilitate standardised evaluation and\nresearch in this important area, and evaluate promising baseline approaches\nbased on predictive uncertainty and dedicated negative embeddings on a range of\nVLM classifiers and object detectors.\n","authors":["Dimity Miller","Niko Sünderhauf","Alex Kenna","Keita Mason"],"pdf_url":"https://arxiv.org/pdf/2403.16528v1.pdf","comment":"31 pages, under review"},{"id":"http://arxiv.org/abs/2403.16526v1","updated":"2024-03-25T08:09:22Z","published":"2024-03-25T08:09:22Z","title":"ModeTv2: GPU-accelerated Motion Decomposition Transformer for Pairwise\n  Optimization in Medical Image Registration","summary":"  Deformable image registration plays a crucial role in medical imaging, aiding\nin disease diagnosis and image-guided interventions. Traditional iterative\nmethods are slow, while deep learning (DL) accelerates solutions but faces\nusability and precision challenges. This study introduces a pyramid network\nwith the enhanced motion decomposition Transformer (ModeTv2) operator,\nshowcasing superior pairwise optimization (PO) akin to traditional methods. We\nre-implement ModeT operator with CUDA extensions to enhance its computational\nefficiency. We further propose RegHead module which refines deformation fields,\nimproves the realism of deformation and reduces parameters. By adopting the PO,\nthe proposed network balances accuracy, efficiency, and generalizability.\nExtensive experiments on two public brain MRI datasets and one abdominal CT\ndataset demonstrate the network's suitability for PO, providing a DL model with\nenhanced usability and interpretability. The code is publicly available.\n","authors":["Haiqiao Wang","Zhuoyuan Wang","Dong Ni","Yi Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08863v3","updated":"2024-03-25T08:05:16Z","published":"2023-11-15T10:49:15Z","title":"Toulouse Hyperspectral Data Set: a benchmark data set to assess\n  semi-supervised spectral representation learning and pixel-wise\n  classification techniques","summary":"  Airborne hyperspectral images can be used to map the land cover in large\nurban areas, thanks to their very high spatial and spectral resolutions on a\nwide spectral domain. While the spectral dimension of hyperspectral images is\nhighly informative of the chemical composition of the land surface, the use of\nstate-of-the-art machine learning algorithms to map the land cover has been\ndramatically limited by the availability of training data. To cope with the\nscarcity of annotations, semi-supervised and self-supervised techniques have\nlately raised a lot of interest in the community. Yet, the publicly available\nhyperspectral data sets commonly used to benchmark machine learning models are\nnot totally suited to evaluate their generalization performances due to one or\nseveral of the following properties: a limited geographical coverage (which\ndoes not reflect the spectral diversity in metropolitan areas), a small number\nof land cover classes and a lack of appropriate standard train / test splits\nfor semi-supervised and self-supervised learning. Therefore, we release in this\npaper the Toulouse Hyperspectral Data Set that stands out from other data sets\nin the above-mentioned respects in order to meet key issues in spectral\nrepresentation learning and classification over large-scale hyperspectral\nimages with very few labeled pixels. Besides, we discuss and experiment\nself-supervised techniques for spectral representation learning, including the\nMasked Autoencoder, and establish a baseline for pixel-wise classification\nachieving 85% overall accuracy and 77% F1 score. The Toulouse Hyperspectral\nData Set and our code are publicly available at\nhttps://www.toulouse-hyperspectral-data-set.com and\nhttps://www.github.com/Romain3Ch216/tlse-experiments, respectively.\n","authors":["Romain Thoreau","Laurent Risser","Véronique Achard","Béatrice Berthelot","Xavier Briottet"],"pdf_url":"https://arxiv.org/pdf/2311.08863v3.pdf","comment":"17 pages, 13 figures"},{"id":"http://arxiv.org/abs/2403.16520v1","updated":"2024-03-25T08:02:41Z","published":"2024-03-25T08:02:41Z","title":"CMViM: Contrastive Masked Vim Autoencoder for 3D Multi-modal\n  Representation Learning for AD classification","summary":"  Alzheimer's disease (AD) is an incurable neurodegenerative condition leading\nto cognitive and functional deterioration. Given the lack of a cure, prompt and\nprecise AD diagnosis is vital, a complex process dependent on multiple factors\nand multi-modal data. While successful efforts have been made to integrate\nmulti-modal representation learning into medical datasets, scant attention has\nbeen given to 3D medical images. In this paper, we propose Contrastive Masked\nVim Autoencoder (CMViM), the first efficient representation learning method\ntailored for 3D multi-modal data. Our proposed framework is built on a masked\nVim autoencoder to learn a unified multi-modal representation and\nlong-dependencies contained in 3D medical images. We also introduce an\nintra-modal contrastive learning module to enhance the capability of the\nmulti-modal Vim encoder for modeling the discriminative features in the same\nmodality, and an inter-modal contrastive learning module to alleviate\nmisaligned representation among modalities. Our framework consists of two main\nsteps: 1) incorporate the Vision Mamba (Vim) into the mask autoencoder to\nreconstruct 3D masked multi-modal data efficiently. 2) align the multi-modal\nrepresentations with contrastive learning mechanisms from both intra-modal and\ninter-modal aspects. Our framework is pre-trained and validated ADNI2 dataset\nand validated on the downstream task for AD classification. The proposed CMViM\nyields 2.7\\% AUC performance improvement compared with other state-of-the-art\nmethods.\n","authors":["Guangqian Yang","Kangrui Du","Zhihan Yang","Ye Du","Yongping Zheng","Shujun Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16520v1.pdf","comment":"11 pages, 1 figure"},{"id":"http://arxiv.org/abs/2403.16516v1","updated":"2024-03-25T08:00:43Z","published":"2024-03-25T08:00:43Z","title":"Visually Guided Generative Text-Layout Pre-training for Document\n  Intelligence","summary":"  Prior study shows that pre-training techniques can boost the performance of\nvisual document understanding (VDU), which typically requires models to gain\nabilities to perceive and reason both document texts and layouts (e.g.,\nlocations of texts and table-cells). To this end, we propose visually guided\ngenerative text-layout pre-training, named ViTLP. Given a document image, the\nmodel optimizes hierarchical language and layout modeling objectives to\ngenerate the interleaved text and layout sequence. In addition, to address the\nlimitation of processing long documents by Transformers, we introduce a\nstraightforward yet effective multi-segment generative pre-training scheme,\nfacilitating ViTLP to process word-intensive documents of any length. ViTLP can\nfunction as a native OCR model to localize and recognize texts of document\nimages. Besides, ViTLP can be effectively applied to various downstream VDU\ntasks. Extensive experiments show that ViTLP achieves competitive performance\nover existing baselines on benchmark VDU tasks, including information\nextraction, document classification, and document question answering.\n","authors":["Zhiming Mao","Haoli Bai","Lu Hou","Jiansheng Wei","Xin Jiang","Qun Liu","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2403.16516v1.pdf","comment":"Accepted to NAACL 2024 main conference. The first version of this\n  paper was submitted to OpenReview\n  (https://openreview.net/forum?id=ARtBIBAmNR) in June 2023"},{"id":"http://arxiv.org/abs/2403.16513v1","updated":"2024-03-25T07:58:58Z","published":"2024-03-25T07:58:58Z","title":"Let Real Images be as a Judger, Spotting Fake Images Synthesized with\n  Generative Models","summary":"  In the last few years, generative models have shown their powerful\ncapabilities in synthesizing realistic images in both quality and diversity\n(i.e., facial images, and natural subjects). Unfortunately, the artifact\npatterns in fake images synthesized by different generative models are\ninconsistent, leading to the failure of previous research that relied on\nspotting subtle differences between real and fake. In our preliminary\nexperiments, we find that the artifacts in fake images always change with the\ndevelopment of the generative model, while natural images exhibit stable\nstatistical properties. In this paper, we employ natural traces shared only by\nreal images as an additional predictive target in the detector. Specifically,\nthe natural traces are learned from the wild real images and we introduce\nextended supervised contrastive learning to bring them closer to real images\nand further away from fake ones. This motivates the detector to make decisions\nbased on the proximity of images to the natural traces. To conduct a\ncomprehensive experiment, we built a high-quality and diverse dataset that\nincludes generative models comprising 6 GAN and 6 diffusion models, to evaluate\nthe effectiveness in generalizing unknown forgery techniques and robustness in\nsurviving different transformations. Experimental results show that our\nproposed method gives 96.1% mAP significantly outperforms the baselines.\nExtensive experiments conducted on the widely recognized platform Midjourney\nreveal that our proposed method achieves an accuracy exceeding 78.4%,\nunderscoring its practicality for real-world application deployment. The source\ncode and partial self-built dataset are available in supplementary material.\n","authors":["Ziyou Liang","Run Wang","Weifeng Liu","Yuyang Zhang","Wenyuan Yang","Lina Wang","Xingkai Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16513v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16510v1","updated":"2024-03-25T07:54:18Z","published":"2024-03-25T07:54:18Z","title":"Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework","summary":"  Despite the remarkable process of talking-head-based avatar-creating\nsolutions, directly generating anchor-style videos with full-body motions\nremains challenging. In this study, we propose Make-Your-Anchor, a novel system\nnecessitating only a one-minute video clip of an individual for training,\nsubsequently enabling the automatic generation of anchor-style videos with\nprecise torso and hand movements. Specifically, we finetune a proposed\nstructure-guided diffusion model on input video to render 3D mesh conditions\ninto human appearances. We adopt a two-stage training strategy for the\ndiffusion model, effectively binding movements with specific appearances. To\nproduce arbitrary long temporal video, we extend the 2D U-Net in the frame-wise\ndiffusion model to a 3D style without additional training cost, and a simple\nyet effective batch-overlapped temporal denoising module is proposed to bypass\nthe constraints on video length during inference. Finally, a novel\nidentity-specific face enhancement module is introduced to improve the visual\nquality of facial regions in the output videos. Comparative experiments\ndemonstrate the effectiveness and superiority of the system in terms of visual\nquality, temporal coherence, and identity preservation, outperforming SOTA\ndiffusion/non-diffusion methods. Project page:\n\\url{https://github.com/ICTMCG/Make-Your-Anchor}.\n","authors":["Ziyao Huang","Fan Tang","Yong Zhang","Xiaodong Cun","Juan Cao","Jintao Li","Tong-Yee Lee"],"pdf_url":"https://arxiv.org/pdf/2403.16510v1.pdf","comment":"accepted at CVPR2024"},{"id":"http://arxiv.org/abs/2305.01309v2","updated":"2024-03-25T07:53:54Z","published":"2023-05-02T10:35:20Z","title":"Geometric Prior Based Deep Human Point Cloud Geometry Compression","summary":"  The emergence of digital avatars has raised an exponential increase in the\ndemand for human point clouds with realistic and intricate details. The\ncompression of such data becomes challenging with overwhelming data amounts\ncomprising millions of points. Herein, we leverage the human geometric prior in\ngeometry redundancy removal of point clouds, greatly promoting the compression\nperformance. More specifically, the prior provides topological constraints as\ngeometry initialization, allowing adaptive adjustments with a compact parameter\nset that could be represented with only a few bits. Therefore, we can envisage\nhigh-resolution human point clouds as a combination of geometric priors and\nstructural deviations. The priors could first be derived with an aligned point\ncloud, and subsequently the difference of features is compressed into a compact\nlatent code. The proposed framework can operate in a play-and-plug fashion with\nexisting learning based point cloud compression methods. Extensive experimental\nresults show that our approach significantly improves the compression\nperformance without deteriorating the quality, demonstrating its promise in a\nvariety of applications.\n","authors":["Xinju Wu","Pingping Zhang","Meng Wang","Peilin Chen","Shiqi Wang","Sam Kwong"],"pdf_url":"https://arxiv.org/pdf/2305.01309v2.pdf","comment":"Accepted by TCSVT 2024"},{"id":"http://arxiv.org/abs/2311.17315v3","updated":"2024-03-25T07:51:14Z","published":"2023-11-29T02:10:31Z","title":"Explaining CLIP's performance disparities on data from blind/low vision\n  users","summary":"  Large multi-modal models (LMMs) hold the potential to usher in a new era of\nautomated visual assistance for people who are blind or low vision (BLV). Yet,\nthese models have not been systematically evaluated on data captured by BLV\nusers. We address this by empirically assessing CLIP, a widely-used LMM likely\nto underpin many assistive technologies. Testing 25 CLIP variants in a\nzero-shot classification task, we find that their accuracy is 15 percentage\npoints lower on average for images captured by BLV users than web-crawled\nimages. This disparity stems from CLIP's sensitivities to 1) image content\n(e.g. not recognizing disability objects as well as other objects); 2) image\nquality (e.g. not being robust to lighting variation); and 3) text content\n(e.g. not recognizing objects described by tactile adjectives as well as visual\nones). We delve deeper with a textual analysis of three common pre-training\ndatasets: LAION-400M, LAION-2B and DataComp-1B, showing that disability content\nis rarely mentioned. We then provide three examples that illustrate how the\nperformance disparities extend to three downstream models underpinned by CLIP:\nOWL-ViT, CLIPSeg and DALL-E2. We find that few-shot learning with as few as 5\nimages can mitigate CLIP's quality-of-service disparities for BLV users in some\nscenarios, which we discuss alongside a set of other possible mitigations.\n","authors":["Daniela Massiceti","Camilla Longden","Agnieszka Słowik","Samuel Wills","Martin Grayson","Cecily Morrison"],"pdf_url":"https://arxiv.org/pdf/2311.17315v3.pdf","comment":"Accepted at 2024 IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR)"},{"id":"http://arxiv.org/abs/2403.16502v1","updated":"2024-03-25T07:35:28Z","published":"2024-03-25T07:35:28Z","title":"Medical Image Registration and Its Application in Retinal Images: A\n  Review","summary":"  Medical image registration is vital for disease diagnosis and treatment with\nits ability to merge diverse information of images, which may be captured under\ndifferent times, angles, or modalities. Although several surveys have reviewed\nthe development of medical image registration, these surveys have not\nsystematically summarized methodologies of existing medical image registration\nmethods. To this end, we provide a comprehensive review of these methods from\ntraditional and deep learning-based directions, aiming to help audiences\nunderstand the development of medical image registration quickly. In\nparticular, we review recent advances in retinal image registration at the end\nof each section, which has not attracted much attention. Additionally, we also\ndiscuss the current challenges of retinal image registration and provide\ninsights and prospects for future research.\n","authors":["Qiushi Nie","Xiaoqing Zhang","Yan Hu","Mingdao Gong","Jiang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.16502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16499v1","updated":"2024-03-25T07:34:06Z","published":"2024-03-25T07:34:06Z","title":"Self-Supervised Learning for Medical Image Data with Anatomy-Oriented\n  Imaging Planes","summary":"  Self-supervised learning has emerged as a powerful tool for pretraining deep\nnetworks on unlabeled data, prior to transfer learning of target tasks with\nlimited annotation. The relevance between the pretraining pretext and target\ntasks is crucial to the success of transfer learning. Various pretext tasks\nhave been proposed to utilize properties of medical image data (e.g., three\ndimensionality), which are more relevant to medical image analysis than generic\nones for natural images. However, previous work rarely paid attention to data\nwith anatomy-oriented imaging planes, e.g., standard cardiac magnetic resonance\nimaging views. As these imaging planes are defined according to the anatomy of\nthe imaged organ, pretext tasks effectively exploiting this information can\npretrain the networks to gain knowledge on the organ of interest. In this work,\nwe propose two complementary pretext tasks for this group of medical image data\nbased on the spatial relationship of the imaging planes. The first is to learn\nthe relative orientation between the imaging planes and implemented as\nregressing their intersecting lines. The second exploits parallel imaging\nplanes to regress their relative slice locations within a stack. Both pretext\ntasks are conceptually straightforward and easy to implement, and can be\ncombined in multitask learning for better representation learning. Thorough\nexperiments on two anatomical structures (heart and knee) and representative\ntarget tasks (semantic segmentation and classification) demonstrate that the\nproposed pretext tasks are effective in pretraining deep networks for\nremarkably boosted performance on the target tasks, and superior to other\nrecent approaches.\n","authors":["Tianwei Zhang","Dong Wei","Mengmeng Zhua","Shi Gu","Yefeng Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.16499v1.pdf","comment":"Medical Image Analysis"},{"id":"http://arxiv.org/abs/2403.16497v1","updated":"2024-03-25T07:29:18Z","published":"2024-03-25T07:29:18Z","title":"PathoTune: Adapting Visual Foundation Model to Pathological Specialists","summary":"  As natural image understanding moves towards the pretrain-finetune era,\nresearch in pathology imaging is concurrently evolving. Despite the predominant\nfocus on pretraining pathological foundation models, how to adapt foundation\nmodels to downstream tasks is little explored. For downstream adaptation, we\npropose the existence of two domain gaps, i.e., the Foundation-Task Gap and the\nTask-Instance Gap. To mitigate these gaps, we introduce PathoTune, a framework\ndesigned to efficiently adapt pathological or even visual foundation models to\npathology-specific tasks via multi-modal prompt tuning. The proposed framework\nleverages Task-specific Visual Prompts and Task-specific Textual Prompts to\nidentify task-relevant features, along with Instance-specific Visual Prompts\nfor encoding single pathological image features. Results across multiple\ndatasets at both patch-level and WSI-level demonstrate its superior performance\nover single-modality prompt tuning approaches. Significantly, PathoTune\nfacilitates the direct adaptation of natural visual foundation models to\npathological tasks, drastically outperforming pathological foundation models\nwith simple linear probing. The code will be available upon acceptance.\n","authors":["Jiaxuan Lu","Fang Yan","Xiaofan Zhang","Yue Gao","Shaoting Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16497v1.pdf","comment":"Submitted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2403.16494v1","updated":"2024-03-25T07:22:22Z","published":"2024-03-25T07:22:22Z","title":"CT-Bound: Fast Boundary Estimation From Noisy Images Via Hybrid\n  Convolution and Transformer Neural Networks","summary":"  We present CT-Bound, a fast boundary estimation method for noisy images using\na hybrid Convolution and Transformer neural network. The proposed architecture\ndecomposes boundary estimation into two tasks: local detection and global\nregularization of image boundaries. It first estimates a parametric\nrepresentation of boundary structures only using the input image within a small\nreceptive field and then refines the boundary structure in the parameter domain\nwithout accessing the input image. Because of this, a part of the network can\nbe easily trained using naive, synthetic images and still generalized to real\nimages, and the entire architecture is computationally efficient as the\nboundary refinement is non-iterative and not in the image domain. Compared with\nthe previous highest accuracy methods, our experiment shows that CT-Bound is\n100 times faster, producing comparably accurate, high-quality boundary and\ncolor maps. We also demonstrate that CT-Bound can produce boundary and color\nmaps on real captured images without extra fine-tuning and real-time boundary\nmap and color map videos at ten frames per second.\n","authors":["Wei Xu","Junjie Luo","Qi Guo"],"pdf_url":"https://arxiv.org/pdf/2403.16494v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.16481v1","updated":"2024-03-25T07:07:50Z","published":"2024-03-25T07:07:50Z","title":"REFRAME: Reflective Surface Real-Time Rendering for Mobile Devices","summary":"  This work tackles the challenging task of achieving real-time novel view\nsynthesis on various scenes, including highly reflective objects and unbounded\noutdoor scenes. Existing real-time rendering methods, especially those based on\nmeshes, often have subpar performance in modeling surfaces with rich\nview-dependent appearances. Our key idea lies in leveraging meshes for\nrendering acceleration while incorporating a novel approach to parameterize\nview-dependent information. We decompose the color into diffuse and specular,\nand model the specular color in the reflected direction based on a neural\nenvironment map. Our experiments demonstrate that our method achieves\ncomparable reconstruction quality for highly reflective surfaces compared to\nstate-of-the-art offline methods, while also efficiently enabling real-time\nrendering on edge devices such as smartphones.\n","authors":["Chaojie Ji","Yufeng Li","Yiyi Liao"],"pdf_url":"https://arxiv.org/pdf/2403.16481v1.pdf","comment":"Project Page:https://xdimlab.github.io/REFRAME/"},{"id":"http://arxiv.org/abs/2403.06606v2","updated":"2024-03-25T06:57:57Z","published":"2024-03-11T10:50:53Z","title":"Distributionally Generative Augmentation for Fair Facial Attribute\n  Classification","summary":"  Facial Attribute Classification (FAC) holds substantial promise in widespread\napplications. However, FAC models trained by traditional methodologies can be\nunfair by exhibiting accuracy inconsistencies across varied data\nsubpopulations. This unfairness is largely attributed to bias in data, where\nsome spurious attributes (e.g., Male) statistically correlate with the target\nattribute (e.g., Smiling). Most of existing fairness-aware methods rely on the\nlabels of spurious attributes, which may be unavailable in practice. This work\nproposes a novel, generation-based two-stage framework to train a fair FAC\nmodel on biased data without additional annotation. Initially, we identify the\npotential spurious attributes based on generative models. Notably, it enhances\ninterpretability by explicitly showing the spurious attributes in image space.\nFollowing this, for each image, we first edit the spurious attributes with a\nrandom degree sampled from a uniform distribution, while keeping target\nattribute unchanged. Then we train a fair FAC model by fostering model\ninvariance to these augmentation. Extensive experiments on three common\ndatasets demonstrate the effectiveness of our method in promoting fairness in\nFAC without compromising accuracy. Codes are in\nhttps://github.com/heqianpei/DiGA.\n","authors":["Fengda Zhang","Qianpei He","Kun Kuang","Jiashuo Liu","Long Chen","Chao Wu","Jun Xiao","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.06606v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.10066v2","updated":"2024-03-25T06:27:57Z","published":"2024-03-15T07:16:07Z","title":"Contrastive Pre-Training with Multi-View Fusion for No-Reference Point\n  Cloud Quality Assessment","summary":"  No-reference point cloud quality assessment (NR-PCQA) aims to automatically\nevaluate the perceptual quality of distorted point clouds without available\nreference, which have achieved tremendous improvements due to the utilization\nof deep neural networks. However, learning-based NR-PCQA methods suffer from\nthe scarcity of labeled data and usually perform suboptimally in terms of\ngeneralization. To solve the problem, we propose a novel contrastive\npre-training framework tailored for PCQA (CoPA), which enables the pre-trained\nmodel to learn quality-aware representations from unlabeled data. To obtain\nanchors in the representation space, we project point clouds with different\ndistortions into images and randomly mix their local patches to form mixed\nimages with multiple distortions. Utilizing the generated anchors, we constrain\nthe pre-training process via a quality-aware contrastive loss following the\nphilosophy that perceptual quality is closely related to both content and\ndistortion. Furthermore, in the model fine-tuning stage, we propose a\nsemantic-guided multi-view fusion module to effectively integrate the features\nof projected images from multiple perspectives. Extensive experiments show that\nour method outperforms the state-of-the-art PCQA methods on popular benchmarks.\nFurther investigations demonstrate that CoPA can also benefit existing\nlearning-based PCQA models.\n","authors":["Ziyu Shan","Yujie Zhang","Qi Yang","Haichen Yang","Yiling Xu","Jenq-Neng Hwang","Xiaozhong Xu","Shan Liu"],"pdf_url":"https://arxiv.org/pdf/2403.10066v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16450v1","updated":"2024-03-25T06:22:27Z","published":"2024-03-25T06:22:27Z","title":"Camera-aware Label Refinement for Unsupervised Person Re-identification","summary":"  Unsupervised person re-identification aims to retrieve images of a specified\nperson without identity labels. Many recent unsupervised Re-ID approaches adopt\nclustering-based methods to measure cross-camera feature similarity to roughly\ndivide images into clusters. They ignore the feature distribution discrepancy\ninduced by camera domain gap, resulting in the unavoidable performance\ndegradation. Camera information is usually available, and the feature\ndistribution in the single camera usually focuses more on the appearance of the\nindividual and has less intra-identity variance. Inspired by the observation,\nwe introduce a \\textbf{C}amera-\\textbf{A}ware \\textbf{L}abel\n\\textbf{R}efinement~(CALR) framework that reduces camera discrepancy by\nclustering intra-camera similarity. Specifically, we employ intra-camera\ntraining to obtain reliable local pseudo labels within each camera, and then\nrefine global labels generated by inter-camera clustering and train the\ndiscriminative model using more reliable global pseudo labels in a self-paced\nmanner. Meanwhile, we develop a camera-alignment module to align feature\ndistributions under different cameras, which could help deal with the camera\nvariance further. Extensive experiments validate the superiority of our\nproposed method over state-of-the-art approaches. The code is accessible at\nhttps://github.com/leeBooMla/CALR.\n","authors":["Pengna Li","Kangyi Wu","Wenli Huang","Sanping Zhou","Jinjun Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16450v1.pdf","comment":"submitted to IEEE TMM"},{"id":"http://arxiv.org/abs/2312.02480v2","updated":"2024-03-25T06:22:09Z","published":"2023-12-05T04:13:31Z","title":"Differentiable Point-based Inverse Rendering","summary":"  We present differentiable point-based inverse rendering, DPIR, an\nanalysis-by-synthesis method that processes images captured under diverse\nilluminations to estimate shape and spatially-varying BRDF. To this end, we\nadopt point-based rendering, eliminating the need for multiple samplings per\nray, typical of volumetric rendering, thus significantly enhancing the speed of\ninverse rendering. To realize this idea, we devise a hybrid point-volumetric\nrepresentation for geometry and a regularized basis-BRDF representation for\nreflectance. The hybrid geometric representation enables fast rendering through\npoint-based splatting while retaining the geometric details and stability\ninherent to SDF-based representations. The regularized basis-BRDF mitigates the\nill-posedness of inverse rendering stemming from limited light-view angular\nsamples. We also propose an efficient shadow detection method using point-based\nshadow map rendering. Our extensive evaluations demonstrate that DPIR\noutperforms prior works in terms of reconstruction accuracy, computational\nefficiency, and memory footprint. Furthermore, our explicit point-based\nrepresentation and rendering enables intuitive geometry and reflectance\nediting.\n","authors":["Hoon-Gyu Chung","Seokjun Choi","Seung-Hwan Baek"],"pdf_url":"https://arxiv.org/pdf/2312.02480v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16442v1","updated":"2024-03-25T06:05:50Z","published":"2024-03-25T06:05:50Z","title":"If CLIP Could Talk: Understanding Vision-Language Model Representations\n  Through Their Preferred Concept Descriptions","summary":"  Recent works often assume that Vision-Language Model (VLM) representations\nare based on visual attributes like shape. However, it is unclear to what\nextent VLMs prioritize this information to represent concepts. We propose\nExtract and Explore (EX2), a novel approach to characterize important textual\nfeatures for VLMs. EX2 uses reinforcement learning to align a large language\nmodel with VLM preferences and generates descriptions that incorporate the\nimportant features for the VLM. Then, we inspect the descriptions to identify\nthe features that contribute to VLM representations. We find that spurious\ndescriptions have a major role in VLM representations despite providing no\nhelpful information, e.g., Click to enlarge photo of CONCEPT. More importantly,\namong informative descriptions, VLMs rely significantly on non-visual\nattributes like habitat to represent visual concepts. Also, our analysis\nreveals that different VLMs prioritize different attributes in their\nrepresentations. Overall, we show that VLMs do not simply match images to scene\ndescriptions and that non-visual or even spurious descriptions significantly\ninfluence their representations.\n","authors":["Reza Esfandiarpoor","Cristina Menghini","Stephen H. Bach"],"pdf_url":"https://arxiv.org/pdf/2403.16442v1.pdf","comment":"Code: https://github.com/BatsResearch/ex2"},{"id":"http://arxiv.org/abs/2310.14566v5","updated":"2024-03-25T06:05:24Z","published":"2023-10-23T04:49:09Z","title":"HallusionBench: An Advanced Diagnostic Suite for Entangled Language\n  Hallucination and Visual Illusion in Large Vision-Language Models","summary":"  We introduce HallusionBench, a comprehensive benchmark designed for the\nevaluation of image-context reasoning. This benchmark presents significant\nchallenges to advanced large visual-language models (LVLMs), such as\nGPT-4V(Vision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, by emphasizing\nnuanced understanding and interpretation of visual data. The benchmark\ncomprises 346 images paired with 1129 questions, all meticulously crafted by\nhuman experts. We introduce a novel structure for these visual questions\ndesigned to establish control groups. This structure enables us to conduct a\nquantitative analysis of the models' response tendencies, logical consistency,\nand various failure modes. In our evaluation on HallusionBench, we benchmarked\n15 different models, highlighting a 31.42% question-pair accuracy achieved by\nthe state-of-the-art GPT-4V. Notably, all other evaluated models achieve\naccuracy below 16%. Moreover, our analysis not only highlights the observed\nfailure modes, including language hallucination and visual illusion, but also\ndeepens an understanding of these pitfalls. Our comprehensive case studies\nwithin HallusionBench shed light on the challenges of hallucination and\nillusion in LVLMs. Based on these insights, we suggest potential pathways for\ntheir future improvement. The benchmark and codebase can be accessed at\nhttps://github.com/tianyi-lab/HallusionBench.\n","authors":["Tianrui Guan","Fuxiao Liu","Xiyang Wu","Ruiqi Xian","Zongxia Li","Xiaoyu Liu","Xijun Wang","Lichang Chen","Furong Huang","Yaser Yacoob","Dinesh Manocha","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.14566v5.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.16440v1","updated":"2024-03-25T06:02:05Z","published":"2024-03-25T06:02:05Z","title":"RCBEVDet: Radar-camera Fusion in Bird's Eye View for 3D Object Detection","summary":"  Three-dimensional object detection is one of the key tasks in autonomous\ndriving. To reduce costs in practice, low-cost multi-view cameras for 3D object\ndetection are proposed to replace the expansive LiDAR sensors. However, relying\nsolely on cameras is difficult to achieve highly accurate and robust 3D object\ndetection. An effective solution to this issue is combining multi-view cameras\nwith the economical millimeter-wave radar sensor to achieve more reliable\nmulti-modal 3D object detection. In this paper, we introduce RCBEVDet, a\nradar-camera fusion 3D object detection method in the bird's eye view (BEV).\nSpecifically, we first design RadarBEVNet for radar BEV feature extraction.\nRadarBEVNet consists of a dual-stream radar backbone and a Radar Cross-Section\n(RCS) aware BEV encoder. In the dual-stream radar backbone, a point-based\nencoder and a transformer-based encoder are proposed to extract radar features,\nwith an injection and extraction module to facilitate communication between the\ntwo encoders. The RCS-aware BEV encoder takes RCS as the object size prior to\nscattering the point feature in BEV. Besides, we present the Cross-Attention\nMulti-layer Fusion module to automatically align the multi-modal BEV feature\nfrom radar and camera with the deformable attention mechanism, and then fuse\nthe feature with channel and spatial fusion layers. Experimental results show\nthat RCBEVDet achieves new state-of-the-art radar-camera fusion results on\nnuScenes and view-of-delft (VoD) 3D object detection benchmarks. Furthermore,\nRCBEVDet achieves better 3D detection results than all real-time camera-only\nand radar-camera 3D object detectors with a faster inference speed at 21~28\nFPS. The source code will be released at https://github.com/VDIGPKU/RCBEVDet.\n","authors":["Zhiwei Lin","Zhe Liu","Zhongyu Xia","Xinhao Wang","Yongtao Wang","Shengxiang Qi","Yang Dong","Nan Dong","Le Zhang","Ce Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.16440v1.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.16439v1","updated":"2024-03-25T05:58:33Z","published":"2024-03-25T05:58:33Z","title":"Producing and Leveraging Online Map Uncertainty in Trajectory Prediction","summary":"  High-definition (HD) maps have played an integral role in the development of\nmodern autonomous vehicle (AV) stacks, albeit with high associated labeling and\nmaintenance costs. As a result, many recent works have proposed methods for\nestimating HD maps online from sensor data, enabling AVs to operate outside of\npreviously-mapped regions. However, current online map estimation approaches\nare developed in isolation of their downstream tasks, complicating their\nintegration in AV stacks. In particular, they do not produce uncertainty or\nconfidence estimates. In this work, we extend multiple state-of-the-art online\nmap estimation methods to additionally estimate uncertainty and show how this\nenables more tightly integrating online mapping with trajectory forecasting. In\ndoing so, we find that incorporating uncertainty yields up to 50% faster\ntraining convergence and up to 15% better prediction performance on the\nreal-world nuScenes driving dataset.\n","authors":["Xunjiang Gu","Guanyu Song","Igor Gilitschenski","Marco Pavone","Boris Ivanovic"],"pdf_url":"https://arxiv.org/pdf/2403.16439v1.pdf","comment":"14 pages, 14 figures, 6 tables. CVPR 2024"},{"id":"http://arxiv.org/abs/2403.07371v2","updated":"2024-03-25T05:48:28Z","published":"2024-03-12T07:15:29Z","title":"Time-Efficient and Identity-Consistent Virtual Try-On Using A Variant of\n  Altered Diffusion Models","summary":"  This study discusses the critical issues of Virtual Try-On in contemporary\ne-commerce and the prospective metaverse, emphasizing the challenges of\npreserving intricate texture details and distinctive features of the target\nperson and the clothes in various scenarios, such as clothing texture and\nidentity characteristics like tattoos or accessories. In addition to the\nfidelity of the synthesized images, the efficiency of the synthesis process\npresents a significant hurdle. Various existing approaches are explored,\nhighlighting the limitations and unresolved aspects, e.g., identity information\nomission, uncontrollable artifacts, and low synthesis speed. It then proposes a\nnovel diffusion-based solution that addresses garment texture preservation and\nuser identity retention during virtual try-on. The proposed network comprises\ntwo primary modules - a warping module aligning clothing with individual\nfeatures and a try-on module refining the attire and generating missing parts\nintegrated with a mask-aware post-processing technique ensuring the integrity\nof the individual's identity. It demonstrates impressive results, surpassing\nthe state-of-the-art in speed by nearly 20 times during inference, with\nsuperior fidelity in qualitative assessments. Quantitative evaluations confirm\ncomparable performance with the recent SOTA method on the VITON-HD and\nDresscode datasets.\n","authors":["Phuong Dam","Jihoon Jeong","Anh Tran","Daeyoung Kim"],"pdf_url":"https://arxiv.org/pdf/2403.07371v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16438v1","updated":"2024-03-25T05:46:06Z","published":"2024-03-25T05:46:06Z","title":"Real-time Neuron Segmentation for Voltage Imaging","summary":"  In voltage imaging, where the membrane potentials of individual neurons are\nrecorded at from hundreds to thousand frames per second using fluorescence\nmicroscopy, data processing presents a challenge. Even a fraction of a minute\nof recording with a limited image size yields gigabytes of video data\nconsisting of tens of thousands of frames, which can be time-consuming to\nprocess. Moreover, millisecond-level short exposures lead to noisy video\nframes, obscuring neuron footprints especially in deep-brain samples where\nnoisy signals are buried in background fluorescence. To address this challenge,\nwe propose a fast neuron segmentation method able to detect multiple,\npotentially overlapping, spiking neurons from noisy video frames, and implement\na data processing pipeline incorporating the proposed segmentation method along\nwith GPU-accelerated motion correction. By testing on existing datasets as well\nas on new datasets we introduce, we show that our pipeline extracts neuron\nfootprints that agree well with human annotation even from cluttered datasets,\nand demonstrate real-time processing of voltage imaging data on a single\ndesktop computer for the first time.\n","authors":["Yosuke Bando","Ramdas Pillai","Atsushi Kajita","Farhan Abdul Hakeem","Yves Quemener","Hua-an Tseng","Kiryl D. Piatkevich","Changyang Linghu","Xue Han","Edward S. Boyden"],"pdf_url":"https://arxiv.org/pdf/2403.16438v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06199v4","updated":"2024-03-25T05:36:56Z","published":"2024-03-10T12:43:27Z","title":"Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small\n  Language Models","summary":"  Multimodal Large Language Models (MLLMs) have showcased impressive skills in\ntasks related to visual understanding and reasoning. Yet, their widespread\napplication faces obstacles due to the high computational demands during both\nthe training and inference phases, restricting their use to a limited audience\nwithin the research and user communities. In this paper, we investigate the\ndesign aspects of Multimodal Small Language Models (MSLMs) and propose an\nefficient multimodal assistant named Mipha, which is designed to create synergy\namong various aspects: visual representation, language models, and optimization\nstrategies. We show that without increasing the volume of training data, our\nMipha-3B outperforms the state-of-the-art large MLLMs, especially\nLLaVA-1.5-13B, on multiple benchmarks. Through detailed discussion, we provide\ninsights and guidelines for developing strong MSLMs that rival the capabilities\nof MLLMs. Our code is available at https://github.com/zhuyiche/llava-phi.\n","authors":["Minjie Zhu","Yichen Zhu","Xin Liu","Ning Liu","Zhiyuan Xu","Chaomin Shen","Yaxin Peng","Zhicai Ou","Feifei Feng","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2403.06199v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.18287v2","updated":"2024-03-25T05:34:58Z","published":"2023-11-30T06:45:52Z","title":"Dispersed Structured Light for Hyperspectral 3D Imaging","summary":"  Hyperspectral 3D imaging aims to acquire both depth and spectral information\nof a scene. However, existing methods are either prohibitively expensive and\nbulky or compromise on spectral and depth accuracy. In this work, we present\nDispersed Structured Light (DSL), a cost-effective and compact method for\naccurate hyperspectral 3D imaging. DSL modifies a traditional projector-camera\nsystem by placing a sub-millimeter thick diffraction grating film front of the\nprojector. The grating disperses structured light based on light wavelength. To\nutilize the dispersed structured light, we devise a model for dispersive\nprojection image formation and a per-pixel hyperspectral 3D reconstruction\nmethod. We validate DSL by instantiating a compact experimental prototype. DSL\nachieves spectral accuracy of 18.8nm full-width half-maximum (FWHM) and depth\nerror of 1mm. We demonstrate that DSL outperforms prior work on practical\nhyperspectral 3D imaging. DSL promises accurate and practical hyperspectral 3D\nimaging for diverse application domains, including computer vision and\ngraphics, cultural heritage, geology, and biology.\n","authors":["Suhyun Shin","Seokjun Choi","Felix Heide","Seung-Hwan Baek"],"pdf_url":"https://arxiv.org/pdf/2311.18287v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16431v1","updated":"2024-03-25T05:22:34Z","published":"2024-03-25T05:22:34Z","title":"DOCTR: Disentangled Object-Centric Transformer for Point Scene\n  Understanding","summary":"  Point scene understanding is a challenging task to process real-world scene\npoint cloud, which aims at segmenting each object, estimating its pose, and\nreconstructing its mesh simultaneously. Recent state-of-the-art method first\nsegments each object and then processes them independently with multiple stages\nfor the different sub-tasks. This leads to a complex pipeline to optimize and\nmakes it hard to leverage the relationship constraints between multiple\nobjects. In this work, we propose a novel Disentangled Object-Centric\nTRansformer (DOCTR) that explores object-centric representation to facilitate\nlearning with multiple objects for the multiple sub-tasks in a unified manner.\nEach object is represented as a query, and a Transformer decoder is adapted to\niteratively optimize all the queries involving their relationship. In\nparticular, we introduce a semantic-geometry disentangled query (SGDQ) design\nthat enables the query features to attend separately to semantic information\nand geometric information relevant to the corresponding sub-tasks. A hybrid\nbipartite matching module is employed to well use the supervisions from all the\nsub-tasks during training. Qualitative and quantitative experimental results\ndemonstrate that our method achieves state-of-the-art performance on the\nchallenging ScanNet dataset. Code is available at\nhttps://github.com/SAITPublic/DOCTR.\n","authors":["Xiaoxuan Yu","Hao Wang","Weiming Li","Qiang Wang","Soonyong Cho","Younghun Sung"],"pdf_url":"https://arxiv.org/pdf/2403.16431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.13964v3","updated":"2024-03-25T05:18:04Z","published":"2023-12-21T15:51:12Z","title":"PIA: Your Personalized Image Animator via Plug-and-Play Modules in\n  Text-to-Image Models","summary":"  Recent advancements in personalized text-to-image (T2I) models have\nrevolutionized content creation, empowering non-experts to generate stunning\nimages with unique styles. While promising, adding realistic motions into these\npersonalized images by text poses significant challenges in preserving distinct\nstyles, high-fidelity details, and achieving motion controllability by text. In\nthis paper, we present PIA, a Personalized Image Animator that excels in\naligning with condition images, achieving motion controllability by text, and\nthe compatibility with various personalized T2I models without specific tuning.\nTo achieve these goals, PIA builds upon a base T2I model with well-trained\ntemporal alignment layers, allowing for the seamless transformation of any\npersonalized T2I model into an image animation model. A key component of PIA is\nthe introduction of the condition module, which utilizes the condition frame\nand inter-frame affinity as input to transfer appearance information guided by\nthe affinity hint for individual frame synthesis in the latent space. This\ndesign mitigates the challenges of appearance-related image alignment within\nand allows for a stronger focus on aligning with motion-related guidance.\n","authors":["Yiming Zhang","Zhening Xing","Yanhong Zeng","Youqing Fang","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2312.13964v3.pdf","comment":"Project page: https://pi-animator.github.io/"},{"id":"http://arxiv.org/abs/2403.16428v1","updated":"2024-03-25T05:12:21Z","published":"2024-03-25T05:12:21Z","title":"Benchmarks and Challenges in Pose Estimation for Egocentric Hand\n  Interactions with Objects","summary":"  We interact with the world with our hands and see it through our own\n(egocentric) perspective. A holistic 3D understanding of such interactions from\negocentric views is important for tasks in robotics, AR/VR, action recognition\nand motion generation. Accurately reconstructing such interactions in 3D is\nchallenging due to heavy occlusion, viewpoint bias, camera distortion, and\nmotion blur from the head movement. To this end, we designed the HANDS23\nchallenge based on the AssemblyHands and ARCTIC datasets with carefully\ndesigned training and testing splits. Based on the results of the top submitted\nmethods and more recent baselines on the leaderboards, we perform a thorough\nanalysis on 3D hand(-object) reconstruction tasks. Our analysis demonstrates\nthe effectiveness of addressing distortion specific to egocentric cameras,\nadopting high-capacity transformers to learn complex hand-object interactions,\nand fusing predictions from different views. Our study further reveals\nchallenging scenarios intractable with state-of-the-art methods, such as fast\nhand motion, object reconstruction from narrow egocentric views, and close\ncontact between two hands and objects. Our efforts will enrich the community's\nknowledge foundation and facilitate future hand studies on egocentric\nhand-object interactions.\n","authors":["Zicong Fan","Takehiko Ohkawa","Linlin Yang","Nie Lin","Zhishan Zhou","Shihao Zhou","Jiajun Liang","Zhong Gao","Xuanyang Zhang","Xue Zhang","Fei Li","Liu Zheng","Feng Lu","Karim Abou Zeid","Bastian Leibe","Jeongwan On","Seungryul Baek","Aditya Prakash","Saurabh Gupta","Kun He","Yoichi Sato","Otmar Hilliges","Hyung Jin Chang","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2403.16428v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16425v1","updated":"2024-03-25T05:10:34Z","published":"2024-03-25T05:10:34Z","title":"Enhancing Visual Place Recognition via Fast and Slow Adaptive Biasing in\n  Event Cameras","summary":"  Event cameras are increasingly popular in robotics due to their beneficial\nfeatures, such as low latency, energy efficiency, and high dynamic range.\nNevertheless, their downstream task performance is greatly influenced by the\noptimization of bias parameters. These parameters, for instance, regulate the\nnecessary change in light intensity to trigger an event, which in turn depends\non factors such as the environment lighting and camera motion. This paper\nintroduces feedback control algorithms that automatically tune the bias\nparameters through two interacting methods: 1) An immediate, on-the-fly fast\nadaptation of the refractory period, which sets the minimum interval between\nconsecutive events, and 2) if the event rate exceeds the specified bounds even\nafter changing the refractory period repeatedly, the controller adapts the\npixel bandwidth and event thresholds, which stabilizes after a short period of\nnoise events across all pixels (slow adaptation). Our evaluation focuses on the\nvisual place recognition task, where incoming query images are compared to a\ngiven reference database. We conducted comprehensive evaluations of our\nalgorithms' adaptive feedback control in real-time. To do so, we collected the\nQCR-Fast-and-Slow dataset that contains DAVIS346 event camera streams from 366\nrepeated traversals of a Scout Mini robot navigating through a 100 meter long\nindoor lab setting (totaling over 35km distance traveled) in varying brightness\nconditions with ground truth location information. Our proposed feedback\ncontrollers result in superior performance when compared to the standard bias\nsettings and prior feedback control methods. Our findings also detail the\nimpact of bias adjustments on task performance and feature ablation studies on\nthe fast and slow adaptation mechanisms.\n","authors":["Gokul B. Nair","Michael Milford","Tobias Fischer"],"pdf_url":"https://arxiv.org/pdf/2403.16425v1.pdf","comment":"8 pages, 9 figures, paper under review"},{"id":"http://arxiv.org/abs/2312.03009v2","updated":"2024-03-25T05:04:04Z","published":"2023-12-04T19:01:19Z","title":"I-PHYRE: Interactive Physical Reasoning","summary":"  Current evaluation protocols predominantly assess physical reasoning in\nstationary scenes, creating a gap in evaluating agents' abilities to interact\nwith dynamic events. While contemporary methods allow agents to modify initial\nscene configurations and observe consequences, they lack the capability to\ninteract with events in real time. To address this, we introduce I-PHYRE, a\nframework that challenges agents to simultaneously exhibit intuitive physical\nreasoning, multi-step planning, and in-situ intervention. Here, intuitive\nphysical reasoning refers to a quick, approximate understanding of physics to\naddress complex problems; multi-step denotes the need for extensive sequence\nplanning in I-PHYRE, considering each intervention can significantly alter\nsubsequent choices; and in-situ implies the necessity for timely object\nmanipulation within a scene, where minor timing deviations can result in task\nfailure. We formulate four game splits to scrutinize agents' learning and\ngeneralization of essential principles of interactive physical reasoning,\nfostering learning through interaction with representative scenarios. Our\nexploration involves three planning strategies and examines several supervised\nand reinforcement agents' zero-shot generalization proficiency on I-PHYRE. The\noutcomes highlight a notable gap between existing learning algorithms and human\nperformance, emphasizing the imperative for more research in enhancing agents\nwith interactive physical reasoning capabilities. The environment and baselines\nwill be made publicly available.\n","authors":["Shiqian Li","Kewen Wu","Chi Zhang","Yixin Zhu"],"pdf_url":"https://arxiv.org/pdf/2312.03009v2.pdf","comment":"21 pages, ICLR 2024"},{"id":"http://arxiv.org/abs/2403.16422v1","updated":"2024-03-25T04:54:49Z","published":"2024-03-25T04:54:49Z","title":"Refining Text-to-Image Generation: Towards Accurate Training-Free\n  Glyph-Enhanced Image Generation","summary":"  Over the past few years, Text-to-Image (T2I) generation approaches based on\ndiffusion models have gained significant attention. However, vanilla diffusion\nmodels often suffer from spelling inaccuracies in the text displayed within the\ngenerated images. The capability to generate visual text is crucial, offering\nboth academic interest and a wide range of practical applications. To produce\naccurate visual text images, state-of-the-art techniques adopt a\nglyph-controlled image generation approach, consisting of a text layout\ngenerator followed by an image generator that is conditioned on the generated\ntext layout. Nevertheless, our study reveals that these models still face three\nprimary challenges, prompting us to develop a testbed to facilitate future\nresearch. We introduce a benchmark, LenCom-Eval, specifically designed for\ntesting models' capability in generating images with Lengthy and Complex visual\ntext. Subsequently, we introduce a training-free framework to enhance the\ntwo-stage generation approaches. We examine the effectiveness of our approach\non both LenCom-Eval and MARIO-Eval benchmarks and demonstrate notable\nimprovements across a range of evaluation metrics, including CLIPScore, OCR\nprecision, recall, F1 score, accuracy, and edit distance scores. For instance,\nour proposed framework improves the backbone model, TextDiffuser, by more than\n23\\% and 13.5\\% in terms of OCR word F1 on LenCom-Eval and MARIO-Eval,\nrespectively. Our work makes a unique contribution to the field by focusing on\ngenerating images with long and rare text sequences, a niche previously\nunexplored by existing literature\n","authors":["Sanyam Lakhanpal","Shivang Chopra","Vinija Jain","Aman Chadha","Man Luo"],"pdf_url":"https://arxiv.org/pdf/2403.16422v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03173v3","updated":"2024-03-25T04:42:22Z","published":"2024-03-05T18:08:29Z","title":"Solving the bongard-logo problem by modeling a probabilistic model","summary":"  Abstract reasoning problems challenge the perceptual and cognitive abilities\nof AI algorithms, demanding deeper pattern discernment and inductive reasoning\nbeyond explicit image features. This study introduces PMoC, a tailored\nprobability model for the Bongard-Logo problem, achieving high reasoning\naccuracy by constructing independent probability models. Additionally, we\npresent Pose-Transformer, an enhanced Transformer-Encoder designed for complex\nabstract reasoning tasks, including Bongard-Logo, RAVEN, I-RAVEN, and PGM.\nPose-Transformer incorporates positional information learning, inspired by\ncapsule networks' pose matrices, enhancing its focus on local positional\nrelationships in image data processing. When integrated with PMoC, it further\nimproves reasoning accuracy. Our approach effectively addresses reasoning\ndifficulties associated with abstract entities' positional changes,\noutperforming previous models on the OIG, D3$\\times$3 subsets of RAVEN, and PGM\ndatabases. This research contributes to advancing AI's capabilities in abstract\nreasoning and cognitive pattern recognition.\n","authors":["Ruizhuo Song","Beiming Yuan"],"pdf_url":"https://arxiv.org/pdf/2403.03173v3.pdf","comment":"14 pages, 11 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.03190v4","updated":"2024-03-25T04:40:39Z","published":"2024-03-05T18:29:17Z","title":"Triple-CFN: Restructuring Conceptual Spaces for Enhancing Abstract\n  Reasoning process","summary":"  Abstract reasoning problems pose significant challenges to artificial\nintelligence algorithms, demanding cognitive capabilities beyond those required\nfor perception tasks. This study introduces the Triple-CFN approach to tackle\nthe Bongard-Logo problem, achieving notable reasoning accuracy by implicitly\nreorganizing the concept space of conflicting instances. Additionally, the\nTriple-CFN paradigm proves effective for the RPM problem with necessary\nmodifications, yielding competitive results. To further enhance performance on\nthe RPM issue, we develop the Meta Triple-CFN network, which explicitly\nstructures the problem space while maintaining interpretability on progressive\npatterns. The success of Meta Triple-CFN is attributed to its paradigm of\nmodeling the conceptual space, equivalent to normalizing reasoning information.\nBased on this ideology, we introduce the Re-space layer, enhancing the\nperformance of both Meta Triple-CFN and Triple-CFN. This paper aims to\ncontribute to advancements in machine intelligence by exploring innovative\nnetwork designs for addressing abstract reasoning problems, paving the way for\nfurther breakthroughs in this domain.\n","authors":["Ruizhuo Song","Beiming Yuan"],"pdf_url":"https://arxiv.org/pdf/2403.03190v4.pdf","comment":"14 pages, 14 figures, 5 tables"},{"id":"http://arxiv.org/abs/2403.03452v4","updated":"2024-03-25T04:38:42Z","published":"2024-03-06T04:36:43Z","title":"D4C glove-train: solving the RPM and Bongard-logo problem by\n  distributing and Circumscribing concepts","summary":"  This paper achieves noteworthy progress in the realm of abstract reasoning,\nparticularly in addressing Raven's Progressive Matrices (RPM) and Bongard-Logo\nchallenges. Initially, we introduce Lico-Net, a novel baseline model that\nresolves RPM problems with remarkable accuracy. Leveraging this foundation, we\nadvance with the D3C approach, which advocates representing the underlying\nconcepts in abstract reasoning problems through distributions. This perspective\nenhances the performance of both Lico-Net and a baseline model excelling in\nBongard-Logo tasks. To bolster the computational efficiency of D3C, we present\nthe D3C-cos variant, offering a streamlined yet precise solution. Furthermore,\nwe propose the D2C method, redefining conceptual boundaries within these\ndomains and bridging the divide between high-level abstractions and their\nlower-dimensional counterparts. Finally, we extend our methodology to D4C,\nemploying adversarial techniques to refine conceptual boundaries further and\ndemonstrate substantial improvements in both RPM and Bongard-Logo challenges.\nOverall, our contributions present a fresh outlook and practical advancements\nin the field of abstract reasoning.\n","authors":["Ruizhuo Song","Beiming Yuan"],"pdf_url":"https://arxiv.org/pdf/2403.03452v4.pdf","comment":"18 pages, 19 figures, 6 tables"},{"id":"http://arxiv.org/abs/2403.16412v1","updated":"2024-03-25T04:14:07Z","published":"2024-03-25T04:14:07Z","title":"Unsupervised Template-assisted Point Cloud Shape Correspondence Network","summary":"  Unsupervised point cloud shape correspondence aims to establish point-wise\ncorrespondences between source and target point clouds. Existing methods obtain\ncorrespondences directly by computing point-wise feature similarity between\npoint clouds. However, non-rigid objects possess strong deformability and\nunusual shapes, making it a longstanding challenge to directly establish\ncorrespondences between point clouds with unconventional shapes. To address\nthis challenge, we propose an unsupervised Template-Assisted point cloud shape\ncorrespondence Network, termed TANet, including a template generation module\nand a template assistance module. The proposed TANet enjoys several merits.\nFirstly, the template generation module establishes a set of learnable\ntemplates with explicit structures. Secondly, we introduce a template\nassistance module that extensively leverages the generated templates to\nestablish more accurate shape correspondences from multiple perspectives.\nExtensive experiments on four human and animal datasets demonstrate that TANet\nachieves favorable performance against state-of-the-art methods.\n","authors":["Jiacheng Deng","Jiahao Lu","Tianzhu Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16412v1.pdf","comment":"Accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2403.16410v1","updated":"2024-03-25T04:05:23Z","published":"2024-03-25T04:05:23Z","title":"Spike-NeRF: Neural Radiance Field Based On Spike Camera","summary":"  As a neuromorphic sensor with high temporal resolution, spike cameras offer\nnotable advantages over traditional cameras in high-speed vision applications\nsuch as high-speed optical estimation, depth estimation, and object tracking.\nInspired by the success of the spike camera, we proposed Spike-NeRF, the first\nNeural Radiance Field derived from spike data, to achieve 3D reconstruction and\nnovel viewpoint synthesis of high-speed scenes. Instead of the multi-view\nimages at the same time of NeRF, the inputs of Spike-NeRF are continuous spike\nstreams captured by a moving spike camera in a very short time. To reconstruct\na correct and stable 3D scene from high-frequency but unstable spike data, we\ndevised spike masks along with a distinctive loss function. We evaluate our\nmethod qualitatively and numerically on several challenging synthetic scenes\ngenerated by blender with the spike camera simulator. Our results demonstrate\nthat Spike-NeRF produces more visually appealing results than the existing\nmethods and the baseline we proposed in high-speed scenes. Our code and data\nwill be released soon.\n","authors":["Yijia Guo","Yuanxi Bai","Liwen Hu","Mianzhi Liu","Ziyi Guo","Lei Ma","Tiejun Huang"],"pdf_url":"https://arxiv.org/pdf/2403.16410v1.pdf","comment":"This paper is accepted by ICME2024"},{"id":"http://arxiv.org/abs/2403.16407v1","updated":"2024-03-25T03:47:53Z","published":"2024-03-25T03:47:53Z","title":"A Survey on Long Video Generation: Challenges, Methods, and Prospects","summary":"  Video generation is a rapidly advancing research area, garnering significant\nattention due to its broad range of applications. One critical aspect of this\nfield is the generation of long-duration videos, which presents unique\nchallenges and opportunities. This paper presents the first survey of recent\nadvancements in long video generation and summarises them into two key\nparadigms: divide and conquer temporal autoregressive.\n  We delve into the common models employed in each paradigm, including aspects\nof network design and conditioning techniques. Furthermore, we offer a\ncomprehensive overview and classification of the datasets and evaluation\nmetrics which are crucial for advancing long video generation research.\nConcluding with a summary of existing studies, we also discuss the emerging\nchallenges and future directions in this dynamic field. We hope that this\nsurvey will serve as an essential reference for researchers and practitioners\nin the realm of long video generation.\n","authors":["Chengxuan Li","Di Huang","Zeyu Lu","Yang Xiao","Qingqi Pei","Lei Bai"],"pdf_url":"https://arxiv.org/pdf/2403.16407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16405v1","updated":"2024-03-25T03:44:36Z","published":"2024-03-25T03:44:36Z","title":"Ensemble Adversarial Defense via Integration of Multiple Dispersed Low\n  Curvature Models","summary":"  The integration of an ensemble of deep learning models has been extensively\nexplored to enhance defense against adversarial attacks. The diversity among\nsub-models increases the attack cost required to deceive the majority of the\nensemble, thereby improving the adversarial robustness. While existing\napproaches mainly center on increasing diversity in feature representations or\ndispersion of first-order gradients with respect to input, the limited\ncorrelation between these diversity metrics and adversarial robustness\nconstrains the performance of ensemble adversarial defense. In this work, we\naim to enhance ensemble diversity by reducing attack transferability. We\nidentify second-order gradients, which depict the loss curvature, as a key\nfactor in adversarial robustness. Computing the Hessian matrix involved in\nsecond-order gradients is computationally expensive. To address this, we\napproximate the Hessian-vector product using differential approximation. Given\nthat low curvature provides better robustness, our ensemble model was designed\nto consider the influence of curvature among different sub-models. We introduce\na novel regularizer to train multiple more-diverse low-curvature network\nmodels. Extensive experiments across various datasets demonstrate that our\nensemble model exhibits superior robustness against a range of attacks,\nunderscoring the effectiveness of our approach.\n","authors":["Kaikang Zhao","Xi Chen","Wei Huang","Liuxin Ding","Xianglong Kong","Fan Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16405v1.pdf","comment":"Accepted to The 2024 International Joint Conference on Neural\n  Networks (IJCNN)"},{"id":"http://arxiv.org/abs/2304.06928v2","updated":"2024-03-25T03:40:19Z","published":"2023-04-14T05:25:52Z","title":"CiPR: An Efficient Framework with Cross-instance Positive Relations for\n  Generalized Category Discovery","summary":"  We tackle the issue of generalized category discovery (GCD). GCD considers\nthe open-world problem of automatically clustering a partially labelled\ndataset, in which the unlabelled data may contain instances from both novel\ncategories and labelled classes. In this paper, we address the GCD problem with\nan unknown category number for the unlabelled data. We propose a framework,\nnamed CiPR, to bootstrap the representation by exploiting Cross-instance\nPositive Relations in the partially labelled data for contrastive learning,\nwhich have been neglected in existing methods. To obtain reliable\ncross-instance relations to facilitate representation learning, we introduce a\nsemi-supervised hierarchical clustering algorithm, named selective neighbor\nclustering (SNC), which can produce a clustering hierarchy directly from the\nconnected components of a graph constructed from selective neighbors. We\nfurther present a method to estimate the unknown class number using SNC with a\njoint reference score that considers clustering indexes of both labelled and\nunlabelled data, and extend SNC to allow label assignment for the unlabelled\ninstances with a given class number. We thoroughly evaluate our framework on\npublic generic image recognition datasets and challenging fine-grained\ndatasets, and establish a new state-of-the-art. Code:\nhttps://github.com/haoosz/CiPR\n","authors":["Shaozhe Hao","Kai Han","Kwan-Yee K. Wong"],"pdf_url":"https://arxiv.org/pdf/2304.06928v2.pdf","comment":"Accepted to TMLR. Code: https://github.com/haoosz/CiPR"},{"id":"http://arxiv.org/abs/2311.13614v2","updated":"2024-03-25T03:39:45Z","published":"2023-11-22T04:52:58Z","title":"HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction\n  Data","summary":"  Multi-modal Large Language Models (MLLMs) tuned on machine-generated\ninstruction-following data have demonstrated remarkable performance in various\nmulti-modal understanding and generation tasks. However, the hallucinations\ninherent in machine-generated data, which could lead to hallucinatory outputs\nin MLLMs, remain under-explored. This work aims to investigate various\nhallucinations (i.e., object, relation, attribute hallucinations) and mitigate\nthose hallucinatory toxicities in large-scale machine-generated visual\ninstruction datasets. Drawing on the human ability to identify factual errors,\nwe present a novel hallucination detection and elimination framework,\nHalluciDoctor, based on the cross-checking paradigm. We use our framework to\nidentify and eliminate hallucinations in the training data automatically.\nInterestingly, HalluciDoctor also indicates that spurious correlations arising\nfrom long-tail object co-occurrences contribute to hallucinations. Based on\nthat, we execute counterfactual visual instruction expansion to balance data\ndistribution, thereby enhancing MLLMs' resistance to hallucinations.\nComprehensive experiments on hallucination evaluation benchmarks show that our\nmethod successfully mitigates 44.6% hallucinations relatively and maintains\ncompetitive performance compared to LLaVA. The data and code for this paper are\npublicly available. \\url{https://github.com/Yuqifan1117/HalluciDoctor}.\n","authors":["Qifan Yu","Juncheng Li","Longhui Wei","Liang Pang","Wentao Ye","Bosheng Qin","Siliang Tang","Qi Tian","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2311.13614v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.16400v1","updated":"2024-03-25T03:30:37Z","published":"2024-03-25T03:30:37Z","title":"ASDF: Assembly State Detection Utilizing Late Fusion by Integrating 6D\n  Pose Estimation","summary":"  In medical and industrial domains, providing guidance for assembly processes\nis critical to ensure efficiency and safety. Errors in assembly can lead to\nsignificant consequences such as extended surgery times, and prolonged\nmanufacturing or maintenance times in industry. Assembly scenarios can benefit\nfrom in-situ AR visualization to provide guidance, reduce assembly times and\nminimize errors. To enable in-situ visualization 6D pose estimation can be\nleveraged. Existing 6D pose estimation techniques primarily focus on individual\nobjects and static captures. However, assembly scenarios have various dynamics\nincluding occlusion during assembly and dynamics in the assembly objects\nappearance. Existing work, combining object detection/6D pose estimation and\nassembly state detection focuses either on pure deep learning-based approaches,\nor limit the assembly state detection to building blocks. To address the\nchallenges of 6D pose estimation in combination with assembly state detection,\nour approach ASDF builds upon the strengths of YOLOv8, a real-time capable\nobject detection framework. We extend this framework, refine the object pose\nand fuse pose knowledge with network-detected pose information. Utilizing our\nlate fusion in our Pose2State module results in refined 6D pose estimation and\nassembly state detection. By combining both pose and state information, our\nPose2State module predicts the final assembly state with precision. Our\nevaluation on our ASDF dataset shows that our Pose2State module leads to an\nimproved assembly state detection and that the improvement of the assembly\nstate further leads to a more robust 6D pose estimation. Moreover, on the GBOT\ndataset, we outperform the pure deep learning-based network, and even\noutperform the hybrid and pure tracking-based approaches.\n","authors":["Hannah Schieber","Shiyu Li","Niklas Corell","Philipp Beckerle","Julian Kreimeier","Daniel Roth"],"pdf_url":"https://arxiv.org/pdf/2403.16400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17460v3","updated":"2024-03-25T03:21:39Z","published":"2023-11-29T09:02:07Z","title":"W-HMR: Human Mesh Recovery in World Space with Weak-supervised Camera\n  Calibration and Orientation Correction","summary":"  For a long time, in reconstructing 3D human bodies from monocular images,\nmost methods opted to simplify the task by minimizing the influence of the\ncamera. Using a coarse focal length setting results in the reconstructed bodies\nnot aligning well with distorted images. Ignoring camera rotation leads to an\nunrealistic reconstructed body pose in world space. Consequently, the\napplication scenarios of existing methods are confined to controlled\nenvironments. When confronted with complex and diverse in-the-wild images, they\nstruggle to achieve accurate and reasonable reconstruction in world space. To\naddress the above issues, we propose W-HMR, which decouples global body\nrecovery into camera calibration, local body recovery, and global body\norientation correction. We design the first weak-supervised camera calibration\nmethod for body distortion, eliminating dependence on focal length labels and\nachieving finer mesh-image alignment. We propose a novel orientation correction\nmodule to allow the reconstructed human body to remain normal in world space.\nDecoupling body orientation and body pose enables our model to consider the\naccuracy in camera coordinate and the reasonableness in world coordinate\nsimultaneously, expanding the range of applications. As a result, W-HMR\nachieves high-quality reconstruction in dual coordinate systems, particularly\nin challenging scenes. Codes and demos have been released on the project page\nhttps://yw0208.github.io/w-hmr/.\n","authors":["Wei Yao","Hongwen Zhang","Yunlian Sun","Jinhui Tang"],"pdf_url":"https://arxiv.org/pdf/2311.17460v3.pdf","comment":"Project Page: https://yw0208.github.io/w-hmr/"},{"id":"http://arxiv.org/abs/2403.16395v1","updated":"2024-03-25T03:18:58Z","published":"2024-03-25T03:18:58Z","title":"Multi-attention Associate Prediction Network for Visual Tracking","summary":"  Classification-regression prediction networks have realized impressive\nsuccess in several modern deep trackers. However, there is an inherent\ndifference between classification and regression tasks, so they have diverse\neven opposite demands for feature matching. Existed models always ignore the\nkey issue and only employ a unified matching block in two task branches,\ndecaying the decision quality. Besides, these models also struggle with\ndecision misalignment situation. In this paper, we propose a multi-attention\nassociate prediction network (MAPNet) to tackle the above problems. Concretely,\ntwo novel matchers, i.e., category-aware matcher and spatial-aware matcher, are\nfirst designed for feature comparison by integrating self, cross, channel or\nspatial attentions organically. They are capable of fully capturing the\ncategory-related semantics for classification and the local spatial contexts\nfor regression, respectively. Then, we present a dual alignment module to\nenhance the correspondences between two branches, which is useful to find the\noptimal tracking solution. Finally, we describe a Siamese tracker built upon\nthe proposed prediction network, which achieves the leading performance on five\ntracking benchmarks, consisting of LaSOT, TrackingNet, GOT-10k, TNL2k and\nUAV123, and surpasses other state-of-the-art approaches.\n","authors":["Xinglong Sun","Haijiang Sun","Shan Jiang","Jiacheng Wang","Xilai Wei","Zhonghe Hu"],"pdf_url":"https://arxiv.org/pdf/2403.16395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16387v1","updated":"2024-03-25T03:06:45Z","published":"2024-03-25T03:06:45Z","title":"Text-IF: Leveraging Semantic Text Guidance for Degradation-Aware and\n  Interactive Image Fusion","summary":"  Image fusion aims to combine information from different source images to\ncreate a comprehensively representative image. Existing fusion methods are\ntypically helpless in dealing with degradations in low-quality source images\nand non-interactive to multiple subjective and objective needs. To solve them,\nwe introduce a novel approach that leverages semantic text guidance image\nfusion model for degradation-aware and interactive image fusion task, termed as\nText-IF. It innovatively extends the classical image fusion to the text guided\nimage fusion along with the ability to harmoniously address the degradation and\ninteraction issues during fusion. Through the text semantic encoder and\nsemantic interaction fusion decoder, Text-IF is accessible to the all-in-one\ninfrared and visible image degradation-aware processing and the interactive\nflexible fusion outcomes. In this way, Text-IF achieves not only multi-modal\nimage fusion, but also multi-modal information fusion. Extensive experiments\nprove that our proposed text guided image fusion strategy has obvious\nadvantages over SOTA methods in the image fusion performance and degradation\ntreatment. The code is available at https://github.com/XunpengYi/Text-IF.\n","authors":["Xunpeng Yi","Han Xu","Hao Zhang","Linfeng Tang","Jiayi Ma"],"pdf_url":"https://arxiv.org/pdf/2403.16387v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.09065v3","updated":"2024-03-25T03:04:44Z","published":"2024-03-14T03:12:02Z","title":"When Semantic Segmentation Meets Frequency Aliasing","summary":"  Despite recent advancements in semantic segmentation, where and what pixels\nare hard to segment remains largely unexplored. Existing research only\nseparates an image into easy and hard regions and empirically observes the\nlatter are associated with object boundaries. In this paper, we conduct a\ncomprehensive analysis of hard pixel errors, categorizing them into three\ntypes: false responses, merging mistakes, and displacements. Our findings\nreveal a quantitative association between hard pixels and aliasing, which is\ndistortion caused by the overlapping of frequency components in the Fourier\ndomain during downsampling. To identify the frequencies responsible for\naliasing, we propose using the equivalent sampling rate to calculate the\nNyquist frequency, which marks the threshold for aliasing. Then, we introduce\nthe aliasing score as a metric to quantify the extent of aliasing. While\npositively correlated with the proposed aliasing score, three types of hard\npixels exhibit different patterns. Here, we propose two novel de-aliasing\nfilter (DAF) and frequency mixing (FreqMix) modules to alleviate aliasing\ndegradation by accurately removing or adjusting frequencies higher than the\nNyquist frequency. The DAF precisely removes the frequencies responsible for\naliasing before downsampling, while the FreqMix dynamically selects\nhigh-frequency components within the encoder block. Experimental results\ndemonstrate consistent improvements in semantic segmentation and low-light\ninstance segmentation tasks. The code is available at:\nhttps://github.com/Linwei-Chen/Seg-Aliasing.\n","authors":["Linwei Chen","Lin Gu","Ying Fu"],"pdf_url":"https://arxiv.org/pdf/2403.09065v3.pdf","comment":"Accepted by ICLR 2024"},{"id":"http://arxiv.org/abs/2403.16386v1","updated":"2024-03-25T03:02:51Z","published":"2024-03-25T03:02:51Z","title":"Dia-LLaMA: Towards Large Language Model-driven CT Report Generation","summary":"  Medical report generation has achieved remarkable advancements yet has still\nbeen faced with several challenges. First, the inherent imbalance in the\ndistribution of normal and abnormal cases may lead models to exhibit a biased\nfocus on normal samples, resulting in unreliable diagnoses. Second, the\nfrequent occurrence of common template sentences in the reports may overwhelm\nthe critical abnormal information. Moreover, existing works focus on 2D chest\nX-rays, leaving CT report generation underexplored due to the high-dimensional\nnature of CT images and the limited availability of CT-report pairs. Recently,\nLLM has shown a great ability to generate reliable answers with appropriate\nprompts, which shed light on addressing the aforementioned challenges. In this\npaper, we propose Dia-LLaMA, a framework to adapt the LLaMA2-7B for CT report\ngeneration by incorporating diagnostic information as guidance prompts.\nConsidering the high dimension of CT, we leverage a pre-trained ViT3D with\nperceiver to extract the visual information. To tailor the LLM for report\ngeneration and emphasize abnormality, we extract additional diagnostic\ninformation by referring to a disease prototype memory bank, which is updated\nduring training to capture common disease representations. Furthermore, we\nintroduce disease-aware attention to enable the model to adjust attention for\ndifferent diseases. Experiments on the chest CT dataset demonstrated that our\nproposed method outperformed previous methods and achieved state-of-the-art on\nboth clinical efficacy performance and natural language generation metrics. The\ncode will be made publically available.\n","authors":["Zhixuan Chen","Luyang Luo","Yequan Bie","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2403.16386v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2403.16385v1","updated":"2024-03-25T03:02:27Z","published":"2024-03-25T03:02:27Z","title":"Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators\n  for Reasoning-Based Chart VQA","summary":"  Understanding data visualizations like charts and plots requires reasoning\nabout both visual elements and numerics. Although strong in extractive\nquestions, current chart visual question answering (chart VQA) models suffer on\ncomplex reasoning questions. In this work, we address the lack of reasoning\nability by data augmentation. We leverage Large Language Models (LLMs), which\nhave shown to have strong reasoning ability, as an automatic data annotator\nthat generates question-answer annotations for chart images. The key innovation\nin our method lies in the Synthesize Step-by-Step strategy: our LLM-based data\ngenerator learns to decompose the complex question into step-by-step\nsub-questions (rationales), which are then used to derive the final answer\nusing external tools, i.e. Python. This step-wise generation procedure is\ntrained on synthetic data generated using a template-based QA generation\npipeline. Experimental results highlight the significance of the proposed\nstep-by-step generation. By training with the LLM-augmented data (LAMENDA), we\nsignificantly enhance the chart VQA models, achieving the state-of-the-art\naccuracy on the ChartQA and PlotQA datasets. In particular, our approach\nimproves the accuracy of the previous state-of-the-art approach from 38% to 54%\non the human-written questions in the ChartQA dataset, which needs strong\nreasoning. We hope our work underscores the potential of synthetic data and\nencourages further exploration of data augmentation using LLMs for\nreasoning-heavy tasks.\n","authors":["Li Zhuowan","Jasani Bhavan","Tang Peng","Ghadar Shabnam"],"pdf_url":"https://arxiv.org/pdf/2403.16385v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.16384v1","updated":"2024-03-25T03:01:53Z","published":"2024-03-25T03:01:53Z","title":"Residual Dense Swin Transformer for Continuous Depth-Independent\n  Ultrasound Imaging","summary":"  Ultrasound imaging is crucial for evaluating organ morphology and function,\nyet depth adjustment can degrade image quality and field-of-view, presenting a\ndepth-dependent dilemma. Traditional interpolation-based zoom-in techniques\noften sacrifice detail and introduce artifacts. Motivated by the potential of\narbitrary-scale super-resolution to naturally address these inherent\nchallenges, we present the Residual Dense Swin Transformer Network (RDSTN),\ndesigned to capture the non-local characteristics and long-range dependencies\nintrinsic to ultrasound images. It comprises a linear embedding module for\nfeature enhancement, an encoder with shifted-window attention for modeling\nnon-locality, and an MLP decoder for continuous detail reconstruction. This\nstrategy streamlines balancing image quality and field-of-view, which offers\nsuperior textures over traditional methods. Experimentally, RDSTN outperforms\nexisting approaches while requiring fewer parameters. In conclusion, RDSTN\nshows promising potential for ultrasound image enhancement by overcoming the\nlimitations of conventional interpolation-based methods and achieving\ndepth-independent imaging.\n","authors":["Jintong Hu","Hui Che","Zishuo Li","Wenming Yang"],"pdf_url":"https://arxiv.org/pdf/2403.16384v1.pdf","comment":"Accepted by ICASSP2024, https://ieeexplore.ieee.org/document/10447712"},{"id":"http://arxiv.org/abs/2403.16379v1","updated":"2024-03-25T02:53:32Z","published":"2024-03-25T02:53:32Z","title":"FlashEval: Towards Fast and Accurate Evaluation of Text-to-image\n  Diffusion Generative Models","summary":"  In recent years, there has been significant progress in the development of\ntext-to-image generative models. Evaluating the quality of the generative\nmodels is one essential step in the development process. Unfortunately, the\nevaluation process could consume a significant amount of computational\nresources, making the required periodic evaluation of model performance (e.g.,\nmonitoring training progress) impractical. Therefore, we seek to improve the\nevaluation efficiency by selecting the representative subset of the text-image\ndataset. We systematically investigate the design choices, including the\nselection criteria (textural features or image-based metrics) and the selection\ngranularity (prompt-level or set-level). We find that the insights from prior\nwork on subset selection for training data do not generalize to this problem,\nand we propose FlashEval, an iterative search algorithm tailored to evaluation\ndata selection. We demonstrate the effectiveness of FlashEval on ranking\ndiffusion models with various configurations, including architectures,\nquantization levels, and sampler schedules on COCO and DiffusionDB datasets.\nOur searched 50-item subset could achieve comparable evaluation quality to the\nrandomly sampled 500-item subset for COCO annotations on unseen models,\nachieving a 10x evaluation speedup. We release the condensed subset of these\ncommonly used datasets to help facilitate diffusion algorithm design and\nevaluation, and open-source FlashEval as a tool for condensing future datasets,\naccessible at https://github.com/thu-nics/FlashEval.\n","authors":["Lin Zhao","Tianchen Zhao","Zinan Lin","Xuefei Ning","Guohao Dai","Huazhong Yang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16379v1.pdf","comment":"The paper is accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.15082v2","updated":"2024-03-25T02:50:07Z","published":"2024-03-22T10:06:31Z","title":"Cell Variational Information Bottleneck Network","summary":"  In this work, we propose Cell Variational Information Bottleneck Network\n(cellVIB), a convolutional neural network using information bottleneck\nmechanism, which can be combined with the latest feedforward network\narchitecture in an end-to-end training method. Our Cell Variational Information\nBottleneck Network is constructed by stacking VIB cells, which generate feature\nmaps with uncertainty. As layers going deeper, the regularization effect will\ngradually increase, instead of directly adding excessive regular constraints to\nthe output layer of the model as in Deep VIB. Under each VIB cell, the\nfeedforward process learns an independent mean term and an standard deviation\nterm, and predicts the Gaussian distribution based on them. The feedback\nprocess is based on reparameterization trick for effective training. This work\nperforms an extensive analysis on MNIST dataset to verify the effectiveness of\neach VIB cells, and provides an insightful analysis on how the VIB cells affect\nmutual information. Experiments conducted on CIFAR-10 also prove that our\ncellVIB is robust against noisy labels during training and against corrupted\nimages during testing. Then, we validate our method on PACS dataset, whose\nresults show that the VIB cells can significantly improve the generalization\nperformance of the basic model. Finally, in a more complex representation\nlearning task, face recognition, our network structure has also achieved very\ncompetitive results.\n","authors":["Zhonghua Zhai","Chen Ju","Jinsong Lan","Shuai Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.15082v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16376v1","updated":"2024-03-25T02:46:57Z","published":"2024-03-25T02:46:57Z","title":"Elite360D: Towards Efficient 360 Depth Estimation via Semantic- and\n  Distance-Aware Bi-Projection Fusion","summary":"  360 depth estimation has recently received great attention for 3D\nreconstruction owing to its omnidirectional field of view (FoV). Recent\napproaches are predominantly focused on cross-projection fusion with\ngeometry-based re-projection: they fuse 360 images with equirectangular\nprojection (ERP) and another projection type, e.g., cubemap projection to\nestimate depth with the ERP format. However, these methods suffer from 1)\nlimited local receptive fields, making it hardly possible to capture large FoV\nscenes, and 2) prohibitive computational cost, caused by the complex\ncross-projection fusion module design. In this paper, we propose Elite360D, a\nnovel framework that inputs the ERP image and icosahedron projection (ICOSAP)\npoint set, which is undistorted and spatially continuous. Elite360D is superior\nin its capacity in learning a representation from a local-with-global\nperspective. With a flexible ERP image encoder, it includes an ICOSAP point\nencoder, and a Bi-projection Bi-attention Fusion (B2F) module (totally ~1M\nparameters). Specifically, the ERP image encoder can take various perspective\nimage-trained backbones (e.g., ResNet, Transformer) to extract local features.\nThe point encoder extracts the global features from the ICOSAP. Then, the B2F\nmodule captures the semantic- and distance-aware dependencies between each\npixel of the ERP feature and the entire ICOSAP feature set. Without specific\nbackbone design and obvious computational cost increase, Elite360D outperforms\nthe prior arts on several benchmark datasets.\n","authors":["Hao Ai","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16376v1.pdf","comment":"8 pages, accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.09506v2","updated":"2024-03-25T02:45:35Z","published":"2024-03-14T15:53:04Z","title":"Don't Judge by the Look: Towards Motion Coherent Video Representation","summary":"  Current training pipelines in object recognition neglect Hue Jittering when\ndoing data augmentation as it not only brings appearance changes that are\ndetrimental to classification, but also the implementation is inefficient in\npractice. In this study, we investigate the effect of hue variance in the\ncontext of video understanding and find this variance to be beneficial since\nstatic appearances are less important in videos that contain motion\ninformation. Based on this observation, we propose a data augmentation method\nfor video understanding, named Motion Coherent Augmentation (MCA), that\nintroduces appearance variation in videos and implicitly encourages the model\nto prioritize motion patterns, rather than static appearances. Concretely, we\npropose an operation SwapMix to efficiently modify the appearance of video\nsamples, and introduce Variation Alignment (VA) to resolve the distribution\nshift caused by SwapMix, enforcing the model to learn appearance invariant\nrepresentations. Comprehensive empirical evaluation across various\narchitectures and different datasets solidly validates the effectiveness and\ngeneralization ability of MCA, and the application of VA in other augmentation\nmethods. Code is available at https://github.com/BeSpontaneous/MCA-pytorch.\n","authors":["Yitian Zhang","Yue Bai","Huan Wang","Yizhou Wang","Yun Fu"],"pdf_url":"https://arxiv.org/pdf/2403.09506v2.pdf","comment":"Accepted by ICLR2024"},{"id":"http://arxiv.org/abs/2403.16370v1","updated":"2024-03-25T02:30:32Z","published":"2024-03-25T02:30:32Z","title":"GoodSAM: Bridging Domain and Capacity Gaps via Segment Anything Model\n  for Distortion-aware Panoramic Semantic Segmentation","summary":"  This paper tackles a novel yet challenging problem: how to transfer knowledge\nfrom the emerging Segment Anything Model (SAM) -- which reveals impressive\nzero-shot instance segmentation capacity -- to learn a compact panoramic\nsemantic segmentation model, i.e., student, without requiring any labeled data.\nThis poses considerable challenges due to SAM's inability to provide semantic\nlabels and the large capacity gap between SAM and the student. To this end, we\npropose a novel framework, called GoodSAM, that introduces a teacher assistant\n(TA) to provide semantic information, integrated with SAM to generate ensemble\nlogits to achieve knowledge transfer. Specifically, we propose a\nDistortion-Aware Rectification (DAR) module that first addresses the distortion\nproblem of panoramic images by imposing prediction-level consistency and\nboundary enhancement. This subtly enhances TA's prediction capacity on\npanoramic images. DAR then incorporates a cross-task complementary fusion block\nto adaptively merge the predictions of SAM and TA to obtain more reliable\nensemble logits. Moreover, we introduce a Multi-level Knowledge Adaptation\n(MKA) module to efficiently transfer the multi-level feature knowledge from TA\nand ensemble logits to learn a compact student model. Extensive experiments on\ntwo benchmarks show that our GoodSAM achieves a remarkable +3.75\\% mIoU\nimprovement over the state-of-the-art (SOTA) domain adaptation methods. Also,\nour most lightweight model achieves comparable performance to the SOTA methods\nwith only 3.7M parameters.\n","authors":["Weiming Zhang","Yexin Liu","Xu Zheng","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16370v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.16368v1","updated":"2024-03-25T02:17:20Z","published":"2024-03-25T02:17:20Z","title":"Distilling Semantic Priors from SAM to Efficient Image Restoration\n  Models","summary":"  In image restoration (IR), leveraging semantic priors from segmentation\nmodels has been a common approach to improve performance. The recent segment\nanything model (SAM) has emerged as a powerful tool for extracting advanced\nsemantic priors to enhance IR tasks. However, the computational cost of SAM is\nprohibitive for IR, compared to existing smaller IR models. The incorporation\nof SAM for extracting semantic priors considerably hampers the model inference\nefficiency. To address this issue, we propose a general framework to distill\nSAM's semantic knowledge to boost exiting IR models without interfering with\ntheir inference process. Specifically, our proposed framework consists of the\nsemantic priors fusion (SPF) scheme and the semantic priors distillation (SPD)\nscheme. SPF fuses two kinds of information between the restored image predicted\nby the original IR model and the semantic mask predicted by SAM for the refined\nrestored image. SPD leverages a self-distillation manner to distill the fused\nsemantic priors to boost the performance of original IR models. Additionally,\nwe design a semantic-guided relation (SGR) module for SPD, which ensures\nsemantic feature representation space consistency to fully distill the priors.\nWe demonstrate the effectiveness of our framework across multiple IR models and\ntasks, including deraining, deblurring, and denoising.\n","authors":["Quan Zhang","Xiaoyu Liu","Wei Li","Hanting Chen","Junchao Liu","Jie Hu","Zhiwei Xiong","Chun Yuan","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16368v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15048v2","updated":"2024-03-25T02:08:01Z","published":"2024-03-22T09:13:09Z","title":"Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning","summary":"  Large-scale Text-to-Image (TTI) models have become a common approach for\ngenerating training data in various generative fields. However, visual\nhallucinations, which contain perceptually critical defects, remain a concern,\nespecially in non-photorealistic styles like cartoon characters. We propose a\nnovel visual hallucination detection system for cartoon character images\ngenerated by TTI models. Our approach leverages pose-aware in-context visual\nlearning (PA-ICVL) with Vision-Language Models (VLMs), utilizing both RGB\nimages and pose information. By incorporating pose guidance from a fine-tuned\npose estimator, we enable VLMs to make more accurate decisions. Experimental\nresults demonstrate significant improvements in identifying visual\nhallucinations compared to baseline methods relying solely on RGB images. This\nresearch advances TTI models by mitigating visual hallucinations, expanding\ntheir potential in non-photorealistic domains.\n","authors":["Bumsoo Kim","Wonseop Shin","Kyuchul Lee","Sanghyun Seo"],"pdf_url":"https://arxiv.org/pdf/2403.15048v2.pdf","comment":"11 pages, 12 figures, 1 table, Project page:\n  https://gh-bumsookim.github.io/Cartoon-Hallucinations-Detection/"},{"id":"http://arxiv.org/abs/2403.16365v1","updated":"2024-03-25T02:03:38Z","published":"2024-03-25T02:03:38Z","title":"Generating Potent Poisons and Backdoors from Scratch with Guided\n  Diffusion","summary":"  Modern neural networks are often trained on massive datasets that are web\nscraped with minimal human inspection. As a result of this insecure curation\npipeline, an adversary can poison or backdoor the resulting model by uploading\nmalicious data to the internet and waiting for a victim to scrape and train on\nit. Existing approaches for creating poisons and backdoors start with randomly\nsampled clean data, called base samples, and then modify those samples to craft\npoisons. However, some base samples may be significantly more amenable to\npoisoning than others. As a result, we may be able to craft more potent poisons\nby carefully choosing the base samples. In this work, we use guided diffusion\nto synthesize base samples from scratch that lead to significantly more potent\npoisons and backdoors than previous state-of-the-art attacks. Our Guided\nDiffusion Poisoning (GDP) base samples can be combined with any downstream\npoisoning or backdoor attack to boost its effectiveness. Our implementation\ncode is publicly available at: https://github.com/hsouri/GDP .\n","authors":["Hossein Souri","Arpit Bansal","Hamid Kazemi","Liam Fowl","Aniruddha Saha","Jonas Geiping","Andrew Gordon Wilson","Rama Chellappa","Tom Goldstein","Micah Goldblum"],"pdf_url":"https://arxiv.org/pdf/2403.16365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17516v3","updated":"2024-03-25T01:55:03Z","published":"2023-11-29T10:39:53Z","title":"MMA-Diffusion: MultiModal Attack on Diffusion Models","summary":"  In recent years, Text-to-Image (T2I) models have seen remarkable\nadvancements, gaining widespread adoption. However, this progress has\ninadvertently opened avenues for potential misuse, particularly in generating\ninappropriate or Not-Safe-For-Work (NSFW) content. Our work introduces\nMMA-Diffusion, a framework that presents a significant and realistic threat to\nthe security of T2I models by effectively circumventing current defensive\nmeasures in both open-source models and commercial online services. Unlike\nprevious approaches, MMA-Diffusion leverages both textual and visual modalities\nto bypass safeguards like prompt filters and post-hoc safety checkers, thus\nexposing and highlighting the vulnerabilities in existing defense mechanisms.\n","authors":["Yijun Yang","Ruiyuan Gao","Xiaosen Wang","Tsung-Yi Ho","Nan Xu","Qiang Xu"],"pdf_url":"https://arxiv.org/pdf/2311.17516v3.pdf","comment":"CVPR 2024. Code is available at\n  https://github.com/yangyijune/MMA-Diffusion"},{"id":"http://arxiv.org/abs/2403.16361v1","updated":"2024-03-25T01:54:57Z","published":"2024-03-25T01:54:57Z","title":"RSTAR: Rotational Streak Artifact Reduction in 4D CBCT using Separable\n  and Circular Convolutions","summary":"  Four-dimensional cone-beam computed tomography (4D CBCT) provides\nrespiration-resolved images and can be used for image-guided radiation therapy.\nHowever, the ability to reveal respiratory motion comes at the cost of image\nartifacts. As raw projection data are sorted into multiple respiratory phases,\nthere is a limited number of cone-beam projections available for image\nreconstruction. Consequently, the 4D CBCT images are covered by severe streak\nartifacts. Although several deep learning-based methods have been proposed to\naddress this issue, most algorithms employ ordinary network models, neglecting\nthe intrinsic structural prior within 4D CBCT images. In this paper, we first\nexplore the origin and appearance of streak artifacts in 4D CBCT\nimages.Specifically, we find that streak artifacts exhibit a periodic\nrotational motion along with the patient's respiration. This unique motion\npattern inspires us to distinguish the artifacts from the desired anatomical\nstructures in the spatiotemporal domain. Thereafter, we propose a\nspatiotemporal neural network named RSTAR-Net with separable and circular\nconvolutions for Rotational Streak Artifact Reduction. The specially designed\nmodel effectively encodes dynamic image features, facilitating the recovery of\n4D CBCT images. Moreover, RSTAR-Net is also lightweight and computationally\nefficient. Extensive experiments substantiate the effectiveness of our proposed\nmethod, and RSTAR-Net shows superior performance to comparison methods.\n","authors":["Ziheng Deng","Hua Chen","Haibo Hu","Zhiyong Xu","Tianling Lyu","Yan Xi","Yang Chen","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.16361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.09911v2","updated":"2024-03-25T01:54:41Z","published":"2023-08-19T05:34:13Z","title":"Noisy-Correspondence Learning for Text-to-Image Person Re-identification","summary":"  Text-to-image person re-identification (TIReID) is a compelling topic in the\ncross-modal community, which aims to retrieve the target person based on a\ntextual query. Although numerous TIReID methods have been proposed and achieved\npromising performance, they implicitly assume the training image-text pairs are\ncorrectly aligned, which is not always the case in real-world scenarios. In\npractice, the image-text pairs inevitably exist under-correlated or even\nfalse-correlated, a.k.a noisy correspondence (NC), due to the low quality of\nthe images and annotation errors. To address this problem, we propose a novel\nRobust Dual Embedding method (RDE) that can learn robust visual-semantic\nassociations even with NC. Specifically, RDE consists of two main components:\n1) A Confident Consensus Division (CCD) module that leverages the dual-grained\ndecisions of dual embedding modules to obtain a consensus set of clean training\ndata, which enables the model to learn correct and reliable visual-semantic\nassociations. 2) A Triplet Alignment Loss (TAL) relaxes the conventional\nTriplet Ranking loss with the hardest negative samples to a log-exponential\nupper bound over all negative ones, thus preventing the model collapse under NC\nand can also focus on hard-negative samples for promising performance. We\nconduct extensive experiments on three public benchmarks, namely CUHK-PEDES,\nICFG-PEDES, and RSTPReID, to evaluate the performance and robustness of our\nRDE. Our method achieves state-of-the-art results both with and without\nsynthetic noisy correspondences on all three datasets. Code is available at\nhttps://github.com/QinYang79/RDE.\n","authors":["Yang Qin","Yingke Chen","Dezhong Peng","Xi Peng","Joey Tianyi Zhou","Peng Hu"],"pdf_url":"https://arxiv.org/pdf/2308.09911v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16358v1","updated":"2024-03-25T01:44:34Z","published":"2024-03-25T01:44:34Z","title":"ChebMixer: Efficient Graph Representation Learning with MLP Mixer","summary":"  Graph neural networks have achieved remarkable success in learning graph\nrepresentations, especially graph Transformer, which has recently shown\nsuperior performance on various graph mining tasks. However, graph Transformer\ngenerally treats nodes as tokens, which results in quadratic complexity\nregarding the number of nodes during self-attention computation. The graph MLP\nMixer addresses this challenge by using the efficient MLP Mixer technique from\ncomputer vision. However, the time-consuming process of extracting graph tokens\nlimits its performance. In this paper, we present a novel architecture named\nChebMixer, a newly graph MLP Mixer that uses fast Chebyshev polynomials-based\nspectral filtering to extract a sequence of tokens. Firstly, we produce\nmultiscale representations of graph nodes via fast Chebyshev polynomial-based\nspectral filtering. Next, we consider each node's multiscale representations as\na sequence of tokens and refine the node representation with an effective MLP\nMixer. Finally, we aggregate the multiscale representations of nodes through\nChebyshev interpolation. Owing to the powerful representation capabilities and\nfast computational properties of MLP Mixer, we can quickly extract more\ninformative node representations to improve the performance of downstream\ntasks. The experimental results prove our significant improvements in a variety\nof scenarios ranging from graph node classification to medical image\nsegmentation.\n","authors":["Xiaoyan Kui","Haonan Yan","Qinsong Li","Liming Chen","Beiji Zou"],"pdf_url":"https://arxiv.org/pdf/2403.16358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11614v2","updated":"2024-03-25T01:23:07Z","published":"2024-03-18T09:44:44Z","title":"CRS-Diff: Controllable Generative Remote Sensing Foundation Model","summary":"  The emergence of diffusion models has revolutionized the field of image\ngeneration, providing new methods for creating high-quality, high-resolution\nimages across various applications. However, the potential of these models for\ngenerating domain-specific images, particularly remote sensing (RS) images,\nremains largely untapped. RS images that are notable for their high resolution,\nextensive coverage, and rich information content, bring new challenges that\ngeneral diffusion models may not adequately address. This paper proposes\nCRS-Diff, a pioneering diffusion modeling framework specifically tailored for\ngenerating remote sensing imagery, leveraging the inherent advantages of\ndiffusion models while integrating advanced control mechanisms to ensure that\nthe imagery is not only visually clear but also enriched with geographic and\ntemporal information. The model integrates global and local control inputs,\nenabling precise combinations of generation conditions to refine the generation\nprocess. A comprehensive evaluation of CRS-Diff has demonstrated its superior\ncapability to generate RS imagery both in a single condition and multiple\nconditions compared with previous methods in terms of image quality and\ndiversity.\n","authors":["Datao Tang","Xiangyong Cao","Xingsong Hou","Zhongyuan Jiang","Deyu Meng"],"pdf_url":"https://arxiv.org/pdf/2403.11614v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17034v2","updated":"2024-03-25T01:21:18Z","published":"2023-11-28T18:45:13Z","title":"Telling Left from Right: Identifying Geometry-Aware Semantic\n  Correspondence","summary":"  While pre-trained large-scale vision models have shown significant promise\nfor semantic correspondence, their features often struggle to grasp the\ngeometry and orientation of instances. This paper identifies the importance of\nbeing geometry-aware for semantic correspondence and reveals a limitation of\nthe features of current foundation models under simple post-processing. We show\nthat incorporating this information can markedly enhance semantic\ncorrespondence performance with simple but effective solutions in both\nzero-shot and supervised settings. We also construct a new challenging\nbenchmark for semantic correspondence built from an existing animal pose\nestimation dataset, for both pre-training validating models. Our method\nachieves a PCK@0.10 score of 65.4 (zero-shot) and 85.6 (supervised) on the\nchallenging SPair-71k dataset, outperforming the state of the art by 5.5p and\n11.0p absolute gains, respectively. Our code and datasets are publicly\navailable at: https://telling-left-from-right.github.io/.\n","authors":["Junyi Zhang","Charles Herrmann","Junhwa Hur","Eric Chen","Varun Jampani","Deqing Sun","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2311.17034v2.pdf","comment":"Accepted by CVPR 24, project page:\n  https://telling-left-from-right.github.io/"},{"id":"http://arxiv.org/abs/2403.14743v2","updated":"2024-03-25T01:18:37Z","published":"2024-03-21T18:00:00Z","title":"VURF: A General-purpose Reasoning and Self-refinement Framework for\n  Video Understanding","summary":"  Recent studies have demonstrated the effectiveness of Large Language Models\n(LLMs) as reasoning modules that can deconstruct complex tasks into more\nmanageable sub-tasks, particularly when applied to visual reasoning tasks for\nimages. In contrast, this paper introduces a Video Understanding and Reasoning\nFramework (VURF) based on the reasoning power of LLMs. Ours is a novel approach\nto extend the utility of LLMs in the context of video tasks, leveraging their\ncapacity to generalize from minimal input and output demonstrations within a\ncontextual framework. By presenting LLMs with pairs of instructions and their\ncorresponding high-level programs, we harness their contextual learning\ncapabilities to generate executable visual programs for video understanding. To\nenhance program's accuracy and robustness, we implement two important\nstrategies. Firstly, we employ a feedback-generation approach, powered by\nGPT-3.5, to rectify errors in programs utilizing unsupported functions.\nSecondly, taking motivation from recent works on self refinement of LLM\noutputs, we introduce an iterative procedure for improving the quality of the\nin-context examples by aligning the initial outputs to the outputs that would\nhave been generated had the LLM not been bound by the structure of the\nin-context examples. Our results on several video-specific tasks, including\nvisual QA, video anticipation, pose estimation and multi-video QA illustrate\nthe efficacy of these enhancements in improving the performance of visual\nprogramming approaches for video tasks.\n","authors":["Ahmad Mahmood","Ashmal Vayani","Muzammal Naseer","Salman Khan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2403.14743v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10119v2","updated":"2024-03-25T01:08:14Z","published":"2024-03-15T09:08:27Z","title":"URS-NeRF: Unordered Rolling Shutter Bundle Adjustment for Neural\n  Radiance Fields","summary":"  We propose a novel rolling shutter bundle adjustment method for neural\nradiance fields (NeRF), which utilizes the unordered rolling shutter (RS)\nimages to obtain the implicit 3D representation. Existing NeRF methods suffer\nfrom low-quality images and inaccurate initial camera poses due to the RS\neffect in the image, whereas, the previous method that incorporates the RS into\nNeRF requires strict sequential data input, limiting its widespread\napplicability. In constant, our method recovers the physical formation of RS\nimages by estimating camera poses and velocities, thereby removing the input\nconstraints on sequential data. Moreover, we adopt a coarse-to-fine training\nstrategy, in which the RS epipolar constraints of the pairwise frames in the\nscene graph are used to detect the camera poses that fall into local minima.\nThe poses detected as outliers are corrected by the interpolation method with\nneighboring poses. The experimental results validate the effectiveness of our\nmethod over state-of-the-art works and demonstrate that the reconstruction of\n3D representations is not constrained by the requirement of video sequence\ninput.\n","authors":["Bo Xu","Ziao Liu","Mengqi Guo","Jiancheng Li","Gim Hee Lee"],"pdf_url":"https://arxiv.org/pdf/2403.10119v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16350v1","updated":"2024-03-25T00:59:35Z","published":"2024-03-25T00:59:35Z","title":"3D-EffiViTCaps: 3D Efficient Vision Transformer with Capsule for Medical\n  Image Segmentation","summary":"  Medical image segmentation (MIS) aims to finely segment various organs. It\nrequires grasping global information from both parts and the entire image for\nbetter segmenting, and clinically there are often certain requirements for\nsegmentation efficiency. Convolutional neural networks (CNNs) have made\nconsiderable achievements in MIS. However, they are difficult to fully collect\nglobal context information and their pooling layer may cause information loss.\nCapsule networks, which combine the benefits of CNNs while taking into account\nadditional information such as relative location that CNNs do not, have lately\ndemonstrated some advantages in MIS. Vision Transformer (ViT) employs\ntransformers in visual tasks. Transformer based on attention mechanism has\nexcellent global inductive modeling capabilities and is expected to capture\nlongrange information. Moreover, there have been resent studies on making ViT\nmore lightweight to minimize model complexity and increase efficiency. In this\npaper, we propose a U-shaped 3D encoder-decoder network named 3D-EffiViTCaps,\nwhich combines 3D capsule blocks with 3D EfficientViT blocks for MIS. Our\nencoder uses capsule blocks and EfficientViT blocks to jointly capture local\nand global semantic information more effectively and efficiently with less\ninformation loss, while the decoder employs CNN blocks and EfficientViT blocks\nto catch ffner details for segmentation. We conduct experiments on various\ndatasets, including iSeg-2017, Hippocampus and Cardiac to verify the\nperformance and efficiency of 3D-EffiViTCaps, which performs better than\nprevious 3D CNN-based, 3D Capsule-based and 3D Transformer-based models. We\nfurther implement a series of ablation experiments on the main blocks. Our code\nis available at: https://github.com/HidNeuron/3D-EffiViTCaps.\n","authors":["Dongwei Gan","Ming Chang","Juan Chen"],"pdf_url":"https://arxiv.org/pdf/2403.16350v1.pdf","comment":"15 pages, 4 figures, submitted to ICPR2024"},{"id":"http://arxiv.org/abs/2301.06626v2","updated":"2024-03-25T00:45:30Z","published":"2023-01-16T22:30:53Z","title":"Masked Vector Quantization","summary":"  Generative models with discrete latent representations have recently\ndemonstrated an impressive ability to learn complex high-dimensional data\ndistributions. However, their performance relies on a long sequence of tokens\nper instance and a large number of codebook entries, resulting in long sampling\ntimes and considerable computation to fit the categorical posterior. To address\nthese issues, we propose the Masked Vector Quantization (MVQ) framework which\nincreases the representational capacity of each code vector by learning mask\nconfigurations via a stochastic winner-takes-all training regime called\nMultiple Hypothese Dropout (MH-Dropout). On ImageNet 64$\\times$64, MVQ reduces\nFID in existing vector quantization architectures by up to $68\\%$ at 2 tokens\nper instance and $57\\%$ at 5 tokens. These improvements widen as codebook\nentries is reduced and allows for $7\\textit{--}45\\times$ speed-up in token\nsampling during inference. As an additional benefit, we find that smaller\nlatent spaces lead to MVQ identifying transferable visual representations where\nmultiple can be smoothly combined.\n","authors":["David D. Nguyen","David Leibowitz","Surya Nepal","Salil S. Kanhere"],"pdf_url":"https://arxiv.org/pdf/2301.06626v2.pdf","comment":"A newer version of this manuscript was archived under 2312.11735"},{"id":"http://arxiv.org/abs/2403.16338v1","updated":"2024-03-25T00:24:10Z","published":"2024-03-25T00:24:10Z","title":"Impact of Video Compression Artifacts on Fisheye Camera Visual\n  Perception Tasks","summary":"  Autonomous driving systems require extensive data collection schemes to cover\nthe diverse scenarios needed for building a robust and safe system. The data\nvolumes are in the order of Exabytes and have to be stored for a long period of\ntime (i.e., more than 10 years of the vehicle's life cycle). Lossless\ncompression doesn't provide sufficient compression ratios, hence, lossy video\ncompression has been explored. It is essential to prove that lossy video\ncompression artifacts do not impact the performance of the perception\nalgorithms. However, there is limited work in this area to provide a solid\nconclusion. In particular, there is no such work for fisheye cameras, which\nhave high radial distortion and where compression may have higher artifacts.\nFisheye cameras are commonly used in automotive systems for 3D object detection\ntask. In this work, we provide the first analysis of the impact of standard\nvideo compression codecs on wide FOV fisheye camera images. We demonstrate that\nthe achievable compression with negligible impact depends on the dataset and\ntemporal prediction of the video codec. We propose a radial distortion-aware\nzonal metric to evaluate the performance of artifacts in fisheye images. In\naddition, we present a novel method for estimating affine mode parameters of\nthe latest VVC codec, and suggest some areas for improvement in video codecs\nfor the application to fisheye imagery.\n","authors":["Madhumitha Sakthi","Louis Kerofsky","Varun Ravi Kumar","Senthil Yogamani"],"pdf_url":"https://arxiv.org/pdf/2403.16338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16335v1","updated":"2024-03-25T00:17:43Z","published":"2024-03-25T00:17:43Z","title":"MEDDAP: Medical Dataset Enhancement via Diversified Augmentation\n  Pipeline","summary":"  The effectiveness of Deep Neural Networks (DNNs) heavily relies on the\nabundance and accuracy of available training data. However, collecting and\nannotating data on a large scale is often both costly and time-intensive,\nparticularly in medical cases where practitioners are already occupied with\ntheir duties. Moreover, ensuring that the model remains robust across various\nscenarios of image capture is crucial in medical domains, especially when\ndealing with ultrasound images that vary based on the settings of different\ndevices and the manual operation of the transducer. To address this challenge,\nwe introduce a novel pipeline called MEDDAP, which leverages Stable Diffusion\n(SD) models to augment existing small datasets by automatically generating new\ninformative labeled samples. Pretrained checkpoints for SD are typically based\non natural images, and training them for medical images requires significant\nGPU resources due to their heavy parameters. To overcome this challenge, we\nintroduce USLoRA (Ultrasound Low-Rank Adaptation), a novel fine-tuning method\ntailored specifically for ultrasound applications. USLoRA allows for selective\nfine-tuning of weights within SD, requiring fewer than 0.1\\% of parameters\ncompared to fully fine-tuning only the UNet portion of SD. To enhance dataset\ndiversity, we incorporate different adjectives into the generation process\nprompts, thereby desensitizing the classifiers to intensity changes across\ndifferent images. This approach is inspired by clinicians' decision-making\nprocesses regarding breast tumors, where tumor shape often plays a more crucial\nrole than intensity. In conclusion, our pipeline not only outperforms\nclassifiers trained on the original dataset but also demonstrates superior\nperformance when encountering unseen datasets. The source code is available at\nhttps://github.com/yasamin-med/MEDDAP.\n","authors":["Yasamin Medghalchi","Niloufar Zakariaei","Arman Rahmim","Ilker Hacihaliloglu"],"pdf_url":"https://arxiv.org/pdf/2403.16335v1.pdf","comment":"submitted to miccai 2024 submitted to miccai 2024 Submitted to\n  MICCAI-2024"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2403.16702v1","updated":"2024-03-25T12:34:33Z","published":"2024-03-25T12:34:33Z","title":"ProCQA: A Large-scale Community-based Programming Question Answering\n  Dataset for Code Search","summary":"  Retrieval-based code question answering seeks to match user queries in\nnatural language to relevant code snippets. Previous approaches typically rely\non pretraining models using crafted bi-modal and uni-modal datasets to align\ntext and code representations. In this paper, we introduce ProCQA, a\nlarge-scale programming question answering dataset extracted from the\nStackOverflow community, offering naturally structured mixed-modal QA pairs. To\nvalidate its effectiveness, we propose a modality-agnostic contrastive\npre-training approach to improve the alignment of text and code representations\nof current code language models. Compared to previous models that primarily\nemploy bimodal and unimodal pairs extracted from CodeSearchNet for\npre-training, our model exhibits significant performance improvements across a\nwide range of code retrieval benchmarks.\n","authors":["Zehan Li","Jianfei Zhang","Chuantao Yin","Yuanxin Ouyang","Wenge Rong"],"pdf_url":"https://arxiv.org/pdf/2403.16702v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2311.16515v2","updated":"2024-03-25T12:01:59Z","published":"2023-11-25T14:24:49Z","title":"Word4Per: Zero-shot Composed Person Retrieval","summary":"  Searching for specific person has great social benefits and security value,\nand it often involves a combination of visual and textual information.\nConventional person retrieval methods, whether image-based or text-based,\nusually fall short in effectively harnessing both types of information, leading\nto the loss of accuracy. In this paper, a whole new task called Composed Person\nRetrieval (CPR) is proposed to jointly utilize both image and text information\nfor target person retrieval. However, the supervised CPR requires very costly\nmanual annotation dataset, while there are currently no available resources. To\nmitigate this issue, we firstly introduce the Zero-shot Composed Person\nRetrieval (ZS-CPR), which leverages existing domain-related data to resolve the\nCPR problem without expensive annotations. Secondly, to learn ZS-CPR model, we\npropose a two-stage learning framework, Word4Per, where a lightweight Textual\nInversion Network (TINet) and a text-based person retrieval model based on\nfine-tuned Contrastive Language-Image Pre-training (CLIP) network are learned\nwithout utilizing any CPR data. Thirdly, a finely annotated Image-Text Composed\nPerson Retrieval (ITCPR) dataset is built as the benchmark to assess the\nperformance of the proposed Word4Per framework. Extensive experiments under\nboth Rank-1 and mAP demonstrate the effectiveness of Word4Per for the ZS-CPR\ntask, surpassing the comparative methods by over 10\\%. The code and ITCPR\ndataset will be publicly available at\nhttps://github.com/Delong-liu-bupt/Word4Per.\n","authors":["Delong Liu","Haiwen Li","Zhicheng Zhao","Fei Su","Yuan Dong"],"pdf_url":"https://arxiv.org/pdf/2311.16515v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16630v1","updated":"2024-03-25T11:20:23Z","published":"2024-03-25T11:20:23Z","title":"A comparative analysis of embedding models for patent similarity","summary":"  This paper makes two contributions to the field of text-based patent\nsimilarity. First, it compares the performance of different kinds of\npatent-specific pretrained embedding models, namely static word embeddings\n(such as word2vec and doc2vec models) and contextual word embeddings (such as\ntransformers based models), on the task of patent similarity calculation.\nSecond, it compares specifically the performance of Sentence Transformers\n(SBERT) architectures with different training phases on the patent similarity\ntask. To assess the models' performance, we use information about patent\ninterferences, a phenomenon in which two or more patent claims belonging to\ndifferent patent applications are proven to be overlapping by patent examiners.\nTherefore, we use these interferences cases as a proxy for maximum similarity\nbetween two patents, treating them as ground-truth to evaluate the performance\nof the different embedding models. Our results point out that, first, Patent\nSBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer\narchitecture proposed in this research, outperforms the current\nstate-of-the-art in patent similarity. Second, they show that, in some cases,\nlarge static models performances are still comparable to contextual ones when\ntrained on extensive data; thus, we believe that the superiority in the\nperformance of contextual embeddings may not be related to the actual\narchitecture but rather to the way the training phase is performed.\n","authors":["Grazia Sveva Ascione","Valerio Sterzi"],"pdf_url":"https://arxiv.org/pdf/2403.16630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.08929v2","updated":"2024-03-25T08:00:38Z","published":"2023-06-15T08:02:07Z","title":"On the resilience of Collaborative Learning-based Recommender Systems\n  Against Community Detection Attack","summary":"  Collaborative-learning-based recommender systems emerged following the\nsuccess of collaborative learning techniques such as Federated Learning (FL)\nand Gossip Learning (GL). In these systems, users participate in the training\nof a recommender system while maintaining their history of consumed items on\ntheir devices. While these solutions seemed appealing for preserving the\nprivacy of the participants at first glance, recent studies have revealed that\ncollaborative learning can be vulnerable to various privacy attacks. In this\npaper, we study the resilience of collaborative learning-based recommender\nsystems against a novel privacy attack called Community Detection Attack (CDA).\nThis attack enables an adversary to identify community members based on a\nchosen set of items (eg., identifying users interested in specific\npoints-of-interest). Through experiments on three real recommendation datasets\nusing two state-of-the-art recommendation models, we evaluate the sensitivity\nof an FL-based recommender system as well as two flavors of Gossip\nLearning-based recommender systems to CDA. The results show that across all\nmodels and datasets, the FL setting is more vulnerable to CDA compared to\nGossip settings. Furthermore, we assess two off-the-shelf mitigation\nstrategies, namely differential privacy (DP) and a \\emph{Share less} policy,\nwhich consists of sharing a subset of less sensitive model parameters. The\nfindings indicate a more favorable privacy-utility trade-off for the\n\\emph{Share less} strategy, particularly in FedRecs.\n","authors":["Yacine Belal","Sonia Ben Mokhtar","Mohamed Maouche","Anthony Simonet-Boulogne"],"pdf_url":"https://arxiv.org/pdf/2306.08929v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16504v1","updated":"2024-03-25T07:38:40Z","published":"2024-03-25T07:38:40Z","title":"LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent\n  Classification","summary":"  Following the significant achievements of large language models (LLMs),\nresearchers have employed in-context learning for text classification tasks.\nHowever, these studies focused on monolingual, single-turn classification\ntasks. In this paper, we introduce LARA (Linguistic-Adaptive\nRetrieval-Augmented Language Models), designed to enhance accuracy in\nmulti-turn classification tasks across six languages, accommodating numerous\nintents in chatbot interactions. Multi-turn intent classification is notably\nchallenging due to the complexity and evolving nature of conversational\ncontexts. LARA tackles these issues by combining a fine-tuned smaller model\nwith a retrieval-augmented mechanism, integrated within the architecture of\nLLMs. This integration allows LARA to dynamically utilize past dialogues and\nrelevant intents, thereby improving the understanding of the context.\nFurthermore, our adaptive retrieval techniques bolster the cross-lingual\ncapabilities of LLMs without extensive retraining and fine-tune. Comprehensive\nexperiments demonstrate that LARA achieves state-of-the-art performance on\nmulti-turn intent classification tasks, enhancing the average accuracy by 3.67%\ncompared to existing methods.\n","authors":["Liu Junhua","Tan Yong Keat","Fu Bin"],"pdf_url":"https://arxiv.org/pdf/2403.16504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16435v1","updated":"2024-03-25T05:31:22Z","published":"2024-03-25T05:31:22Z","title":"InstUPR : Instruction-based Unsupervised Passage Reranking with Large\n  Language Models","summary":"  This paper introduces InstUPR, an unsupervised passage reranking method based\non large language models (LLMs). Different from existing approaches that rely\non extensive training with query-document pairs or retrieval-specific\ninstructions, our method leverages the instruction-following capabilities of\ninstruction-tuned LLMs for passage reranking without any additional\nfine-tuning. To achieve this, we introduce a soft score aggregation technique\nand employ pairwise reranking for unsupervised passage reranking. Experiments\non the BEIR benchmark demonstrate that InstUPR outperforms unsupervised\nbaselines as well as an instruction-tuned reranker, highlighting its\neffectiveness and superiority. Source code to reproduce all experiments is\nopen-sourced at https://github.com/MiuLab/InstUPR\n","authors":["Chao-Wei Huang","Yun-Nung Chen"],"pdf_url":"https://arxiv.org/pdf/2403.16435v1.pdf","comment":"Preprint. This manuscript was originally written and submitted in\n  June 2023"},{"id":"http://arxiv.org/abs/2403.16424v1","updated":"2024-03-25T05:04:52Z","published":"2024-03-25T05:04:52Z","title":"An Experiment with the Use of ChatGPT for LCSH Subject Assignment on\n  Electronic Theses and Dissertations","summary":"  This study delves into the potential use of Large Language Models (LLMs) for\ngenerating Library of Congress Subject Headings (LCSH). The authors employed\nChatGPT to generate subject headings for electronic theses and dissertations\n(ETDs) based on their titles and summaries. The results revealed that although\nsome generated subject headings were valid, there were issues regarding\nspecificity and exhaustiveness. The study showcases that LLMs can serve as a\nstrategic response to the backlog of items awaiting cataloging in academic\nlibraries, while also offering a cost-effective approach for promptly\ngenerating LCSH. Nonetheless, human catalogers remain essential for verifying\nand enhancing the validity, exhaustiveness, and specificity of LCSH generated\nby LLMs.\n","authors":["Eric H. C. Chow","TJ Kao","Xiaoli Li"],"pdf_url":"https://arxiv.org/pdf/2403.16424v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2403.16378v1","updated":"2024-03-25T02:52:42Z","published":"2024-03-25T02:52:42Z","title":"Play to Your Strengths: Collaborative Intelligence of Conventional\n  Recommender Models and Large Language Models","summary":"  The rise of large language models (LLMs) has opened new opportunities in\nRecommender Systems (RSs) by enhancing user behavior modeling and content\nunderstanding. However, current approaches that integrate LLMs into RSs solely\nutilize either LLM or conventional recommender model (CRM) to generate final\nrecommendations, without considering which data segments LLM or CRM excel in.\nTo fill in this gap, we conduct experiments on MovieLens-1M and Amazon-Books\ndatasets, and compare the performance of a representative CRM (DCNv2) and an\nLLM (LLaMA2-7B) on various groups of data samples. Our findings reveal that\nLLMs excel in data segments where CRMs exhibit lower confidence and precision,\nwhile samples where CRM excels are relatively challenging for LLM, requiring\nsubstantial training data and a long training time for comparable performance.\nThis suggests potential synergies in the combination between LLM and CRM.\nMotivated by these insights, we propose Collaborative Recommendation with\nconventional Recommender and Large Language Model (dubbed \\textit{CoReLLa}). In\nthis framework, we first jointly train LLM and CRM and address the issue of\ndecision boundary shifts through alignment loss. Then, the resource-efficient\nCRM, with a shorter inference time, handles simple and moderate samples, while\nLLM processes the small subset of challenging samples for CRM. Our experimental\nresults demonstrate that CoReLLa outperforms state-of-the-art CRM and LLM\nmethods significantly, underscoring its effectiveness in recommendation tasks.\n","authors":["Yunjia Xi","Weiwen Liu","Jianghao Lin","Chuhan Wu","Bo Chen","Ruiming Tang","Weinan Zhang","Yong Yu"],"pdf_url":"https://arxiv.org/pdf/2403.16378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16371v1","updated":"2024-03-25T02:31:57Z","published":"2024-03-25T02:31:57Z","title":"Uncovering Selective State Space Model's Capabilities in Lifelong\n  Sequential Recommendation","summary":"  Sequential Recommenders have been widely applied in various online services,\naiming to model users' dynamic interests from their sequential interactions.\nWith users increasingly engaging with online platforms, vast amounts of\nlifelong user behavioral sequences have been generated. However, existing\nsequential recommender models often struggle to handle such lifelong sequences.\nThe primary challenges stem from computational complexity and the ability to\ncapture long-range dependencies within the sequence. Recently, a state space\nmodel featuring a selective mechanism (i.e., Mamba) has emerged. In this work,\nwe investigate the performance of Mamba for lifelong sequential recommendation\n(i.e., length>=2k). More specifically, we leverage the Mamba block to model\nlifelong user sequences selectively. We conduct extensive experiments to\nevaluate the performance of representative sequential recommendation models in\nthe setting of lifelong sequences. Experiments on two real-world datasets\ndemonstrate the superiority of Mamba. We found that RecMamba achieves\nperformance comparable to the representative model while significantly reducing\ntraining duration by approximately 70% and memory costs by 80%. Codes and data\nare available at \\url{https://github.com/nancheng58/RecMamba}.\n","authors":["Jiyuan Yang","Yuanzi Li","Jingyu Zhao","Hanbing Wang","Muyang Ma","Jun Ma","Zhaochun Ren","Mengqi Zhang","Xin Xin","Zhumin Chen","Pengjie Ren"],"pdf_url":"https://arxiv.org/pdf/2403.16371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16345v1","updated":"2024-03-25T00:43:44Z","published":"2024-03-25T00:43:44Z","title":"Enhanced Facet Generation with LLM Editing","summary":"  In information retrieval, facet identification of a user query is an\nimportant task. If a search service can recognize the facets of a user's query,\nit has the potential to offer users a much broader range of search results.\nPrevious studies can enhance facet prediction by leveraging retrieved documents\nand related queries obtained through a search engine. However, there are\nchallenges in extending it to other applications when a search engine operates\nas part of the model. First, search engines are constantly updated. Therefore,\nadditional information may change during training and test, which may reduce\nperformance. The second challenge is that public search engines cannot search\nfor internal documents. Therefore, a separate search system needs to be built\nto incorporate documents from private domains within the company. We propose\ntwo strategies that focus on a framework that can predict facets by taking only\nqueries as input without a search engine. The first strategy is multi-task\nlearning to predict SERP. By leveraging SERP as a target instead of a source,\nthe proposed model deeply understands queries without relying on external\nmodules. The second strategy is to enhance the facets by combining Large\nLanguage Model (LLM) and the small model. Overall performance improves when\nsmall model and LLM are combined rather than facet generation individually.\n","authors":["Joosung Lee","Jinhong Kim"],"pdf_url":"https://arxiv.org/pdf/2403.16345v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.09738v4","updated":"2024-03-25T23:53:01Z","published":"2024-03-13T18:16:21Z","title":"Evaluating Large Language Models as Generative User Simulators for\n  Conversational Recommendation","summary":"  Synthetic users are cost-effective proxies for real users in the evaluation\nof conversational recommender systems. Large language models show promise in\nsimulating human-like behavior, raising the question of their ability to\nrepresent a diverse population of users. We introduce a new protocol to measure\nthe degree to which language models can accurately emulate human behavior in\nconversational recommendation. This protocol is comprised of five tasks, each\ndesigned to evaluate a key property that a synthetic user should exhibit:\nchoosing which items to talk about, expressing binary preferences, expressing\nopen-ended preferences, requesting recommendations, and giving feedback.\nThrough evaluation of baseline simulators, we demonstrate these tasks\neffectively reveal deviations of language models from human behavior, and offer\ninsights on how to reduce the deviations with model selection and prompting\nstrategies.\n","authors":["Se-eun Yoon","Zhankui He","Jessica Maria Echterhoff","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2403.09738v4.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.17264v1","updated":"2024-03-25T23:15:13Z","published":"2024-03-25T23:15:13Z","title":"EXPLORA: A teacher-apprentice methodology for eliciting natural\n  child-computer interactions","summary":"  Investigating child-computer interactions within their contexts is vital for\ndesigning technology that caters to children's needs. However, determining what\naspects of context are relevant for designing child-centric technology remains\na challenge. We introduce EXPLORA, a multimodal, multistage online methodology\ncomprising three pivotal stages: (1) building a teacher-apprentice\nrelationship,(2) learning from child-teachers, and (3) assessing and\nreinforcing researcher-apprentice learning. Central to EXPLORA is the\ncollection of attitudinal data through pre-observation interviews, offering\nresearchers a deeper understanding of children's characteristics and contexts.\nThis informs subsequent online observations, allowing researchers to focus on\nfrequent interactions. Furthermore, researchers can validate preliminary\nassumptions with children. A means-ends analysis framework aids in the\nsystematic analysis of data, shedding light on context, agency and\nhomework-information searching processes children employ in their activities.\nTo illustrate EXPLORA's capabilities, we present nine single case studies\ninvestigating Brazilian child-caregiver dyads' (children ages 9-11) use of\ntechnology in homework information-searching.\n","authors":["Vanessa Figueiredo","Catherine Ann Cameron"],"pdf_url":"https://arxiv.org/pdf/2403.17264v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17210v1","updated":"2024-03-25T21:37:31Z","published":"2024-03-25T21:37:31Z","title":"CADGL: Context-Aware Deep Graph Learning for Predicting Drug-Drug\n  Interactions","summary":"  Examining Drug-Drug Interactions (DDIs) is a pivotal element in the process\nof drug development. DDIs occur when one drug's properties are affected by the\ninclusion of other drugs. Detecting favorable DDIs has the potential to pave\nthe way for creating and advancing innovative medications applicable in\npractical settings. However, existing DDI prediction models continue to face\nchallenges related to generalization in extreme cases, robust feature\nextraction, and real-life application possibilities. We aim to address these\nchallenges by leveraging the effectiveness of context-aware deep graph learning\nby introducing a novel framework named CADGL. Based on a customized variational\ngraph autoencoder (VGAE), we capture critical structural and physio-chemical\ninformation using two context preprocessors for feature extraction from two\ndifferent perspectives: local neighborhood and molecular context, in a\nheterogeneous graphical structure. Our customized VGAE consists of a graph\nencoder, a latent information encoder, and an MLP decoder. CADGL surpasses\nother state-of-the-art DDI prediction models, excelling in predicting\nclinically valuable novel DDIs, supported by rigorous case studies.\n","authors":["Azmine Toushik Wasi","Taki Hasan Rafi","Raima Islam","Serbetar Karlo","Dong-Kyu Chae"],"pdf_url":"https://arxiv.org/pdf/2403.17210v1.pdf","comment":"8 Pages, 4 Figures; In review in IEEE/ACM Transactions on\n  Computational Biology and Bioinformatics"},{"id":"http://arxiv.org/abs/2403.17209v1","updated":"2024-03-25T21:37:30Z","published":"2024-03-25T21:37:30Z","title":"Generation of Asset Administration Shell with Large Language Model\n  Agents: Interoperability in Digital Twins with Semantic Node","summary":"  This research introduces a novel approach for assisting the creation of Asset\nAdministration Shell (AAS) instances for digital twin modeling within the\ncontext of Industry 4.0, aiming to enhance interoperability in smart\nmanufacturing and reduce manual effort. We construct a \"semantic node\" data\nstructure to capture the semantic essence of textual data. Then, a system\npowered by large language models is designed and implemented to process\n\"semantic node\" and generate AAS instance models from textual technical data.\nOur evaluation demonstrates a 62-79% effective generation rate, indicating a\nsubstantial proportion of manual creation effort can be converted into easier\nvalidation effort, thereby reducing the time and cost in creating AAS instance\nmodels. In our evaluation, a comparative analysis of different LLMs and an\nin-depth ablation study of Retrieval-Augmented Generation (RAG) mechanisms\nprovide insights into the effectiveness of LLM systems for interpreting\ntechnical concepts. Our findings emphasize LLMs' capability in automating AAS\ninstance creation, enhancing semantic interoperability, and contributing to the\nbroader field of semantic interoperability for digital twins in industrial\napplications. The prototype implementation and evaluation results are released\non our GitHub Repository with the link: https://github.com/YuchenXia/AASbyLLM\n","authors":["Yuchen Xia","Zhewen Xiao","Nasser Jazdi","Michael Weyrich"],"pdf_url":"https://arxiv.org/pdf/2403.17209v1.pdf","comment":"Pre-print, submitted to IEEE ACCESS, under peer-review"},{"id":"http://arxiv.org/abs/2403.17089v1","updated":"2024-03-25T18:25:10Z","published":"2024-03-25T18:25:10Z","title":"GOLF: Goal-Oriented Long-term liFe tasks supported by human-AI\n  collaboration","summary":"  The advent of ChatGPT and similar large language models (LLMs) has\nrevolutionized the human-AI interaction and information-seeking process.\nLeveraging LLMs as an alternative to search engines, users can now access\nsummarized information tailored to their queries, significantly reducing the\ncognitive load associated with navigating vast information resources. This\nshift underscores the potential of LLMs in redefining information access\nparadigms. Drawing on the foundation of task-focused information retrieval and\nLLMs' task planning ability, this research extends the scope of LLM\ncapabilities beyond routine task automation to support users in navigating\nlong-term and significant life tasks. It introduces the GOLF framework\n(Goal-Oriented Long-term liFe tasks), which focuses on enhancing LLMs' ability\nto assist in significant life decisions through goal orientation and long-term\nplanning. The methodology encompasses a comprehensive simulation study to test\nthe framework's efficacy, followed by model and human evaluations to develop a\ndataset benchmark for long-term life tasks, and experiments across different\nmodels and settings. By shifting the focus from short-term tasks to the broader\nspectrum of long-term life goals, this research underscores the transformative\npotential of LLMs in enhancing human decision-making processes and task\nmanagement, marking a significant step forward in the evolution of human-AI\ncollaboration.\n","authors":["Ben Wang"],"pdf_url":"https://arxiv.org/pdf/2403.17089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16948v1","updated":"2024-03-25T17:10:39Z","published":"2024-03-25T17:10:39Z","title":"Reinforcement Learning-based Recommender Systems with Large Language\n  Models for State Reward and Action Modeling","summary":"  Reinforcement Learning (RL)-based recommender systems have demonstrated\npromising performance in meeting user expectations by learning to make accurate\nnext-item recommendations from historical user-item interactions. However,\nexisting offline RL-based sequential recommendation methods face the challenge\nof obtaining effective user feedback from the environment. Effectively modeling\nthe user state and shaping an appropriate reward for recommendation remains a\nchallenge. In this paper, we leverage language understanding capabilities and\nadapt large language models (LLMs) as an environment (LE) to enhance RL-based\nrecommenders. The LE is learned from a subset of user-item interaction data,\nthus reducing the need for large training data, and can synthesise user\nfeedback for offline data by: (i) acting as a state model that produces high\nquality states that enrich the user representation, and (ii) functioning as a\nreward model to accurately capture nuanced user preferences on actions.\nMoreover, the LE allows to generate positive actions that augment the limited\noffline training data. We propose a LE Augmentation (LEA) method to further\nimprove recommendation performance by optimising jointly the supervised\ncomponent and the RL policy, using the augmented actions and historical user\nsignals. We use LEA, the state and reward models in conjunction with\nstate-of-the-art RL recommenders and report experimental results on two\npublicly available datasets.\n","authors":["Jie Wang","Alexandros Karatzoglou","Ioannis Arapakis","Joemon M. Jose"],"pdf_url":"https://arxiv.org/pdf/2403.16948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16778v1","updated":"2024-03-25T13:54:44Z","published":"2024-03-25T13:54:44Z","title":"GloSIS: The Global Soil Information System Web Ontology","summary":"  Established in 2012 by members of the Food and Agriculture Organisation\n(FAO), the Global Soil Partnership (GSP) is a global network of stakeholders\npromoting sound land and soil management practices towards a sustainable world\nfood system. However, soil survey largely remains a local or regional activity,\nbound to heterogeneous methods and conventions. Recognising the relevance of\nglobal and trans-national policies towards sustainable land management\npractices, the GSP elected data harmonisation and exchange as one of its key\nlines of action. Building upon international standards and previous work\ntowards a global soil data ontology, an improved domain model was eventually\ndeveloped within the GSP [54], the basis for a Global Soil Information System\n(GloSIS). This work also identified the Semantic Web as a possible avenue to\noperationalise the domain model. This article presents the GloSIS web ontology,\nan implementation of the GloSIS domain model with the Web Ontology Language\n(OWL). Thoroughly employing a host of Semantic Web standards (SOSA, SKOS,\nGeoSPARQL, QUDT), GloSIS lays out not only a soil data ontology but also an\nextensive set of ready-to-use code-lists for soil description and\nphysio-chemical analysis. Various examples are provided on the provision and\nuse of GloSIS-compliant linked data, showcasing the contribution of this\nontology to the discovery, exploration, integration and access of soil data.\n","authors":["Raul Palma","Bogusz Janiak","Luís Moreira de Sousa","Kathi Schleidt","Tomáš Řezník","Fenny van Egmond","Johan Leenaars","Dimitrios Moshou","Abdul Mouazen","Peter Wilson","David Medyckyj-Scott","Alistair Ritchie","Yusuf Yigini","Ronald Vargas"],"pdf_url":"https://arxiv.org/pdf/2403.16778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01744v2","updated":"2024-03-25T13:16:43Z","published":"2024-03-04T05:41:29Z","title":"NoteLLM: A Retrievable Large Language Model for Note Recommendation","summary":"  People enjoy sharing \"notes\" including their experiences within online\ncommunities. Therefore, recommending notes aligned with user interests has\nbecome a crucial task. Existing online methods only input notes into BERT-based\nmodels to generate note embeddings for assessing similarity. However, they may\nunderutilize some important cues, e.g., hashtags or categories, which represent\nthe key concepts of notes. Indeed, learning to generate hashtags/categories can\npotentially enhance note embeddings, both of which compress key note\ninformation into limited content. Besides, Large Language Models (LLMs) have\nsignificantly outperformed BERT in understanding natural languages. It is\npromising to introduce LLMs into note recommendation. In this paper, we propose\na novel unified framework called NoteLLM, which leverages LLMs to address the\nitem-to-item (I2I) note recommendation. Specifically, we utilize Note\nCompression Prompt to compress a note into a single special token, and further\nlearn the potentially related notes' embeddings via a contrastive learning\napproach. Moreover, we use NoteLLM to summarize the note and generate the\nhashtag/category automatically through instruction tuning. Extensive\nvalidations on real scenarios demonstrate the effectiveness of our proposed\nmethod compared with the online baseline and show major improvements in the\nrecommendation system of Xiaohongshu.\n","authors":["Chao Zhang","Shiwei Wu","Haoxin Zhang","Tong Xu","Yan Gao","Yao Hu","Di Wu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2403.01744v2.pdf","comment":"Published as a WWW'24 full paper"},{"id":"http://arxiv.org/abs/2403.16656v1","updated":"2024-03-25T11:47:53Z","published":"2024-03-25T11:47:53Z","title":"Graph Augmentation for Recommendation","summary":"  Graph augmentation with contrastive learning has gained significant attention\nin the field of recommendation systems due to its ability to learn expressive\nuser representations, even when labeled data is limited. However, directly\napplying existing GCL models to real-world recommendation environments poses\nchallenges. There are two primary issues to address. Firstly, the lack of\nconsideration for data noise in contrastive learning can result in noisy\nself-supervised signals, leading to degraded performance. Secondly, many\nexisting GCL approaches rely on graph neural network (GNN) architectures, which\ncan suffer from over-smoothing problems due to non-adaptive message passing. To\naddress these challenges, we propose a principled framework called GraphAug.\nThis framework introduces a robust data augmentor that generates denoised\nself-supervised signals, enhancing recommender systems. The GraphAug framework\nincorporates a graph information bottleneck (GIB)-regularized augmentation\nparadigm, which automatically distills informative self-supervision information\nand adaptively adjusts contrastive view generation. Through rigorous\nexperimentation on real-world datasets, we thoroughly assessed the performance\nof our novel GraphAug model. The outcomes consistently unveil its superiority\nover existing baseline methods. The source code for our model is publicly\navailable at: https://github.com/HKUDS/GraphAug.\n","authors":["Qianru Zhang","Lianghao Xia","Xuheng Cai","Siuming Yiu","Chao Huang","Christian S. Jensen"],"pdf_url":"https://arxiv.org/pdf/2403.16656v1.pdf","comment":"13 pages and accepted by ICDE 2024"},{"id":"http://arxiv.org/abs/2304.06182v4","updated":"2024-03-25T09:43:30Z","published":"2023-04-12T22:46:52Z","title":"GNNUERS: Fairness Explanation in GNNs for Recommendation via\n  Counterfactual Reasoning","summary":"  Nowadays, research into personalization has been focusing on explainability\nand fairness. Several approaches proposed in recent works are able to explain\nindividual recommendations in a post-hoc manner or by explanation paths.\nHowever, explainability techniques applied to unfairness in recommendation have\nbeen limited to finding user/item features mostly related to biased\nrecommendations. In this paper, we devised a novel algorithm that leverages\ncounterfactuality methods to discover user unfairness explanations in the form\nof user-item interactions. In our counterfactual framework, interactions are\nrepresented as edges in a bipartite graph, with users and items as nodes. Our\nbipartite graph explainer perturbs the topological structure to find an altered\nversion that minimizes the disparity in utility between the protected and\nunprotected demographic groups. Experiments on four real-world graphs coming\nfrom various domains showed that our method can systematically explain user\nunfairness on three state-of-the-art GNN-based recommendation models. Moreover,\nan empirical evaluation of the perturbed network uncovered relevant patterns\nthat justify the nature of the unfairness discovered by the generated\nexplanations. The source code and the preprocessed data sets are available at\nhttps://github.com/jackmedda/RS-BGExplainer.\n","authors":["Giacomo Medda","Francesco Fabbri","Mirko Marras","Ludovico Boratto","Gianni Fenu"],"pdf_url":"https://arxiv.org/pdf/2304.06182v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19710v1","updated":"2024-03-25T18:32:44Z","published":"2024-03-25T18:32:44Z","title":"STRUM-LLM: Attributed and Structured Contrastive Summarization","summary":"  Users often struggle with decision-making between two options (A vs B), as it\nusually requires time-consuming research across multiple web pages. We propose\nSTRUM-LLM that addresses this challenge by generating attributed, structured,\nand helpful contrastive summaries that highlight key differences between the\ntwo options. STRUM-LLM identifies helpful contrast: the specific attributes\nalong which the two options differ significantly and which are most likely to\ninfluence the user's decision. Our technique is domain-agnostic, and does not\nrequire any human-labeled data or fixed attribute list as supervision.\nSTRUM-LLM attributes all extractions back to the input sources along with\ntextual evidence, and it does not have a limit on the length of input sources\nthat it can process. STRUM-LLM Distilled has 100x more throughput than the\nmodels with comparable performance while being 10x smaller. In this paper, we\nprovide extensive evaluations for our method and lay out future directions for\nour currently deployed system.\n","authors":["Beliz Gunel","James B. Wendt","Jing Xie","Yichao Zhou","Nguyen Vo","Zachary Fisher","Sandeep Tata"],"pdf_url":"https://arxiv.org/pdf/2403.19710v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2403.16707v1","updated":"2024-03-25T12:44:52Z","published":"2024-03-25T12:44:52Z","title":"One-Shot Domain Incremental Learning","summary":"  Domain incremental learning (DIL) has been discussed in previous studies on\ndeep neural network models for classification. In DIL, we assume that samples\non new domains are observed over time. The models must classify inputs on all\ndomains. In practice, however, we may encounter a situation where we need to\nperform DIL under the constraint that the samples on the new domain are\nobserved only infrequently. Therefore, in this study, we consider the extreme\ncase where we have only one sample from the new domain, which we call one-shot\nDIL. We first empirically show that existing DIL methods do not work well in\none-shot DIL. We have analyzed the reason for this failure through various\ninvestigations. According to our analysis, we clarify that the difficulty of\none-shot DIL is caused by the statistics in the batch normalization layers.\nTherefore, we propose a technique regarding these statistics and demonstrate\nthe effectiveness of our technique through experiments on open datasets.\n","authors":["Yasushi Esaki","Satoshi Koide","Takuro Kutsuna"],"pdf_url":"https://arxiv.org/pdf/2403.16707v1.pdf","comment":"accepted at IEEE International Joint Conference on Neural Networks\n  (IJCNN) 2024"},{"id":"http://arxiv.org/abs/2403.16695v1","updated":"2024-03-25T12:26:32Z","published":"2024-03-25T12:26:32Z","title":"Assessing the Performance of Deep Learning for Automated Gleason Grading\n  in Prostate Cancer","summary":"  Prostate cancer is a dominant health concern calling for advanced diagnostic\ntools. Utilizing digital pathology and artificial intelligence, this study\nexplores the potential of 11 deep neural network architectures for automated\nGleason grading in prostate carcinoma focusing on comparing traditional and\nrecent architectures. A standardized image classification pipeline, based on\nthe AUCMEDI framework, facilitated robust evaluation using an in-house dataset\nconsisting of 34,264 annotated tissue tiles. The results indicated varying\nsensitivity across architectures, with ConvNeXt demonstrating the strongest\nperformance. Notably, newer architectures achieved superior performance, even\nthough with challenges in differentiating closely related Gleason grades. The\nConvNeXt model was capable of learning a balance between complexity and\ngeneralizability. Overall, this study lays the groundwork for enhanced Gleason\ngrading systems, potentially improving diagnostic efficiency for prostate\ncancer.\n","authors":["Dominik Müller","Philip Meyer","Lukas Rentschler","Robin Manz","Daniel Hieber","Jonas Bäcker","Samantha Cramer","Christoph Wengenmayr","Bruno Märkl","Ralf Huss","Frank Kramer","Iñaki Soto-Rey","Johannes Raffler"],"pdf_url":"https://arxiv.org/pdf/2403.16695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16689v1","updated":"2024-03-25T12:23:39Z","published":"2024-03-25T12:23:39Z","title":"Synapse: Learning Preferential Concepts from Visual Demonstrations","summary":"  This paper addresses the problem of preference learning, which aims to learn\nuser-specific preferences (e.g., \"good parking spot\", \"convenient drop-off\nlocation\") from visual input. Despite its similarity to learning factual\nconcepts (e.g., \"red cube\"), preference learning is a fundamentally harder\nproblem due to its subjective nature and the paucity of person-specific\ntraining data. We address this problem using a new framework called Synapse,\nwhich is a neuro-symbolic approach designed to efficiently learn preferential\nconcepts from limited demonstrations. Synapse represents preferences as\nneuro-symbolic programs in a domain-specific language (DSL) that operates over\nimages, and leverages a novel combination of visual parsing, large language\nmodels, and program synthesis to learn programs representing individual\npreferences. We evaluate Synapse through extensive experimentation including a\nuser case study focusing on mobility-related concepts in mobile robotics and\nautonomous driving. Our evaluation demonstrates that Synapse significantly\noutperforms existing baselines as well as its own ablations. The code and other\ndetails can be found on the project website https://amrl.cs.utexas.edu/synapse .\n","authors":["Sadanand Modak","Noah Patton","Isil Dillig","Joydeep Biswas"],"pdf_url":"https://arxiv.org/pdf/2403.16689v1.pdf","comment":"23 pages, 7 figures; Preprint"},{"id":"http://arxiv.org/abs/2403.16681v1","updated":"2024-03-25T12:15:55Z","published":"2024-03-25T12:15:55Z","title":"A note on generalization bounds for losses with finite moments","summary":"  This paper studies the truncation method from Alquier [1] to derive\nhigh-probability PAC-Bayes bounds for unbounded losses with heavy tails.\nAssuming that the $p$-th moment is bounded, the resulting bounds interpolate\nbetween a slow rate $1 / \\sqrt{n}$ when $p=2$, and a fast rate $1 / n$ when $p\n\\to \\infty$ and the loss is essentially bounded. Moreover, the paper derives a\nhigh-probability PAC-Bayes bound for losses with a bounded variance. This bound\nhas an exponentially better dependence on the confidence parameter and the\ndependency measure than previous bounds in the literature. Finally, the paper\nextends all results to guarantees in expectation and single-draw PAC-Bayes. In\norder to so, it obtains analogues of the PAC-Bayes fast rate bound for bounded\nlosses from [2] in these settings.\n","authors":["Borja Rodríguez-Gálvez","Omar Rivasplata","Ragnar Thobaben","Mikael Skoglund"],"pdf_url":"https://arxiv.org/pdf/2403.16681v1.pdf","comment":"9 pages: 5 of main text, 1 of references, and 3 of appendices"},{"id":"http://arxiv.org/abs/2403.16680v1","updated":"2024-03-25T12:15:47Z","published":"2024-03-25T12:15:47Z","title":"Symmetric Basis Convolutions for Learning Lagrangian Fluid Mechanics","summary":"  Learning physical simulations has been an essential and central aspect of\nmany recent research efforts in machine learning, particularly for\nNavier-Stokes-based fluid mechanics. Classic numerical solvers have\ntraditionally been computationally expensive and challenging to use in inverse\nproblems, whereas Neural solvers aim to address both concerns through machine\nlearning. We propose a general formulation for continuous convolutions using\nseparable basis functions as a superset of existing methods and evaluate a\nlarge set of basis functions in the context of (a) a compressible 1D SPH\nsimulation, (b) a weakly compressible 2D SPH simulation, and (c) an\nincompressible 2D SPH Simulation. We demonstrate that even and odd symmetries\nincluded in the basis functions are key aspects of stability and accuracy. Our\nbroad evaluation shows that Fourier-based continuous convolutions outperform\nall other architectures regarding accuracy and generalization. Finally, using\nthese Fourier-based networks, we show that prior inductive biases, such as\nwindow functions, are no longer necessary. An implementation of our approach,\nas well as complete datasets and solver implementations, is available at\nhttps://github.com/tum-pbs/SFBC.\n","authors":["Rene Winchenbach","Nils Thuerey"],"pdf_url":"https://arxiv.org/pdf/2403.16680v1.pdf","comment":"Published at International Conference on Learning Representation\n  (ICLR) 2024, 54 pages, 39 figures"},{"id":"http://arxiv.org/abs/2403.16678v1","updated":"2024-03-25T12:15:42Z","published":"2024-03-25T12:15:42Z","title":"DeepGleason: a System for Automated Gleason Grading of Prostate Cancer\n  using Deep Neural Networks","summary":"  Advances in digital pathology and artificial intelligence (AI) offer\npromising opportunities for clinical decision support and enhancing diagnostic\nworkflows. Previous studies already demonstrated AI's potential for automated\nGleason grading, but lack state-of-the-art methodology and model reusability.\nTo address this issue, we propose DeepGleason: an open-source deep neural\nnetwork based image classification system for automated Gleason grading using\nwhole-slide histopathology images from prostate tissue sections. Implemented\nwith the standardized AUCMEDI framework, our tool employs a tile-wise\nclassification approach utilizing fine-tuned image preprocessing techniques in\ncombination with a ConvNeXt architecture which was compared to various\nstate-of-the-art architectures. The neural network model was trained and\nvalidated on an in-house dataset of 34,264 annotated tiles from 369 prostate\ncarcinoma slides. We demonstrated that DeepGleason is capable of highly\naccurate and reliable Gleason grading with a macro-averaged F1-score of 0.806,\nAUC of 0.991, and Accuracy of 0.974. The internal architecture comparison\nrevealed that the ConvNeXt model was superior performance-wise on our dataset\nto established and other modern architectures like transformers. Furthermore,\nwe were able to outperform the current state-of-the-art in tile-wise\nfine-classification with a sensitivity and specificity of 0.94 and 0.98 for\nbenign vs malignant detection as well as of 0.91 and 0.75 for Gleason 3 vs\nGleason 4 & 5 classification, respectively. Our tool contributes to the wider\nadoption of AI-based Gleason grading within the research community and paves\nthe way for broader clinical application of deep learning models in digital\npathology. DeepGleason is open-source and publicly available for research\napplication in the following Git repository:\nhttps://github.com/frankkramer-lab/DeepGleason.\n","authors":["Dominik Müller","Philip Meyer","Lukas Rentschler","Robin Manz","Jonas Bäcker","Samantha Cramer","Christoph Wengenmayr","Bruno Märkl","Ralf Huss","Iñaki Soto-Rey","Johannes Raffler"],"pdf_url":"https://arxiv.org/pdf/2403.16678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16677v1","updated":"2024-03-25T12:14:48Z","published":"2024-03-25T12:14:48Z","title":"FOOL: Addressing the Downlink Bottleneck in Satellite Computing with\n  Neural Feature Compression","summary":"  Nanosatellite constellations equipped with sensors capturing large geographic\nregions provide unprecedented opportunities for Earth observation. As\nconstellation sizes increase, network contention poses a downlink bottleneck.\nOrbital Edge Computing (OEC) leverages limited onboard compute resources to\nreduce transfer costs by processing the raw captures at the source. However,\ncurrent solutions have limited practicability due to reliance on crude\nfiltering methods or over-prioritizing particular downstream tasks.\n  This work presents FOOL, an OEC-native and task-agnostic feature compression\nmethod that preserves prediction performance. FOOL partitions high-resolution\nsatellite imagery to maximize throughput. Further, it embeds context and\nleverages inter-tile dependencies to lower transfer costs with negligible\noverhead. While FOOL is a feature compressor, it can recover images with\ncompetitive scores on perceptual quality measures at lower bitrates. We\nextensively evaluate transfer cost reduction by including the peculiarity of\nintermittently available network connections in low earth orbit. Lastly, we\ntest the feasibility of our system for standardized nanosatellite form factors.\nWe demonstrate that FOOL permits downlinking over 100x the data volume without\nrelying on prior information on the downstream tasks.\n","authors":["Alireza Furutanpey","Qiyang Zhang","Philipp Raith","Tobias Pfandzelter","Shangguang Wang","Schahram Dustdar"],"pdf_url":"https://arxiv.org/pdf/2403.16677v1.pdf","comment":"18 pages, double column, 19 figures, 7 tables, Initial Submission to\n  IEEE Transactions on Mobile Computing"},{"id":"http://arxiv.org/abs/2403.16674v1","updated":"2024-03-25T12:13:20Z","published":"2024-03-25T12:13:20Z","title":"Understanding the Functional Roles of Modelling Components in Spiking\n  Neural Networks","summary":"  Spiking neural networks (SNNs), inspired by the neural circuits of the brain,\nare promising in achieving high computational efficiency with biological\nfidelity. Nevertheless, it is quite difficult to optimize SNNs because the\nfunctional roles of their modelling components remain unclear. By designing and\nevaluating several variants of the classic model, we systematically investigate\nthe functional roles of key modelling components, leakage, reset, and\nrecurrence, in leaky integrate-and-fire (LIF) based SNNs. Through extensive\nexperiments, we demonstrate how these components influence the accuracy,\ngeneralization, and robustness of SNNs. Specifically, we find that the leakage\nplays a crucial role in balancing memory retention and robustness, the reset\nmechanism is essential for uninterrupted temporal processing and computational\nefficiency, and the recurrence enriches the capability to model complex\ndynamics at a cost of robustness degradation. With these interesting\nobservations, we provide optimization suggestions for enhancing the performance\nof SNNs in different scenarios. This work deepens the understanding of how SNNs\nwork, which offers valuable guidance for the development of more effective and\nrobust neuromorphic models.\n","authors":["Huifeng Yin","Hanle Zheng","Jiayi Mao","Siyuan Ding","Xing Liu","Mingkun Xu","Yifan Hu","Jing Pei","Lei Deng"],"pdf_url":"https://arxiv.org/pdf/2403.16674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02930v2","updated":"2024-03-25T12:07:13Z","published":"2024-03-05T12:48:29Z","title":"A Second Look on BASS -- Boosting Abstractive Summarization with Unified\n  Semantic Graphs -- A Replication Study","summary":"  We present a detailed replication study of the BASS framework, an abstractive\nsummarization system based on the notion of Unified Semantic Graphs. Our\ninvestigation includes challenges in replicating key components and an ablation\nstudy to systematically isolate error sources rooted in replicating novel\ncomponents. Our findings reveal discrepancies in performance compared to the\noriginal work. We highlight the significance of paying careful attention even\nto reasonably omitted details for replicating advanced frameworks like BASS,\nand emphasize key practices for writing replicable papers.\n","authors":["Osman Alperen Koraş","Jörg Schlötterer","Christin Seifert"],"pdf_url":"https://arxiv.org/pdf/2403.02930v2.pdf","comment":"This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Advances in Information Retrieval, 46th European Conference on\n  Information Retrieval, ECIR 2024. 16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.14587v2","updated":"2024-03-25T12:00:19Z","published":"2024-03-21T17:42:45Z","title":"An Analysis of Linear Time Series Forecasting Models","summary":"  Despite their simplicity, linear models perform well at time series\nforecasting, even when pitted against deeper and more expensive models. A\nnumber of variations to the linear model have been proposed, often including\nsome form of feature normalisation that improves model generalisation. In this\npaper we analyse the sets of functions expressible using these linear model\narchitectures. In so doing we show that several popular variants of linear\nmodels for time series forecasting are equivalent and functionally\nindistinguishable from standard, unconstrained linear regression. We\ncharacterise the model classes for each linear variant. We demonstrate that\neach model can be reinterpreted as unconstrained linear regression over a\nsuitably augmented feature set, and therefore admit closed-form solutions when\nusing a mean-squared loss function. We provide experimental evidence that the\nmodels under inspection learn nearly identical solutions, and finally\ndemonstrate that the simpler closed form solutions are superior forecasters\nacross 72% of test settings.\n","authors":["William Toner","Luke Darlow"],"pdf_url":"https://arxiv.org/pdf/2403.14587v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16656v1","updated":"2024-03-25T11:47:53Z","published":"2024-03-25T11:47:53Z","title":"Graph Augmentation for Recommendation","summary":"  Graph augmentation with contrastive learning has gained significant attention\nin the field of recommendation systems due to its ability to learn expressive\nuser representations, even when labeled data is limited. However, directly\napplying existing GCL models to real-world recommendation environments poses\nchallenges. There are two primary issues to address. Firstly, the lack of\nconsideration for data noise in contrastive learning can result in noisy\nself-supervised signals, leading to degraded performance. Secondly, many\nexisting GCL approaches rely on graph neural network (GNN) architectures, which\ncan suffer from over-smoothing problems due to non-adaptive message passing. To\naddress these challenges, we propose a principled framework called GraphAug.\nThis framework introduces a robust data augmentor that generates denoised\nself-supervised signals, enhancing recommender systems. The GraphAug framework\nincorporates a graph information bottleneck (GIB)-regularized augmentation\nparadigm, which automatically distills informative self-supervision information\nand adaptively adjusts contrastive view generation. Through rigorous\nexperimentation on real-world datasets, we thoroughly assessed the performance\nof our novel GraphAug model. The outcomes consistently unveil its superiority\nover existing baseline methods. The source code for our model is publicly\navailable at: https://github.com/HKUDS/GraphAug.\n","authors":["Qianru Zhang","Lianghao Xia","Xuheng Cai","Siuming Yiu","Chao Huang","Christian S. Jensen"],"pdf_url":"https://arxiv.org/pdf/2403.16656v1.pdf","comment":"13 pages and accepted by ICDE 2024"},{"id":"http://arxiv.org/abs/2403.16654v1","updated":"2024-03-25T11:42:01Z","published":"2024-03-25T11:42:01Z","title":"A Novel Loss Function-based Support Vector Machine for Binary\n  Classification","summary":"  The previous support vector machine(SVM) including $0/1$ loss SVM, hinge loss\nSVM, ramp loss SVM, truncated pinball loss SVM, and others, overlooked the\ndegree of penalty for the correctly classified samples within the margin. This\noversight affects the generalization ability of the SVM classifier to some\nextent. To address this limitation, from the perspective of confidence margin,\nwe propose a novel Slide loss function ($\\ell_s$) to construct the support\nvector machine classifier($\\ell_s$-SVM). By introducing the concept of proximal\nstationary point, and utilizing the property of Lipschitz continuity, we derive\nthe first-order optimality conditions for $\\ell_s$-SVM. Based on this, we\ndefine the $\\ell_s$ support vectors and working set of $\\ell_s$-SVM. To\nefficiently handle $\\ell_s$-SVM, we devise a fast alternating direction method\nof multipliers with the working set ($\\ell_s$-ADMM), and provide the\nconvergence analysis. The numerical experiments on real world datasets confirm\nthe robustness and effectiveness of the proposed method.\n","authors":["Yan Li","Liping Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11922v3","updated":"2024-03-25T11:39:57Z","published":"2024-02-19T08:11:26Z","title":"Spatio-Temporal Few-Shot Learning via Diffusive Neural Network\n  Generation","summary":"  Spatio-temporal modeling is foundational for smart city applications, yet it\nis often hindered by data scarcity in many cities and regions. To bridge this\ngap, we propose a novel generative pre-training framework, GPD, for\nspatio-temporal few-shot learning with urban knowledge transfer. Unlike\nconventional approaches that heavily rely on common feature extraction or\nintricate few-shot learning designs, our solution takes a novel approach by\nperforming generative pre-training on a collection of neural network parameters\noptimized with data from source cities. We recast spatio-temporal few-shot\nlearning as pre-training a generative diffusion model, which generates tailored\nneural networks guided by prompts, allowing for adaptability to diverse data\ndistributions and city-specific characteristics. GPD employs a\nTransformer-based denoising diffusion model, which is model-agnostic to\nintegrate with powerful spatio-temporal neural networks. By addressing\nchallenges arising from data gaps and the complexity of generalizing knowledge\nacross cities, our framework consistently outperforms state-of-the-art\nbaselines on multiple real-world datasets for tasks such as traffic speed\nprediction and crowd flow prediction. The implementation of our approach is\navailable: https://github.com/tsinghua-fib-lab/GPD.\n","authors":["Yuan Yuan","Chenyang Shao","Jingtao Ding","Depeng Jin","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2402.11922v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16644v1","updated":"2024-03-25T11:29:32Z","published":"2024-03-25T11:29:32Z","title":"Bridging the Sim-to-Real Gap with Bayesian Inference","summary":"  We present SIM-FSVGD for learning robot dynamics from data. As opposed to\ntraditional methods, SIM-FSVGD leverages low-fidelity physical priors, e.g., in\nthe form of simulators, to regularize the training of neural network models.\nWhile learning accurate dynamics already in the low data regime, SIM-FSVGD\nscales and excels also when more data is available. We empirically show that\nlearning with implicit physical priors results in accurate mean model\nestimation as well as precise uncertainty quantification. We demonstrate the\neffectiveness of SIM-FSVGD in bridging the sim-to-real gap on a\nhigh-performance RC racecar system. Using model-based RL, we demonstrate a\nhighly dynamic parking maneuver with drifting, using less than half the data\ncompared to the state of the art.\n","authors":["Jonas Rothfuss","Bhavya Sukhija","Lenart Treven","Florian Dörfler","Stelian Coros","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2403.16644v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16640v1","updated":"2024-03-25T11:28:52Z","published":"2024-03-25T11:28:52Z","title":"Multi-Scale Texture Loss for CT denoising with GANs","summary":"  Generative Adversarial Networks (GANs) have proved as a powerful framework\nfor denoising applications in medical imaging. However, GAN-based denoising\nalgorithms still suffer from limitations in capturing complex relationships\nwithin the images. In this regard, the loss function plays a crucial role in\nguiding the image generation process, encompassing how much a synthetic image\ndiffers from a real image. To grasp highly complex and non-linear textural\nrelationships in the training process, this work presents a loss function that\nleverages the intrinsic multi-scale nature of the Gray-Level-Co-occurrence\nMatrix (GLCM). Although the recent advances in deep learning have demonstrated\nsuperior performance in classification and detection tasks, we hypothesize that\nits information content can be valuable when integrated into GANs' training. To\nthis end, we propose a differentiable implementation of the GLCM suited for\ngradient-based optimization. Our approach also introduces a self-attention\nlayer that dynamically aggregates the multi-scale texture information extracted\nfrom the images. We validate our approach by carrying out extensive experiments\nin the context of low-dose CT denoising, a challenging application that aims to\nenhance the quality of noisy CT scans. We utilize three publicly available\ndatasets, including one simulated and two real datasets. The results are\npromising as compared to other well-established loss functions, being also\nconsistent across three different GAN architectures. The code is available at:\nhttps://github.com/FrancescoDiFeola/DenoTextureLoss\n","authors":["Francesco Di Feola","Lorenzo Tronchin","Valerio Guarrasi","Paolo Soda"],"pdf_url":"https://arxiv.org/pdf/2403.16640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16630v1","updated":"2024-03-25T11:20:23Z","published":"2024-03-25T11:20:23Z","title":"A comparative analysis of embedding models for patent similarity","summary":"  This paper makes two contributions to the field of text-based patent\nsimilarity. First, it compares the performance of different kinds of\npatent-specific pretrained embedding models, namely static word embeddings\n(such as word2vec and doc2vec models) and contextual word embeddings (such as\ntransformers based models), on the task of patent similarity calculation.\nSecond, it compares specifically the performance of Sentence Transformers\n(SBERT) architectures with different training phases on the patent similarity\ntask. To assess the models' performance, we use information about patent\ninterferences, a phenomenon in which two or more patent claims belonging to\ndifferent patent applications are proven to be overlapping by patent examiners.\nTherefore, we use these interferences cases as a proxy for maximum similarity\nbetween two patents, treating them as ground-truth to evaluate the performance\nof the different embedding models. Our results point out that, first, Patent\nSBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer\narchitecture proposed in this research, outperforms the current\nstate-of-the-art in patent similarity. Second, they show that, in some cases,\nlarge static models performances are still comparable to contextual ones when\ntrained on extensive data; thus, we believe that the superiority in the\nperformance of contextual embeddings may not be related to the actual\narchitecture but rather to the way the training phase is performed.\n","authors":["Grazia Sveva Ascione","Valerio Sterzi"],"pdf_url":"https://arxiv.org/pdf/2403.16630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.03472v2","updated":"2024-03-25T10:58:22Z","published":"2023-08-07T11:02:44Z","title":"Improving the forecast accuracy of wind power by leveraging multiple\n  hierarchical structure","summary":"  Renewable energy generation is of utmost importance for global\ndecarbonization. Forecasting renewable energies, particularly wind energy, is\nchallenging due to the inherent uncertainty in wind energy generation, which\ndepends on weather conditions. Recent advances in hierarchical forecasting\nthrough reconciliation have demonstrated a significant increase in the quality\nof wind energy forecasts for short-term periods. We leverage the\ncross-sectional and temporal hierarchical structure of turbines in wind farms\nand build cross-temporal hierarchies to further investigate how integrated\ncross-sectional and temporal dimensions can add value to forecast accuracy in\nwind farms. We found that cross-temporal reconciliation was superior to\nindividual cross-sectional reconciliation at multiple temporal aggregations.\nAdditionally, machine learning based forecasts that were cross-temporally\nreconciled demonstrated high accuracy at coarser temporal granularities, which\nmay encourage adoption for short-term wind forecasts. Empirically, we provide\ninsights for decision-makers on the best methods for forecasting high-frequency\nwind data across different forecasting horizons and levels.\n","authors":["Lucas English","Mahdi Abolghasemi"],"pdf_url":"https://arxiv.org/pdf/2308.03472v2.pdf","comment":"41 pages, 14 figures"},{"id":"http://arxiv.org/abs/2403.16612v1","updated":"2024-03-25T10:42:48Z","published":"2024-03-25T10:42:48Z","title":"Calibrating Bayesian UNet++ for Sub-Seasonal Forecasting","summary":"  Seasonal forecasting is a crucial task when it comes to detecting the extreme\nheat and colds that occur due to climate change. Confidence in the predictions\nshould be reliable since a small increase in the temperatures in a year has a\nbig impact on the world. Calibration of the neural networks provides a way to\nensure our confidence in the predictions. However, calibrating regression\nmodels is an under-researched topic, especially in forecasters. We calibrate a\nUNet++ based architecture, which was shown to outperform physics-based models\nin temperature anomalies. We show that with a slight trade-off between\nprediction error and calibration error, it is possible to get more reliable and\nsharper forecasts. We believe that calibration should be an important part of\nsafety-critical machine learning applications such as weather forecasters.\n","authors":["Busra Asan","Abdullah Akgul","Alper Unal","Melih Kandemir","Gozde Unal"],"pdf_url":"https://arxiv.org/pdf/2403.16612v1.pdf","comment":"Accepted as a workshop paper at \"ICLR 2024 Tackling Climate Change\n  with Machine Learning\""},{"id":"http://arxiv.org/abs/2403.16610v1","updated":"2024-03-25T10:40:04Z","published":"2024-03-25T10:40:04Z","title":"Distributed collaborative anomalous sound detection by embedding sharing","summary":"  To develop a machine sound monitoring system, a method for detecting\nanomalous sound is proposed. In this paper, we explore a method for multiple\nclients to collaboratively learn an anomalous sound detection model while\nkeeping their raw data private from each other. In the context of industrial\nmachine anomalous sound detection, each client possesses data from different\nmachines or different operational states, making it challenging to learn\nthrough federated learning or split learning. In our proposed method, each\nclient calculates embeddings using a common pre-trained model developed for\nsound data classification, and these calculated embeddings are aggregated on\nthe server to perform anomalous sound detection through outlier exposure.\nExperiments showed that our proposed method improves the AUC of anomalous sound\ndetection by an average of 6.8%.\n","authors":["Kota Dohi","Yohei Kawaguchi"],"pdf_url":"https://arxiv.org/pdf/2403.16610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16607v1","updated":"2024-03-25T10:38:17Z","published":"2024-03-25T10:38:17Z","title":"Enhancing Industrial Transfer Learning with Style Filter: Cost Reduction\n  and Defect-Focus","summary":"  Addressing the challenge of data scarcity in industrial domains, transfer\nlearning emerges as a pivotal paradigm. This work introduces Style Filter, a\ntailored methodology for industrial contexts. By selectively filtering source\ndomain data before knowledge transfer, Style Filter reduces the quantity of\ndata while maintaining or even enhancing the performance of transfer learning\nstrategy. Offering label-free operation, minimal reliance on prior knowledge,\nindependence from specific models, and re-utilization, Style Filter is\nevaluated on authentic industrial datasets, highlighting its effectiveness when\nemployed before conventional transfer strategies in the deep learning domain.\nThe results underscore the effectiveness of Style Filter in real-world\nindustrial applications.\n","authors":["Chen Li","Ruijie Ma","Xiang Qian","Xiaohao Wang","Xinghui Li"],"pdf_url":"https://arxiv.org/pdf/2403.16607v1.pdf","comment":"17 pages, 11 figures,4 tables"},{"id":"http://arxiv.org/abs/2403.16594v1","updated":"2024-03-25T10:13:52Z","published":"2024-03-25T10:13:52Z","title":"EDUE: Expert Disagreement-Guided One-Pass Uncertainty Estimation for\n  Medical Image Segmentation","summary":"  Deploying deep learning (DL) models in medical applications relies on\npredictive performance and other critical factors, such as conveying\ntrustworthy predictive uncertainty. Uncertainty estimation (UE) methods provide\npotential solutions for evaluating prediction reliability and improving the\nmodel confidence calibration. Despite increasing interest in UE, challenges\npersist, such as the need for explicit methods to capture aleatoric uncertainty\nand align uncertainty estimates with real-life disagreements among domain\nexperts. This paper proposes an Expert Disagreement-Guided Uncertainty\nEstimation (EDUE) for medical image segmentation. By leveraging variability in\nground-truth annotations from multiple raters, we guide the model during\ntraining and incorporate random sampling-based strategies to enhance\ncalibration confidence. Our method achieves 55% and 23% improvement in\ncorrelation on average with expert disagreements at the image and pixel levels,\nrespectively, better calibration, and competitive segmentation performance\ncompared to the state-of-the-art deep ensembles, requiring only a single\nforward pass.\n","authors":["Kudaibergen Abutalip","Numan Saeed","Ikboljon Sobirov","Vincent Andrearczyk","Adrien Depeursinge","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2403.16594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16591v1","updated":"2024-03-25T10:06:45Z","published":"2024-03-25T10:06:45Z","title":"Deciphering the Interplay between Local Differential Privacy, Average\n  Bayesian Privacy, and Maximum Bayesian Privacy","summary":"  The swift evolution of machine learning has led to emergence of various\ndefinitions of privacy due to the threats it poses to privacy, including the\nconcept of local differential privacy (LDP). Although widely embraced and\nutilized across numerous domains, this conventional approach to measure privacy\nstill exhibits certain limitations, spanning from failure to prevent\ninferential disclosure to lack of consideration for the adversary's background\nknowledge. In this comprehensive study, we introduce Bayesian privacy and delve\ninto the intricate relationship between local differential privacy and its\nBayesian counterparts, unveiling novel insights into utility-privacy\ntrade-offs. We introduce a framework that encapsulates both attack and defense\nstrategies, highlighting their interplay and effectiveness. Our theoretical\ncontributions are anchored in the rigorous definitions and relationships\nbetween Average Bayesian Privacy (ABP) and Maximum Bayesian Privacy (MBP),\nencapsulated by equations $\\epsilon_{p,a} \\leq\n\\frac{1}{\\sqrt{2}}\\sqrt{(\\epsilon_{p,m} + \\epsilon)\\cdot(e^{\\epsilon_{p,m} +\n\\epsilon} - 1)}$ and the equivalence between $\\xi$-MBP and $2\\xi$-LDP\nestablished under uniform prior distribution. These relationships fortify our\nunderstanding of the privacy guarantees provided by various mechanisms, leading\nto the realization that a mechanism satisfying $\\xi$-LDP also confers\n$\\xi$-MBP, and vice versa. Our work not only lays the groundwork for future\nempirical exploration but also promises to enhance the design of\nprivacy-preserving algorithms that do not compromise on utility, thereby\nfostering the development of trustworthy machine learning solutions.\n","authors":["Xiaojin Zhang","Yulin Fei","Wei Chen","Hai Jin"],"pdf_url":"https://arxiv.org/pdf/2403.16591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16582v1","updated":"2024-03-25T09:49:42Z","published":"2024-03-25T09:49:42Z","title":"In the Search for Optimal Multi-view Learning Models for Crop\n  Classification with Global Remote Sensing Data","summary":"  Crop classification is of critical importance due to its role in studying\ncrop pattern changes, resource management, and carbon sequestration. When\nemploying data-driven techniques for its prediction, utilizing various temporal\ndata sources is necessary. Deep learning models have proven to be effective for\nthis task by mapping time series data to high-level representation for\nprediction. However, they face substantial challenges when dealing with\nmultiple input patterns. The literature offers limited guidance for Multi-View\nLearning (MVL) scenarios, as it has primarily focused on exploring fusion\nstrategies with specific encoders and validating them in local regions. In\ncontrast, we investigate the impact of simultaneous selection of the fusion\nstrategy and the encoder architecture evaluated on a global-scale cropland and\ncrop-type classifications. We use a range of five fusion strategies (Input,\nFeature, Decision, Ensemble, Hybrid) and five temporal encoder architectures\n(LSTM, GRU, TempCNN, TAE, L-TAE) as possible MVL model configurations. The\nvalidation is on the CropHarvest dataset that provides optical, radar, and\nweather time series, and topographic information as input data. We found that\nin scenarios with a limited number of labeled samples, a unique configuration\nis insufficient for all the cases. Instead, a specialized combination,\nincluding encoder and fusion strategy, should be meticulously sought. To\nstreamline this search process, we suggest initially identifying the optimal\nencoder architecture tailored for a particular fusion strategy, and then\ndetermining the most suitable fusion strategy for the classification task. We\nprovide a technical framework for researchers exploring crop classification or\nrelated tasks through a MVL approach.\n","authors":["Francisco Mena","Diego Arenas","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2403.16582v1.pdf","comment":"submitted to journal"},{"id":"http://arxiv.org/abs/2403.10615v2","updated":"2024-03-25T09:42:13Z","published":"2024-03-15T18:26:33Z","title":"LightIt: Illumination Modeling and Control for Diffusion Models","summary":"  We introduce LightIt, a method for explicit illumination control for image\ngeneration. Recent generative methods lack lighting control, which is crucial\nto numerous artistic aspects of image generation such as setting the overall\nmood or cinematic appearance. To overcome these limitations, we propose to\ncondition the generation on shading and normal maps. We model the lighting with\nsingle bounce shading, which includes cast shadows. We first train a shading\nestimation module to generate a dataset of real-world images and shading pairs.\nThen, we train a control network using the estimated shading and normals as\ninput. Our method demonstrates high-quality image generation and lighting\ncontrol in numerous scenes. Additionally, we use our generated dataset to train\nan identity-preserving relighting model, conditioned on an image and a target\nshading. Our method is the first that enables the generation of images with\ncontrollable, consistent lighting and performs on par with specialized\nrelighting state-of-the-art methods.\n","authors":["Peter Kocsis","Julien Philip","Kalyan Sunkavalli","Matthias Nießner","Yannick Hold-Geoffroy"],"pdf_url":"https://arxiv.org/pdf/2403.10615v2.pdf","comment":"Project page: https://peter-kocsis.github.io/LightIt/ Video:\n  https://youtu.be/cCfSBD5aPLI"},{"id":"http://arxiv.org/abs/2403.16576v1","updated":"2024-03-25T09:41:49Z","published":"2024-03-25T09:41:49Z","title":"Antigen-Specific Antibody Design via Direct Energy-based Preference\n  Optimization","summary":"  Antibody design, a crucial task with significant implications across various\ndisciplines such as therapeutics and biology, presents considerable challenges\ndue to its intricate nature. In this paper, we tackle antigen-specific antibody\ndesign as a protein sequence-structure co-design problem, considering both\nrationality and functionality. Leveraging a pre-trained conditional diffusion\nmodel that jointly models sequences and structures of\ncomplementarity-determining regions (CDR) in antibodies with equivariant neural\nnetworks, we propose direct energy-based preference optimization to guide the\ngeneration of antibodies with both rational structures and considerable binding\naffinities to given antigens. Our method involves fine-tuning the pre-trained\ndiffusion model using a residue-level decomposed energy preference.\nAdditionally, we employ gradient surgery to address conflicts between various\ntypes of energy, such as attraction and repulsion. Experiments on RAbD\nbenchmark show that our approach effectively optimizes the energy of generated\nantibodies and achieves state-of-the-art performance in designing high-quality\nantibodies with low total energy and high binding affinity, demonstrating the\nsuperiority of our approach.\n","authors":["Xiangxin Zhou","Dongyu Xue","Ruizhe Chen","Zaixiang Zheng","Liang Wang","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2403.16576v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16571v1","updated":"2024-03-25T09:36:51Z","published":"2024-03-25T09:36:51Z","title":"NSINA: A News Corpus for Sinhala","summary":"  The introduction of large language models (LLMs) has advanced natural\nlanguage processing (NLP), but their effectiveness is largely dependent on\npre-training resources. This is especially evident in low-resource languages,\nsuch as Sinhala, which face two primary challenges: the lack of substantial\ntraining data and limited benchmarking datasets. In response, this study\nintroduces NSINA, a comprehensive news corpus of over 500,000 articles from\npopular Sinhala news websites, along with three NLP tasks: news media\nidentification, news category prediction, and news headline generation. The\nrelease of NSINA aims to provide a solution to challenges in adapting LLMs to\nSinhala, offering valuable resources and benchmarks for improving NLP in the\nSinhala language. NSINA is the largest news corpus for Sinhala, available up to\ndate.\n","authors":["Hansi Hettiarachchi","Damith Premasiri","Lasitha Uyangodage","Tharindu Ranasinghe"],"pdf_url":"https://arxiv.org/pdf/2403.16571v1.pdf","comment":"Accepted to LREC-COLING 2024 (The 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation)"},{"id":"http://arxiv.org/abs/2403.16569v1","updated":"2024-03-25T09:36:10Z","published":"2024-03-25T09:36:10Z","title":"Revealing Vulnerabilities of Neural Networks in Parameter Learning and\n  Defense Against Explanation-Aware Backdoors","summary":"  Explainable Artificial Intelligence (XAI) strategies play a crucial part in\nincreasing the understanding and trustworthiness of neural networks.\nNonetheless, these techniques could potentially generate misleading\nexplanations. Blinding attacks can drastically alter a machine learning\nalgorithm's prediction and explanation, providing misleading information by\nadding visually unnoticeable artifacts into the input, while maintaining the\nmodel's accuracy. It poses a serious challenge in ensuring the reliability of\nXAI methods. To ensure the reliability of XAI methods poses a real challenge,\nwe leverage statistical analysis to highlight the changes in CNN weights within\na CNN following blinding attacks. We introduce a method specifically designed\nto limit the effectiveness of such attacks during the evaluation phase,\navoiding the need for extra training. The method we suggest defences against\nmost modern explanation-aware adversarial attacks, achieving an approximate\ndecrease of ~99\\% in the Attack Success Rate (ASR) and a ~91\\% reduction in the\nMean Square Error (MSE) between the original explanation and the defended\n(post-attack) explanation across three unique types of attacks.\n","authors":["Md Abdul Kadir","GowthamKrishna Addluri","Daniel Sonntag"],"pdf_url":"https://arxiv.org/pdf/2403.16569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16561v1","updated":"2024-03-25T09:24:05Z","published":"2024-03-25T09:24:05Z","title":"FedFixer: Mitigating Heterogeneous Label Noise in Federated Learning","summary":"  Federated Learning (FL) heavily depends on label quality for its performance.\nHowever, the label distribution among individual clients is always both noisy\nand heterogeneous. The high loss incurred by client-specific samples in\nheterogeneous label noise poses challenges for distinguishing between\nclient-specific and noisy label samples, impacting the effectiveness of\nexisting label noise learning approaches. To tackle this issue, we propose\nFedFixer, where the personalized model is introduced to cooperate with the\nglobal model to effectively select clean client-specific samples. In the dual\nmodels, updating the personalized model solely at a local level can lead to\noverfitting on noisy data due to limited samples, consequently affecting both\nthe local and global models' performance. To mitigate overfitting, we address\nthis concern from two perspectives. Firstly, we employ a confidence regularizer\nto alleviate the impact of unconfident predictions caused by label noise.\nSecondly, a distance regularizer is implemented to constrain the disparity\nbetween the personalized and global models. We validate the effectiveness of\nFedFixer through extensive experiments on benchmark datasets. The results\ndemonstrate that FedFixer can perform well in filtering noisy label samples on\ndifferent clients, especially in highly heterogeneous label noise scenarios.\n","authors":["Xinyuan Ji","Zhaowei Zhu","Wei Xi","Olga Gadyatskaya","Zilong Song","Yong Cai","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.16561v1.pdf","comment":"accepted by AAA24"},{"id":"http://arxiv.org/abs/2403.16557v1","updated":"2024-03-25T09:16:59Z","published":"2024-03-25T09:16:59Z","title":"Accelerating Federated Learning by Selecting Beneficial Herd of Local\n  Gradients","summary":"  Federated Learning (FL) is a distributed machine learning framework in\ncommunication network systems. However, the systems' Non-Independent and\nIdentically Distributed (Non-IID) data negatively affect the convergence\nefficiency of the global model, since only a subset of these data samples are\nbeneficial for model convergence. In pursuit of this subset, a reliable\napproach involves determining a measure of validity to rank the samples within\nthe dataset. In this paper, We propose the BHerd strategy which selects a\nbeneficial herd of local gradients to accelerate the convergence of the FL\nmodel. Specifically, we map the distribution of the local dataset to the local\ngradients and use the Herding strategy to obtain a permutation of the set of\ngradients, where the more advanced gradients in the permutation are closer to\nthe average of the set of gradients. These top portion of the gradients will be\nselected and sent to the server for global aggregation. We conduct experiments\non different datasets, models and scenarios by building a prototype system, and\nexperimental results demonstrate that our BHerd strategy is effective in\nselecting beneficial local gradients to mitigate the effects brought by the\nNon-IID dataset, thus accelerating model convergence.\n","authors":["Ping Luo","Xiaoge Deng","Ziqing Wen","Tao Sun","Dongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2403.16557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14714v4","updated":"2024-03-25T08:58:39Z","published":"2023-10-23T08:51:05Z","title":"BatteryML:An Open-source platform for Machine Learning on Battery\n  Degradation","summary":"  Battery degradation remains a pivotal concern in the energy storage domain,\nwith machine learning emerging as a potent tool to drive forward insights and\nsolutions. However, this intersection of electrochemical science and machine\nlearning poses complex challenges. Machine learning experts often grapple with\nthe intricacies of battery science, while battery researchers face hurdles in\nadapting intricate models tailored to specific datasets. Beyond this, a\ncohesive standard for battery degradation modeling, inclusive of data formats\nand evaluative benchmarks, is conspicuously absent. Recognizing these\nimpediments, we present BatteryML - a one-step, all-encompass, and open-source\nplatform designed to unify data preprocessing, feature extraction, and the\nimplementation of both traditional and state-of-the-art models. This\nstreamlined approach promises to enhance the practicality and efficiency of\nresearch applications. BatteryML seeks to fill this void, fostering an\nenvironment where experts from diverse specializations can collaboratively\ncontribute, thus elevating the collective understanding and advancement of\nbattery research.The code for our project is publicly available on GitHub at\nhttps://github.com/microsoft/BatteryML.\n","authors":["Han Zhang","Xiaofan Gui","Shun Zheng","Ziheng Lu","Yuqi Li","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2310.14714v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02760v2","updated":"2024-03-25T08:57:47Z","published":"2023-11-05T20:33:18Z","title":"Causal Question Answering with Reinforcement Learning","summary":"  Causal questions inquire about causal relationships between different events\nor phenomena. They are important for a variety of use cases, including virtual\nassistants and search engines. However, many current approaches to causal\nquestion answering cannot provide explanations or evidence for their answers.\nHence, in this paper, we aim to answer causal questions with a causality graph,\na large-scale dataset of causal relations between noun phrases along with the\nrelations' provenance data. Inspired by recent, successful applications of\nreinforcement learning to knowledge graph tasks, such as link prediction and\nfact-checking, we explore the application of reinforcement learning on a\ncausality graph for causal question answering. We introduce an\nActor-Critic-based agent which learns to search through the graph to answer\ncausal questions. We bootstrap the agent with a supervised learning procedure\nto deal with large action spaces and sparse rewards. Our evaluation shows that\nthe agent successfully prunes the search space to answer binary causal\nquestions by visiting less than 30 nodes per question compared to over 3,000\nnodes by a naive breadth-first search. Our ablation study indicates that our\nsupervised learning strategy provides a strong foundation upon which our\nreinforcement learning agent improves. The paths returned by our agent explain\nthe mechanisms by which a cause produces an effect. Moreover, for each edge on\na path, our causality graph provides its original source allowing for easy\nverification of paths.\n","authors":["Lukas Blübaum","Stefan Heindorf"],"pdf_url":"https://arxiv.org/pdf/2311.02760v2.pdf","comment":"Accepted at WWW 2024"},{"id":"http://arxiv.org/abs/2402.09132v3","updated":"2024-03-25T08:46:02Z","published":"2024-02-14T12:28:38Z","title":"Exploring the Adversarial Capabilities of Large Language Models","summary":"  The proliferation of large language models (LLMs) has sparked widespread and\ngeneral interest due to their strong language generation capabilities, offering\ngreat potential for both industry and research. While previous research delved\ninto the security and privacy issues of LLMs, the extent to which these models\ncan exhibit adversarial behavior remains largely unexplored. Addressing this\ngap, we investigate whether common publicly available LLMs have inherent\ncapabilities to perturb text samples to fool safety measures, so-called\nadversarial examples resp.~attacks. More specifically, we investigate whether\nLLMs are inherently able to craft adversarial examples out of benign samples to\nfool existing safe rails. Our experiments, which focus on hate speech\ndetection, reveal that LLMs succeed in finding adversarial perturbations,\neffectively undermining hate speech detection systems. Our findings carry\nsignificant implications for (semi-)autonomous systems relying on LLMs,\nhighlighting potential challenges in their interaction with existing systems\nand safety measures.\n","authors":["Lukas Struppek","Minh Hieu Le","Dominik Hintersdorf","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2402.09132v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16542v1","updated":"2024-03-25T08:35:19Z","published":"2024-03-25T08:35:19Z","title":"Differentially Private Online Federated Learning with Correlated Noise","summary":"  We propose a novel differentially private algorithm for online federated\nlearning that employs temporally correlated noise to improve the utility while\nensuring the privacy of the continuously released models. To address challenges\nstemming from DP noise and local updates with streaming noniid data, we develop\na perturbed iterate analysis to control the impact of the DP noise on the\nutility. Moreover, we demonstrate how the drift errors from local updates can\nbe effectively managed under a quasi-strong convexity condition. Subject to an\n$(\\epsilon, \\delta)$-DP budget, we establish a dynamic regret bound over the\nentire time horizon that quantifies the impact of key parameters and the\nintensity of changes in dynamic environments. Numerical experiments validate\nthe efficacy of the proposed algorithm.\n","authors":["Jiaojiao Zhang","Linglingzhi Zhu","Mikael Johansson"],"pdf_url":"https://arxiv.org/pdf/2403.16542v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2403.16523v1","updated":"2024-03-25T08:06:08Z","published":"2024-03-25T08:06:08Z","title":"Causal Discovery from Poisson Branching Structural Causal Model Using\n  High-Order Cumulant with Path Analysis","summary":"  Count data naturally arise in many fields, such as finance, neuroscience, and\nepidemiology, and discovering causal structure among count data is a crucial\ntask in various scientific and industrial scenarios. One of the most common\ncharacteristics of count data is the inherent branching structure described by\na binomial thinning operator and an independent Poisson distribution that\ncaptures both branching and noise. For instance, in a population count\nscenario, mortality and immigration contribute to the count, where survival\nfollows a Bernoulli distribution, and immigration follows a Poisson\ndistribution. However, causal discovery from such data is challenging due to\nthe non-identifiability issue: a single causal pair is Markov equivalent, i.e.,\n$X\\rightarrow Y$ and $Y\\rightarrow X$ are distributed equivalent. Fortunately,\nin this work, we found that the causal order from $X$ to its child $Y$ is\nidentifiable if $X$ is a root vertex and has at least two directed paths to\n$Y$, or the ancestor of $X$ with the most directed path to $X$ has a directed\npath to $Y$ without passing $X$. Specifically, we propose a Poisson Branching\nStructure Causal Model (PB-SCM) and perform a path analysis on PB-SCM using\nhigh-order cumulants. Theoretical results establish the connection between the\npath and cumulant and demonstrate that the path information can be obtained\nfrom the cumulant. With the path information, causal order is identifiable\nunder some graphical conditions. A practical algorithm for learning causal\nstructure under PB-SCM is proposed and the experiments demonstrate and verify\nthe effectiveness of the proposed method.\n","authors":["Jie Qiao","Yu Xiang","Zhengming Chen","Ruichu Cai","Zhifeng Hao"],"pdf_url":"https://arxiv.org/pdf/2403.16523v1.pdf","comment":"Accepted by AAAI-2024"},{"id":"http://arxiv.org/abs/2306.08929v2","updated":"2024-03-25T08:00:38Z","published":"2023-06-15T08:02:07Z","title":"On the resilience of Collaborative Learning-based Recommender Systems\n  Against Community Detection Attack","summary":"  Collaborative-learning-based recommender systems emerged following the\nsuccess of collaborative learning techniques such as Federated Learning (FL)\nand Gossip Learning (GL). In these systems, users participate in the training\nof a recommender system while maintaining their history of consumed items on\ntheir devices. While these solutions seemed appealing for preserving the\nprivacy of the participants at first glance, recent studies have revealed that\ncollaborative learning can be vulnerable to various privacy attacks. In this\npaper, we study the resilience of collaborative learning-based recommender\nsystems against a novel privacy attack called Community Detection Attack (CDA).\nThis attack enables an adversary to identify community members based on a\nchosen set of items (eg., identifying users interested in specific\npoints-of-interest). Through experiments on three real recommendation datasets\nusing two state-of-the-art recommendation models, we evaluate the sensitivity\nof an FL-based recommender system as well as two flavors of Gossip\nLearning-based recommender systems to CDA. The results show that across all\nmodels and datasets, the FL setting is more vulnerable to CDA compared to\nGossip settings. Furthermore, we assess two off-the-shelf mitigation\nstrategies, namely differential privacy (DP) and a \\emph{Share less} policy,\nwhich consists of sharing a subset of less sensitive model parameters. The\nfindings indicate a more favorable privacy-utility trade-off for the\n\\emph{Share less} strategy, particularly in FedRecs.\n","authors":["Yacine Belal","Sonia Ben Mokhtar","Mohamed Maouche","Anthony Simonet-Boulogne"],"pdf_url":"https://arxiv.org/pdf/2306.08929v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16509v1","updated":"2024-03-25T07:48:34Z","published":"2024-03-25T07:48:34Z","title":"Human Understanding AI Paper Challenge 2024 -- Dataset Design","summary":"  In 2024, we will hold a research paper competition (the third Human\nUnderstanding AI Paper Challenge) for the research and development of\nartificial intelligence technologies to understand human daily life. This\ndocument introduces the datasets that will be provided to participants in the\ncompetition, and summarizes the issues to consider in data processing and\nlearning model development.\n","authors":["Se Won Oh","Hyuntae Jeong","Jeong Mook Lim","Seungeun Chung","Kyoung Ju Noh"],"pdf_url":"https://arxiv.org/pdf/2403.16509v1.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.16497v1","updated":"2024-03-25T07:29:18Z","published":"2024-03-25T07:29:18Z","title":"PathoTune: Adapting Visual Foundation Model to Pathological Specialists","summary":"  As natural image understanding moves towards the pretrain-finetune era,\nresearch in pathology imaging is concurrently evolving. Despite the predominant\nfocus on pretraining pathological foundation models, how to adapt foundation\nmodels to downstream tasks is little explored. For downstream adaptation, we\npropose the existence of two domain gaps, i.e., the Foundation-Task Gap and the\nTask-Instance Gap. To mitigate these gaps, we introduce PathoTune, a framework\ndesigned to efficiently adapt pathological or even visual foundation models to\npathology-specific tasks via multi-modal prompt tuning. The proposed framework\nleverages Task-specific Visual Prompts and Task-specific Textual Prompts to\nidentify task-relevant features, along with Instance-specific Visual Prompts\nfor encoding single pathological image features. Results across multiple\ndatasets at both patch-level and WSI-level demonstrate its superior performance\nover single-modality prompt tuning approaches. Significantly, PathoTune\nfacilitates the direct adaptation of natural visual foundation models to\npathological tasks, drastically outperforming pathological foundation models\nwith simple linear probing. The code will be available upon acceptance.\n","authors":["Jiaxuan Lu","Fang Yan","Xiaofan Zhang","Yue Gao","Shaoting Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16497v1.pdf","comment":"Submitted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2403.16495v1","updated":"2024-03-25T07:23:23Z","published":"2024-03-25T07:23:23Z","title":"LSTTN: A Long-Short Term Transformer-based Spatio-temporal Neural\n  Network for Traffic Flow Forecasting","summary":"  Accurate traffic forecasting is a fundamental problem in intelligent\ntransportation systems and learning long-range traffic representations with key\ninformation through spatiotemporal graph neural networks (STGNNs) is a basic\nassumption of current traffic flow prediction models. However, due to\nstructural limitations, existing STGNNs can only utilize short-range traffic\nflow data; therefore, the models cannot adequately learn the complex trends and\nperiodic features in traffic flow. Besides, it is challenging to extract the\nkey temporal information from the long historical traffic series and obtain a\ncompact representation. To solve the above problems, we propose a novel LSTTN\n(Long-Short Term Transformer-based Network) framework comprehensively\nconsidering the long- and short-term features in historical traffic flow.\nFirst, we employ a masked subseries Transformer to infer the content of masked\nsubseries from a small portion of unmasked subseries and their temporal context\nin a pretraining manner, forcing the model to efficiently learn compressed and\ncontextual subseries temporal representations from long historical series.\nThen, based on the learned representations, long-term trend is extracted by\nusing stacked 1D dilated convolution layers, and periodic features are\nextracted by dynamic graph convolution layers. For the difficulties in making\ntime-step level prediction, LSTTN adopts a short-term trend extractor to learn\nfine-grained short-term temporal features. Finally, LSTTN fuses the long-term\ntrend, periodic features and short-term features to obtain the prediction\nresults. Experiments on four real-world datasets show that in 60-minute-ahead\nlong-term forecasting, the LSTTN model achieves a minimum improvement of 5.63\\%\nand a maximum improvement of 16.78\\% over baseline models. The source code is\navailable at https://github.com/GeoX-Lab/LSTTN.\n","authors":["Qinyao Luo","Silu He","Xing Han","Yuhan Wang","Haifeng Li"],"pdf_url":"https://arxiv.org/pdf/2403.16495v1.pdf","comment":"15 pages, 10 figures, 6 tables"},{"id":"http://arxiv.org/abs/2403.16482v1","updated":"2024-03-25T07:08:01Z","published":"2024-03-25T07:08:01Z","title":"Determined Multi-Label Learning via Similarity-Based Prompt","summary":"  In multi-label classification, each training instance is associated with\nmultiple class labels simultaneously. Unfortunately, collecting the fully\nprecise class labels for each training instance is time- and labor-consuming\nfor real-world applications. To alleviate this problem, a novel labeling\nsetting termed \\textit{Determined Multi-Label Learning} (DMLL) is proposed,\naiming to effectively alleviate the labeling cost inherent in multi-label\ntasks. In this novel labeling setting, each training instance is associated\nwith a \\textit{determined label} (either \"Yes\" or \"No\"), which indicates\nwhether the training instance contains the provided class label. The provided\nclass label is randomly and uniformly selected from the whole candidate labels\nset. Besides, each training instance only need to be determined once, which\nsignificantly reduce the annotation cost of the labeling task for multi-label\ndatasets. In this paper, we theoretically derive an risk-consistent estimator\nto learn a multi-label classifier from these determined-labeled training data.\nAdditionally, we introduce a similarity-based prompt learning method for the\nfirst time, which minimizes the risk-consistent loss of large-scale pre-trained\nmodels to learn a supplemental prompt with richer semantic information.\nExtensive experimental validation underscores the efficacy of our approach,\ndemonstrating superior performance compared to existing state-of-the-art\nmethods.\n","authors":["Meng Wei","Zhongnian Li","Peng Ying","Yong Zhou","Xinzheng Xu"],"pdf_url":"https://arxiv.org/pdf/2403.16482v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2312.11834v2","updated":"2024-03-25T07:00:29Z","published":"2023-12-19T04:02:50Z","title":"Multi-agent reinforcement learning using echo-state network and its\n  application to pedestrian dynamics","summary":"  In recent years, simulations of pedestrians using the multi-agent\nreinforcement learning (MARL) have been studied. This study considered the\nroads on a grid-world environment, and implemented pedestrians as MARL agents\nusing an echo-state network and the least squares policy iteration method.\nUnder this environment, the ability of these agents to learn to move forward by\navoiding other agents was investigated. Specifically, we considered two types\nof tasks: the choice between a narrow direct route and a broad detour, and the\nbidirectional pedestrian flow in a corridor. The simulations results indicated\nthat the learning was successful when the density of the agents was not that\nhigh.\n","authors":["Hisato Komatsu"],"pdf_url":"https://arxiv.org/pdf/2312.11834v2.pdf","comment":"26 pages, 17 figures"},{"id":"http://arxiv.org/abs/2403.06606v2","updated":"2024-03-25T06:57:57Z","published":"2024-03-11T10:50:53Z","title":"Distributionally Generative Augmentation for Fair Facial Attribute\n  Classification","summary":"  Facial Attribute Classification (FAC) holds substantial promise in widespread\napplications. However, FAC models trained by traditional methodologies can be\nunfair by exhibiting accuracy inconsistencies across varied data\nsubpopulations. This unfairness is largely attributed to bias in data, where\nsome spurious attributes (e.g., Male) statistically correlate with the target\nattribute (e.g., Smiling). Most of existing fairness-aware methods rely on the\nlabels of spurious attributes, which may be unavailable in practice. This work\nproposes a novel, generation-based two-stage framework to train a fair FAC\nmodel on biased data without additional annotation. Initially, we identify the\npotential spurious attributes based on generative models. Notably, it enhances\ninterpretability by explicitly showing the spurious attributes in image space.\nFollowing this, for each image, we first edit the spurious attributes with a\nrandom degree sampled from a uniform distribution, while keeping target\nattribute unchanged. Then we train a fair FAC model by fostering model\ninvariance to these augmentation. Extensive experiments on three common\ndatasets demonstrate the effectiveness of our method in promoting fairness in\nFAC without compromising accuracy. Codes are in\nhttps://github.com/heqianpei/DiGA.\n","authors":["Fengda Zhang","Qianpei He","Kun Kuang","Jiashuo Liu","Long Chen","Chao Wu","Jun Xiao","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.06606v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.16469v1","updated":"2024-03-25T06:50:25Z","published":"2024-03-25T06:50:25Z","title":"Learning from Reduced Labels for Long-Tailed Data","summary":"  Long-tailed data is prevalent in real-world classification tasks and heavily\nrelies on supervised information, which makes the annotation process\nexceptionally labor-intensive and time-consuming. Unfortunately, despite being\na common approach to mitigate labeling costs, existing weakly supervised\nlearning methods struggle to adequately preserve supervised information for\ntail samples, resulting in a decline in accuracy for the tail classes. To\nalleviate this problem, we introduce a novel weakly supervised labeling setting\ncalled Reduced Label. The proposed labeling setting not only avoids the decline\nof supervised information for the tail samples, but also decreases the labeling\ncosts associated with long-tailed data. Additionally, we propose an\nstraightforward and highly efficient unbiased framework with strong theoretical\nguarantees to learn from these Reduced Labels. Extensive experiments conducted\non benchmark datasets including ImageNet validate the effectiveness of our\napproach, surpassing the performance of state-of-the-art weakly supervised\nmethods.\n","authors":["Meng Wei","Zhongnian Li","Yong Zhou","Xinzheng Xu"],"pdf_url":"https://arxiv.org/pdf/2403.16469v1.pdf","comment":"12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.16464v1","updated":"2024-03-25T06:46:27Z","published":"2024-03-25T06:46:27Z","title":"Training Generative Adversarial Network-Based Vocoder with Limited Data\n  Using Augmentation-Conditional Discriminator","summary":"  A generative adversarial network (GAN)-based vocoder trained with an\nadversarial discriminator is commonly used for speech synthesis because of its\nfast, lightweight, and high-quality characteristics. However, this data-driven\nmodel requires a large amount of training data incurring high data-collection\ncosts. This fact motivates us to train a GAN-based vocoder on limited data. A\npromising solution is to augment the training data to avoid overfitting.\nHowever, a standard discriminator is unconditional and insensitive to\ndistributional changes caused by data augmentation. Thus, augmented speech\n(which can be extraordinary) may be considered real speech. To address this\nissue, we propose an augmentation-conditional discriminator (AugCondD) that\nreceives the augmentation state as input in addition to speech, thereby\nassessing the input speech according to the augmentation state, without\ninhibiting the learning of the original non-augmented distribution.\nExperimental results indicate that AugCondD improves speech quality under\nlimited data conditions while achieving comparable speech quality under\nsufficient data conditions. Audio samples are available at\nhttps://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/augcondd/.\n","authors":["Takuhiro Kaneko","Hirokazu Kameoka","Kou Tanaka"],"pdf_url":"https://arxiv.org/pdf/2403.16464v1.pdf","comment":"Accepted to ICASSP 2024. Project page:\n  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/augcondd/"},{"id":"http://arxiv.org/abs/2403.16460v1","updated":"2024-03-25T06:43:28Z","published":"2024-03-25T06:43:28Z","title":"FedAC: A Adaptive Clustered Federated Learning Framework for\n  Heterogeneous Data","summary":"  Clustered federated learning (CFL) is proposed to mitigate the performance\ndeterioration stemming from data heterogeneity in federated learning (FL) by\ngrouping similar clients for cluster-wise model training. However, current CFL\nmethods struggle due to inadequate integration of global and intra-cluster\nknowledge and the absence of an efficient online model similarity metric, while\ntreating the cluster count as a fixed hyperparameter limits flexibility and\nrobustness. In this paper, we propose an adaptive CFL framework, named FedAC,\nwhich (1) efficiently integrates global knowledge into intra-cluster learning\nby decoupling neural networks and utilizing distinct aggregation methods for\neach submodule, significantly enhancing performance; (2) includes a\ncosteffective online model similarity metric based on dimensionality reduction;\n(3) incorporates a cluster number fine-tuning module for improved adaptability\nand scalability in complex, heterogeneous environments. Extensive experiments\nshow that FedAC achieves superior empirical performance, increasing the test\naccuracy by around 1.82% and 12.67% on CIFAR-10 and CIFAR-100 datasets,\nrespectively, under different non-IID settings compared to SOTA methods.\n","authors":["Yuxin Zhang","Haoyu Chen","Zheng Lin","Zhe Chen","Jin Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.16460v1.pdf","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.16459v1","updated":"2024-03-25T06:42:02Z","published":"2024-03-25T06:42:02Z","title":"On the rates of convergence for learning with convolutional neural\n  networks","summary":"  We study the approximation and learning capacities of convolutional neural\nnetworks (CNNs). Our first result proves a new approximation bound for CNNs\nwith certain constraint on the weights. Our second result gives a new analysis\non the covering number of feed-forward neural networks, which include CNNs as\nspecial cases. The analysis carefully takes into account the size of the\nweights and hence gives better bounds than existing literature in some\nsituations. Using these two results, we are able to derive rates of convergence\nfor estimators based on CNNs in many learning problems. In particular, we\nestablish minimax optimal convergence rates of the least squares based on CNNs\nfor learning smooth functions in the nonparametric regression setting. For\nbinary classification, we derive convergence rates for CNN classifiers with\nhinge loss and logistic loss. It is also shown that the obtained rates are\nminimax optimal in several settings.\n","authors":["Yunfei Yang","Han Feng","Ding-Xuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.16459v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16430v5","updated":"2024-03-25T06:32:49Z","published":"2023-12-27T06:34:54Z","title":"Preference as Reward, Maximum Preference Optimization with Importance\n  Sampling","summary":"  Preference learning is a key technology for aligning language models with\nhuman values. Reinforcement Learning from Human Feedback (RLHF) is a\nmodel-based algorithm to optimize preference learning, which first fits a\nreward model for preference scores and then optimizes the generating policy\nwith an on-policy PPO algorithm to maximize the reward. The processing of RLHF\nis complex, time-consuming, and unstable. The Direct Preference Optimization\n(DPO) algorithm uses an off-policy algorithm to directly optimize the\ngenerating policy and eliminates the need for a reward model. DPO is more\ndata-efficient and stable. However, DPO has a drawback of overfitting to the\npreference data and ignoring the KL-regularization term when the preference is\ndeterministic. Identity mapping Preference Optimization(IPO) uses a\nroot-finding MSE loss to incorporate KL-regularization. However, both DPO and\nIPO fail to properly address the KL-regularization term because the support of\nthe preference distribution is not equal to the reference distribution. In this\npaper, we propose a simple and intuitive off-policy preference optimization\nalgorithm from an importance sampling view, which we call Maximum Preference\nOptimization (MPO). MPO incorporates the off-policy KL-regularization term,\nmaking regularization truly effective. MPO achieves the best of both worlds by\ncombining the objectives of RLHF and IPO while being an off-policy algorithm.\nFurthermore, MPO eliminates the need for a reward model and reference policy,\nsimplifying the learning process and reducing memory usage.\n","authors":["Zaifan Jiang","Xing Huang","Chao Wei"],"pdf_url":"https://arxiv.org/pdf/2312.16430v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16451v1","updated":"2024-03-25T06:30:54Z","published":"2024-03-25T06:30:54Z","title":"DeepMachining: Online Prediction of Machining Errors of Lathe Machines","summary":"  We describe DeepMachining, a deep learning-based AI system for online\nprediction of machining errors of lathe machine operations. We have built and\nevaluated DeepMachining based on manufacturing data from factories.\nSpecifically, we first pretrain a deep learning model for a given lathe\nmachine's operations to learn the salient features of machining states. Then,\nwe fine-tune the pretrained model to adapt to specific machining tasks. We\ndemonstrate that DeepMachining achieves high prediction accuracy for multiple\ntasks that involve different workpieces and cutting tools. To the best of our\nknowledge, this work is one of the first factory experiments using pre-trained\ndeep-learning models to predict machining errors of lathe machines.\n","authors":["Xiang-Li Lu","Hwai-Jung Hsu","Che-Wei Chou","H. T. Kung","Chen-Hsin Lee"],"pdf_url":"https://arxiv.org/pdf/2403.16451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08282v3","updated":"2024-03-25T06:21:37Z","published":"2023-10-12T12:39:08Z","title":"Data driven modeling for self-similar dynamics","summary":"  Multiscale modeling of complex systems is crucial for understanding their\nintricacies. Data-driven multiscale modeling has emerged as a promising\napproach to tackle challenges associated with complex systems. On the other\nhand, self-similarity is prevalent in complex systems, hinting that large-scale\ncomplex systems can be modeled at a reduced cost. In this paper, we introduce a\nmultiscale neural network framework that incorporates self-similarity as prior\nknowledge, facilitating the modeling of self-similar dynamical systems. For\ndeterministic dynamics, our framework can discern whether the dynamics are\nself-similar. For uncertain dynamics, it can compare and determine which\nparameter set is closer to self-similarity. The framework allows us to extract\nscale-invariant kernels from the dynamics for modeling at any scale. Moreover,\nour method can identify the power law exponents in self-similar systems.\nPreliminary tests on the Ising model yielded critical exponents consistent with\ntheoretical expectations, providing valuable insights for addressing critical\nphase transitions in non-equilibrium systems.\n","authors":["Ruyi Tao","Ningning Tao","Yi-zhuang You","Jiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.08282v3.pdf","comment":"10 pages,7 figures,1 table"},{"id":"http://arxiv.org/abs/2403.16442v1","updated":"2024-03-25T06:05:50Z","published":"2024-03-25T06:05:50Z","title":"If CLIP Could Talk: Understanding Vision-Language Model Representations\n  Through Their Preferred Concept Descriptions","summary":"  Recent works often assume that Vision-Language Model (VLM) representations\nare based on visual attributes like shape. However, it is unclear to what\nextent VLMs prioritize this information to represent concepts. We propose\nExtract and Explore (EX2), a novel approach to characterize important textual\nfeatures for VLMs. EX2 uses reinforcement learning to align a large language\nmodel with VLM preferences and generates descriptions that incorporate the\nimportant features for the VLM. Then, we inspect the descriptions to identify\nthe features that contribute to VLM representations. We find that spurious\ndescriptions have a major role in VLM representations despite providing no\nhelpful information, e.g., Click to enlarge photo of CONCEPT. More importantly,\namong informative descriptions, VLMs rely significantly on non-visual\nattributes like habitat to represent visual concepts. Also, our analysis\nreveals that different VLMs prioritize different attributes in their\nrepresentations. Overall, we show that VLMs do not simply match images to scene\ndescriptions and that non-visual or even spurious descriptions significantly\ninfluence their representations.\n","authors":["Reza Esfandiarpoor","Cristina Menghini","Stephen H. Bach"],"pdf_url":"https://arxiv.org/pdf/2403.16442v1.pdf","comment":"Code: https://github.com/BatsResearch/ex2"},{"id":"http://arxiv.org/abs/2403.16439v1","updated":"2024-03-25T05:58:33Z","published":"2024-03-25T05:58:33Z","title":"Producing and Leveraging Online Map Uncertainty in Trajectory Prediction","summary":"  High-definition (HD) maps have played an integral role in the development of\nmodern autonomous vehicle (AV) stacks, albeit with high associated labeling and\nmaintenance costs. As a result, many recent works have proposed methods for\nestimating HD maps online from sensor data, enabling AVs to operate outside of\npreviously-mapped regions. However, current online map estimation approaches\nare developed in isolation of their downstream tasks, complicating their\nintegration in AV stacks. In particular, they do not produce uncertainty or\nconfidence estimates. In this work, we extend multiple state-of-the-art online\nmap estimation methods to additionally estimate uncertainty and show how this\nenables more tightly integrating online mapping with trajectory forecasting. In\ndoing so, we find that incorporating uncertainty yields up to 50% faster\ntraining convergence and up to 15% better prediction performance on the\nreal-world nuScenes driving dataset.\n","authors":["Xunjiang Gu","Guanyu Song","Igor Gilitschenski","Marco Pavone","Boris Ivanovic"],"pdf_url":"https://arxiv.org/pdf/2403.16439v1.pdf","comment":"14 pages, 14 figures, 6 tables. CVPR 2024"},{"id":"http://arxiv.org/abs/2312.03009v2","updated":"2024-03-25T05:04:04Z","published":"2023-12-04T19:01:19Z","title":"I-PHYRE: Interactive Physical Reasoning","summary":"  Current evaluation protocols predominantly assess physical reasoning in\nstationary scenes, creating a gap in evaluating agents' abilities to interact\nwith dynamic events. While contemporary methods allow agents to modify initial\nscene configurations and observe consequences, they lack the capability to\ninteract with events in real time. To address this, we introduce I-PHYRE, a\nframework that challenges agents to simultaneously exhibit intuitive physical\nreasoning, multi-step planning, and in-situ intervention. Here, intuitive\nphysical reasoning refers to a quick, approximate understanding of physics to\naddress complex problems; multi-step denotes the need for extensive sequence\nplanning in I-PHYRE, considering each intervention can significantly alter\nsubsequent choices; and in-situ implies the necessity for timely object\nmanipulation within a scene, where minor timing deviations can result in task\nfailure. We formulate four game splits to scrutinize agents' learning and\ngeneralization of essential principles of interactive physical reasoning,\nfostering learning through interaction with representative scenarios. Our\nexploration involves three planning strategies and examines several supervised\nand reinforcement agents' zero-shot generalization proficiency on I-PHYRE. The\noutcomes highlight a notable gap between existing learning algorithms and human\nperformance, emphasizing the imperative for more research in enhancing agents\nwith interactive physical reasoning capabilities. The environment and baselines\nwill be made publicly available.\n","authors":["Shiqian Li","Kewen Wu","Chi Zhang","Yixin Zhu"],"pdf_url":"https://arxiv.org/pdf/2312.03009v2.pdf","comment":"21 pages, ICLR 2024"},{"id":"http://arxiv.org/abs/2403.16418v1","updated":"2024-03-25T04:43:47Z","published":"2024-03-25T04:43:47Z","title":"An incremental MaxSAT-based model to learn balanced rules","summary":"  The increasing advancements in the field of machine learning have led to the\ndevelopment of numerous applications that effectively address a wide range of\nproblems with accurate predictions. However, in certain cases, accuracy alone\nmay not be sufficient. Many real-world problems also demand explanations and\ninterpretability behind the predictions. One of the most popular interpretable\nmodels that are classification rules. This work aims to propose an incremental\nmodel for learning interpretable and balanced rules based on MaxSAT, called\nIMLIB. This new model was based on two other approaches, one based on SAT and\nthe other on MaxSAT. The one based on SAT limits the size of each generated\nrule, making it possible to balance them. We suggest that such a set of rules\nseem more natural to be understood compared to a mixture of large and small\nrules. The approach based on MaxSAT, called IMLI, presents a technique to\nincrease performance that involves learning a set of rules by incrementally\napplying the model in a dataset. Finally, IMLIB and IMLI are compared using\ndiverse databases. IMLIB obtained results comparable to IMLI in terms of\naccuracy, generating more balanced rules with smaller sizes.\n","authors":["Antônio Carlos Souza Ferreira Júnior","Thiago Alves Rocha"],"pdf_url":"https://arxiv.org/pdf/2403.16418v1.pdf","comment":"16 pages, 5 tables, submitted to BRACIS 2023 (Brazilian Conference on\n  Intelligent Systems), accepted version published in Intelligent Systems,\n  LNCS, vol 14195"},{"id":"http://arxiv.org/abs/2312.05654v3","updated":"2024-03-25T04:32:19Z","published":"2023-12-09T19:42:36Z","title":"Spectral methods for Neural Integral Equations","summary":"  Neural integral equations are deep learning models based on the theory of\nintegral equations, where the model consists of an integral operator and the\ncorresponding equation (of the second kind) which is learned through an\noptimization procedure. This approach allows to leverage the nonlocal\nproperties of integral operators in machine learning, but it is computationally\nexpensive. In this article, we introduce a framework for neural integral\nequations based on spectral methods that allows us to learn an operator in the\nspectral domain, resulting in a cheaper computational cost, as well as in high\ninterpolation accuracy. We study the properties of our methods and show various\ntheoretical guarantees regarding the approximation capabilities of the model,\nand convergence to solutions of the numerical methods. We provide numerical\nexperiments to demonstrate the practical effectiveness of the resulting model.\n","authors":["Emanuele Zappala"],"pdf_url":"https://arxiv.org/pdf/2312.05654v3.pdf","comment":"15 pages, 3 figures and 2 tables. v3: Missing hypotheses for the\n  framework have been now added"},{"id":"http://arxiv.org/abs/2403.14689v2","updated":"2024-03-25T04:21:13Z","published":"2024-03-13T22:38:08Z","title":"Developing and Deploying Industry Standards for Artificial Intelligence\n  in Education (AIED): Challenges, Strategies, and Future Directions","summary":"  The adoption of Artificial Intelligence in Education (AIED) holds the promise\nof revolutionizing educational practices by offering personalized learning\nexperiences, automating administrative and pedagogical tasks, and reducing the\ncost of content creation. However, the lack of standardized practices in the\ndevelopment and deployment of AIED solutions has led to fragmented ecosystems,\nwhich presents challenges in interoperability, scalability, and ethical\ngovernance. This article aims to address the critical need to develop and\nimplement industry standards in AIED, offering a comprehensive analysis of the\ncurrent landscape, challenges, and strategic approaches to overcome these\nobstacles. We begin by examining the various applications of AIED in various\neducational settings and identify key areas lacking in standardization,\nincluding system interoperability, ontology mapping, data integration,\nevaluation, and ethical governance. Then, we propose a multi-tiered framework\nfor establishing robust industry standards for AIED. In addition, we discuss\nmethodologies for the iterative development and deployment of standards,\nincorporating feedback loops from real-world applications to refine and adapt\nstandards over time. The paper also highlights the role of emerging\ntechnologies and pedagogical theories in shaping future standards for AIED.\nFinally, we outline a strategic roadmap for stakeholders to implement these\nstandards, fostering a cohesive and ethical AIED ecosystem. By establishing\ncomprehensive industry standards, such as those by IEEE Artificial Intelligence\nStandards Committee (AISC) and International Organization for Standardization\n(ISO), we can accelerate and scale AIED solutions to improve educational\noutcomes, ensuring that technological advances align with the principles of\ninclusivity, fairness, and educational excellence.\n","authors":["Richard Tong","Haoyang Li","Joleen Liang","Qingsong Wen"],"pdf_url":"https://arxiv.org/pdf/2403.14689v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2310.01777v2","updated":"2024-03-25T04:04:05Z","published":"2023-10-03T03:56:26Z","title":"SEA: Sparse Linear Attention with Estimated Attention Mask","summary":"  The transformer architecture has driven breakthroughs in recent years on\ntasks which require modeling pairwise relationships between sequential\nelements, as is the case in natural language understanding. However, long\nseqeuences pose a problem due to the quadratic complexity of the attention\noperation. Previous research has aimed to lower the complexity by sparsifying\nor linearly approximating the attention matrix. Yet, these approaches cannot\nstraightforwardly distill knowledge from a teacher's attention matrix and often\nrequire complete retraining from scratch. Furthermore, previous sparse and\nlinear approaches lose interpretability if they cannot produce full attention\nmatrices. To address these challenges, we propose SEA: Sparse linear attention\nwith an Estimated Attention mask. SEA estimates the attention matrix with\nlinear complexity via kernel-based linear attention, then subsequently creates\na sparse attention matrix with a top-k selection to perform a sparse attention\noperation. For language modeling tasks (Wikitext2), previous linear and sparse\nattention methods show roughly two-fold worse perplexity scores over the\nquadratic OPT-1.3B baseline, while SEA achieves better perplexity than\nOPT-1.3B, using roughly half the memory of OPT-1.3B, providing interpretable\nattention matrix. We believe that our work will have a large practical impact,\nas it opens the possibility of running large transformers on resource-limited\ndevices with less memory.\n","authors":["Heejun Lee","Jina Kim","Jeffrey Willette","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2310.01777v2.pdf","comment":"9 main pages"},{"id":"http://arxiv.org/abs/2403.16405v1","updated":"2024-03-25T03:44:36Z","published":"2024-03-25T03:44:36Z","title":"Ensemble Adversarial Defense via Integration of Multiple Dispersed Low\n  Curvature Models","summary":"  The integration of an ensemble of deep learning models has been extensively\nexplored to enhance defense against adversarial attacks. The diversity among\nsub-models increases the attack cost required to deceive the majority of the\nensemble, thereby improving the adversarial robustness. While existing\napproaches mainly center on increasing diversity in feature representations or\ndispersion of first-order gradients with respect to input, the limited\ncorrelation between these diversity metrics and adversarial robustness\nconstrains the performance of ensemble adversarial defense. In this work, we\naim to enhance ensemble diversity by reducing attack transferability. We\nidentify second-order gradients, which depict the loss curvature, as a key\nfactor in adversarial robustness. Computing the Hessian matrix involved in\nsecond-order gradients is computationally expensive. To address this, we\napproximate the Hessian-vector product using differential approximation. Given\nthat low curvature provides better robustness, our ensemble model was designed\nto consider the influence of curvature among different sub-models. We introduce\na novel regularizer to train multiple more-diverse low-curvature network\nmodels. Extensive experiments across various datasets demonstrate that our\nensemble model exhibits superior robustness against a range of attacks,\nunderscoring the effectiveness of our approach.\n","authors":["Kaikang Zhao","Xi Chen","Wei Huang","Liuxin Ding","Xianglong Kong","Fan Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16405v1.pdf","comment":"Accepted to The 2024 International Joint Conference on Neural\n  Networks (IJCNN)"},{"id":"http://arxiv.org/abs/2403.16398v1","updated":"2024-03-25T03:26:01Z","published":"2024-03-25T03:26:01Z","title":"Rethinking the Representation in Federated Unsupervised Learning with\n  Non-IID Data","summary":"  Federated learning achieves effective performance in modeling decentralized\ndata. In practice, client data are not well-labeled, which makes it potential\nfor federated unsupervised learning (FUSL) with non-IID data. However, the\nperformance of existing FUSL methods suffers from insufficient representations,\ni.e., (1) representation collapse entanglement among local and global models,\nand (2) inconsistent representation spaces among local models. The former\nindicates that representation collapse in local model will subsequently impact\nthe global model and other local models. The latter means that clients model\ndata representation with inconsistent parameters due to the deficiency of\nsupervision signals. In this work, we propose FedU2 which enhances generating\nuniform and unified representation in FUSL with non-IID data. Specifically,\nFedU2 consists of flexible uniform regularizer (FUR) and efficient unified\naggregator (EUA). FUR in each client avoids representation collapse via\ndispersing samples uniformly, and EUA in server promotes unified representation\nby constraining consistent client model updating. To extensively validate the\nperformance of FedU2, we conduct both cross-device and cross-silo evaluation\nexperiments on two benchmark datasets, i.e., CIFAR10 and CIFAR100.\n","authors":["Xinting Liao","Weiming Liu","Chaochao Chen","Pengyang Zhou","Fengyuan Yu","Huabin Zhu","Binhui Yao","Tao Wang","Xiaolin Zheng","Yanchao Tan"],"pdf_url":"https://arxiv.org/pdf/2403.16398v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2309.02836v2","updated":"2024-03-25T03:17:30Z","published":"2023-09-06T08:48:03Z","title":"BigVSAN: Enhancing GAN-based Neural Vocoders with Slicing Adversarial\n  Network","summary":"  Generative adversarial network (GAN)-based vocoders have been intensively\nstudied because they can synthesize high-fidelity audio waveforms faster than\nreal-time. However, it has been reported that most GANs fail to obtain the\noptimal projection for discriminating between real and fake data in the feature\nspace. In the literature, it has been demonstrated that slicing adversarial\nnetwork (SAN), an improved GAN training framework that can find the optimal\nprojection, is effective in the image generation task. In this paper, we\ninvestigate the effectiveness of SAN in the vocoding task. For this purpose, we\npropose a scheme to modify least-squares GAN, which most GAN-based vocoders\nadopt, so that their loss functions satisfy the requirements of SAN. Through\nour experiments, we demonstrate that SAN can improve the performance of\nGAN-based vocoders, including BigVGAN, with small modifications. Our code is\navailable at https://github.com/sony/bigvsan.\n","authors":["Takashi Shibuya","Yuhta Takida","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2309.02836v2.pdf","comment":"Accepted at ICASSP 2024. Equation (5) in the previous version is\n  wrong. We modified it"},{"id":"http://arxiv.org/abs/2403.16393v1","updated":"2024-03-25T03:17:27Z","published":"2024-03-25T03:17:27Z","title":"Concurrent Linguistic Error Detection (CLED) for Large Language Models","summary":"  The wide adoption of Large language models (LLMs) makes their dependability a\npressing concern. Detection of errors is the first step to mitigating their\nimpact on a system and thus, efficient error detection for LLMs is an important\nissue. In many settings, the LLM is considered as a black box with no access to\nthe internal nodes; this prevents the use of many error detection schemes that\nneed access to the model's internal nodes. An interesting observation is that\nthe output of LLMs in error-free operation should be valid and normal text.\nTherefore, when the text is not valid or differs significantly from normal\ntext, it is likely that there is an error. Based on this observation we propose\nto perform Concurrent Linguistic Error Detection (CLED); this scheme extracts\nsome linguistic features of the text generated by the LLM and feeds them to a\nconcurrent classifier that detects errors. Since the proposed error detection\nmechanism only relies on the outputs of the model, then it can be used on LLMs\nin which there is no access to the internal nodes. The proposed CLED scheme has\nbeen evaluated on the T5 model when used for news summarization and on the\nOPUS-MT model when used for translation. In both cases, the same set of\nlinguistic features has been used for error detection to illustrate the\napplicability of the proposed scheme beyond a specific case. The results show\nthat CLED can detect most of the errors at a low overhead penalty. The use of\nthe concurrent classifier also enables a trade-off between error detection\neffectiveness and its associated overhead, so providing flexibility to a\ndesigner.\n","authors":["Jinhua Zhu","Javier Conde","Zhen Gao","Pedro Reviriego","Shanshan Liu","Fabrizio Lombardi"],"pdf_url":"https://arxiv.org/pdf/2403.16393v1.pdf","comment":"11 pages, 6 figures, 30 references"},{"id":"http://arxiv.org/abs/2403.16391v1","updated":"2024-03-25T03:13:56Z","published":"2024-03-25T03:13:56Z","title":"Physics-informed RL for Maximal Safety Probability Estimation","summary":"  Accurate risk quantification and reachability analysis are crucial for safe\ncontrol and learning, but sampling from rare events, risky states, or long-term\ntrajectories can be prohibitively costly. Motivated by this, we study how to\nestimate the long-term safety probability of maximally safe actions without\nsufficient coverage of samples from risky states and long-term trajectories.\nThe use of maximal safety probability in control and learning is expected to\navoid conservative behaviors due to over-approximation of risk. Here, we first\nshow that long-term safety probability, which is multiplicative in time, can be\nconverted into additive costs and be solved using standard reinforcement\nlearning methods. We then derive this probability as solutions of partial\ndifferential equations (PDEs) and propose Physics-Informed Reinforcement\nLearning (PIRL) algorithm. The proposed method can learn using sparse rewards\nbecause the physics constraints help propagate risk information through\nneighbors. This suggests that, for the purpose of extracting more information\nfor efficient learning, physics constraints can serve as an alternative to\nreward shaping. The proposed method can also estimate long-term risk using\nshort-term samples and deduce the risk of unsampled states. This feature is in\nstark contrast with the unconstrained deep RL that demands sufficient data\ncoverage. These merits of the proposed method are demonstrated in numerical\nsimulation.\n","authors":["Hikaru Hoshino","Yorie Nakahira"],"pdf_url":"https://arxiv.org/pdf/2403.16391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15365v2","updated":"2024-03-25T03:06:08Z","published":"2024-03-22T17:33:11Z","title":"A Transfer Attack to Image Watermarks","summary":"  Watermark has been widely deployed by industry to detect AI-generated images.\nThe robustness of such watermark-based detector against evasion attacks in the\nwhite-box and black-box settings is well understood in the literature. However,\nthe robustness in the no-box setting is much less understood. In particular,\nmultiple studies claimed that image watermark is robust in such setting. In\nthis work, we propose a new transfer evasion attack to image watermark in the\nno-box setting. Our transfer attack adds a perturbation to a watermarked image\nto evade multiple surrogate watermarking models trained by the attacker itself,\nand the perturbed watermarked image also evades the target watermarking model.\nOur major contribution is to show that, both theoretically and empirically,\nwatermark-based AI-generated image detector is not robust to evasion attacks\neven if the attacker does not have access to the watermarking model nor the\ndetection API.\n","authors":["Yuepeng Hu","Zhengyuan Jiang","Moyang Guo","Neil Gong"],"pdf_url":"https://arxiv.org/pdf/2403.15365v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16377v1","updated":"2024-03-25T02:47:29Z","published":"2024-03-25T02:47:29Z","title":"Real-time Adaptation for Condition Monitoring Signal Prediction using\n  Label-aware Neural Processes","summary":"  Building a predictive model that rapidly adapts to real-time condition\nmonitoring (CM) signals is critical for engineering systems/units.\nUnfortunately, many current methods suffer from a trade-off between\nrepresentation power and agility in online settings. For instance, parametric\nmethods that assume an underlying functional form for CM signals facilitate\nefficient online prediction updates. However, this simplification leads to\nvulnerability to model specifications and an inability to capture complex\nsignals. On the other hand, approaches based on over-parameterized or\nnon-parametric models can excel at explaining complex nonlinear signals, but\nreal-time updates for such models pose a challenging task. In this paper, we\npropose a neural process-based approach that addresses this trade-off. It\nencodes available observations within a CM signal into a representation space\nand then reconstructs the signal's history and evolution for prediction. Once\ntrained, the model can encode an arbitrary number of observations without\nrequiring retraining, enabling on-the-spot real-time predictions along with\nquantified uncertainty and can be readily updated as more online data is\ngathered. Furthermore, our model is designed to incorporate qualitative\ninformation (i.e., labels) from individual units. This integration not only\nenhances individualized predictions for each unit but also enables joint\ninference for both signals and their associated labels. Numerical studies on\nboth synthetic and real-world data in reliability engineering highlight the\nadvantageous features of our model in real-time adaptation, enhanced signal\nprediction with uncertainty quantification, and joint prediction for labels and\nsignals.\n","authors":["Seokhyun Chung","Raed Al Kontar"],"pdf_url":"https://arxiv.org/pdf/2403.16377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09506v2","updated":"2024-03-25T02:45:35Z","published":"2024-03-14T15:53:04Z","title":"Don't Judge by the Look: Towards Motion Coherent Video Representation","summary":"  Current training pipelines in object recognition neglect Hue Jittering when\ndoing data augmentation as it not only brings appearance changes that are\ndetrimental to classification, but also the implementation is inefficient in\npractice. In this study, we investigate the effect of hue variance in the\ncontext of video understanding and find this variance to be beneficial since\nstatic appearances are less important in videos that contain motion\ninformation. Based on this observation, we propose a data augmentation method\nfor video understanding, named Motion Coherent Augmentation (MCA), that\nintroduces appearance variation in videos and implicitly encourages the model\nto prioritize motion patterns, rather than static appearances. Concretely, we\npropose an operation SwapMix to efficiently modify the appearance of video\nsamples, and introduce Variation Alignment (VA) to resolve the distribution\nshift caused by SwapMix, enforcing the model to learn appearance invariant\nrepresentations. Comprehensive empirical evaluation across various\narchitectures and different datasets solidly validates the effectiveness and\ngeneralization ability of MCA, and the application of VA in other augmentation\nmethods. Code is available at https://github.com/BeSpontaneous/MCA-pytorch.\n","authors":["Yitian Zhang","Yue Bai","Huan Wang","Yizhou Wang","Yun Fu"],"pdf_url":"https://arxiv.org/pdf/2403.09506v2.pdf","comment":"Accepted by ICLR2024"},{"id":"http://arxiv.org/abs/2403.16374v1","updated":"2024-03-25T02:38:34Z","published":"2024-03-25T02:38:34Z","title":"ProIn: Learning to Predict Trajectory Based on Progressive Interactions\n  for Autonomous Driving","summary":"  Accurate motion prediction of pedestrians, cyclists, and other surrounding\nvehicles (all called agents) is very important for autonomous driving. Most\nexisting works capture map information through an one-stage interaction with\nmap by vector-based attention, to provide map constraints for social\ninteraction and multi-modal differentiation. However, these methods have to\nencode all required map rules into the focal agent's feature, so as to retain\nall possible intentions' paths while at the meantime to adapt to potential\nsocial interaction. In this work, a progressive interaction network is proposed\nto enable the agent's feature to progressively focus on relevant maps, in order\nto better learn agents' feature representation capturing the relevant map\nconstraints. The network progressively encode the complex influence of map\nconstraints into the agent's feature through graph convolutions at the\nfollowing three stages: after historical trajectory encoder, after social\ninteraction, and after multi-modal differentiation. In addition, a weight\nallocation mechanism is proposed for multi-modal training, so that each mode\ncan obtain learning opportunities from a single-mode ground truth. Experiments\nhave validated the superiority of progressive interactions to the existing\none-stage interaction, and demonstrate the effectiveness of each component.\nEncouraging results were obtained in the challenging benchmarks.\n","authors":["Yinke Dong","Haifeng Yuan","Hongkun Liu","Wei Jing","Fangzhen Li","Hongmin Liu","Bin Fan"],"pdf_url":"https://arxiv.org/pdf/2403.16374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.10438v4","updated":"2024-03-25T02:37:09Z","published":"2022-04-21T23:59:17Z","title":"EVOTER: Evolution of Transparent Explainable Rule-sets","summary":"  Most AI systems are black boxes generating reasonable outputs for given\ninputs. Some domains, however, have explainability and trustworthiness\nrequirements that cannot be directly met by these approaches. Various methods\nhave therefore been developed to interpret black-box models after training.\nThis paper advocates an alternative approach where the models are transparent\nand explainable to begin with. This approach, EVOTER, evolves rule-sets based\non simple logical expressions. The approach is evaluated in several\nprediction/classification and prescription/policy search domains with and\nwithout a surrogate. It is shown to discover meaningful rule sets that perform\nsimilarly to black-box models. The rules can provide insight into the domain,\nand make biases hidden in the data explicit. It may also be possible to edit\nthem directly to remove biases and add constraints. EVOTER thus forms a\npromising foundation for building trustworthy AI systems for real-world\napplications in the future.\n","authors":["Hormoz Shahrzad","Babak Hodjat","Risto Miikkulainen"],"pdf_url":"https://arxiv.org/pdf/2204.10438v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16372v1","updated":"2024-03-25T02:32:43Z","published":"2024-03-25T02:32:43Z","title":"SignSGD with Federated Voting","summary":"  Distributed learning is commonly used for accelerating model training by\nharnessing the computational capabilities of multiple-edge devices. However, in\npractical applications, the communication delay emerges as a bottleneck due to\nthe substantial information exchange required between workers and a central\nparameter server. SignSGD with majority voting (signSGD-MV) is an effective\ndistributed learning algorithm that can significantly reduce communication\ncosts by one-bit quantization. However, due to heterogeneous computational\ncapabilities, it fails to converge when the mini-batch sizes differ among\nworkers. To overcome this, we propose a novel signSGD optimizer with\n\\textit{federated voting} (signSGD-FV). The idea of federated voting is to\nexploit learnable weights to perform weighted majority voting. The server\nlearns the weights assigned to the edge devices in an online fashion based on\ntheir computational capabilities. Subsequently, these weights are employed to\ndecode the signs of the aggregated local gradients in such a way to minimize\nthe sign decoding error probability. We provide a unified convergence rate\nanalysis framework applicable to scenarios where the estimated weights are\nknown to the parameter server either perfectly or imperfectly. We demonstrate\nthat the proposed signSGD-FV algorithm has a theoretical convergence guarantee\neven when edge devices use heterogeneous mini-batch sizes. Experimental results\nshow that signSGD-FV outperforms signSGD-MV, exhibiting a faster convergence\nrate, especially in heterogeneous mini-batch sizes.\n","authors":["Chanho Park","H. Vincent Poor","Namyoon Lee"],"pdf_url":"https://arxiv.org/pdf/2403.16372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16369v1","updated":"2024-03-25T02:17:54Z","published":"2024-03-25T02:17:54Z","title":"Learning Action-based Representations Using Invariance","summary":"  Robust reinforcement learning agents using high-dimensional observations must\nbe able to identify relevant state features amidst many exogeneous distractors.\nA representation that captures controllability identifies these state elements\nby determining what affects agent control. While methods such as inverse\ndynamics and mutual information capture controllability for a limited number of\ntimesteps, capturing long-horizon elements remains a challenging problem.\nMyopic controllability can capture the moment right before an agent crashes\ninto a wall, but not the control-relevance of the wall while the agent is still\nsome distance away. To address this we introduce action-bisimulation encoding,\na method inspired by the bisimulation invariance pseudometric, that extends\nsingle-step controllability with a recursive invariance constraint. By doing\nthis, action-bisimulation learns a multi-step controllability metric that\nsmoothly discounts distant state features that are relevant for control. We\ndemonstrate that action-bisimulation pretraining on reward-free, uniformly\nrandom data improves sample efficiency in several environments, including a\nphotorealistic 3D simulation domain, Habitat. Additionally, we provide\ntheoretical analysis and qualitative results demonstrating the information\ncaptured by action-bisimulation.\n","authors":["Max Rudolph","Caleb Chuck","Kevin Black","Misha Lvovsky","Scott Niekum","Amy Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15048v2","updated":"2024-03-25T02:08:01Z","published":"2024-03-22T09:13:09Z","title":"Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning","summary":"  Large-scale Text-to-Image (TTI) models have become a common approach for\ngenerating training data in various generative fields. However, visual\nhallucinations, which contain perceptually critical defects, remain a concern,\nespecially in non-photorealistic styles like cartoon characters. We propose a\nnovel visual hallucination detection system for cartoon character images\ngenerated by TTI models. Our approach leverages pose-aware in-context visual\nlearning (PA-ICVL) with Vision-Language Models (VLMs), utilizing both RGB\nimages and pose information. By incorporating pose guidance from a fine-tuned\npose estimator, we enable VLMs to make more accurate decisions. Experimental\nresults demonstrate significant improvements in identifying visual\nhallucinations compared to baseline methods relying solely on RGB images. This\nresearch advances TTI models by mitigating visual hallucinations, expanding\ntheir potential in non-photorealistic domains.\n","authors":["Bumsoo Kim","Wonseop Shin","Kyuchul Lee","Sanghyun Seo"],"pdf_url":"https://arxiv.org/pdf/2403.15048v2.pdf","comment":"11 pages, 12 figures, 1 table, Project page:\n  https://gh-bumsookim.github.io/Cartoon-Hallucinations-Detection/"},{"id":"http://arxiv.org/abs/2403.16365v1","updated":"2024-03-25T02:03:38Z","published":"2024-03-25T02:03:38Z","title":"Generating Potent Poisons and Backdoors from Scratch with Guided\n  Diffusion","summary":"  Modern neural networks are often trained on massive datasets that are web\nscraped with minimal human inspection. As a result of this insecure curation\npipeline, an adversary can poison or backdoor the resulting model by uploading\nmalicious data to the internet and waiting for a victim to scrape and train on\nit. Existing approaches for creating poisons and backdoors start with randomly\nsampled clean data, called base samples, and then modify those samples to craft\npoisons. However, some base samples may be significantly more amenable to\npoisoning than others. As a result, we may be able to craft more potent poisons\nby carefully choosing the base samples. In this work, we use guided diffusion\nto synthesize base samples from scratch that lead to significantly more potent\npoisons and backdoors than previous state-of-the-art attacks. Our Guided\nDiffusion Poisoning (GDP) base samples can be combined with any downstream\npoisoning or backdoor attack to boost its effectiveness. Our implementation\ncode is publicly available at: https://github.com/hsouri/GDP .\n","authors":["Hossein Souri","Arpit Bansal","Hamid Kazemi","Liam Fowl","Aniruddha Saha","Jonas Geiping","Andrew Gordon Wilson","Rama Chellappa","Tom Goldstein","Micah Goldblum"],"pdf_url":"https://arxiv.org/pdf/2403.16365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01391v2","updated":"2024-03-25T01:55:36Z","published":"2023-04-03T21:42:42Z","title":"Counterfactual Learning on Graphs: A Survey","summary":"  Graph-structured data are pervasive in the real-world such as social\nnetworks, molecular graphs and transaction networks. Graph neural networks\n(GNNs) have achieved great success in representation learning on graphs,\nfacilitating various downstream tasks. However, GNNs have several drawbacks\nsuch as lacking interpretability, can easily inherit the bias of data and\ncannot model casual relations. Recently, counterfactual learning on graphs has\nshown promising results in alleviating these drawbacks. Various approaches have\nbeen proposed for counterfactual fairness, explainability, link prediction and\nother applications on graphs. To facilitate the development of this promising\ndirection, in this survey, we categorize and comprehensively review papers on\ngraph counterfactual learning. We divide existing methods into four categories\nbased on problems studied. For each category, we provide background and\nmotivating examples, a general framework summarizing existing works and a\ndetailed review of these works. We point out promising future research\ndirections at the intersection of graph-structured data, counterfactual\nlearning, and real-world applications. To offer a comprehensive view of\nresources for future studies, we compile a collection of open-source\nimplementations, public datasets, and commonly-used evaluation metrics. This\nsurvey aims to serve as a ``one-stop-shop'' for building a unified\nunderstanding of graph counterfactual learning categories and current\nresources. We also maintain a repository for papers and resources and will keep\nupdating the repository\nhttps://github.com/TimeLovercc/Awesome-Graph-Causal-Learning.\n","authors":["Zhimeng Guo","Teng Xiao","Zongyu Wu","Charu Aggarwal","Hui Liu","Suhang Wang"],"pdf_url":"https://arxiv.org/pdf/2304.01391v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16354v1","updated":"2024-03-25T01:12:57Z","published":"2024-03-25T01:12:57Z","title":"ChatDBG: An AI-Powered Debugging Assistant","summary":"  This paper presents ChatDBG, the first AI-powered debugging assistant.\nChatDBG integrates large language models (LLMs) to significantly enhance the\ncapabilities and user-friendliness of conventional debuggers. ChatDBG lets\nprogrammers engage in a collaborative dialogue with the debugger, allowing them\nto pose complex questions about program state, perform root cause analysis for\ncrashes or assertion failures, and explore open-ended queries like \"why is x\nnull?\". To handle these queries, ChatDBG grants the LLM autonomy to take the\nwheel and drive debugging by issuing commands to navigate through stacks and\ninspect program state; it then reports its findings and yields back control to\nthe programmer. Our ChatDBG prototype integrates with standard debuggers\nincluding LLDB, GDB, and WinDBG for native code and Pdb for Python. Our\nevaluation across a diverse set of code, including C/C++ code with known bugs\nand a suite of Python code including standalone scripts and Jupyter notebooks,\ndemonstrates that ChatDBG can successfully analyze root causes, explain bugs,\nand generate accurate fixes for a wide range of real-world errors. For the\nPython programs, a single query led to an actionable bug fix 67% of the time;\none additional follow-up query increased the success rate to 85%. ChatDBG has\nseen rapid uptake; it has already been downloaded nearly 30,000 times.\n","authors":["Kyla Levin","Nicolas van Kempen","Emery D. Berger","Stephen N. Freund"],"pdf_url":"https://arxiv.org/pdf/2403.16354v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2310.00290v5","updated":"2024-03-25T01:07:23Z","published":"2023-09-30T07:46:47Z","title":"Universality of almost periodicity in bounded discrete time series","summary":"  We consider arbitrary bounded discrete time series. From its statistical\nfeature, without any use of the Fourier transform, we find an almost periodic\nfunction which suitably characterizes the corresponding time series.\n","authors":["Chikara Nakayama","Tsuyoshi Yoneda"],"pdf_url":"https://arxiv.org/pdf/2310.00290v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16347v1","updated":"2024-03-25T00:50:27Z","published":"2024-03-25T00:50:27Z","title":"ChatGPT Incorrectness Detection in Software Reviews","summary":"  We conducted a survey of 135 software engineering (SE) practitioners to\nunderstand how they use Generative AI-based chatbots like ChatGPT for SE tasks.\nWe find that they want to use ChatGPT for SE tasks like software library\nselection but often worry about the truthfulness of ChatGPT responses. We\ndeveloped a suite of techniques and a tool called CID (ChatGPT Incorrectness\nDetector) to automatically test and detect the incorrectness in ChatGPT\nresponses. CID is based on the iterative prompting to ChatGPT by asking it\ncontextually similar but textually divergent questions (using an approach that\nutilizes metamorphic relationships in texts). The underlying principle in CID\nis that for a given question, a response that is different from other responses\n(across multiple incarnations of the question) is likely an incorrect response.\nIn a benchmark study of library selection, we show that CID can detect\nincorrect responses from ChatGPT with an F1-score of 0.74 - 0.75.\n","authors":["Minaoar Hossain Tanzil","Junaed Younus Khan","Gias Uddin"],"pdf_url":"https://arxiv.org/pdf/2403.16347v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.06626v2","updated":"2024-03-25T00:45:30Z","published":"2023-01-16T22:30:53Z","title":"Masked Vector Quantization","summary":"  Generative models with discrete latent representations have recently\ndemonstrated an impressive ability to learn complex high-dimensional data\ndistributions. However, their performance relies on a long sequence of tokens\nper instance and a large number of codebook entries, resulting in long sampling\ntimes and considerable computation to fit the categorical posterior. To address\nthese issues, we propose the Masked Vector Quantization (MVQ) framework which\nincreases the representational capacity of each code vector by learning mask\nconfigurations via a stochastic winner-takes-all training regime called\nMultiple Hypothese Dropout (MH-Dropout). On ImageNet 64$\\times$64, MVQ reduces\nFID in existing vector quantization architectures by up to $68\\%$ at 2 tokens\nper instance and $57\\%$ at 5 tokens. These improvements widen as codebook\nentries is reduced and allows for $7\\textit{--}45\\times$ speed-up in token\nsampling during inference. As an additional benefit, we find that smaller\nlatent spaces lead to MVQ identifying transferable visual representations where\nmultiple can be smoothly combined.\n","authors":["David D. Nguyen","David Leibowitz","Surya Nepal","Salil S. Kanhere"],"pdf_url":"https://arxiv.org/pdf/2301.06626v2.pdf","comment":"A newer version of this manuscript was archived under 2312.11735"},{"id":"http://arxiv.org/abs/2403.16336v1","updated":"2024-03-25T00:21:34Z","published":"2024-03-25T00:21:34Z","title":"Predictive Inference in Multi-environment Scenarios","summary":"  We address the challenge of constructing valid confidence intervals and sets\nin problems of prediction across multiple environments. We investigate two\ntypes of coverage suitable for these problems, extending the jackknife and\nsplit-conformal methods to show how to obtain distribution-free coverage in\nsuch non-traditional, hierarchical data-generating scenarios. Our contributions\nalso include extensions for settings with non-real-valued responses and a\ntheory of consistency for predictive inference in these general problems. We\ndemonstrate a novel resizing method to adapt to problem difficulty, which\napplies both to existing approaches for predictive inference with hierarchical\ndata and the methods we develop; this reduces prediction set sizes using\nlimited information from the test environment, a key to the methods' practical\nperformance, which we evaluate through neurochemical sensing and species\nclassification datasets.\n","authors":["John C. Duchi","Suyash Gupta","Kuanhao Jiang","Pragya Sur"],"pdf_url":"https://arxiv.org/pdf/2403.16336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08774v2","updated":"2024-03-25T00:18:35Z","published":"2023-10-12T23:46:08Z","title":"PhyloGFN: Phylogenetic inference with generative flow networks","summary":"  Phylogenetics is a branch of computational biology that studies the\nevolutionary relationships among biological entities. Its long history and\nnumerous applications notwithstanding, inference of phylogenetic trees from\nsequence data remains challenging: the high complexity of tree space poses a\nsignificant obstacle for the current combinatorial and probabilistic\ntechniques. In this paper, we adopt the framework of generative flow networks\n(GFlowNets) to tackle two core problems in phylogenetics: parsimony-based and\nBayesian phylogenetic inference. Because GFlowNets are well-suited for sampling\ncomplex combinatorial structures, they are a natural choice for exploring and\nsampling from the multimodal posterior distribution over tree topologies and\nevolutionary distances. We demonstrate that our amortized posterior sampler,\nPhyloGFN, produces diverse and high-quality evolutionary hypotheses on real\nbenchmark datasets. PhyloGFN is competitive with prior works in marginal\nlikelihood estimation and achieves a closer fit to the target distribution than\nstate-of-the-art variational inference methods. Our code is available at\nhttps://github.com/zmy1116/phylogfn.\n","authors":["Mingyang Zhou","Zichao Yan","Elliot Layne","Nikolay Malkin","Dinghuai Zhang","Moksh Jain","Mathieu Blanchette","Yoshua Bengio"],"pdf_url":"https://arxiv.org/pdf/2310.08774v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16335v1","updated":"2024-03-25T00:17:43Z","published":"2024-03-25T00:17:43Z","title":"MEDDAP: Medical Dataset Enhancement via Diversified Augmentation\n  Pipeline","summary":"  The effectiveness of Deep Neural Networks (DNNs) heavily relies on the\nabundance and accuracy of available training data. However, collecting and\nannotating data on a large scale is often both costly and time-intensive,\nparticularly in medical cases where practitioners are already occupied with\ntheir duties. Moreover, ensuring that the model remains robust across various\nscenarios of image capture is crucial in medical domains, especially when\ndealing with ultrasound images that vary based on the settings of different\ndevices and the manual operation of the transducer. To address this challenge,\nwe introduce a novel pipeline called MEDDAP, which leverages Stable Diffusion\n(SD) models to augment existing small datasets by automatically generating new\ninformative labeled samples. Pretrained checkpoints for SD are typically based\non natural images, and training them for medical images requires significant\nGPU resources due to their heavy parameters. To overcome this challenge, we\nintroduce USLoRA (Ultrasound Low-Rank Adaptation), a novel fine-tuning method\ntailored specifically for ultrasound applications. USLoRA allows for selective\nfine-tuning of weights within SD, requiring fewer than 0.1\\% of parameters\ncompared to fully fine-tuning only the UNet portion of SD. To enhance dataset\ndiversity, we incorporate different adjectives into the generation process\nprompts, thereby desensitizing the classifiers to intensity changes across\ndifferent images. This approach is inspired by clinicians' decision-making\nprocesses regarding breast tumors, where tumor shape often plays a more crucial\nrole than intensity. In conclusion, our pipeline not only outperforms\nclassifiers trained on the original dataset but also demonstrates superior\nperformance when encountering unseen datasets. The source code is available at\nhttps://github.com/yasamin-med/MEDDAP.\n","authors":["Yasamin Medghalchi","Niloufar Zakariaei","Arman Rahmim","Ilker Hacihaliloglu"],"pdf_url":"https://arxiv.org/pdf/2403.16335v1.pdf","comment":"submitted to miccai 2024 submitted to miccai 2024 Submitted to\n  MICCAI-2024"},{"id":"http://arxiv.org/abs/2403.16334v1","updated":"2024-03-25T00:15:34Z","published":"2024-03-25T00:15:34Z","title":"Graphs Generalization under Distribution Shifts","summary":"  Traditional machine learning methods heavily rely on the independent and\nidentically distribution assumption, which imposes limitations when the test\ndistribution deviates from the training distribution. To address this crucial\nissue, out-of-distribution (OOD) generalization, which aims to achieve\nsatisfactory generalization performance when faced with unknown distribution\nshifts, has made a significant process. However, the OOD method for\ngraph-structured data currently lacks clarity and remains relatively unexplored\ndue to two primary challenges. Firstly, distribution shifts on graphs often\noccur simultaneously on node attributes and graph topology. Secondly, capturing\ninvariant information amidst diverse distribution shifts proves to be a\nformidable challenge. To overcome these obstacles, in this paper, we introduce\na novel framework, namely Graph Learning Invariant Domain genERation (GLIDER).\nThe goal is to (1) diversify variations across domains by modeling the\npotential seen or unseen variations of attribute distribution and topological\nstructure and (2) minimize the discrepancy of the variation in a representation\nspace where the target is to predict semantic labels. Extensive experiment\nresults indicate that our model outperforms baseline methods on node-level OOD\ngeneralization across domains in distribution shift on node features and\ntopological structures simultaneously.\n","authors":["Qin Tian","Wenjun Wang","Chen Zhao","Minglai Shao","Wang Zhang","Dong Li"],"pdf_url":"https://arxiv.org/pdf/2403.16334v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2304.02970v5","updated":"2024-03-25T08:50:42Z","published":"2023-04-06T09:54:06Z","title":"Unraveling Instance Associations: A Closer Look for Audio-Visual\n  Segmentation","summary":"  Audio-visual segmentation (AVS) is a challenging task that involves\naccurately segmenting sounding objects based on audio-visual cues. The\neffectiveness of audio-visual learning critically depends on achieving accurate\ncross-modal alignment between sound and visual objects. Successful audio-visual\nlearning requires two essential components: 1) a challenging dataset with\nhigh-quality pixel-level multi-class annotated images associated with audio\nfiles, and 2) a model that can establish strong links between audio information\nand its corresponding visual object. However, these requirements are only\npartially addressed by current methods, with training sets containing biased\naudio-visual data, and models that generalise poorly beyond this biased\ntraining set. In this work, we propose a new cost-effective strategy to build\nchallenging and relatively unbiased high-quality audio-visual segmentation\nbenchmarks. We also propose a new informative sample mining method for\naudio-visual supervised contrastive learning to leverage discriminative\ncontrastive samples to enforce cross-modal understanding. We show empirical\nresults that demonstrate the effectiveness of our benchmark. Furthermore,\nexperiments conducted on existing AVS datasets and on our new benchmark show\nthat our method achieves state-of-the-art (SOTA) segmentation accuracy.\n","authors":["Yuanhong Chen","Yuyuan Liu","Hu Wang","Fengbei Liu","Chong Wang","Helen Frazer","Gustavo Carneiro"],"pdf_url":"https://arxiv.org/pdf/2304.02970v5.pdf","comment":"Code is available at https://github.com/cyh-0/CAVP"},{"id":"http://arxiv.org/abs/2403.10066v2","updated":"2024-03-25T06:27:57Z","published":"2024-03-15T07:16:07Z","title":"Contrastive Pre-Training with Multi-View Fusion for No-Reference Point\n  Cloud Quality Assessment","summary":"  No-reference point cloud quality assessment (NR-PCQA) aims to automatically\nevaluate the perceptual quality of distorted point clouds without available\nreference, which have achieved tremendous improvements due to the utilization\nof deep neural networks. However, learning-based NR-PCQA methods suffer from\nthe scarcity of labeled data and usually perform suboptimally in terms of\ngeneralization. To solve the problem, we propose a novel contrastive\npre-training framework tailored for PCQA (CoPA), which enables the pre-trained\nmodel to learn quality-aware representations from unlabeled data. To obtain\nanchors in the representation space, we project point clouds with different\ndistortions into images and randomly mix their local patches to form mixed\nimages with multiple distortions. Utilizing the generated anchors, we constrain\nthe pre-training process via a quality-aware contrastive loss following the\nphilosophy that perceptual quality is closely related to both content and\ndistortion. Furthermore, in the model fine-tuning stage, we propose a\nsemantic-guided multi-view fusion module to effectively integrate the features\nof projected images from multiple perspectives. Extensive experiments show that\nour method outperforms the state-of-the-art PCQA methods on popular benchmarks.\nFurther investigations demonstrate that CoPA can also benefit existing\nlearning-based PCQA models.\n","authors":["Ziyu Shan","Yujie Zhang","Qi Yang","Haichen Yang","Yiling Xu","Jenq-Neng Hwang","Xiaozhong Xu","Shan Liu"],"pdf_url":"https://arxiv.org/pdf/2403.10066v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18107v2","updated":"2024-03-25T05:28:20Z","published":"2024-02-28T06:54:35Z","title":"Multimodal Interaction Modeling via Self-Supervised Multi-Task Learning\n  for Review Helpfulness Prediction","summary":"  In line with the latest research, the task of identifying helpful reviews\nfrom a vast pool of user-generated textual and visual data has become a\nprominent area of study. Effective modal representations are expected to\npossess two key attributes: consistency and differentiation. Current methods\ndesigned for Multimodal Review Helpfulness Prediction (MRHP) face limitations\nin capturing distinctive information due to their reliance on uniform\nmultimodal annotation. The process of adding varied multimodal annotations is\nnot only time-consuming but also labor-intensive. To tackle these challenges,\nwe propose an auto-generated scheme based on multi-task learning to generate\npseudo labels. This approach allows us to simultaneously train for the global\nmultimodal interaction task and the separate cross-modal interaction subtasks,\nenabling us to learn and leverage both consistency and differentiation\neffectively. Subsequently, experimental results validate the effectiveness of\npseudo labels, and our approach surpasses previous textual and multimodal\nbaseline models on two widely accessible benchmark datasets, providing a\nsolution to the MRHP problem.\n","authors":["HongLin Gong","Mengzhao Jia","Liqiang Jing"],"pdf_url":"https://arxiv.org/pdf/2402.18107v2.pdf","comment":"10 pages,4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.15048v2","updated":"2024-03-25T02:08:01Z","published":"2024-03-22T09:13:09Z","title":"Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning","summary":"  Large-scale Text-to-Image (TTI) models have become a common approach for\ngenerating training data in various generative fields. However, visual\nhallucinations, which contain perceptually critical defects, remain a concern,\nespecially in non-photorealistic styles like cartoon characters. We propose a\nnovel visual hallucination detection system for cartoon character images\ngenerated by TTI models. Our approach leverages pose-aware in-context visual\nlearning (PA-ICVL) with Vision-Language Models (VLMs), utilizing both RGB\nimages and pose information. By incorporating pose guidance from a fine-tuned\npose estimator, we enable VLMs to make more accurate decisions. Experimental\nresults demonstrate significant improvements in identifying visual\nhallucinations compared to baseline methods relying solely on RGB images. This\nresearch advances TTI models by mitigating visual hallucinations, expanding\ntheir potential in non-photorealistic domains.\n","authors":["Bumsoo Kim","Wonseop Shin","Kyuchul Lee","Sanghyun Seo"],"pdf_url":"https://arxiv.org/pdf/2403.15048v2.pdf","comment":"11 pages, 12 figures, 1 table, Project page:\n  https://gh-bumsookim.github.io/Cartoon-Hallucinations-Detection/"},{"id":"http://arxiv.org/abs/2308.09911v2","updated":"2024-03-25T01:54:41Z","published":"2023-08-19T05:34:13Z","title":"Noisy-Correspondence Learning for Text-to-Image Person Re-identification","summary":"  Text-to-image person re-identification (TIReID) is a compelling topic in the\ncross-modal community, which aims to retrieve the target person based on a\ntextual query. Although numerous TIReID methods have been proposed and achieved\npromising performance, they implicitly assume the training image-text pairs are\ncorrectly aligned, which is not always the case in real-world scenarios. In\npractice, the image-text pairs inevitably exist under-correlated or even\nfalse-correlated, a.k.a noisy correspondence (NC), due to the low quality of\nthe images and annotation errors. To address this problem, we propose a novel\nRobust Dual Embedding method (RDE) that can learn robust visual-semantic\nassociations even with NC. Specifically, RDE consists of two main components:\n1) A Confident Consensus Division (CCD) module that leverages the dual-grained\ndecisions of dual embedding modules to obtain a consensus set of clean training\ndata, which enables the model to learn correct and reliable visual-semantic\nassociations. 2) A Triplet Alignment Loss (TAL) relaxes the conventional\nTriplet Ranking loss with the hardest negative samples to a log-exponential\nupper bound over all negative ones, thus preventing the model collapse under NC\nand can also focus on hard-negative samples for promising performance. We\nconduct extensive experiments on three public benchmarks, namely CUHK-PEDES,\nICFG-PEDES, and RSTPReID, to evaluate the performance and robustness of our\nRDE. Our method achieves state-of-the-art results both with and without\nsynthetic noisy correspondences on all three datasets. Code is available at\nhttps://github.com/QinYang79/RDE.\n","authors":["Yang Qin","Yingke Chen","Dezhong Peng","Xi Peng","Joey Tianyi Zhou","Peng Hu"],"pdf_url":"https://arxiv.org/pdf/2308.09911v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17005v1","updated":"2024-03-25T17:59:40Z","published":"2024-03-25T17:59:40Z","title":"TRIP: Temporal Residual Learning with Image Noise Prior for\n  Image-to-Video Diffusion Models","summary":"  Recent advances in text-to-video generation have demonstrated the utility of\npowerful diffusion models. Nevertheless, the problem is not trivial when\nshaping diffusion models to animate static image (i.e., image-to-video\ngeneration). The difficulty originates from the aspect that the diffusion\nprocess of subsequent animated frames should not only preserve the faithful\nalignment with the given image but also pursue temporal coherence among\nadjacent frames. To alleviate this, we present TRIP, a new recipe of\nimage-to-video diffusion paradigm that pivots on image noise prior derived from\nstatic image to jointly trigger inter-frame relational reasoning and ease the\ncoherent temporal modeling via temporal residual learning. Technically, the\nimage noise prior is first attained through one-step backward diffusion process\nbased on both static image and noised video latent codes. Next, TRIP executes a\nresidual-like dual-path scheme for noise prediction: 1) a shortcut path that\ndirectly takes image noise prior as the reference noise of each frame to\namplify the alignment between the first frame and subsequent frames; 2) a\nresidual path that employs 3D-UNet over noised video and static image latent\ncodes to enable inter-frame relational reasoning, thereby easing the learning\nof the residual noise for each frame. Furthermore, both reference and residual\nnoise of each frame are dynamically merged via attention mechanism for final\nvideo generation. Extensive experiments on WebVid-10M, DTDB and MSR-VTT\ndatasets demonstrate the effectiveness of our TRIP for image-to-video\ngeneration. Please see our project page at https://trip-i2v.github.io/TRIP/.\n","authors":["Zhongwei Zhang","Fuchen Long","Yingwei Pan","Zhaofan Qiu","Ting Yao","Yang Cao","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2403.17005v1.pdf","comment":"CVPR 2024; Project page: https://trip-i2v.github.io/TRIP/"},{"id":"http://arxiv.org/abs/2403.17004v1","updated":"2024-03-25T17:59:35Z","published":"2024-03-25T17:59:35Z","title":"SD-DiT: Unleashing the Power of Self-supervised Discrimination in\n  Diffusion Transformer","summary":"  Diffusion Transformer (DiT) has emerged as the new trend of generative\ndiffusion models on image generation. In view of extremely slow convergence in\ntypical DiT, recent breakthroughs have been driven by mask strategy that\nsignificantly improves the training efficiency of DiT with additional\nintra-image contextual learning. Despite this progress, mask strategy still\nsuffers from two inherent limitations: (a) training-inference discrepancy and\n(b) fuzzy relations between mask reconstruction & generative diffusion process,\nresulting in sub-optimal training of DiT. In this work, we address these\nlimitations by novelly unleashing the self-supervised discrimination knowledge\nto boost DiT training. Technically, we frame our DiT in a teacher-student\nmanner. The teacher-student discriminative pairs are built on the diffusion\nnoises along the same Probability Flow Ordinary Differential Equation (PF-ODE).\nInstead of applying mask reconstruction loss over both DiT encoder and decoder,\nwe decouple DiT encoder and decoder to separately tackle discriminative and\ngenerative objectives. In particular, by encoding discriminative pairs with\nstudent and teacher DiT encoders, a new discriminative loss is designed to\nencourage the inter-image alignment in the self-supervised embedding space.\nAfter that, student samples are fed into student DiT decoder to perform the\ntypical generative diffusion task. Extensive experiments are conducted on\nImageNet dataset, and our method achieves a competitive balance between\ntraining cost and generative capacity.\n","authors":["Rui Zhu","Yingwei Pan","Yehao Li","Ting Yao","Zhenglong Sun","Tao Mei","Chang Wen Chen"],"pdf_url":"https://arxiv.org/pdf/2403.17004v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17001v1","updated":"2024-03-25T17:59:31Z","published":"2024-03-25T17:59:31Z","title":"VP3D: Unleashing 2D Visual Prompt for Text-to-3D Generation","summary":"  Recent innovations on text-to-3D generation have featured Score Distillation\nSampling (SDS), which enables the zero-shot learning of implicit 3D models\n(NeRF) by directly distilling prior knowledge from 2D diffusion models.\nHowever, current SDS-based models still struggle with intricate text prompts\nand commonly result in distorted 3D models with unrealistic textures or\ncross-view inconsistency issues. In this work, we introduce a novel Visual\nPrompt-guided text-to-3D diffusion model (VP3D) that explicitly unleashes the\nvisual appearance knowledge in 2D visual prompt to boost text-to-3D generation.\nInstead of solely supervising SDS with text prompt, VP3D first capitalizes on\n2D diffusion model to generate a high-quality image from input text, which\nsubsequently acts as visual prompt to strengthen SDS optimization with explicit\nvisual appearance. Meanwhile, we couple the SDS optimization with additional\ndifferentiable reward function that encourages rendering images of 3D models to\nbetter visually align with 2D visual prompt and semantically match with text\nprompt. Through extensive experiments, we show that the 2D Visual Prompt in our\nVP3D significantly eases the learning of visual appearance of 3D models and\nthus leads to higher visual fidelity with more detailed textures. It is also\nappealing in view that when replacing the self-generating visual prompt with a\ngiven reference image, VP3D is able to trigger a new task of stylized\ntext-to-3D generation. Our project page is available at\nhttps://vp3d-cvpr24.github.io.\n","authors":["Yang Chen","Yingwei Pan","Haibo Yang","Ting Yao","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2403.17001v1.pdf","comment":"CVPR 2024; Project page: https://vp3d-cvpr24.github.io"},{"id":"http://arxiv.org/abs/2403.17000v1","updated":"2024-03-25T17:59:26Z","published":"2024-03-25T17:59:26Z","title":"Learning Spatial Adaptation and Temporal Coherence in Diffusion Models\n  for Video Super-Resolution","summary":"  Diffusion models are just at a tipping point for image super-resolution task.\nNevertheless, it is not trivial to capitalize on diffusion models for video\nsuper-resolution which necessitates not only the preservation of visual\nappearance from low-resolution to high-resolution videos, but also the temporal\nconsistency across video frames. In this paper, we propose a novel approach,\npursuing Spatial Adaptation and Temporal Coherence (SATeCo), for video\nsuper-resolution. SATeCo pivots on learning spatial-temporal guidance from\nlow-resolution videos to calibrate both latent-space high-resolution video\ndenoising and pixel-space video reconstruction. Technically, SATeCo freezes all\nthe parameters of the pre-trained UNet and VAE, and only optimizes two\ndeliberately-designed spatial feature adaptation (SFA) and temporal feature\nalignment (TFA) modules, in the decoder of UNet and VAE. SFA modulates frame\nfeatures via adaptively estimating affine parameters for each pixel,\nguaranteeing pixel-wise guidance for high-resolution frame synthesis. TFA\ndelves into feature interaction within a 3D local window (tubelet) through\nself-attention, and executes cross-attention between tubelet and its\nlow-resolution counterpart to guide temporal feature alignment. Extensive\nexperiments conducted on the REDS4 and Vid4 datasets demonstrate the\neffectiveness of our approach.\n","authors":["Zhikai Chen","Fuchen Long","Zhaofan Qiu","Ting Yao","Wengang Zhou","Jiebo Luo","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2403.17000v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.16985v1","updated":"2024-03-25T17:46:51Z","published":"2024-03-25T17:46:51Z","title":"Towards Low-Latency and Energy-Efficient Hybrid P2P-CDN Live Video\n  Streaming","summary":"  Streaming segmented videos over the Hypertext Transfer Protocol (HTTP) is an\nincreasingly popular approach in both live and video-on-demand (VoD)\napplications. However, designing a scalable and adaptable framework that\nreduces servers energy consumption and supports low latency and high quality\nservices, particularly for live video streaming scenarios, is still challenging\nfor Over-The-Top (OTT) service providers. To address such challenges, this\npaper introduces a new hybrid P2P-CDN framework that leverages new networking\nand computing paradigms, i.e., Network Function Virtualization (NFV) and edge\ncomputing for live video streaming. The proposed framework introduces a\nmulti-layer architecture and a tree of possible actions therein (an action\ntree), taking into account all available resources from peers, edge, and CDN\nservers to efficiently distribute video fetching and transcoding tasks across a\nhybrid P2P-CDN network, consequently enhancing the users latency and video\nquality. We also discuss our testbed designed to validate the framework and\ncompare it with baseline methods. The experimental results indicate that the\nproposed framework improves user Quality of Experience (QoE), reduces client\nserving latency, and improves edge server energy consumption compared to\nbaseline approaches.\n","authors":["Reza Farahani","Christian Timmerer","Hermann Hellwagner"],"pdf_url":"https://arxiv.org/pdf/2403.16985v1.pdf","comment":"6 pages, 3 figures, Special Issue on Sustainable Multimedia\n  Communications and Services, IEEE MMTC Communications"},{"id":"http://arxiv.org/abs/2403.16951v1","updated":"2024-03-25T17:12:43Z","published":"2024-03-25T17:12:43Z","title":"Network-Assisted Delivery of Adaptive Video Streaming Services through\n  CDN, SDN, and MEC","summary":"  Multimedia applications, mainly video streaming services, are currently the\ndominant source of network load worldwide. In recent Video-on-Demand (VoD) and\nlive video streaming services, traditional streaming delivery techniques have\nbeen replaced by adaptive solutions based on the HTTP protocol. Current trends\ntoward high-resolution (e.g., 8K) and/or low-latency VoD and live video\nstreaming pose new challenges to end-to-end (E2E) bandwidth demand and have\nstringent delay requirements. To do this, video providers typically rely on\nContent Delivery Networks (CDNs) to ensure that they provide scalable video\nstreaming services. To support future streaming scenarios involving millions of\nusers, it is necessary to increase the CDNs' efficiency. It is widely agreed\nthat these requirements may be satisfied by adopting emerging networking\ntechniques to present Network-Assisted Video Streaming (NAVS) methods.\nMotivated by this, this thesis goes one step beyond traditional pure\nclient-based HAS algorithms by incorporating (an) in-network component(s) with\na broader view of the network to present completely transparent NAVS solutions\nfor HAS clients.\n","authors":["Reza Farahani"],"pdf_url":"https://arxiv.org/pdf/2403.16951v1.pdf","comment":"PhD thesis defended in 22.08.2023\n  (https://netlibrary.aau.at/obvuklhs/content/titleinfo/9173622)"}]},"2024-03-26T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2402.13452v2","updated":"2024-03-26T17:59:14Z","published":"2024-02-21T01:11:28Z","title":"LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based\n  on Twitter Data","summary":"  Prior research on Twitter (now X) data has provided positive evidence of its\nutility in developing supplementary health surveillance systems. In this study,\nwe present a new framework to surveil public health, focusing on mental health\n(MH) outcomes. We hypothesize that locally posted tweets are indicative of\nlocal MH outcomes and collect tweets posted from 765 neighborhoods (census\nblock groups) in the USA. We pair these tweets from each neighborhood with the\ncorresponding MH outcome reported by the Center for Disease Control (CDC) to\ncreate a benchmark dataset, LocalTweets. With LocalTweets, we present the first\npopulation-level evaluation task for Twitter-based MH surveillance systems. We\nthen develop an efficient and effective method, LocalHealth, for predicting MH\noutcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the\nhighest F1-score and accuracy of 0.7429 and 79.78\\%, respectively, a 59\\%\nimprovement in F1-score over the GPT3.5 in zero-shot setting. We also utilize\nLocalHealth to extrapolate CDC's estimates to proxy unreported neighborhoods,\nachieving an F1-score of 0.7291. Our work suggests that Twitter data can be\neffectively leveraged to simulate neighborhood-level MH outcomes.\n","authors":["Vijeta Deshpande","Minhwa Lee","Zonghai Yao","Zihao Zhang","Jason Brian Gibbons","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2402.13452v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08763v3","updated":"2024-03-26T17:58:48Z","published":"2024-03-13T17:58:57Z","title":"Simple and Scalable Strategies to Continually Pre-train Large Language\n  Models","summary":"  Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by the final loss\nand the average score on several language model (LM) evaluation benchmarks.\nSpecifically, we show this for a weak but realistic distribution shift between\ntwo commonly used LLM pre-training datasets (English$\\rightarrow$English) and a\nstronger distribution shift (English$\\rightarrow$German) at the $405$M\nparameter model scale with large dataset sizes (hundreds of billions of\ntokens). Selecting the weak but realistic shift for larger-scale experiments,\nwe also find that our continual learning strategies match the re-training\nbaseline for a 10B parameter LLM. Our results demonstrate that LLMs can be\nsuccessfully updated via simple and scalable continual learning strategies,\nmatching the re-training baseline using only a fraction of the compute.\nFinally, inspired by previous work, we propose alternatives to the cosine\nlearning rate schedule that help circumvent forgetting induced by LR re-warming\nand that are not bound to a fixed token budget.\n","authors":["Adam Ibrahim","Benjamin Thérien","Kshitij Gupta","Mats L. Richter","Quentin Anthony","Timothée Lesort","Eugene Belilovsky","Irina Rish"],"pdf_url":"https://arxiv.org/pdf/2403.08763v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17919v1","updated":"2024-03-26T17:55:02Z","published":"2024-03-26T17:55:02Z","title":"LISA: Layerwise Importance Sampling for Memory-Efficient Large Language\n  Model Fine-Tuning","summary":"  The machine learning community has witnessed impressive advancements since\nthe first appearance of large language models (LLMs), yet their huge memory\nconsumption has become a major roadblock to large-scale training. Parameter\nEfficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been\nproposed to alleviate this problem, but their performance still fails to match\nfull parameter training in most large-scale fine-tuning settings. Attempting to\ncomplement this deficiency, we investigate layerwise properties of LoRA on\nfine-tuning tasks and observe an uncommon skewness of weight norms across\ndifferent layers. Utilizing this key observation, a surprisingly simple\ntraining strategy is discovered, which outperforms both LoRA and full parameter\ntraining in a wide range of settings with memory costs as low as LoRA. We name\nit Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA,\nwhich applies the idea of importance sampling to different layers in LLMs and\nrandomly freeze most middle layers during optimization. Experimental results\nshow that with similar or less GPU memory consumption, LISA surpasses LoRA or\neven full parameter tuning in downstream fine-tuning tasks, where LISA\nconsistently outperforms LoRA by over $11\\%$-$37\\%$ in terms of MT-Bench\nscores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or\nbetter performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating\nits effectiveness across different domains.\n","authors":["Rui Pan","Xiang Liu","Shizhe Diao","Renjie Pi","Jipeng Zhang","Chi Han","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.17919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16248v2","updated":"2024-03-26T17:46:26Z","published":"2024-03-24T17:39:51Z","title":"Large Language Models Offer an Alternative to the Traditional Approach\n  of Topic Modelling","summary":"  Topic modelling, as a well-established unsupervised technique, has found\nextensive use in automatically detecting significant topics within a corpus of\ndocuments. However, classic topic modelling approaches (e.g., LDA) have certain\ndrawbacks, such as the lack of semantic understanding and the presence of\noverlapping topics. In this work, we investigate the untapped potential of\nlarge language models (LLMs) as an alternative for uncovering the underlying\ntopics within extensive text corpora. To this end, we introduce a framework\nthat prompts LLMs to generate topics from a given set of documents and\nestablish evaluation protocols to assess the clustering efficacy of LLMs. Our\nfindings indicate that LLMs with appropriate prompts can stand out as a viable\nalternative, capable of generating relevant topic titles and adhering to human\nguidelines to refine and merge topics. Through in-depth experiments and\nevaluation, we summarise the advantages and constraints of employing LLMs in\ntopic extraction.\n","authors":["Yida Mu","Chun Dong","Kalina Bontcheva","Xingyi Song"],"pdf_url":"https://arxiv.org/pdf/2403.16248v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17887v1","updated":"2024-03-26T17:20:04Z","published":"2024-03-26T17:20:04Z","title":"The Unreasonable Ineffectiveness of the Deeper Layers","summary":"  We empirically study a simple layer-pruning strategy for popular families of\nopen-weight pretrained LLMs, finding minimal degradation of performance on\ndifferent question-answering benchmarks until after a large fraction (up to\nhalf) of the layers are removed. To prune these models, we identify the optimal\nblock of layers to prune by considering similarity across layers; then, to\n\"heal\" the damage, we perform a small amount of finetuning. In particular, we\nuse parameter-efficient finetuning (PEFT) methods, specifically quantization\nand Low Rank Adapters (QLoRA), such that each of our experiments can be\nperformed on a single A100 GPU. From a practical perspective, these results\nsuggest that layer pruning methods can complement other PEFT strategies to\nfurther reduce computational resources of finetuning on the one hand, and can\nimprove the memory and latency of inference on the other hand. From a\nscientific perspective, the robustness of these LLMs to the deletion of layers\nimplies either that current pretraining methods are not properly leveraging the\nparameters in the deeper layers of the network or that the shallow layers play\na critical role in storing knowledge.\n","authors":["Andrey Gromov","Kushal Tirumala","Hassan Shapourian","Paolo Glorioso","Daniel A. Roberts"],"pdf_url":"https://arxiv.org/pdf/2403.17887v1.pdf","comment":"12 + 10 pages, 5 + 4 figures"},{"id":"http://arxiv.org/abs/2403.17860v1","updated":"2024-03-26T16:49:25Z","published":"2024-03-26T16:49:25Z","title":"Exploring LLMs as a Source of Targeted Synthetic Textual Data to\n  Minimize High Confidence Misclassifications","summary":"  Natural Language Processing (NLP) models optimized for predictive performance\noften make high confidence errors and suffer from vulnerability to adversarial\nand out-of-distribution data. Existing work has mainly focused on mitigation of\nsuch errors using either humans or an automated approach. In this study, we\nexplore the usage of large language models (LLMs) for data augmentation as a\npotential solution to the issue of NLP models making wrong predictions with\nhigh confidence during classification tasks. We compare the effectiveness of\nsynthetic data generated by LLMs with that of human data obtained via the same\nprocedure. For mitigation, humans or LLMs provide natural language\ncharacterizations of high confidence misclassifications to generate synthetic\ndata, which are then used to extend the training set. We conduct an extensive\nevaluation of our approach on three classification tasks and demonstrate its\neffectiveness in reducing the number of high confidence misclassifications\npresent in the model, all while maintaining the same level of accuracy.\nMoreover, we find that the cost gap between humans and LLMs surpasses an order\nof magnitude, as LLMs attain human-like performance while being more scalable.\n","authors":["Philip Lippmann","Matthijs Spaan","Jie Yang"],"pdf_url":"https://arxiv.org/pdf/2403.17860v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17859v1","updated":"2024-03-26T16:48:13Z","published":"2024-03-26T16:48:13Z","title":"ChroniclingAmericaQA: A Large-scale Question Answering Dataset based on\n  Historical American Newspaper Pages","summary":"  Question answering (QA) and Machine Reading Comprehension (MRC) tasks have\nsignificantly advanced in recent years due to the rapid development of deep\nlearning techniques and, more recently, large language models. At the same\ntime, many benchmark datasets have become available for QA and MRC tasks.\nHowever, most existing large-scale benchmark datasets have been created\npredominantly using synchronous document collections like Wikipedia or the Web.\nArchival document collections, such as historical newspapers, contain valuable\ninformation from the past that is still not widely used to train large language\nmodels. To further contribute to advancing QA and MRC tasks and to overcome the\nlimitation of previous datasets, we introduce ChroniclingAmericaQA, a\nlarge-scale dataset with 485K question-answer pairs created based on the\nhistorical newspaper collection Chronicling America. Our dataset is constructed\nfrom a subset of the Chronicling America newspaper collection spanning 120\nyears. One of the significant challenges for utilizing digitized historical\nnewspaper collections is the low quality of OCR text. Therefore, to enable\nrealistic testing of QA models, our dataset can be used in three different\nways: answering questions from raw and noisy content, answering questions from\ncleaner, corrected version of the content, as well as answering questions from\nscanned images of newspaper pages. This and the fact that ChroniclingAmericaQA\nspans the longest time period among available QA datasets make it quite a\nunique and useful resource.\n","authors":["Bhawna Piryani","Jamshid Mozafari","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2403.17859v1.pdf","comment":"Accepted at SIGIR 2024"},{"id":"http://arxiv.org/abs/2403.17856v1","updated":"2024-03-26T16:45:27Z","published":"2024-03-26T16:45:27Z","title":"Verbing Weirds Language (Models): Evaluation of English Zero-Derivation\n  in Five LLMs","summary":"  Lexical-syntactic flexibility, in the form of conversion (or zero-derivation)\nis a hallmark of English morphology. In conversion, a word with one part of\nspeech is placed in a non-prototypical context, where it is coerced to behave\nas if it had a different part of speech. However, while this process affects a\nlarge part of the English lexicon, little work has been done to establish the\ndegree to which language models capture this type of generalization. This paper\nreports the first study on the behavior of large language models with reference\nto conversion. We design a task for testing lexical-syntactic flexibility --\nthe degree to which models can generalize over words in a construction with a\nnon-prototypical part of speech. This task is situated within a natural\nlanguage inference paradigm. We test the abilities of five language models --\ntwo proprietary models (GPT-3.5 and GPT-4), three open-source models (Mistral\n7B, Falcon 40B, and Llama 2 70B). We find that GPT-4 performs best on the task,\nfollowed by GPT-3.5, but that the open source language models are also able to\nperform it and that the 7B parameter Mistral displays as little difference\nbetween its baseline performance on the natural language inference task and the\nnon-prototypical syntactic category task, as the massive GPT-4.\n","authors":["David R. Mortensen","Valentina Izrailevitch","Yunze Xiao","Hinrich Schütze","Leonie Weissweiler"],"pdf_url":"https://arxiv.org/pdf/2403.17856v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2401.06795v2","updated":"2024-03-26T16:44:34Z","published":"2024-01-08T18:42:55Z","title":"AI and Generative AI for Research Discovery and Summarization","summary":"  AI and generative AI tools, including chatbots like ChatGPT that rely on\nlarge language models (LLMs), have burst onto the scene this year, creating\nincredible opportunities to increase work productivity and improve our lives.\nStatisticians and data scientists have begun experiencing the benefits from the\navailability of these tools in numerous ways, such as the generation of\nprogramming code from text prompts to analyze data or fit statistical models.\nOne area that these tools can make a substantial impact is in research\ndiscovery and summarization. Standalone tools and plugins to chatbots are being\ndeveloped that allow researchers to more quickly find relevant literature than\npre-2023 search tools. Furthermore, generative AI tools have improved to the\npoint where they can summarize and extract the key points from research\narticles in succinct language. Finally, chatbots based on highly parameterized\nLLMs can be used to simulate abductive reasoning, which provides researchers\nthe ability to make connections among related technical topics, which can also\nbe used for research discovery. We review the developments in AI and generative\nAI for research discovery and summarization, and propose directions where these\ntypes of tools are likely to head in the future that may be of interest to\nstatistician and data scientists.\n","authors":["Mark Glickman","Yi Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.06795v2.pdf","comment":"29 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.17853v1","updated":"2024-03-26T16:42:30Z","published":"2024-03-26T16:42:30Z","title":"Using Domain Knowledge to Guide Dialog Structure Induction via Neural\n  Probabilistic Soft Logic","summary":"  Dialog Structure Induction (DSI) is the task of inferring the latent dialog\nstructure (i.e., a set of dialog states and their temporal transitions) of a\ngiven goal-oriented dialog. It is a critical component for modern dialog system\ndesign and discourse analysis. Existing DSI approaches are often purely\ndata-driven, deploy models that infer latent states without access to domain\nknowledge, underperform when the training corpus is limited/noisy, or have\ndifficulty when test dialogs exhibit distributional shifts from the training\ndomain. This work explores a neural-symbolic approach as a potential solution\nto these problems. We introduce Neural Probabilistic Soft Logic Dialogue\nStructure Induction (NEUPSL DSI), a principled approach that injects symbolic\nknowledge into the latent space of a generative neural model. We conduct a\nthorough empirical investigation on the effect of NEUPSL DSI learning on hidden\nrepresentation quality, few-shot learning, and out-of-domain generalization\nperformance. Over three dialog structure induction datasets and across\nunsupervised and semi-supervised settings for standard and cross-domain\ngeneralization, the injection of symbolic knowledge using NEUPSL DSI provides a\nconsistent boost in performance over the canonical baselines.\n","authors":["Connor Pryor","Quan Yuan","Jeremiah Liu","Mehran Kazemi","Deepak Ramachandran","Tania Bedrax-Weiss","Lise Getoor"],"pdf_url":"https://arxiv.org/pdf/2403.17853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.11278v3","updated":"2024-03-26T16:40:50Z","published":"2023-07-21T00:34:38Z","title":"Generator-Retriever-Generator Approach for Open-Domain Question\n  Answering","summary":"  Open-domain question answering (QA) tasks usually require the retrieval of\nrelevant information from a large corpus to generate accurate answers. We\npropose a novel approach called Generator-Retriever-Generator (GRG) that\ncombines document retrieval techniques with a large language model (LLM), by\nfirst prompting the model to generate contextual documents based on a given\nquestion. In parallel, a dual-encoder network retrieves documents that are\nrelevant to the question from an external corpus. The generated and retrieved\ndocuments are then passed to the second LLM, which generates the final answer.\nBy combining document retrieval and LLM generation, our approach addresses the\nchallenges of open-domain QA, such as generating informative and contextually\nrelevant answers. GRG outperforms the state-of-the-art generate-then-read and\nretrieve-then-read pipelines (GENREAD and RFiD) improving their performance by\nat least by +5.2, +4.2, and +1.6 on TriviaQA, NQ, and WebQ datasets,\nrespectively. We provide code, datasets, and checkpoints at\nhttps://github.com/abdoelsayed2016/GRG.\n","authors":["Abdelrahman Abdallah","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2307.11278v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17848v1","updated":"2024-03-26T16:37:54Z","published":"2024-03-26T16:37:54Z","title":"ArabicaQA: A Comprehensive Dataset for Arabic Question Answering","summary":"  In this paper, we address the significant gap in Arabic natural language\nprocessing (NLP) resources by introducing ArabicaQA, the first large-scale\ndataset for machine reading comprehension and open-domain question answering in\nArabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701\nunanswerable questions created by crowdworkers to look similar to answerable\nones, along with additional labels of open-domain questions marks a crucial\nadvancement in Arabic NLP resources. We also present AraDPR, the first dense\npassage retrieval model trained on the Arabic Wikipedia corpus, specifically\ndesigned to tackle the unique challenges of Arabic text retrieval. Furthermore,\nour study includes extensive benchmarking of large language models (LLMs) for\nArabic question answering, critically evaluating their performance in the\nArabic language context. In conclusion, ArabicaQA, AraDPR, and the benchmarking\nof LLMs in Arabic question answering offer significant advancements in the\nfield of Arabic NLP. The dataset and code are publicly accessible for further\nresearch https://github.com/DataScienceUIBK/ArabicaQA.\n","authors":["Abdelrahman Abdallah","Mahmoud Kasem","Mahmoud Abdalla","Mohamed Mahmoud","Mohamed Elkasaby","Yasser Elbendary","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2403.17848v1.pdf","comment":"Accepted at SIGIR 2024"},{"id":"http://arxiv.org/abs/2403.17846v1","updated":"2024-03-26T16:36:43Z","published":"2024-03-26T16:36:43Z","title":"Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot\n  Navigation","summary":"  Recent open-vocabulary robot mapping methods enrich dense geometric maps with\npre-trained visual-language features. While these maps allow for the prediction\nof point-wise saliency maps when queried for a certain language concept,\nlarge-scale environments and abstract queries beyond the object level still\npose a considerable hurdle, ultimately limiting language-grounded robotic\nnavigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D\nscene graph mapping approach for language-grounded robot navigation. Leveraging\nopen-vocabulary vision foundation models, we first obtain state-of-the-art\nopen-vocabulary segment-level maps in 3D and subsequently construct a 3D scene\ngraph hierarchy consisting of floor, room, and object concepts, each enriched\nwith open-vocabulary features. Our approach is able to represent multi-story\nbuildings and allows robotic traversal of those using a cross-floor Voronoi\ngraph. HOV-SG is evaluated on three distinct datasets and surpasses previous\nbaselines in open-vocabulary semantic accuracy on the object, room, and floor\nlevel while producing a 75% reduction in representation size compared to dense\nopen-vocabulary maps. In order to prove the efficacy and generalization\ncapabilities of HOV-SG, we showcase successful long-horizon\nlanguage-conditioned robot navigation within real-world multi-storage\nenvironments. We provide code and trial video data at http://hovsg.github.io/.\n","authors":["Abdelrhman Werby","Chenguang Huang","Martin Büchner","Abhinav Valada","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2403.17846v1.pdf","comment":"Code and video are available at http://hovsg.github.io/"},{"id":"http://arxiv.org/abs/2305.03123v2","updated":"2024-03-26T16:22:54Z","published":"2023-04-13T16:01:28Z","title":"ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and\n  Ethics) Evaluation: A Review","summary":"  ChatGPT is another large language model (LLM) vastly available for the\nconsumers on their devices but due to its performance and ability to converse\neffectively, it has gained a huge popularity amongst research as well as\nindustrial community. Recently, many studies have been published to show the\neffectiveness, efficiency, integration, and sentiments of chatGPT and other\nLLMs. In contrast, this study focuses on the important aspects that are mostly\noverlooked, i.e. sustainability, privacy, digital divide, and ethics and\nsuggests that not only chatGPT but every subsequent entry in the category of\nconversational bots should undergo Sustainability, PrivAcy, Digital divide, and\nEthics (SPADE) evaluation. This paper discusses in detail the issues and\nconcerns raised over chatGPT in line with aforementioned characteristics. We\nalso discuss the recent EU AI Act briefly in accordance with the SPADE\nevaluation. We support our hypothesis by some preliminary data collection and\nvisualizations along with hypothesized facts. We also suggest mitigations and\nrecommendations for each of the concerns. Furthermore, we also suggest some\npolicies and recommendations for AI policy act, if designed by the governments.\n","authors":["Sunder Ali Khowaja","Parus Khuwaja","Kapal Dev","Weizheng Wang","Lewis Nkenyereye"],"pdf_url":"https://arxiv.org/pdf/2305.03123v2.pdf","comment":"29 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2309.09800v3","updated":"2024-03-26T16:05:51Z","published":"2023-09-18T14:18:19Z","title":"AMuRD: Annotated Arabic-English Receipt Dataset for Key Information\n  Extraction and Classification","summary":"  The extraction of key information from receipts is a complex task that\ninvolves the recognition and extraction of text from scanned receipts. This\nprocess is crucial as it enables the retrieval of essential content and\norganizing it into structured documents for easy access and analysis. In this\npaper, we present AMuRD, a novel multilingual human-annotated dataset\nspecifically designed for information extraction from receipts. This dataset\ncomprises $47,720$ samples and addresses the key challenges in information\nextraction and item classification - the two critical aspects of data analysis\nin the retail industry. Each sample includes annotations for item names and\nattributes such as price, brand, and more. This detailed annotation facilitates\na comprehensive understanding of each item on the receipt. Furthermore, the\ndataset provides classification into $44$ distinct product categories. This\nclassification feature allows for a more organized and efficient analysis of\nthe items, enhancing the usability of the dataset for various applications. In\nour study, we evaluated various language model architectures, e.g., by\nfine-tuning LLaMA models on the AMuRD dataset. Our approach yielded exceptional\nresults, with an F1 score of 97.43\\% and accuracy of 94.99\\% in information\nextraction and classification, and an even higher F1 score of 98.51\\% and\naccuracy of 97.06\\% observed in specific tasks. The dataset and code are\npublicly accessible for further\nresearchhttps://github.com/Update-For-Integrated-Business-AI/AMuRD.\n","authors":["Abdelrahman Abdallah","Mahmoud Abdalla","Mohamed Elkasaby","Yasser Elbendary","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2309.09800v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.03742v2","updated":"2024-03-26T16:03:57Z","published":"2023-08-07T17:46:49Z","title":"Training BERT Models to Carry Over a Coding System Developed on One\n  Corpus to Another","summary":"  This paper describes how we train BERT models to carry over a coding system\ndeveloped on the paragraphs of a Hungarian literary journal to another. The aim\nof the coding system is to track trends in the perception of literary\ntranslation around the political transformation in 1989 in Hungary. To evaluate\nnot only task performance but also the consistence of the annotation, moreover,\nto get better predictions from an ensemble, we use 10-fold crossvalidation.\nExtensive hyperparameter tuning is used to obtain the best possible results and\nfair comparisons. To handle label imbalance, we use loss functions and metrics\nrobust to it. Evaluation of the effect of domain shift is carried out by\nsampling a test set from the target domain. We establish the sample size by\nestimating the bootstrapped confidence interval via simulations. This way, we\nshow that our models can carry over one annotation system to the target domain.\nComparisons are drawn to provide insights such as learning multilabel\ncorrelations and confidence penalty improve resistance to domain shift, and\ndomain adaptation on OCR-ed text on another domain improves performance almost\nto the same extent as that on the corpus under study. See our code at\nhttps://codeberg.org/zsamboki/bert-annotator-ensemble.\n","authors":["Dalma Galambos","Pál Zsámboki"],"pdf_url":"https://arxiv.org/pdf/2308.03742v2.pdf","comment":"Camera-ready version, to be presented at the 2024 Joint International\n  Conference on Computational Linguistics, Language Resources and Evaluation\n  (LREC-COLING 2024)"},{"id":"http://arxiv.org/abs/2311.15964v2","updated":"2024-03-26T15:58:26Z","published":"2023-11-27T16:07:37Z","title":"Efficient Pre-training for Localized Instruction Generation of Videos","summary":"  Procedural videos show step-by-step demonstrations of tasks like recipe\npreparation. Understanding such videos is challenging, involving the precise\nlocalization of steps and the generation of textual instructions. Manually\nannotating steps and writing instructions is costly, which limits the size of\ncurrent datasets and hinders effective learning. Leveraging large but noisy\nvideo-transcript datasets for pre-training can boost performance, but demands\nsignificant computational resources. Furthermore, transcripts contain\nirrelevant content and exhibit style variation compared to instructions written\nby human annotators. To mitigate both issues, we propose a technique,\nSieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters\nirrelevant transcripts and (ii) Swap enhances the quality of the text\ninstruction by automatically replacing the transcripts with human-written\ninstructions from a text-only recipe dataset. The curated dataset, three orders\nof magnitude smaller than current web-scale datasets, enables efficient\ntraining of large-scale models with competitive performance. We complement our\nSieve-\\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step\nlocalization and instruction generation for procedural videos. When this model\nis pre-trained on our curated dataset, it achieves state-of-the-art performance\nin zero-shot and finetuning settings on YouCook2 and Tasty, while using a\nfraction of the computational resources.\n","authors":["Anil Batra","Davide Moltisanti","Laura Sevilla-Lara","Marcus Rohrbach","Frank Keller"],"pdf_url":"https://arxiv.org/pdf/2311.15964v2.pdf","comment":"This version has some missing experiments and elaborative technical\n  details"},{"id":"http://arxiv.org/abs/2403.17816v1","updated":"2024-03-26T15:53:02Z","published":"2024-03-26T15:53:02Z","title":"Graph Language Model (GLM): A new graph-based approach to detect social\n  instabilities","summary":"  This scientific report presents a novel methodology for the early prediction\nof important political events using News datasets. The methodology leverages\nnatural language processing, graph theory, clique analysis, and semantic\nrelationships to uncover hidden predictive signals within the data. Initially,\nwe designed a preliminary version of the method and tested it on a few events.\nThis analysis revealed limitations in the initial research phase. We then\nenhanced the model in two key ways: first, we added a filtration step to only\nconsider politically relevant news before further processing; second, we\nadjusted the input features to make the alert system more sensitive to\nsignificant spikes in the data. After finalizing the improved methodology, we\ntested it on eleven events including US protests, the Ukraine war, and French\nprotests. Results demonstrate the superiority of our approach compared to\nbaseline methods. Through targeted refinements, our model can now provide\nearlier and more accurate predictions of major political events based on subtle\npatterns in news data.\n","authors":["Wallyson Lemes de Oliveira","Vahid Shamsaddini","Ali Ghofrani","Rahul Singh Inda","Jithendra Sai Veeramaneni","Étienne Voutaz"],"pdf_url":"https://arxiv.org/pdf/2403.17816v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17811v1","updated":"2024-03-26T15:50:37Z","published":"2024-03-26T15:50:37Z","title":"Are Compressed Language Models Less Subgroup Robust?","summary":"  To reduce the inference cost of large language models, model compression is\nincreasingly used to create smaller scalable models. However, little is known\nabout their robustness to minority subgroups defined by the labels and\nattributes of a dataset. In this paper, we investigate the effects of 18\ndifferent compression methods and settings on the subgroup robustness of BERT\nlanguage models. We show that worst-group performance does not depend on model\nsize alone, but also on the compression method used. Additionally, we find that\nmodel compression does not always worsen the performance on minority subgroups.\nAltogether, our analysis serves to further research into the subgroup\nrobustness of model compression.\n","authors":["Leonidas Gee","Andrea Zugarini","Novi Quadrianto"],"pdf_url":"https://arxiv.org/pdf/2403.17811v1.pdf","comment":"The 2023 Conference on Empirical Methods in Natural Language\n  Processing (EMNLP 2023)"},{"id":"http://arxiv.org/abs/2401.11911v4","updated":"2024-03-26T15:47:14Z","published":"2024-01-22T12:54:04Z","title":"Blinded by Generated Contexts: How Language Models Merge Generated and\n  Retrieved Contexts for Open-Domain QA?","summary":"  While auxiliary information has become a key to enhancing Large Language\nModels (LLMs), relatively little is known about how LLMs merge these contexts,\nspecifically contexts generated by LLMs and those retrieved from external\nsources. To investigate this, we formulate a systematic framework to identify\nwhether LLMs' responses, derived from the integration of generated and\nretrieved contexts, are attributed to either generated or retrieved contexts.\nTo easily trace the origin of the response, we construct datasets with\nconflicting contexts, i.e., each question is paired with both generated and\nretrieved contexts, yet only one of them contains the correct answer. Our\nexperiments reveal a significant bias in several LLMs (GPT-4/3.5 and Llama2) to\nfavor generated contexts, even when they provide incorrect information. We\nfurther identify two key factors contributing to this bias: i) contexts\ngenerated by LLMs typically show greater similarity to the questions,\nincreasing their likelihood of being selected; ii) the segmentation process\nused in retrieved contexts disrupts their completeness, thereby hindering their\nfull utilization in LLMs. Our analysis enhances the understanding of how LLMs\nmerge diverse contexts, offering valuable insights for advancing current\naugmentation methods for LLMs.\n","authors":["Hexiang Tan","Fei Sun","Wanli Yang","Yuanzhuo Wang","Qi Cao","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2401.11911v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17806v1","updated":"2024-03-26T15:44:58Z","published":"2024-03-26T15:44:58Z","title":"Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding\n  Model Mechanisms","summary":"  Many recent language model (LM) interpretability studies have adopted the\ncircuits framework, which aims to find the minimal computational subgraph, or\ncircuit, that explains LM behavior on a given task. Most studies determine\nwhich edges belong in a LM's circuit by performing causal interventions on each\nedge independently, but this scales poorly with model size. Edge attribution\npatching (EAP), gradient-based approximation to interventions, has emerged as a\nscalable but imperfect solution to this problem. In this paper, we introduce a\nnew method - EAP with integrated gradients (EAP-IG) - that aims to better\nmaintain a core property of circuits: faithfulness. A circuit is faithful if\nall model edges outside the circuit can be ablated without changing the model's\nperformance on the task; faithfulness is what justifies studying circuits,\nrather than the full model. Our experiments demonstrate that circuits found\nusing EAP are less faithful than those found using EAP-IG, even though both\nhave high node overlap with circuits found previously using causal\ninterventions. We conclude more generally that when using circuits to compare\nthe mechanisms models use to solve tasks, faithfulness, not overlap, is what\nshould be measured.\n","authors":["Michael Hanna","Sandro Pezzelle","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2403.17806v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17804v1","updated":"2024-03-26T15:42:01Z","published":"2024-03-26T15:42:01Z","title":"Improving Text-to-Image Consistency via Automatic Prompt Optimization","summary":"  Impressive advances in text-to-image (T2I) generative models have yielded a\nplethora of high performing models which are able to generate aesthetically\nappealing, photorealistic images. Despite the progress, these models still\nstruggle to produce images that are consistent with the input prompt,\noftentimes failing to capture object quantities, relations and attributes\nproperly. Existing solutions to improve prompt-image consistency suffer from\nthe following challenges: (1) they oftentimes require model fine-tuning, (2)\nthey only focus on nearby prompt samples, and (3) they are affected by\nunfavorable trade-offs among image quality, representation diversity, and\nprompt-image consistency. In this paper, we address these challenges and\nintroduce a T2I optimization-by-prompting framework, OPT2I, which leverages a\nlarge language model (LLM) to improve prompt-image consistency in T2I models.\nOur framework starts from a user prompt and iteratively generates revised\nprompts with the goal of maximizing a consistency score. Our extensive\nvalidation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost\nthe initial consistency score by up to 24.9% in terms of DSG score while\npreserving the FID and increasing the recall between generated and real data.\nOur work paves the way toward building more reliable and robust T2I systems by\nharnessing the power of LLMs.\n","authors":["Oscar Mañas","Pietro Astolfi","Melissa Hall","Candace Ross","Jack Urbanek","Adina Williams","Aishwarya Agrawal","Adriana Romero-Soriano","Michal Drozdzal"],"pdf_url":"https://arxiv.org/pdf/2403.17804v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07703v2","updated":"2024-03-26T15:31:34Z","published":"2023-11-13T19:41:34Z","title":"Measuring Entrainment in Spontaneous Code-switched Speech","summary":"  It is well-known that speakers who entrain to one another have more\nsuccessful conversations than those who do not. Previous research has shown\nthat interlocutors entrain on linguistic features in both written and spoken\nmonolingual domains. More recent work on code-switched communication has also\nshown preliminary evidence of entrainment on certain aspects of code-switching\n(CSW). However, such studies of entrainment in code-switched domains have been\nextremely few and restricted to human-machine textual interactions. Our work\nstudies code-switched spontaneous speech between humans, finding that (1)\npatterns of written and spoken entrainment in monolingual settings largely\ngeneralize to code-switched settings, and (2) some patterns of entrainment on\ncode-switching in dialogue agent-generated text generalize to spontaneous\ncode-switched speech. Our findings give rise to important implications for the\npotentially \"universal\" nature of entrainment as a communication phenomenon,\nand potential applications in inclusive and interactive speech technology.\n","authors":["Debasmita Bhattacharya","Siying Ding","Alayna Nguyen","Julia Hirschberg"],"pdf_url":"https://arxiv.org/pdf/2311.07703v2.pdf","comment":"Edits: camera-ready manuscript for NAACL 2024"},{"id":"http://arxiv.org/abs/2403.01748v2","updated":"2024-03-26T15:26:21Z","published":"2024-03-04T05:55:01Z","title":"Decode Neural signal as Speech","summary":"  Decoding language from brain dynamics is an important open direction in the\nrealm of brain-computer interface (BCI), especially considering the rapid\ngrowth of large language models. Compared to invasive-based signals which\nrequire electrode implantation surgery, non-invasive neural signals (e.g. EEG,\nMEG) have attracted increasing attention considering their safety and\ngenerality. However, the exploration is not adequate in three aspects: 1)\nprevious methods mainly focus on EEG but none of the previous works address\nthis problem on MEG with better signal quality; 2) prior works have\npredominantly used ``teacher-forcing\" during generative decoding, which is\nimpractical; 3) prior works are mostly ``BART-based\" not fully auto-regressive,\nwhich performs better in other sequence tasks. In this paper, we explore the\nbrain-to-text translation of MEG signals in a speech-decoding formation. Here\nwe are the first to investigate a cross-attention-based ``whisper\" model for\ngenerating text directly from MEG signals without teacher forcing. Our model\nachieves impressive BLEU-1 scores of 60.30 and 52.89 without pretraining \\&\nteacher-forcing on two major datasets (\\textit{GWilliams} and\n\\textit{Schoffelen}). This paper conducts a comprehensive review to understand\nhow speech decoding formation performs on the neural decoding tasks, including\npretraining initialization, training \\& evaluation set splitting, augmentation,\nand scaling law.\n","authors":["Yiqian Yang","Yiqun Duan","Qiang Zhang","Renjing Xu","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2403.01748v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16167v2","updated":"2024-03-26T15:14:25Z","published":"2024-03-24T14:21:06Z","title":"Exploiting Semantic Reconstruction to Mitigate Hallucinations in\n  Vision-Language Models","summary":"  Hallucinations in vision-language models pose a significant challenge to\ntheir reliability, particularly in the generation of long captions. Current\nmethods fall short of accurately identifying and mitigating these\nhallucinations. To address this issue, we introduce ESREAL, a novel\nunsupervised learning framework designed to suppress the generation of\nhallucinations through accurate localization and penalization of hallucinated\ntokens. Initially, ESREAL creates a reconstructed image based on the generated\ncaption and aligns its corresponding regions with those of the original image.\nThis semantic reconstruction aids in identifying both the presence and type of\ntoken-level hallucinations within the generated caption. Subsequently, ESREAL\ncomputes token-level hallucination scores by assessing the semantic similarity\nof aligned regions based on the type of hallucination. Finally, ESREAL employs\na proximal policy optimization algorithm, where it selectively penalizes\nhallucinated tokens according to their token-level hallucination scores. Our\nframework notably reduces hallucinations in LLaVA, InstructBLIP, and mPLUG-Owl2\nby 32.81%, 27.08%, and 7.46% on the CHAIR metric. This improvement is achieved\nsolely through signals derived from the image itself, without the need for any\nimage-text pairs.\n","authors":["Minchan Kim","Minyeong Kim","Junik Bae","Suhwan Choi","Sungkyung Kim","Buru Chang"],"pdf_url":"https://arxiv.org/pdf/2403.16167v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17768v1","updated":"2024-03-26T14:54:48Z","published":"2024-03-26T14:54:48Z","title":"SciNews: From Scholarly Complexities to Public Narratives -- A Dataset\n  for Scientific News Report Generation","summary":"  Scientific news reports serve as a bridge, adeptly translating complex\nresearch articles into reports that resonate with the broader public. The\nautomated generation of such narratives enhances the accessibility of scholarly\ninsights. In this paper, we present a new corpus to facilitate this paradigm\ndevelopment. Our corpus comprises a parallel compilation of academic\npublications and their corresponding scientific news reports across nine\ndisciplines. To demonstrate the utility and reliability of our dataset, we\nconduct an extensive analysis, highlighting the divergences in readability and\nbrevity between scientific news narratives and academic manuscripts. We\nbenchmark our dataset employing state-of-the-art text generation models. The\nevaluation process involves both automatic and human evaluation, which lays the\ngroundwork for future explorations into the automated generation of scientific\nnews reports. The dataset and code related to this work are available at\nhttps://dongqi.me/projects/SciNews.\n","authors":["Dongqi Pu","Yifan Wang","Jia Loy","Vera Demberg"],"pdf_url":"https://arxiv.org/pdf/2403.17768v1.pdf","comment":"LREC-COLING 2024 Main Conference Paper"},{"id":"http://arxiv.org/abs/2403.17760v1","updated":"2024-03-26T14:51:12Z","published":"2024-03-26T14:51:12Z","title":"Constructions Are So Difficult That Even Large Language Models Get Them\n  Right for the Wrong Reasons","summary":"  In this paper, we make a contribution that can be understood from two\nperspectives: from an NLP perspective, we introduce a small challenge dataset\nfor NLI with large lexical overlap, which minimises the possibility of models\ndiscerning entailment solely based on token distinctions, and show that GPT-4\nand Llama 2 fail it with strong bias. We then create further challenging\nsub-tasks in an effort to explain this failure. From a Computational\nLinguistics perspective, we identify a group of constructions with three\nclasses of adjectives which cannot be distinguished by surface features. This\nenables us to probe for LLM's understanding of these constructions in various\nways, and we find that they fail in a variety of ways to distinguish between\nthem, suggesting that they don't adequately represent their meaning or capture\nthe lexical properties of phrasal heads.\n","authors":["Shijia Zhou","Leonie Weissweiler","Taiqi He","Hinrich Schütze","David R. Mortensen","Lori Levin"],"pdf_url":"https://arxiv.org/pdf/2403.17760v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.11996v2","updated":"2024-03-26T14:46:04Z","published":"2024-03-18T17:30:27Z","title":"Accelerating Scientific Discovery with Generative Knowledge Extraction,\n  Graph-Based Representation, and Multimodal Intelligent Graph Reasoning","summary":"  Leveraging generative Artificial Intelligence (AI), we have transformed a\ndataset comprising 1,000 scientific papers into an ontological knowledge graph.\nThrough an in-depth structural analysis, we have calculated node degrees,\nidentified communities and connectivities, and evaluated clustering\ncoefficients and betweenness centrality of pivotal nodes, uncovering\nfascinating knowledge architectures. The graph has an inherently scale-free\nnature, is highly connected, and can be used for graph reasoning by taking\nadvantage of transitive and isomorphic properties that reveal unprecedented\ninterdisciplinary relationships that can be used to answer queries, identify\ngaps in knowledge, propose never-before-seen material designs, and predict\nmaterial behaviors. We compute deep node embeddings for combinatorial node\nsimilarity ranking for use in a path sampling strategy links dissimilar\nconcepts that have previously not been related. One comparison revealed\nstructural parallels between biological materials and Beethoven's 9th Symphony,\nhighlighting shared patterns of complexity through isomorphic mapping. In\nanother example, the algorithm proposed a hierarchical mycelium-based composite\nbased on integrating path sampling with principles extracted from Kandinsky's\n'Composition VII' painting. The resulting material integrates an innovative set\nof concepts that include a balance of chaos/order, adjustable porosity,\nmechanical strength, and complex patterned chemical functionalization. We\nuncover other isomorphisms across science, technology and art, revealing a\nnuanced ontology of immanence that reveal a context-dependent heterarchical\ninterplay of constituents. Graph-based generative AI achieves a far higher\ndegree of novelty, explorative capacity, and technical detail, than\nconventional approaches and establishes a widely useful framework for\ninnovation by revealing hidden connections.\n","authors":["Markus J. Buehler"],"pdf_url":"https://arxiv.org/pdf/2403.11996v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17752v1","updated":"2024-03-26T14:43:48Z","published":"2024-03-26T14:43:48Z","title":"Can multiple-choice questions really be useful in detecting the\n  abilities of LLMs?","summary":"  Multiple-choice questions (MCQs) are widely used in the evaluation of large\nlanguage models (LLMs) due to their simplicity and efficiency. However, there\nare concerns about whether MCQs can truly measure LLM's capabilities,\nparticularly in knowledge-intensive scenarios where long-form generation (LFG)\nanswers are required. The misalignment between the task and the evaluation\nmethod demands a thoughtful analysis of MCQ's efficacy, which we undertake in\nthis paper by evaluating nine LLMs on four question-answering (QA) datasets in\ntwo languages: Chinese and English. We identify a significant issue: LLMs\nexhibit an order sensitivity in bilingual MCQs, favoring answers located at\nspecific positions, i.e., the first position. We further quantify the gap\nbetween MCQs and long-form generation questions (LFGQs) by comparing their\ndirect outputs, token logits, and embeddings. Our results reveal a relatively\nlow correlation between answers from MCQs and LFGQs for identical questions.\nAdditionally, we propose two methods to quantify the consistency and confidence\nof LLMs' output, which can be generalized to other QA evaluation benchmarks.\nNotably, our analysis challenges the idea that the higher the consistency, the\ngreater the accuracy. We also find MCQs to be less reliable than LFGQs in terms\nof expected calibration error. Finally, the misalignment between MCQs and LFGQs\nis not only reflected in the evaluation performance but also in the embedding\nspace. Our code and models can be accessed at\nhttps://github.com/Meetyou-AI-Lab/Can-MC-Evaluate-LLMs.\n","authors":["Wangyue Li","Liangzhi Li","Tong Xiang","Xiao Liu","Wei Deng","Noa Garcia"],"pdf_url":"https://arxiv.org/pdf/2403.17752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17748v1","updated":"2024-03-26T14:40:10Z","published":"2024-03-26T14:40:10Z","title":"UCxn: Typologically Informed Annotation of Constructions Atop Universal\n  Dependencies","summary":"  The Universal Dependencies (UD) project has created an invaluable collection\nof treebanks with contributions in over 140 languages. However, the UD\nannotations do not tell the full story. Grammatical constructions that convey\nmeaning through a particular combination of several morphosyntactic elements --\nfor example, interrogative sentences with special markers and/or word orders --\nare not labeled holistically. We argue for (i) augmenting UD annotations with a\n'UCxn' annotation layer for such meaning-bearing grammatical constructions, and\n(ii) approaching this in a typologically informed way so that morphosyntactic\nstrategies can be compared across languages. As a case study, we consider five\nconstruction families in ten languages, identifying instances of each\nconstruction in UD treebanks through the use of morphosyntactic patterns. In\naddition to findings regarding these particular constructions, our study yields\nimportant insights on methodology for describing and identifying constructions\nin language-general and language-particular ways, and lays the foundation for\nfuture constructional enrichment of UD treebanks.\n","authors":["Leonie Weissweiler","Nina Böbel","Kirian Guiller","Santiago Herrera","Wesley Scivetti","Arthur Lorenzi","Nurit Melnik","Archna Bhatia","Hinrich Schütze","Lori Levin","Amir Zeldes","Joakim Nivre","William Croft","Nathan Schneider"],"pdf_url":"https://arxiv.org/pdf/2403.17748v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2310.02129v4","updated":"2024-03-26T14:38:23Z","published":"2023-10-03T15:10:46Z","title":"Unveiling the Pitfalls of Knowledge Editing for Large Language Models","summary":"  As the cost associated with fine-tuning Large Language Models (LLMs)\ncontinues to rise, recent research efforts have pivoted towards developing\nmethodologies to edit implicit knowledge embedded within LLMs. Yet, there's\nstill a dark cloud lingering overhead -- will knowledge editing trigger\nbutterfly effect? since it is still unclear whether knowledge editing might\nintroduce side effects that pose potential risks or not. This paper pioneers\nthe investigation into the potential pitfalls associated with knowledge editing\nfor LLMs. To achieve this, we introduce new benchmark datasets and propose\ninnovative evaluation metrics. Our results underline two pivotal concerns: (1)\nKnowledge Conflict: Editing groups of facts that logically clash can magnify\nthe inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)\nKnowledge Distortion: Altering parameters with the aim of editing factual\nknowledge can irrevocably warp the innate knowledge structure of LLMs.\nExperimental results vividly demonstrate that knowledge editing might\ninadvertently cast a shadow of unintended consequences on LLMs, which warrant\nattention and efforts for future works. Code and data are available at\nhttps://github.com/zjunlp/PitfallsKnowledgeEditing.\n","authors":["Zhoubo Li","Ningyu Zhang","Yunzhi Yao","Mengru Wang","Xi Chen","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2310.02129v4.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.12721v2","updated":"2024-03-26T14:32:34Z","published":"2024-03-19T13:30:47Z","title":"CLASSLA-web: Comparable Web Corpora of South Slavic Languages Enriched\n  with Linguistic and Genre Annotation","summary":"  This paper presents a collection of highly comparable web corpora of\nSlovenian, Croatian, Bosnian, Montenegrin, Serbian, Macedonian, and Bulgarian,\ncovering thereby the whole spectrum of official languages in the South Slavic\nlanguage space. The collection of these corpora comprises a total of 13 billion\ntokens of texts from 26 million documents. The comparability of the corpora is\nensured by a comparable crawling setup and the usage of identical crawling and\npost-processing technology. All the corpora were linguistically annotated with\nthe state-of-the-art CLASSLA-Stanza linguistic processing pipeline, and\nenriched with document-level genre information via the Transformer-based\nmultilingual X-GENRE classifier, which further enhances comparability at the\nlevel of linguistic annotation and metadata enrichment. The genre-focused\nanalysis of the resulting corpora shows a rather consistent distribution of\ngenres throughout the seven corpora, with variations in the most prominent\ngenre categories being well-explained by the economic strength of each language\ncommunity. A comparison of the distribution of genre categories across the\ncorpora indicates that web corpora from less developed countries primarily\nconsist of news articles. Conversely, web corpora from economically more\ndeveloped countries exhibit a smaller proportion of news content, with a\ngreater presence of promotional and opinionated texts.\n","authors":["Nikola Ljubešić","Taja Kuzman"],"pdf_url":"https://arxiv.org/pdf/2403.12721v2.pdf","comment":"Accepted to the LREC-COLING 2024 conference"},{"id":"http://arxiv.org/abs/2307.05300v4","updated":"2024-03-26T14:32:33Z","published":"2023-07-11T14:45:19Z","title":"Unleashing the Emergent Cognitive Synergy in Large Language Models: A\n  Task-Solving Agent through Multi-Persona Self-Collaboration","summary":"  Human intelligence thrives on cognitive synergy, where collaboration among\ndifferent minds yield superior outcomes compared to isolated individuals. In\nthis work, we propose Solo Performance Prompting (SPP), which transforms a\nsingle LLM into a cognitive synergist by engaging in multi-turn\nself-collaboration with multiple personas. A cognitive synergist is an\nintelligent agent that collaboratively combines multiple minds' strengths and\nknowledge to enhance problem-solving in complex tasks. By dynamically\nidentifying and simulating different personas based on task inputs, SPP\nunleashes the potential of cognitive synergy in LLMs. Our in-depth analysis\nshows that assigning multiple fine-grained personas in LLMs improves\nproblem-solving abilities compared to using a single or fixed number of\npersonas. We evaluate SPP on three challenging tasks: Trivia Creative Writing,\nCodenames Collaborative, and Logic Grid Puzzle, encompassing both\nknowledge-intensive and reasoning-intensive types. Unlike previous works, such\nas Chain-of-Thought, that solely enhance the reasoning abilities in LLMs,\nexperimental results demonstrate that SPP effectively reduces factual\nhallucination, and maintains strong reasoning capabilities. Additionally,\ncomparative experiments show that cognitive synergy only emerges in GPT-4 and\ndoes not appear in less capable models, such as GPT-3.5-turbo and\nLlama2-13b-chat, which draws an interesting analogy to human development. Code,\ndata, and prompts can be found at:\nhttps://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.\n","authors":["Zhenhailong Wang","Shaoguang Mao","Wenshan Wu","Tao Ge","Furu Wei","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2307.05300v4.pdf","comment":"Accepted as a main conference paper at NAACL 2024"},{"id":"http://arxiv.org/abs/2403.17733v1","updated":"2024-03-26T14:20:42Z","published":"2024-03-26T14:20:42Z","title":"Continual Few-shot Event Detection via Hierarchical Augmentation\n  Networks","summary":"  Traditional continual event detection relies on abundant labeled data for\ntraining, which is often impractical to obtain in real-world applications. In\nthis paper, we introduce continual few-shot event detection (CFED), a more\ncommonly encountered scenario when a substantial number of labeled samples are\nnot accessible. The CFED task is challenging as it involves memorizing previous\nevent types and learning new event types with few-shot samples. To mitigate\nthese challenges, we propose a memory-based framework: Hierarchical\nAugmentation Networks (HANet). To memorize previous event types with limited\nmemory, we incorporate prototypical augmentation into the memory set. For the\nissue of learning new event types in few-shot scenarios, we propose a\ncontrastive augmentation module for token representations. Despite comparing\nwith previous state-of-the-art methods, we also conduct comparisons with\nChatGPT. Experiment results demonstrate that our method significantly\noutperforms all of these methods in multiple continual few-shot event detection\ntasks.\n","authors":["Chenlong Zhang","Pengfei Cao","Yubo Chen","Kang Liu","Zhiqiang Zhang","Mengshu Sun","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.17733v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17727v1","updated":"2024-03-26T14:16:56Z","published":"2024-03-26T14:16:56Z","title":"FastPerson: Enhancing Video Learning through Effective Video\n  Summarization that Preserves Linguistic and Visual Contexts","summary":"  Quickly understanding lengthy lecture videos is essential for learners with\nlimited time and interest in various topics to improve their learning\nefficiency. To this end, video summarization has been actively researched to\nenable users to view only important scenes from a video. However, these studies\nfocus on either the visual or audio information of a video and extract\nimportant segments in the video. Therefore, there is a risk of missing\nimportant information when both the teacher's speech and visual information on\nthe blackboard or slides are important, such as in a lecture video. To tackle\nthis issue, we propose FastPerson, a video summarization approach that\nconsiders both the visual and auditory information in lecture videos.\nFastPerson creates summary videos by utilizing audio transcriptions along with\non-screen images and text, minimizing the risk of overlooking crucial\ninformation for learners. Further, it provides a feature that allows learners\nto switch between the summary and original videos for each chapter of the\nvideo, enabling them to adjust the pace of learning based on their interests\nand level of understanding. We conducted an evaluation with 40 participants to\nassess the effectiveness of our method and confirmed that it reduced viewing\ntime by 53\\% at the same level of comprehension as that when using traditional\nvideo playback methods.\n","authors":["Kazuki Kawamura","Jun Rekimoto"],"pdf_url":"https://arxiv.org/pdf/2403.17727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17706v1","updated":"2024-03-26T13:50:34Z","published":"2024-03-26T13:50:34Z","title":"Enhanced Short Text Modeling: Leveraging Large Language Models for Topic\n  Refinement","summary":"  Crafting effective topic models for brief texts, like tweets and news\nheadlines, is essential for capturing the swift shifts in social dynamics.\nTraditional topic models, however, often fall short in accurately representing\nthe semantic intricacies of short texts due to their brevity and lack of\ncontextual data. In our study, we harness the advanced capabilities of Large\nLanguage Models (LLMs) to introduce a novel approach termed \"Topic Refinement\".\nThis approach does not directly involve itself in the initial modeling of\ntopics but focuses on improving topics after they have been mined. By employing\nprompt engineering, we direct LLMs to eliminate off-topic words within a given\ntopic, ensuring that only contextually relevant words are preserved or\nsubstituted with ones that fit better semantically. This method emulates\nhuman-like scrutiny and improvement of topics, thereby elevating the semantic\nquality of the topics generated by various models. Our comprehensive evaluation\nacross three unique datasets has shown that our topic refinement approach\nsignificantly enhances the semantic coherence of topics.\n","authors":["Shuyu Chang","Rui Wang","Peng Ren","Haiping Huang"],"pdf_url":"https://arxiv.org/pdf/2403.17706v1.pdf","comment":"6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.17691v1","updated":"2024-03-26T13:32:32Z","published":"2024-03-26T13:32:32Z","title":"Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to\n  Inform GenAI Copyright Disputes","summary":"  The advent of Generative Artificial Intelligence (GenAI) models, including\nGitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content\ncreation, enabling non-professionals to produce high-quality content across\nvarious domains. This transformative technology has led to a surge of synthetic\ncontent and sparked legal disputes over copyright infringement. To address\nthese challenges, this paper introduces a novel approach that leverages the\nlearning capacity of GenAI models for copyright legal analysis, demonstrated\nwith GPT2 and Stable Diffusion models. Copyright law distinguishes between\noriginal expressions and generic ones (Sc\\`enes \\`a faire), protecting the\nformer and permitting reproduction of the latter. However, this distinction has\nhistorically been challenging to make consistently, leading to over-protection\nof copyrighted works. GenAI offers an unprecedented opportunity to enhance this\nlegal analysis by revealing shared patterns in preexisting works. We propose a\ndata-driven approach to identify the genericity of works created by GenAI,\nemploying \"data-driven bias\" to assess the genericity of expressive\ncompositions. This approach aids in copyright scope determination by utilizing\nthe capabilities of GenAI to identify and prioritize expressive elements and\nrank them according to their frequency in the model's dataset. The potential\nimplications of measuring expressive genericity for copyright law are profound.\nSuch scoring could assist courts in determining copyright scope during\nlitigation, inform the registration practices of Copyright Offices, allowing\nregistration of only highly original synthetic works, and help copyright owners\nsignal the value of their works and facilitate fairer licensing deals. More\ngenerally, this approach offers valuable insights to policymakers grappling\nwith adapting copyright law to the challenges posed by the era of GenAI.\n","authors":["Uri Hacohen","Adi Haviv","Shahar Sarfaty","Bruria Friedman","Niva Elkin-Koren","Roi Livni","Amit H Bermano"],"pdf_url":"https://arxiv.org/pdf/2403.17691v1.pdf","comment":"Presented at ACM CSLAW 2024"},{"id":"http://arxiv.org/abs/2305.14230v2","updated":"2024-03-26T13:16:37Z","published":"2023-05-23T16:46:18Z","title":"Exploring Representational Disparities Between Multilingual and\n  Bilingual Translation Models","summary":"  Multilingual machine translation has proven immensely useful for both\nparameter efficiency and overall performance across many language pairs via\ncomplete multilingual parameter sharing. However, some language pairs in\nmultilingual models can see worse performance than in bilingual models,\nespecially in the one-to-many translation setting. Motivated by their empirical\ndifferences, we examine the geometric differences in representations from\nbilingual models versus those from one-to-many multilingual models.\nSpecifically, we compute the isotropy of these representations using intrinsic\ndimensionality and IsoScore, in order to measure how the representations\nutilize the dimensions in their underlying vector space. Using the same\nevaluation data in both models, we find that for a given language pair, its\nmultilingual model decoder representations are consistently less isotropic and\noccupy fewer dimensions than comparable bilingual model decoder\nrepresentations. Additionally, we show that much of the anisotropy in\nmultilingual decoder representations can be attributed to modeling\nlanguage-specific information, therefore limiting remaining representational\ncapacity.\n","authors":["Neha Verma","Kenton Murray","Kevin Duh"],"pdf_url":"https://arxiv.org/pdf/2305.14230v2.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.02270v2","updated":"2024-03-26T13:14:52Z","published":"2024-03-04T17:57:18Z","title":"FENICE: Factuality Evaluation of summarization based on Natural language\n  Inference and Claim Extraction","summary":"  Recent advancements in text summarization, particularly with the advent of\nLarge Language Models (LLMs), have shown remarkable performance. However, a\nnotable challenge persists as a substantial number of automatically-generated\nsummaries exhibit factual inconsistencies, such as hallucinations. In response\nto this issue, various approaches for the evaluation of consistency for\nsummarization have emerged. Yet, these newly-introduced metrics face several\nlimitations, including lack of interpretability, focus on short document\nsummaries (e.g., news articles), and computational impracticality, especially\nfor LLM-based metrics. To address these shortcomings, we propose Factuality\nEvaluation of summarization based on Natural language Inference and Claim\nExtraction (FENICE), a more interpretable and efficient factuality-oriented\nmetric. FENICE leverages an NLI-based alignment between information in the\nsource document and a set of atomic facts, referred to as claims, extracted\nfrom the summary. Our metric sets a new state of the art on AGGREFACT, the\nde-facto benchmark for factuality evaluation. Moreover, we extend our\nevaluation to a more challenging setting by conducting a human annotation\nprocess of long-form summarization.\n","authors":["Alessandro Scirè","Karim Ghonim","Roberto Navigli"],"pdf_url":"https://arxiv.org/pdf/2403.02270v2.pdf","comment":"9 pages, long paper"},{"id":"http://arxiv.org/abs/2403.16915v2","updated":"2024-03-26T13:11:44Z","published":"2024-03-25T16:32:50Z","title":"Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language\n  Models","summary":"  Fine-tuning in information retrieval systems using pre-trained language\nmodels (PLM-based IR) requires learning query representations and\nquery-document relations, in addition to downstream task-specific learning.\nThis study introduces coarse-tuning as an intermediate learning stage that\nbridges pre-training and fine-tuning. By learning query representations and\nquery-document relations in coarse-tuning, we aim to reduce the load of\nfine-tuning and improve the learning effect of downstream IR tasks. We propose\nQuery-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the\nappropriateness of query-document pairs. Evaluation experiments show that the\nproposed method significantly improves MRR and/or nDCG@5 in four ad-hoc\ndocument retrieval datasets. Furthermore, the results of the query prediction\ntask suggested that coarse-tuning facilitated learning of query representation\nand query-document relations.\n","authors":["Atsushi Keyaki","Ribeka Keyaki"],"pdf_url":"https://arxiv.org/pdf/2403.16915v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.13737v3","updated":"2024-03-26T13:01:38Z","published":"2024-03-20T16:43:42Z","title":"EthioLLM: Multilingual Large Language Models for Ethiopian Languages\n  with Task Evaluation","summary":"  Large language models (LLMs) have gained popularity recently due to their\noutstanding performance in various downstream Natural Language Processing (NLP)\ntasks. However, low-resource languages are still lagging behind current\nstate-of-the-art (SOTA) developments in the field of NLP due to insufficient\nresources to train LLMs. Ethiopian languages exhibit remarkable linguistic\ndiversity, encompassing a wide array of scripts, and are imbued with profound\nreligious and cultural significance. This paper introduces EthioLLM --\nmultilingual large language models for five Ethiopian languages (Amharic,\nGe'ez, Afan Oromo, Somali, and Tigrinya) and English, and Ethiobenchmark -- a\nnew benchmark dataset for various downstream NLP tasks. We evaluate the\nperformance of these models across five downstream NLP tasks. We open-source\nour multilingual language models, new benchmark datasets for various downstream\ntasks, and task-specific fine-tuned language models and discuss the performance\nof the models. Our dataset and models are available at the\nhttps://huggingface.co/EthioNLP repository.\n","authors":["Atnafu Lambebo Tonja","Israel Abebe Azime","Tadesse Destaw Belay","Mesay Gemeda Yigezu","Moges Ahmed Mehamed","Abinew Ali Ayele","Ebrahim Chekol Jibril","Michael Melese Woldeyohannis","Olga Kolesnikova","Philipp Slusallek","Dietrich Klakow","Shengwu Xiong","Seid Muhie Yimam"],"pdf_url":"https://arxiv.org/pdf/2403.13737v3.pdf","comment":"Accepted at LREC-Coling 2024"},{"id":"http://arxiv.org/abs/2403.14117v2","updated":"2024-03-26T12:53:14Z","published":"2024-03-21T04:03:16Z","title":"A Design Space for Intelligent and Interactive Writing Assistants","summary":"  In our era of rapid technological advancement, the research landscape for\nwriting assistants has become increasingly fragmented across various research\ncommunities. We seek to address this challenge by proposing a design space as a\nstructured way to examine and explore the multidimensional space of intelligent\nand interactive writing assistants. Through a large community collaboration, we\nexplore five aspects of writing assistants: task, user, technology,\ninteraction, and ecosystem. Within each aspect, we define dimensions (i.e.,\nfundamental components of an aspect) and codes (i.e., potential options for\neach dimension) by systematically reviewing 115 papers. Our design space aims\nto offer researchers and designers a practical tool to navigate, comprehend,\nand compare the various possibilities of writing assistants, and aid in the\nenvisioning and design of new writing assistants.\n","authors":["Mina Lee","Katy Ilonka Gero","John Joon Young Chung","Simon Buckingham Shum","Vipul Raheja","Hua Shen","Subhashini Venugopalan","Thiemo Wambsganss","David Zhou","Emad A. Alghamdi","Tal August","Avinash Bhat","Madiha Zahrah Choksi","Senjuti Dutta","Jin L. C. Guo","Md Naimul Hoque","Yewon Kim","Simon Knight","Seyed Parsa Neshaei","Agnia Sergeyuk","Antonette Shibani","Disha Shrivastava","Lila Shroff","Jessi Stark","Sarah Sterman","Sitong Wang","Antoine Bosselut","Daniel Buschek","Joseph Chee Chang","Sherol Chen","Max Kreminski","Joonsuk Park","Roy Pea","Eugenia H. Rho","Shannon Zejiang Shen","Pao Siangliulue"],"pdf_url":"https://arxiv.org/pdf/2403.14117v2.pdf","comment":"Published as a conference paper at CHI 2024"},{"id":"http://arxiv.org/abs/2403.17661v1","updated":"2024-03-26T12:47:39Z","published":"2024-03-26T12:47:39Z","title":"Language Models for Text Classification: Is In-Context Learning Enough?","summary":"  Recent foundational language models have shown state-of-the-art performance\nin many NLP tasks in zero- and few-shot settings. An advantage of these models\nover more standard approaches based on fine-tuning is the ability to understand\ninstructions written in natural language (prompts), which helps them generalise\nbetter to different tasks and domains without the need for specific training\ndata. This makes them suitable for addressing text classification problems for\ndomains with limited amounts of annotated instances. However, existing research\nis limited in scale and lacks understanding of how text generation models\ncombined with prompting techniques compare to more established methods for text\nclassification such as fine-tuning masked language models. In this paper, we\naddress this research gap by performing a large-scale evaluation study for 16\ntext classification datasets covering binary, multiclass, and multilabel\nproblems. In particular, we compare zero- and few-shot approaches of large\nlanguage models to fine-tuning smaller language models. We also analyse the\nresults by prompt, classification type, domain, and number of labels. In\ngeneral, the results show how fine-tuning smaller and more efficient language\nmodels can still outperform few-shot approaches of larger language models,\nwhich have room for improvement when it comes to text classification.\n","authors":["Aleksandra Edwards","Jose Camacho-Collados"],"pdf_url":"https://arxiv.org/pdf/2403.17661v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2308.10592v3","updated":"2024-03-26T12:31:35Z","published":"2023-08-21T09:47:31Z","title":"BAN-PL: a Novel Polish Dataset of Banned Harmful and Offensive Content\n  from Wykop.pl web service","summary":"  Since the Internet is flooded with hate, it is one of the main tasks for NLP\nexperts to master automated online content moderation. However, advancements in\nthis field require improved access to publicly available accurate and\nnon-synthetic datasets of social media content. For the Polish language, such\nresources are very limited. In this paper, we address this gap by presenting a\nnew open dataset of offensive social media content for the Polish language. The\ndataset comprises content from Wykop.pl, a popular online service often\nreferred to as the \"Polish Reddit\", reported by users and banned in the\ninternal moderation process. It contains a total of 691,662 posts and comments,\nevenly divided into two categories: \"harmful\" and \"neutral\" (\"non-harmful\").\nThe anonymized subset of the BAN-PL dataset consisting on 24,000 pieces (12,000\nfor each class), along with preprocessing scripts have been made publicly\navailable. Furthermore the paper offers valuable insights into real-life\ncontent moderation processes and delves into an analysis of linguistic features\nand content characteristics of the dataset. Moreover, a comprehensive\nanonymization procedure has been meticulously described and applied. The\nprevalent biases encountered in similar datasets, including post-moderation and\npre-selection biases, are also discussed.\n","authors":["Anna Kołos","Inez Okulska","Kinga Głąbińska","Agnieszka Karlińska","Emilia Wiśnios","Paweł Ellerik","Andrzej Prałat"],"pdf_url":"https://arxiv.org/pdf/2308.10592v3.pdf","comment":"Accepted for LREC-COLING 2024 Conference"},{"id":"http://arxiv.org/abs/2403.17647v1","updated":"2024-03-26T12:29:18Z","published":"2024-03-26T12:29:18Z","title":"Intrinsic Subgraph Generation for Interpretable Graph based Visual\n  Question Answering","summary":"  The large success of deep learning based methods in Visual Question Answering\n(VQA) has concurrently increased the demand for explainable methods. Most\nmethods in Explainable Artificial Intelligence (XAI) focus on generating\npost-hoc explanations rather than taking an intrinsic approach, the latter\ncharacterizing an interpretable model. In this work, we introduce an\ninterpretable approach for graph-based VQA and demonstrate competitive\nperformance on the GQA dataset. This approach bridges the gap between\ninterpretability and performance. Our model is designed to intrinsically\nproduce a subgraph during the question-answering process as its explanation,\nproviding insight into the decision making. To evaluate the quality of these\ngenerated subgraphs, we compare them against established post-hoc\nexplainability methods for graph neural networks, and perform a human\nevaluation. Moreover, we present quantitative metrics that correlate with the\nevaluations of human assessors, acting as automatic metrics for the generated\nexplanatory subgraphs. Our implementation is available at\nhttps://github.com/DigitalPhonetics/Intrinsic-Subgraph-Generation-for-VQA.\n","authors":["Pascal Tilli","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2403.17647v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17645v1","updated":"2024-03-26T12:27:32Z","published":"2024-03-26T12:27:32Z","title":"DANCER: Entity Description Augmented Named Entity Corrector for\n  Automatic Speech Recognition","summary":"  End-to-end automatic speech recognition (E2E ASR) systems often suffer from\nmistranscription of domain-specific phrases, such as named entities, sometimes\nleading to catastrophic failures in downstream tasks. A family of fast and\nlightweight named entity correction (NEC) models for ASR have recently been\nproposed, which normally build on phonetic-level edit distance algorithms and\nhave shown impressive NEC performance. However, as the named entity (NE) list\ngrows, the problems of phonetic confusion in the NE list are exacerbated; for\nexample, homophone ambiguities increase substantially. In view of this, we\nproposed a novel Description Augmented Named entity CorrEctoR (dubbed DANCER),\nwhich leverages entity descriptions to provide additional information to\nfacilitate mitigation of phonetic confusion for NEC on ASR transcription. To\nthis end, an efficient entity description augmented masked language model\n(EDA-MLM) comprised of a dense retrieval model is introduced, enabling MLM to\nadapt swiftly to domain-specific entities for the NEC task. A series of\nexperiments conducted on the AISHELL-1 and Homophone datasets confirm the\neffectiveness of our modeling approach. DANCER outperforms a strong baseline,\nthe phonetic edit-distance-based NEC model (PED-NEC), by a character error rate\n(CER) reduction of about 7% relatively on AISHELL-1 for named entities. More\nnotably, when tested on Homophone that contain named entities of high phonetic\nconfusion, DANCER offers a more pronounced CER reduction of 46% relatively over\nPED-NEC for named entities.\n","authors":["Yi-Cheng Wang","Hsin-Wei Wang","Bi-Cheng Yan","Chi-Han Lin","Berlin Chen"],"pdf_url":"https://arxiv.org/pdf/2403.17645v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13334v2","updated":"2024-03-26T12:24:46Z","published":"2024-03-20T06:37:59Z","title":"Hyacinth6B: A large language model for Traditional Chinese","summary":"  This research's primary motivation of this study is to address the high\nhardware and computational demands typically associated with LLMs.Therefore,our\ngoal is to find a balance between model lightness and performance,striving to\nmaximize performance while using a comparatively lightweight model. Hyacinth6B\nwas developed with this objective in mind,aiming to fully leverage the core\ncapabilities of LLMs without incurring substantial resource costs, effectively\npushing the boundaries of smaller model's performance. The training approach\ninvolves parameter efficient finetuning using the LoRA method.\n","authors":["Chih-Wei Song","Yin-Te Tsai"],"pdf_url":"https://arxiv.org/pdf/2403.13334v2.pdf","comment":"14pages"},{"id":"http://arxiv.org/abs/2403.17640v1","updated":"2024-03-26T12:21:51Z","published":"2024-03-26T12:21:51Z","title":"REFeREE: A REference-FREE Model-Based Metric for Text Simplification","summary":"  Text simplification lacks a universal standard of quality, and annotated\nreference simplifications are scarce and costly. We propose to alleviate such\nlimitations by introducing REFeREE, a reference-free model-based metric with a\n3-stage curriculum. REFeREE leverages an arbitrarily scalable pretraining stage\nand can be applied to any quality standard as long as a small number of human\nannotations are available. Our experiments show that our metric outperforms\nexisting reference-based metrics in predicting overall ratings and reaches\ncompetitive and consistent performance in predicting specific ratings while\nrequiring no reference simplifications at inference time.\n","authors":["Yichen Huang","Ekaterina Kochmar"],"pdf_url":"https://arxiv.org/pdf/2403.17640v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17636v1","updated":"2024-03-26T12:11:29Z","published":"2024-03-26T12:11:29Z","title":"Mix-Initiative Response Generation with Dynamic Prefix Tuning","summary":"  Mixed initiative serves as one of the key factors in controlling conversation\ndirections. For a speaker, responding passively or leading proactively would\nresult in rather different responses. However, most dialogue systems focus on\ntraining a holistic response generation model without any distinction among\ndifferent initiatives. It leads to the cross-contamination problem, where the\nmodel confuses different initiatives and generates inappropriate responses.\nMoreover, obtaining plenty of human annotations for initiative labels can be\nexpensive. To address this issue, we propose a general mix-Initiative Dynamic\nPrefix Tuning framework (IDPT) to decouple different initiatives from the\ngeneration model, which learns initiative-aware prefixes in both supervised and\nunsupervised settings. Specifically, IDPT decouples initiative factors into\ndifferent prefix parameters and uses the attention mechanism to adjust the\nselection of initiatives in guiding generation dynamically. The prefix\nparameters can be tuned towards accurate initiative prediction as well as\nmix-initiative response generation. Extensive experiments on two public\ndialogue datasets show that the proposed IDPT outperforms previous baselines on\nboth automatic metrics and human evaluations. It also manages to generate\nappropriate responses with manipulated initiatives.\n","authors":["Yuxiang Nie","Heyan Huang","Xian-Ling Mao","Lizi Liao"],"pdf_url":"https://arxiv.org/pdf/2403.17636v1.pdf","comment":"Accepted to the main conference of NAACL 2024"},{"id":"http://arxiv.org/abs/2310.12541v3","updated":"2024-03-26T12:04:44Z","published":"2023-10-19T07:46:54Z","title":"Large Language Model for Multi-objective Evolutionary Optimization","summary":"  Multiobjective evolutionary algorithms (MOEAs) are major methods for solving\nmultiobjective optimization problems (MOPs). Many MOEAs have been proposed in\nthe past decades, of which the search operators need a carefully handcrafted\ndesign with domain knowledge. Recently, some attempts have been made to replace\nthe manually designed operators in MOEAs with learning-based operators (e.g.,\nneural network models). However, much effort is still required for designing\nand training such models, and the learned operators might not generalize well\non new problems. To tackle the above challenges, this work investigates a novel\napproach that leverages the powerful large language model (LLM) to design MOEA\noperators. With proper prompt engineering, we successfully let a general LLM\nserve as a black-box search operator for decomposition-based MOEA (MOEA/D) in a\nzero-shot manner. In addition, by learning from the LLM behavior, we further\ndesign an explicit white-box operator with randomness and propose a new version\nof decomposition-based MOEA, termed MOEA/D-LO. Experimental studies on\ndifferent test benchmarks show that our proposed method can achieve competitive\nperformance with widely used MOEAs. It is also promising to see the operator\nonly learned from a few instances can have robust generalization performance on\nunseen problems with quite different patterns and settings. The results reveal\nthe potential benefits of using pre-trained LLMs in the design of MOEAs.To\nfoster reproducibility and accessibility, the source code is\nhttps://github.com/FeiLiu36/LLM4MOEA.\n","authors":["Fei Liu","Xi Lin","Zhenkun Wang","Shunyu Yao","Xialiang Tong","Mingxuan Yuan","Qingfu Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.12541v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.15694v5","updated":"2024-03-26T11:52:59Z","published":"2023-10-24T10:05:32Z","title":"COPR: Continual Learning Human Preference through Optimal Policy\n  Regularization","summary":"  The technique of Reinforcement Learning from Human Feedback (RLHF) is a\ncommonly employed method to improve pre-trained Language Models (LM), enhancing\ntheir ability to conform to human preferences. Nevertheless, the current\nRLHF-based LMs necessitate full retraining each time novel queries or feedback\nare introduced, which becomes a challenging task because human preferences can\nvary between different domains or tasks. Retraining LMs poses practical\ndifficulties in many real-world situations due to the significant time and\ncomputational resources required, along with concerns related to data privacy.\nTo address this limitation, we propose a new method called Continual Optimal\nPolicy Regularization (COPR), in which we compute the distribution of optimal\npolicy bypassing the partition function and then regularize the current policy\nbased on the historically optimal distribution to mitigate Catastrophic\nForgetting (CF). COPR involves a single learning phase and doesn't necessitate\ncomplex reinforcement learning. Importantly, it shares the capability with RLHF\nto learn from unlabeled data by maintaining a scoring module, similar to reward\nmodel, making it flexible for continually learning without human feedback. Our\nexperimental results show that COPR outperforms strong Continuous Learning (CL)\nbaselines when it comes to consistently aligning with human preferences on\nincremental tasks and domains.\n","authors":["Han Zhang","Lin Gui","Yuanzhao Zhai","Hui Wang","Yu Lei","Ruifeng Xu"],"pdf_url":"https://arxiv.org/pdf/2310.15694v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17612v1","updated":"2024-03-26T11:45:22Z","published":"2024-03-26T11:45:22Z","title":"\"You are an expert annotator\": Automatic Best-Worst-Scaling Annotations\n  for Emotion Intensity Modeling","summary":"  Labeling corpora constitutes a bottleneck to create models for new tasks or\ndomains. Large language models mitigate the issue with automatic corpus\nlabeling methods, particularly for categorical annotations. Some NLP tasks such\nas emotion intensity prediction, however, require text regression, but there is\nno work on automating annotations for continuous label assignments. Regression\nis considered more challenging than classification: The fact that humans\nperform worse when tasked to choose values from a rating scale lead to\ncomparative annotation methods, including best-worst scaling. This raises the\nquestion if large language model-based annotation methods show similar\npatterns, namely that they perform worse on rating scale annotation tasks than\non comparative annotation tasks. To study this, we automate emotion intensity\npredictions and compare direct rating scale predictions, pairwise comparisons\nand best-worst scaling. We find that the latter shows the highest reliability.\nA transformer regressor fine-tuned on these data performs nearly on par with a\nmodel trained on the original manual annotations.\n","authors":["Christopher Bagdon","Prathamesh Karmalker","Harsha Gurulingappa","Roman Klinger"],"pdf_url":"https://arxiv.org/pdf/2403.17612v1.pdf","comment":"accepted for publication in NAACL 2024"},{"id":"http://arxiv.org/abs/2403.17611v1","updated":"2024-03-26T11:44:49Z","published":"2024-03-26T11:44:49Z","title":"Denoising Table-Text Retrieval for Open-Domain Question Answering","summary":"  In table-text open-domain question answering, a retriever system retrieves\nrelevant evidence from tables and text to answer questions. Previous studies in\ntable-text open-domain question answering have two common challenges: firstly,\ntheir retrievers can be affected by false-positive labels in training datasets;\nsecondly, they may struggle to provide appropriate evidence for questions that\nrequire reasoning across the table. To address these issues, we propose\nDenoised Table-Text Retriever (DoTTeR). Our approach involves utilizing a\ndenoised training dataset with fewer false positive labels by discarding\ninstances with lower question-relevance scores measured through a false\npositive detection model. Subsequently, we integrate table-level ranking\ninformation into the retriever to assist in finding evidence for questions that\ndemand reasoning across the table. To encode this ranking information, we\nfine-tune a rank-aware column encoder to identify minimum and maximum values\nwithin a column. Experimental results demonstrate that DoTTeR significantly\noutperforms strong baselines on both retrieval recall and downstream QA tasks.\nOur code is available at https://github.com/deokhk/DoTTeR.\n","authors":["Deokhyung Kang","Baikjin Jung","Yunsu Kim","Gary Geunbae Lee"],"pdf_url":"https://arxiv.org/pdf/2403.17611v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2305.14790v2","updated":"2024-03-26T11:29:21Z","published":"2023-05-24T06:43:23Z","title":"Advancing Topic Segmentation and Outline Generation in Chinese Texts:\n  The Paragraph-level Topic Representation, Corpus, and Benchmark","summary":"  Topic segmentation and outline generation strive to divide a document into\ncoherent topic sections and generate corresponding subheadings, unveiling the\ndiscourse topic structure of a document. Compared with sentence-level topic\nstructure, the paragraph-level topic structure can quickly grasp and understand\nthe overall context of the document from a higher level, benefitting many\ndownstream tasks such as summarization, discourse parsing, and information\nretrieval. However, the lack of large-scale, high-quality Chinese\nparagraph-level topic structure corpora restrained relative research and\napplications. To fill this gap, we build the Chinese paragraph-level topic\nrepresentation, corpus, and benchmark in this paper. Firstly, we propose a\nhierarchical paragraph-level topic structure representation with three layers\nto guide the corpus construction. Then, we employ a two-stage man-machine\ncollaborative annotation method to construct the largest Chinese\nParagraph-level Topic Structure corpus (CPTS), achieving high quality. We also\nbuild several strong baselines, including ChatGPT, to validate the\ncomputability of CPTS on two fundamental tasks (topic segmentation and outline\ngeneration) and preliminarily verified its usefulness for the downstream task\n(discourse parsing).\n","authors":["Feng Jiang","Weihao Liu","Xiaomin Chu","Peifeng Li","Qiaoming Zhu","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2305.14790v2.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2309.13318v2","updated":"2024-03-26T11:26:04Z","published":"2023-09-23T09:24:05Z","title":"Spanish Resource Grammar version 2023","summary":"  We present the latest version of the Spanish Resource Grammar (SRG), a\ngrammar of Spanish implemented in the HPSG formalism. Such grammars encode a\ncomplex set of hypotheses about syntax making them a resource for empirical\ntesting of linguistic theory. They also encode a strict notion of\ngrammaticality which makes them a resource for natural language processing\napplications in computer-assisted language learning. This version of the SRG\nuses the recent version of the Freeling morphological analyzer and is released\nalong with an automatically created, manually verified treebank of 2,291\nsentences. We explain the treebanking process, emphasizing how it is different\nfrom treebanking with manual annotation and how it contributes to\nempirically-driven development of syntactic theory. The treebanks' high level\nof consistency and detail makes them a resource for training high-quality\nsemantic parsers and generally systems that benefit from precise and detailed\nsemantics. Finally, we present the grammar's coverage and overgeneration on 100\nsentences from a learner corpus, a new research line related to developing\nmethodologies for robust empirical evaluation of hypotheses in second language\nacquisition.\n","authors":["Olga Zamaraeva","Lorena S. Allegue","Carlos Gómez-Rodríguez"],"pdf_url":"https://arxiv.org/pdf/2309.13318v2.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.13518v2","updated":"2024-03-26T11:16:47Z","published":"2024-03-20T11:38:30Z","title":"Motion Generation from Fine-grained Textual Descriptions","summary":"  The task of text2motion is to generate human motion sequences from given\ntextual descriptions, where the model explores diverse mappings from natural\nlanguage instructions to human body movements. While most existing works are\nconfined to coarse-grained motion descriptions, e.g., \"A man squats.\",\nfine-grained descriptions specifying movements of relevant body parts are\nbarely explored. Models trained with coarse-grained texts may not be able to\nlearn mappings from fine-grained motion-related words to motion primitives,\nresulting in the failure to generate motions from unseen descriptions. In this\npaper, we build a large-scale language-motion dataset specializing in\nfine-grained textual descriptions, FineHumanML3D, by feeding GPT-3.5-turbo with\nstep-by-step instructions with pseudo-code compulsory checks. Accordingly, we\ndesign a new text2motion model, FineMotionDiffuse, making full use of\nfine-grained textual information. Our quantitative evaluation shows that\nFineMotionDiffuse trained on FineHumanML3D improves FID by a large margin of\n0.38, compared with competitive baselines. According to the qualitative\nevaluation and case study, our model outperforms MotionDiffuse in generating\nspatially or chronologically composite motions, by learning the implicit\nmappings from fine-grained descriptions to the corresponding basic motions. We\nrelease our data at https://github.com/KunhangL/finemotiondiffuse.\n","authors":["Kunhang Li","Yansong Feng"],"pdf_url":"https://arxiv.org/pdf/2403.13518v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08644v3","updated":"2024-03-26T11:13:56Z","published":"2024-02-13T18:24:08Z","title":"Tandem Transformers for Inference Efficient LLMs","summary":"  The autoregressive nature of conventional large language models (LLMs)\ninherently limits inference speed, as tokens are generated sequentially. While\nspeculative and parallel decoding techniques attempt to mitigate this, they\nface limitations: either relying on less accurate smaller models for generation\nor failing to fully leverage the base LLM's representations.\n  We introduce a novel architecture, Tandem transformers, to address these\nissues. This architecture uniquely combines (1) a small autoregressive model\nand (2) a large model operating in block mode (processing multiple tokens\nsimultaneously). The small model's predictive accuracy is substantially\nenhanced by granting it attention to the large model's richer representations.\nOn the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko\ndemonstrates a 3.3% improvement in next-token prediction accuracy over a\nstandalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter\nmodel with comparable downstream performance. We further incorporate the tandem\nmodel within the speculative decoding (SPEED) framework where the large model\nvalidates tokens from the small model. This ensures that the Tandem of\nPaLM2-Bison and PaLM2-Gecko achieves substantial speedup (around 1.14x faster\nthan using vanilla PaLM2-Gecko in SPEED) while maintaining identical downstream\ntask accuracy.\n","authors":["Aishwarya P S","Pranav Ajit Nair","Yashas Samaga","Toby Boyd","Sanjiv Kumar","Prateek Jain","Praneeth Netrapalli"],"pdf_url":"https://arxiv.org/pdf/2402.08644v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17599v1","updated":"2024-03-26T11:09:58Z","published":"2024-03-26T11:09:58Z","title":"Coimagining the Future of Voice Assistants with Cultural Sensitivity","summary":"  Voice assistants (VAs) are becoming a feature of our everyday life. Yet, the\nuser experience (UX) is often limited, leading to underuse, disengagement, and\nabandonment. Co-designing interactions for VAs with potential end-users can be\nuseful. Crowdsourcing this process online and anonymously may add value.\nHowever, most work has been done in the English-speaking West on dialogue data\nsets. We must be sensitive to cultural differences in language, social\ninteractions, and attitudes towards technology. Our aims were to explore the\nvalue of co-designing VAs in the non-Western context of Japan and demonstrate\nthe necessity of cultural sensitivity. We conducted an online elicitation study\n(N = 135) where Americans (n = 64) and Japanese people (n = 71) imagined\ndialogues (N = 282) and activities (N = 73) with future VAs. We discuss the\nimplications for coimagining interactions with future VAs, offer design\nguidelines for the Japanese and English-speaking US contexts, and suggest\nopportunities for cultural plurality in VA design and scholarship.\n","authors":["Katie Seaborn","Yuto Sawa","Mizuki Watanabe"],"pdf_url":"https://arxiv.org/pdf/2403.17599v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2403.14438v2","updated":"2024-03-26T11:02:32Z","published":"2024-03-21T14:44:03Z","title":"A Multimodal Approach to Device-Directed Speech Detection with Large\n  Language Models","summary":"  Interactions with virtual assistants typically start with a predefined\ntrigger phrase followed by the user command. To make interactions with the\nassistant more intuitive, we explore whether it is feasible to drop the\nrequirement that users must begin each command with a trigger phrase. We\nexplore this task in three ways: First, we train classifiers using only\nacoustic information obtained from the audio waveform. Second, we take the\ndecoder outputs of an automatic speech recognition (ASR) system, such as 1-best\nhypotheses, as input features to a large language model (LLM). Finally, we\nexplore a multimodal system that combines acoustic and lexical features, as\nwell as ASR decoder signals in an LLM. Using multimodal information yields\nrelative equal-error-rate improvements over text-only and audio-only models of\nup to 39% and 61%. Increasing the size of the LLM and training with low-rank\nadaption leads to further relative EER reductions of up to 18% on our dataset.\n","authors":["Dominik Wagner","Alexander Churchill","Siddharth Sigtia","Panayiotis Georgiou","Matt Mirsamadi","Aarshee Mishra","Erik Marchi"],"pdf_url":"https://arxiv.org/pdf/2403.14438v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2312.03632"},{"id":"http://arxiv.org/abs/2402.11537v2","updated":"2024-03-26T10:45:40Z","published":"2024-02-18T10:36:05Z","title":"Deciphering the Impact of Pretraining Data on Large Language Models\n  through Machine Unlearning","summary":"  Through pretraining on a corpus with various sources, Large Language Models\n(LLMs) have gained impressive performance. However, the impact of each\ncomponent of the pretraining corpus remains opaque. As a result, the\norganization of the pretraining corpus is still empirical and may deviate from\nthe optimal. To address this issue, we systematically analyze the impact of 48\ndatasets from 5 major categories of pretraining data of LLMs and measure their\nimpacts on LLMs using benchmarks about nine major categories of model\ncapabilities. Our analyses provide empirical results about the contribution of\nmultiple corpora on the performances of LLMs, along with their joint impact\npatterns, including complementary, orthogonal, and correlational relationships.\nWe also identify a set of ``high-impact data'' such as Books that is\nsignificantly related to a set of model capabilities. These findings provide\ninsights into the organization of data to support more efficient pretraining of\nLLMs.\n","authors":["Yang Zhao","Li Du","Xiao Ding","Kai Xiong","Zhouhao Sun","Jun Shi","Ting Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2402.11537v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17582v1","updated":"2024-03-26T10:45:11Z","published":"2024-03-26T10:45:11Z","title":"Towards a Zero-Data, Controllable, Adaptive Dialog System","summary":"  Conversational Tree Search (V\\\"ath et al., 2023) is a recent approach to\ncontrollable dialog systems, where domain experts shape the behavior of a\nReinforcement Learning agent through a dialog tree. The agent learns to\nefficiently navigate this tree, while adapting to information needs, e.g.,\ndomain familiarity, of different users. However, the need for additional\ntraining data hinders deployment in new domains. To address this, we explore\napproaches to generate this data directly from dialog trees. We improve the\noriginal approach, and show that agents trained on synthetic data can achieve\ncomparable dialog success to models trained on human data, both when using a\ncommercial Large Language Model for generation, or when using a smaller\nopen-source model, running on a single GPU. We further demonstrate the\nscalability of our approach by collecting and testing on two new datasets:\nONBOARD, a new domain helping foreign residents moving to a new city, and the\nmedical domain DIAGNOSE, a subset of Wikipedia articles related to scalp and\nhead symptoms. Finally, we perform human testing, where no statistically\nsignificant differences were found in either objective or subjective measures\nbetween models trained on human and generated data.\n","authors":["Dirk Väth","Lindsey Vanderlyn","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2403.17582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08274v4","updated":"2024-03-26T10:36:31Z","published":"2023-12-13T16:43:41Z","title":"High-throughput Biomedical Relation Extraction for Semi-Structured Web\n  Articles Empowered by Large Language Models","summary":"  Objective: To develop a high-throughput biomedical relation extraction system\nthat takes advantage of the large language models'(LLMs) reading comprehension\nability and biomedical world knowledge in a scalable and evidential manner.\nMethods: We formulate the relation extraction task as binary classifications\nfor large language models. Specifically, LLMs make the decision based on the\nexternal corpus and its world knowledge, giving the reason for the judgment for\nfactual verification. This method is tailored for semi-structured web articles,\nwherein we designate the main title as the tail entity and explicitly\nincorporate it into the context, and the potential head entities are matched\nbased on a biomedical thesaurus. Moreover, lengthy contents are sliced into\ntext chunks, embedded, and retrieved with additional embedding models. Results:\nUsing an open-source LLM, we extracted 248659 relation triplets of three\ndistinct relation types from three reputable biomedical websites. To assess the\nefficacy of the basic pipeline employed for biomedical relation extraction, we\ncurated a benchmark dataset annotated by a medical expert. Evaluation results\nindicate that the pipeline exhibits performance comparable to that of GPT-4.\nCase studies further illuminate challenges faced by contemporary LLMs in the\ncontext of biomedical relation extraction for semi-structured web articles.\nConclusion: The proposed method has demonstrated its effectiveness in\nleveraging the strengths of LLMs for high-throughput biomedical relation\nextraction. Its adaptability is evident, as it can be seamlessly extended to\ndiverse semi-structured biomedical websites, facilitating the extraction of\nvarious types of biomedical relations with ease.\n","authors":["Songchi Zhou","Sheng Yu"],"pdf_url":"https://arxiv.org/pdf/2312.08274v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15885v2","updated":"2024-03-26T10:26:04Z","published":"2024-03-23T16:45:22Z","title":"STEntConv: Predicting Disagreement with Stance Detection and a Signed\n  Graph Convolutional Network","summary":"  The rise of social media platforms has led to an increase in polarised online\ndiscussions, especially on political and socio-cultural topics such as\nelections and climate change. We propose a simple and novel unsupervised method\nto predict whether the authors of two posts agree or disagree, leveraging user\nstances about named entities obtained from their posts. We present STEntConv, a\nmodel which builds a graph of users and named entities weighted by stance and\ntrains a Signed Graph Convolutional Network (SGCN) to detect disagreement\nbetween comment and reply posts. We run experiments and ablation studies and\nshow that including this information improves disagreement detection\nperformance on a dataset of Reddit posts for a range of controversial subreddit\ntopics, without the need for platform-specific features or user history.\n","authors":["Isabelle Lorge","Li Zhang","Xiaowen Dong","Janet B. Pierrehumbert"],"pdf_url":"https://arxiv.org/pdf/2403.15885v2.pdf","comment":"Accepted for the 2024 Joint International Conference on Computational\n  Linguistics, Language Resources and Evaluation (LREC-COLING 2024)"},{"id":"http://arxiv.org/abs/2403.17564v1","updated":"2024-03-26T10:14:12Z","published":"2024-03-26T10:14:12Z","title":"Task-Oriented Paraphrase Analytics","summary":"  Since paraphrasing is an ill-defined task, the term \"paraphrasing\" covers\ntext transformation tasks with different characteristics. Consequently,\nexisting paraphrasing studies have applied quite different (explicit and\nimplicit) criteria as to when a pair of texts is to be considered a paraphrase,\nall of which amount to postulating a certain level of semantic or lexical\nsimilarity. In this paper, we conduct a literature review and propose a\ntaxonomy to organize the 25~identified paraphrasing (sub-)tasks. Using\nclassifiers trained to identify the tasks that a given paraphrasing instance\nfits, we find that the distributions of task-specific instances in the known\nparaphrase corpora vary substantially. This means that the use of these\ncorpora, without the respective paraphrase conditions being clearly defined\n(which is the normal case), must lead to incomparable and misleading results.\n","authors":["Marcel Gohsen","Matthias Hagen","Martin Potthast","Benno Stein"],"pdf_url":"https://arxiv.org/pdf/2403.17564v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2304.02541v4","updated":"2024-03-26T10:13:06Z","published":"2023-04-05T16:03:42Z","title":"PWESuite: Phonetic Word Embeddings and Tasks They Facilitate","summary":"  Mapping words into a fixed-dimensional vector space is the backbone of modern\nNLP. While most word embedding methods successfully encode semantic\ninformation, they overlook phonetic information that is crucial for many tasks.\nWe develop three methods that use articulatory features to build phonetically\ninformed word embeddings. To address the inconsistent evaluation of existing\nphonetic word embedding methods, we also contribute a task suite to fairly\nevaluate past, current, and future methods. We evaluate both (1) intrinsic\naspects of phonetic word embeddings, such as word retrieval and correlation\nwith sound similarity, and (2) extrinsic performance on tasks such as rhyme and\ncognate detection and sound analogies. We hope our task suite will promote\nreproducibility and inspire future phonetic embedding research.\n","authors":["Vilém Zouhar","Kalvin Chang","Chenxuan Cui","Nathaniel Carlson","Nathaniel Robinson","Mrinmaya Sachan","David Mortensen"],"pdf_url":"https://arxiv.org/pdf/2304.02541v4.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17556v1","updated":"2024-03-26T10:04:24Z","published":"2024-03-26T10:04:24Z","title":"m3P: Towards Multimodal Multilingual Translation with Multimodal Prompt","summary":"  Multilingual translation supports multiple translation directions by\nprojecting all languages in a shared space, but the translation quality is\nundermined by the difference between languages in the text-only modality,\nespecially when the number of languages is large. To bridge this gap, we\nintroduce visual context as the universal language-independent representation\nto facilitate multilingual translation. In this paper, we propose a framework\nto leverage the multimodal prompt to guide the Multimodal Multilingual neural\nMachine Translation (m3P), which aligns the representations of different\nlanguages with the same meaning and generates the conditional vision-language\nmemory for translation. We construct a multilingual multimodal instruction\ndataset (InstrMulti102) to support 102 languages. Our method aims to minimize\nthe representation distance of different languages by regarding the image as a\ncentral language. Experimental results show that m3P outperforms previous\ntext-only baselines and multilingual multimodal methods by a large margin.\nFurthermore, the probing experiments validate the effectiveness of our method\nin enhancing translation under the low-resource and massively multilingual\nscenario.\n","authors":["Jian Yang","Hongcheng Guo","Yuwei Yin","Jiaqi Bai","Bing Wang","Jiaheng Liu","Xinnian Liang","Linzheng Cahi","Liqun Yang","Zhoujun Li"],"pdf_url":"https://arxiv.org/pdf/2403.17556v1.pdf","comment":"COLING 2024"},{"id":"http://arxiv.org/abs/2403.17553v1","updated":"2024-03-26T10:01:01Z","published":"2024-03-26T10:01:01Z","title":"RuBia: A Russian Language Bias Detection Dataset","summary":"  Warning: this work contains upsetting or disturbing content.\n  Large language models (LLMs) tend to learn the social and cultural biases\npresent in the raw pre-training data. To test if an LLM's behavior is fair,\nfunctional datasets are employed, and due to their purpose, these datasets are\nhighly language and culture-specific. In this paper, we address a gap in the\nscope of multilingual bias evaluation by presenting a bias detection dataset\nspecifically designed for the Russian language, dubbed as RuBia. The RuBia\ndataset is divided into 4 domains: gender, nationality, socio-economic status,\nand diverse, each of the domains is further divided into multiple fine-grained\nsubdomains. Every example in the dataset consists of two sentences with the\nfirst reinforcing a potentially harmful stereotype or trope and the second\ncontradicting it. These sentence pairs were first written by volunteers and\nthen validated by native-speaking crowdsourcing workers. Overall, there are\nnearly 2,000 unique sentence pairs spread over 19 subdomains in RuBia. To\nillustrate the dataset's purpose, we conduct a diagnostic evaluation of\nstate-of-the-art or near-state-of-the-art LLMs and discuss the LLMs'\npredisposition to social biases.\n","authors":["Veronika Grigoreva","Anastasiia Ivanova","Ilseyar Alimova","Ekaterina Artemova"],"pdf_url":"https://arxiv.org/pdf/2403.17553v1.pdf","comment":"accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17552v1","updated":"2024-03-26T09:59:45Z","published":"2024-03-26T09:59:45Z","title":"Naive Bayes-based Context Extension for Large Language Models","summary":"  Large Language Models (LLMs) have shown promising in-context learning\nabilities. However, conventional In-Context Learning (ICL) approaches are often\nimpeded by length limitations of transformer architecture, which pose\nchallenges when attempting to effectively integrate supervision from a\nsubstantial number of demonstration examples. In this paper, we introduce a\nnovel framework, called Naive Bayes-based Context Extension (NBCE), to enable\nexisting LLMs to perform ICL with an increased number of demonstrations by\nsignificantly expanding their context size. Importantly, this expansion does\nnot require fine-tuning or dependence on particular model architectures, all\nthe while preserving linear efficiency. NBCE initially splits the context into\nequal-sized windows fitting the target LLM's maximum length. Then, it\nintroduces a voting mechanism to select the most relevant window, regarded as\nthe posterior context. Finally, it employs Bayes' theorem to generate the test\ntask. Our experimental results demonstrate that NBCE substantially enhances\nperformance, particularly as the number of demonstration examples increases,\nconsistently outperforming alternative methods. The NBCE code will be made\npublicly accessible. The code NBCE is available at:\nhttps://github.com/amurtadha/NBCE-master\n","authors":["Jianlin Su","Murtadha Ahmed"," Wenbo","Luo Ao","Mingren Zhu","Yunfeng Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17552v1.pdf","comment":"Accepted to main NAACL 2024"},{"id":"http://arxiv.org/abs/2403.17546v1","updated":"2024-03-26T09:51:43Z","published":"2024-03-26T09:51:43Z","title":"Decoding excellence: Mapping the demand for psychological traits of\n  operations and supply chain professionals through text mining","summary":"  The current study proposes an innovative methodology for the profiling of\npsychological traits of Operations Management (OM) and Supply Chain Management\n(SCM) professionals. We use innovative methods and tools of text mining and\nsocial network analysis to map the demand for relevant skills from a set of job\ndescriptions, with a focus on psychological characteristics. The proposed\napproach aims to evaluate the market demand for specific traits by combining\nrelevant psychological constructs, text mining techniques, and an innovative\nmeasure, namely, the Semantic Brand Score. We apply the proposed methodology to\na dataset of job descriptions for OM and SCM professionals, with the objective\nof providing a mapping of their relevant required skills, including\npsychological characteristics. In addition, the analysis is then detailed by\nconsidering the region of the organization that issues the job description, its\norganizational size, and the seniority level of the open position in order to\nunderstand their nuances. Finally, topic modeling is used to examine key\ncomponents and their relative significance in job descriptions. By employing a\nnovel methodology and considering contextual factors, we provide an innovative\nunderstanding of the attitudinal traits that differentiate professionals. This\nresearch contributes to talent management, recruitment practices, and\nprofessional development initiatives, since it provides new figures and\nperspectives to improve the effectiveness and success of Operations Management\nand Supply Chain Management professionals.\n","authors":["S. Di Luozzo","A. Fronzetti Colladon","M. M. Schiraldi"],"pdf_url":"https://arxiv.org/pdf/2403.17546v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17545v1","updated":"2024-03-26T09:49:35Z","published":"2024-03-26T09:49:35Z","title":"A Gaze-grounded Visual Question Answering Dataset for Clarifying\n  Ambiguous Japanese Questions","summary":"  Situated conversations, which refer to visual information as visual question\nanswering (VQA), often contain ambiguities caused by reliance on directive\ninformation. This problem is exacerbated because some languages, such as\nJapanese, often omit subjective or objective terms. Such ambiguities in\nquestions are often clarified by the contexts in conversational situations,\nsuch as joint attention with a user or user gaze information. In this study, we\npropose the Gaze-grounded VQA dataset (GazeVQA) that clarifies ambiguous\nquestions using gaze information by focusing on a clarification process\ncomplemented by gaze information. We also propose a method that utilizes gaze\ntarget estimation results to improve the accuracy of GazeVQA tasks. Our\nexperimental results showed that the proposed method improved the performance\nin some cases of a VQA system on GazeVQA and identified some typical problems\nof GazeVQA tasks that need to be improved.\n","authors":["Shun Inadumi","Seiya Kawano","Akishige Yuguchi","Yasutomo Kawanishi","Koichiro Yoshino"],"pdf_url":"https://arxiv.org/pdf/2403.17545v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17540v1","updated":"2024-03-26T09:43:15Z","published":"2024-03-26T09:43:15Z","title":"Large Language Models Are State-of-the-Art Evaluator for Grammatical\n  Error Correction","summary":"  Large Language Models (LLMs) have been reported to outperform existing\nautomatic evaluation metrics in some tasks, such as text summarization and\nmachine translation. However, there has been a lack of research on LLMs as\nevaluators in grammatical error correction (GEC). In this study, we investigate\nthe performance of LLMs in GEC evaluation by employing prompts designed to\nincorporate various evaluation criteria inspired by previous research. Our\nextensive experimental results demonstrate that GPT-4 achieved Kendall's rank\ncorrelation of 0.662 with human judgments, surpassing all existing methods.\nFurthermore, in recent GEC evaluations, we have underscored the significance of\nthe LLMs scale and particularly emphasized the importance of fluency among\nevaluation criteria.\n","authors":["Masamune Kobayashi","Masato Mita","Mamoru Komachi"],"pdf_url":"https://arxiv.org/pdf/2403.17540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17536v1","updated":"2024-03-26T09:41:21Z","published":"2024-03-26T09:41:21Z","title":"ILLUMINER: Instruction-tuned Large Language Models as Few-shot Intent\n  Classifier and Slot Filler","summary":"  State-of-the-art intent classification (IC) and slot filling (SF) methods\noften rely on data-intensive deep learning models, limiting their practicality\nfor industry applications. Large language models on the other hand,\nparticularly instruction-tuned models (Instruct-LLMs), exhibit remarkable\nzero-shot performance across various natural language tasks. This study\nevaluates Instruct-LLMs on popular benchmark datasets for IC and SF,\nemphasizing their capacity to learn from fewer examples. We introduce\nILLUMINER, an approach framing IC and SF as language generation tasks for\nInstruct-LLMs, with a more efficient SF-prompting method compared to prior\nwork. A comprehensive comparison with multiple baselines shows that our\napproach, using the FLAN-T5 11B model, outperforms the state-of-the-art joint\nIC+SF method and in-context learning with GPT3.5 (175B), particularly in slot\nfilling by 11.1--32.2 percentage points. Additionally, our in-depth ablation\nstudy demonstrates that parameter-efficient fine-tuning requires less than 6%\nof training data to yield comparable performance with traditional full-weight\nfine-tuning.\n","authors":["Paramita Mirza","Viju Sudhi","Soumya Ranjan Sahoo","Sinchana Ramakanth Bhat"],"pdf_url":"https://arxiv.org/pdf/2403.17536v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17534v1","updated":"2024-03-26T09:39:53Z","published":"2024-03-26T09:39:53Z","title":"Sparse Logistic Regression with High-order Features for Automatic\n  Grammar Rule Extraction from Treebanks","summary":"  Descriptive grammars are highly valuable, but writing them is time-consuming\nand difficult. Furthermore, while linguists typically use corpora to create\nthem, grammar descriptions often lack quantitative data. As for formal\ngrammars, they can be challenging to interpret. In this paper, we propose a new\nmethod to extract and explore significant fine-grained grammar patterns and\npotential syntactic grammar rules from treebanks, in order to create an\neasy-to-understand corpus-based grammar. More specifically, we extract\ndescriptions and rules across different languages for two linguistic phenomena,\nagreement and word order, using a large search space and paying special\nattention to the ranking order of the extracted rules. For that, we use a\nlinear classifier to extract the most salient features that predict the\nlinguistic phenomena under study. We associate statistical information to each\nrule, and we compare the ranking of the model's results to those of other\nquantitative and statistical measures. Our method captures both well-known and\nless well-known significant grammar rules in Spanish, French, and Wolof.\n","authors":["Santiago Herrera","Caio Corro","Sylvain Kahane"],"pdf_url":"https://arxiv.org/pdf/2403.17534v1.pdf","comment":"Published in LREC-Coling 2024 proceedings"},{"id":"http://arxiv.org/abs/2403.17528v1","updated":"2024-03-26T09:31:55Z","published":"2024-03-26T09:31:55Z","title":"Multilingual Sentence-T5: Scalable Sentence Encoders for Multilingual\n  Applications","summary":"  Prior work on multilingual sentence embedding has demonstrated that the\nefficient use of natural language inference (NLI) data to build\nhigh-performance models can outperform conventional methods. However, the\npotential benefits from the recent ``exponential'' growth of language models\nwith billions of parameters have not yet been fully explored. In this paper, we\nintroduce Multilingual Sentence T5 (m-ST5), as a larger model of NLI-based\nmultilingual sentence embedding, by extending Sentence T5, an existing\nmonolingual model. By employing the low-rank adaptation (LoRA) technique, we\nhave achieved a successful scaling of the model's size to 5.7 billion\nparameters. We conducted experiments to evaluate the performance of sentence\nembedding and verified that the method outperforms the NLI-based prior\napproach. Furthermore, we also have confirmed a positive correlation between\nthe size of the model and its performance. It was particularly noteworthy that\nlanguages with fewer resources or those with less linguistic similarity to\nEnglish benefited more from the parameter increase. Our model is available at\nhttps://huggingface.co/pkshatech/m-ST5.\n","authors":["Chihiro Yano","Akihiko Fukuchi","Shoko Fukasawa","Hideyuki Tachibana","Yotaro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2403.17528v1.pdf","comment":"Accepted in LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.08281v4","updated":"2024-03-26T09:29:51Z","published":"2024-03-13T06:18:48Z","title":"Mastering Text, Code and Math Simultaneously via Fusing Highly\n  Specialized Language Models","summary":"  Underlying data distributions of natural language, programming code, and\nmathematical symbols vary vastly, presenting a complex challenge for large\nlanguage models (LLMs) that strive to achieve high performance across all three\ndomains simultaneously. Achieving a very high level of proficiency for an LLM\nwithin a specific domain often requires extensive training with relevant\ncorpora, which is typically accompanied by a sacrifice in performance in other\ndomains. In this paper, we propose to fuse models that are already\nhighly-specialized directly. The proposed fusing framework, UltraFuser,\nconsists of three distinct specialists that are already sufficiently trained on\nlanguage, coding, and mathematics. A token-level gating mechanism is introduced\nto blend the specialists' outputs. A two-stage training strategy accompanied by\nbalanced sampling is designed to ensure stability. To effectively train the\nfused model, we further construct a high-quality supervised instruction tuning\ndataset, UltraChat 2, which includes text, code, and mathematical content. This\ndataset comprises approximately 300,000 instructions and covers a wide range of\ntopics in each domain. Experiments show that our model could simultaneously\nachieve mastery of the three crucial domains.\n","authors":["Ning Ding","Yulin Chen","Ganqu Cui","Xingtai Lv","Weilin Zhao","Ruobing Xie","Bowen Zhou","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2403.08281v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17524v1","updated":"2024-03-26T09:25:57Z","published":"2024-03-26T09:25:57Z","title":"Provably Secure Disambiguating Neural Linguistic Steganography","summary":"  Recent research in provably secure neural linguistic steganography has\noverlooked a crucial aspect: the sender must detokenize stegotexts to avoid\nraising suspicion from the eavesdropper. The segmentation ambiguity problem,\nwhich arises when using language models based on subwords, leads to occasional\ndecoding failures in all neural language steganography implementations based on\nthese models. Current solutions to this issue involve altering the probability\ndistribution of candidate words, rendering them incompatible with provably\nsecure steganography. We propose a novel secure disambiguation method named\nSyncPool, which effectively addresses the segmentation ambiguity problem. We\ngroup all tokens with prefix relationships in the candidate pool before the\nsteganographic embedding algorithm runs to eliminate uncertainty among\nambiguous tokens. To enable the receiver to synchronize the sampling process of\nthe sender, a shared cryptographically-secure pseudorandom number generator\n(CSPRNG) is deployed to select a token from the ambiguity pool. SyncPool does\nnot change the size of the candidate pool or the distribution of tokens and\nthus is applicable to provably secure language steganography methods. We\nprovide theoretical proofs and experimentally demonstrate the applicability of\nour solution to various languages and models, showing its potential to\nsignificantly improve the reliability and security of neural linguistic\nsteganography systems.\n","authors":["Yuang Qi","Kejiang Chen","Kai Zeng","Weiming Zhang","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2403.17524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17516v1","updated":"2024-03-26T09:18:59Z","published":"2024-03-26T09:18:59Z","title":"MapGuide: A Simple yet Effective Method to Reconstruct Continuous\n  Language from Brain Activities","summary":"  Decoding continuous language from brain activity is a formidable yet\npromising field of research. It is particularly significant for aiding people\nwith speech disabilities to communicate through brain signals. This field\naddresses the complex task of mapping brain signals to text. The previous best\nattempt reverse-engineered this process in an indirect way: it began by\nlearning to encode brain activity from text and then guided text generation by\naligning with predicted brain responses. In contrast, we propose a simple yet\neffective method that guides text reconstruction by directly comparing them\nwith the predicted text embeddings mapped from brain activities. Comprehensive\nexperiments reveal that our method significantly outperforms the current\nstate-of-the-art model, showing average improvements of 77% and 54% on BLEU and\nMETEOR scores. We further validate the proposed modules through detailed\nablation studies and case analyses and highlight a critical correlation: the\nmore precisely we map brain activities to text embeddings, the better the text\nreconstruction results. Such insight can simplify the task of reconstructing\nlanguage from brain activities for future work, emphasizing the importance of\nimproving brain-to-text-embedding mapping techniques.\n","authors":["Xinpei Zhao","Jingyuan Sun","Shaonan Wang","Jing Ye","Xiaohan Zhang","Chengqing Zong"],"pdf_url":"https://arxiv.org/pdf/2403.17516v1.pdf","comment":"Accepted to NAACL 2024 main conference"},{"id":"http://arxiv.org/abs/2305.16031v2","updated":"2024-03-26T09:16:36Z","published":"2023-05-25T13:08:10Z","title":"Efficient Document Embeddings via Self-Contrastive Bregman Divergence\n  Learning","summary":"  Learning quality document embeddings is a fundamental problem in natural\nlanguage processing (NLP), information retrieval (IR), recommendation systems,\nand search engines. Despite recent advances in the development of\ntransformer-based models that produce sentence embeddings with self-contrastive\nlearning, the encoding of long documents (Ks of words) is still challenging\nwith respect to both efficiency and quality considerations. Therefore, we train\nLongfomer-based document encoders using a state-of-the-art unsupervised\ncontrastive learning method (SimCSE). Further on, we complement the baseline\nmethod -- siamese neural network -- with additional convex neural networks\nbased on functional Bregman divergence aiming to enhance the quality of the\noutput document representations. We show that overall the combination of a\nself-contrastive siamese network and our proposed neural Bregman network\noutperforms the baselines in two linear classification settings on three long\ndocument topic classification tasks from the legal and biomedical domains.\n","authors":["Daniel Saggau","Mina Rezaei","Bernd Bischl","Ilias Chalkidis"],"pdf_url":"https://arxiv.org/pdf/2305.16031v2.pdf","comment":"5 pages, short paper at Findings of ACL 2023"},{"id":"http://arxiv.org/abs/2403.17497v1","updated":"2024-03-26T08:58:28Z","published":"2024-03-26T08:58:28Z","title":"Sharing the Cost of Success: A Game for Evaluating and Learning\n  Collaborative Multi-Agent Instruction Giving and Following Policies","summary":"  In collaborative goal-oriented settings, the participants are not only\ninterested in achieving a successful outcome, but do also implicitly negotiate\nthe effort they put into the interaction (by adapting to each other). In this\nwork, we propose a challenging interactive reference game that requires two\nplayers to coordinate on vision and language observations. The learning signal\nin this game is a score (given after playing) that takes into account the\nachieved goal and the players' assumed efforts during the interaction. We show\nthat a standard Proximal Policy Optimization (PPO) setup achieves a high\nsuccess rate when bootstrapped with heuristic partner behaviors that implement\ninsights from the analysis of human-human interactions. And we find that a\npairing of neural partners indeed reduces the measured joint effort when\nplaying together repeatedly. However, we observe that in comparison to a\nreasonable heuristic pairing there is still room for improvement -- which\ninvites further research in the direction of cost-sharing in collaborative\ninteractions.\n","authors":["Philipp Sadler","Sherzod Hakimov","David Schlangen"],"pdf_url":"https://arxiv.org/pdf/2403.17497v1.pdf","comment":"9 pages, Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17491v1","updated":"2024-03-26T08:47:23Z","published":"2024-03-26T08:47:23Z","title":"DGoT: Dynamic Graph of Thoughts for Scientific Abstract Generation","summary":"  The method of training language models based on domain datasets has obtained\nsignificant achievements in the task of generating scientific paper abstracts.\nHowever, such models face problems of generalization and expensive training\ncosts. The use of large language models (LLMs) to solve the task of generating\npaper abstracts saves the cost of model training. However, due to the\nhallucination problem of LLM, it is often necessary to improve the reliability\nof the results through multi-round query prompt approach such as Graph of\nThoughts (GoT), which also brings additional reasoning costs. In this paper, we\npropose a Dynamic Graph of Thought (DGoT). It not only inherits the advantages\nof the existing GoT prompt approach, but also dynamically adjust the graph\nstructure according to data characteristics while reducing model reasoning\ncost. Experimental results show that our method's cost-effectiveness in\nabstract generation tasks is only 43.7% to 56.4% of other multi-round query\nprompt approaches. Our code is available at https://github.com/JayceNing/DGoT.\n","authors":["Xinyu Ning","Yutong Zhao","Yitong Liu","Hongwen Yang"],"pdf_url":"https://arxiv.org/pdf/2403.17491v1.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2309.14974v3","updated":"2024-03-26T08:46:07Z","published":"2023-09-25T09:21:25Z","title":"Detecting Sexual Content at the Sentence Level in First Millennium Latin\n  Texts","summary":"  In this study, we propose to evaluate the use of deep learning methods for\nsemantic classification at the sentence level to accelerate the process of\ncorpus building in the field of humanities and linguistics, a traditional and\ntime-consuming task. We introduce a novel corpus comprising around 2500\nsentences spanning from 300 BCE to 900 CE including sexual semantics (medical,\nerotica, etc.). We evaluate various sentence classification approaches and\ndifferent input embedding layers, and show that all consistently outperform\nsimple token-based searches. We explore the integration of idiolectal and\nsociolectal metadata embeddings (centuries, author, type of writing), but find\nthat it leads to overfitting. Our results demonstrate the effectiveness of this\napproach, achieving high precision and true positive rates (TPR) of\nrespectively 70.60% and 86.33% using HAN. We evaluate the impact of the dataset\nsize on the model performances (420 instead of 2013), and show that, while our\nmodels perform worse, they still offer a high enough precision and TPR, even\nwithout MLM, respectively 69% and 51%. Given the result, we provide an analysis\nof the attention mechanism as a supporting added value for humanists in order\nto produce more data.\n","authors":["Thibault Clérice"],"pdf_url":"https://arxiv.org/pdf/2309.14974v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17486v1","updated":"2024-03-26T08:32:39Z","published":"2024-03-26T08:32:39Z","title":"KDMCSE: Knowledge Distillation Multimodal Sentence Embeddings with\n  Adaptive Angular margin Contrastive Learning","summary":"  Previous work on multimodal sentence embedding has proposed multimodal\ncontrastive learning and achieved promising results. However, by taking the\nrest of the batch as negative samples without reviewing when forming\ncontrastive pairs, those studies encountered many suspicious and noisy negative\nexamples, significantly affecting the methods' overall performance. In this\nwork, we propose KDMCSE (Knowledge Distillation Multimodal contrastive learning\nof Sentence Embeddings), a novel approach that enhances the discrimination and\ngeneralizability of multimodal representation and inherits the knowledge from\nthe teacher model to learn the difference between positive and negative\ninstances and via that, can detect noisy and wrong negative samples effectively\nbefore they are calculated in the contrastive objective. Furthermore, to\novercome the limitation of modeling the variation within negative pairs, we\nintroduce a new contrastive objective, AdapACSE (Adaptive Angular Margin\nSupervised Contrastive Learning for Multimodal sentence embeddings), that\nenhances the discriminative representation by strengthening the margin within\nthe angular space while capturing varying semantics within the negative.\nExperimental results on widely used Semantic Textual Similarity (STS)\nbenchmarks demonstrate the effectiveness of our approach.\n","authors":["Cong-Duy Nguyen","Thong Nguyen","Xiaobao Wu","Anh Tuan Luu"],"pdf_url":"https://arxiv.org/pdf/2403.17486v1.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2309.11888v2","updated":"2024-03-26T08:04:36Z","published":"2023-09-21T08:45:41Z","title":"High-order Joint Constituency and Dependency Parsing","summary":"  This work revisits the topic of jointly parsing constituency and dependency\ntrees, i.e., to produce compatible constituency and dependency trees\nsimultaneously for input sentences, which is attractive considering that the\ntwo types of trees are complementary in representing syntax. The original work\nof Zhou and Zhao (2019) performs joint parsing only at the inference phase.\nThey train two separate parsers under the multi-task learning framework (i.e.,\none shared encoder and two independent decoders). They design an ad-hoc dynamic\nprogramming-based decoding algorithm of $O(n^5)$ time complexity for finding\noptimal compatible tree pairs. Compared to their work, we make progress in\nthree aspects: (1) adopting a much more efficient decoding algorithm of\n$O(n^4)$ time complexity, (2) exploring joint modeling at the training phase,\ninstead of only at the inference phase, (3) proposing high-order scoring\ncomponents to promote constituent-dependency interaction. We conduct\nexperiments and analysis on seven languages, covering both rich-resource and\nlow-resource scenarios. Results and analysis show that joint modeling leads to\na modest overall performance boost over separate modeling, but substantially\nimproves the complete matching ratio of whole trees, thanks to the explicit\nmodeling of tree compatibility.\n","authors":["Yanggan Gu","Yang Hou","Zhefeng Wang","Xinyu Duan","Zhenghua Li"],"pdf_url":"https://arxiv.org/pdf/2309.11888v2.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17445v1","updated":"2024-03-26T07:23:46Z","published":"2024-03-26T07:23:46Z","title":"Incorporating Exponential Smoothing into MLP: A Simple but Effective\n  Sequence Model","summary":"  Modeling long-range dependencies in sequential data is a crucial step in\nsequence learning. A recently developed model, the Structured State Space (S4),\ndemonstrated significant effectiveness in modeling long-range sequences.\nHowever, It is unclear whether the success of S4 can be attributed to its\nintricate parameterization and HiPPO initialization or simply due to State\nSpace Models (SSMs). To further investigate the potential of the deep SSMs, we\nstart with exponential smoothing (ETS), a simple SSM, and propose a stacked\narchitecture by directly incorporating it into an element-wise MLP. We augment\nsimple ETS with additional parameters and complex field to reduce the inductive\nbias. Despite increasing less than 1\\% of parameters of element-wise MLP, our\nmodels achieve comparable results to S4 on the LRA benchmark.\n","authors":["Jiqun Chu","Zuoquan Lin"],"pdf_url":"https://arxiv.org/pdf/2403.17445v1.pdf","comment":"12 pages, 5 tables, 3 figures"},{"id":"http://arxiv.org/abs/2403.16662v2","updated":"2024-03-26T07:13:15Z","published":"2024-03-25T11:56:29Z","title":"RU22Fact: Optimizing Evidence for Multilingual Explainable Fact-Checking\n  on Russia-Ukraine Conflict","summary":"  Fact-checking is the task of verifying the factuality of a given claim by\nexamining the available evidence. High-quality evidence plays a vital role in\nenhancing fact-checking systems and facilitating the generation of explanations\nthat are understandable to humans. However, the provision of both sufficient\nand relevant evidence for explainable fact-checking systems poses a challenge.\nTo tackle this challenge, we propose a method based on a Large Language Model\nto automatically retrieve and summarize evidence from the Web. Furthermore, we\nconstruct RU22Fact, a novel multilingual explainable fact-checking dataset on\nthe Russia-Ukraine conflict in 2022 of 16K samples, each containing real-world\nclaims, optimized evidence, and referenced explanation. To establish a baseline\nfor our dataset, we also develop an end-to-end explainable fact-checking system\nto verify claims and generate explanations. Experimental results demonstrate\nthe prospect of optimized evidence in increasing fact-checking performance and\nalso indicate the possibility of further progress in the end-to-end claim\nverification and explanation generation tasks.\n","authors":["Yirong Zeng","Xiao Ding","Yi Zhao","Xiangyu Li","Jie Zhang","Chao Yao","Ting Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2403.16662v2.pdf","comment":"12 pages, 3 figures, accepted by lrec-coling2024"},{"id":"http://arxiv.org/abs/2403.14633v2","updated":"2024-03-26T07:12:40Z","published":"2024-02-16T23:18:19Z","title":"Born With a Silver Spoon? Investigating Socioeconomic Bias in Large\n  Language Models","summary":"  Socioeconomic bias in society exacerbates disparities, influencing access to\nopportunities and resources based on individuals' economic and social\nbackgrounds. This pervasive issue perpetuates systemic inequalities, hindering\nthe pursuit of inclusive progress as a society. In this paper, we investigate\nthe presence of socioeconomic bias, if any, in large language models. To this\nend, we introduce a novel dataset SilverSpoon, consisting of 3000 samples that\nillustrate hypothetical scenarios that involve underprivileged people\nperforming ethically ambiguous actions due to their circumstances, and ask\nwhether the action is ethically justified. Further, this dataset has a\ndual-labeling scheme and has been annotated by people belonging to both ends of\nthe socioeconomic spectrum. Using SilverSpoon, we evaluate the degree of\nsocioeconomic bias expressed in large language models and the variation of this\ndegree as a function of model size. We also perform qualitative analysis to\nanalyze the nature of this bias. Our analysis reveals that while humans\ndisagree on which situations require empathy toward the underprivileged, most\nlarge language models are unable to empathize with the socioeconomically\nunderprivileged regardless of the situation. To foster further research in this\ndomain, we make SilverSpoon and our evaluation harness publicly available.\n","authors":["Smriti Singh","Shuvam Keshari","Vinija Jain","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2403.14633v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17431v1","updated":"2024-03-26T06:57:23Z","published":"2024-03-26T06:57:23Z","title":"Robust and Scalable Model Editing for Large Language Models","summary":"  Large language models (LLMs) can make predictions using parametric\nknowledge--knowledge encoded in the model weights--or contextual\nknowledge--knowledge presented in the context. In many scenarios, a desirable\nbehavior is that LLMs give precedence to contextual knowledge when it conflicts\nwith the parametric knowledge, and fall back to using their parametric\nknowledge when the context is irrelevant. This enables updating and correcting\nthe model's knowledge by in-context editing instead of retraining. Previous\nworks have shown that LLMs are inclined to ignore contextual knowledge and fail\nto reliably fall back to parametric knowledge when presented with irrelevant\ncontext. In this work, we discover that, with proper prompting methods,\ninstruction-finetuned LLMs can be highly controllable by contextual knowledge\nand robust to irrelevant context. Utilizing this feature, we propose EREN (Edit\nmodels by REading Notes) to improve the scalability and robustness of LLM\nediting. To better evaluate the robustness of model editors, we collect a new\ndataset, that contains irrelevant questions that are more challenging than the\nones in existing datasets. Empirical results show that our method outperforms\ncurrent state-of-the-art methods by a large margin. Unlike existing techniques,\nit can integrate knowledge from multiple edits, and correctly respond to\nsyntactically similar but semantically unrelated inputs (and vice versa). The\nsource code can be found at https://github.com/thunlp/EREN.\n","authors":["Yingfa Chen","Zhengyan Zhang","Xu Han","Chaojun Xiao","Zhiyuan Liu","Chen Chen","Kuai Li","Tao Yang","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2403.17431v1.pdf","comment":"LREC-COLING 2024 paper, 16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2207.01262v2","updated":"2024-03-26T06:54:43Z","published":"2022-07-04T08:54:43Z","title":"Understanding Performance of Long-Document Ranking Models through\n  Comprehensive Evaluation and Leaderboarding","summary":"  We evaluated 20+ Transformer models for ranking of long documents (including\nrecent LongP models trained with FlashAttention) and compared them with simple\nFirstP baselines (applying the same model to input truncated to the first 512\ntokens). We used MS MARCO Documents v1 as a primary training set and evaluated\nmodels in the zero-shot scenario as well as after fine-tuning on other\ncollections.\n  In our initial experiments with standard collections we found that\nlong-document models underperformed FirstP or outperformed it by at most 5% on\naverage in terms of MRR or NDCG. We then conjectured that this was not due to\nmodels inability to process long context but rather due to a positional bias of\nrelevant passages, which tended to be among the first 512 document tokens. We\nfound evidence that this bias was, indeed, present in at least two test sets,\nwhich motivated us to create a new collection MS MARCO FarRelevant where the\nrelevant passages were not present among the first 512 tokens.\n  Unlike standard collections where we observed both little benefit from\nincorporating longer contexts and limited variability in model performance\n(within a few %), experiments on MS MARCO FarRelevant uncovered dramatic\ndifferences among models. FirstP models performed roughly at the\nrandom-baseline level in both zero-shot and fine-tuning scenarios. Simple\naggregation models (e.g., MaxP) had good zero-shot accuracy but benefited\nlittle from fine-tuning. Most other models had poor zero-shot performance\n(sometimes at a random baseline level) but outstripped MaxP by as much 13-28\\%\nafter finetuning. Thus, positional bias not only diminishes benefits of\nprocessing longer document contexts but also leads to model overfitting to this\nbias and performing poorly in a zero-shot setting when a distribution of\nrelevant passages changes substantially.\n  We make our software and MS MARCO FarRelevant available.\n","authors":["Leonid Boytsov","David Akinpelu","Tianyi Lin","Fangwei Gao","Yutian Zhao","Jeffrey Huang","Eric Nyberg"],"pdf_url":"https://arxiv.org/pdf/2207.01262v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17428v1","updated":"2024-03-26T06:50:04Z","published":"2024-03-26T06:50:04Z","title":"Aligning Large Language Models for Enhancing Psychiatric Interviews\n  through Symptom Delineation and Summarization","summary":"  Recent advancements in Large Language Models (LLMs) have accelerated their\nusage in various domains. Given the fact that psychiatric interviews are\ngoal-oriented and structured dialogues between the professional interviewer and\nthe interviewee, it is one of the most underexplored areas where LLMs can\ncontribute substantial value. Here, we explore the use of LLMs for enhancing\npsychiatric interviews, by analyzing counseling data from North Korean\ndefectors with traumatic events and mental health issues. Specifically, we\ninvestigate whether LLMs can (1) delineate the part of the conversation that\nsuggests psychiatric symptoms and name the symptoms, and (2) summarize\nstressors and symptoms, based on the interview dialogue transcript. Here, the\ntranscript data was labeled by mental health experts for training and\nevaluation of LLMs. Our experimental results show that appropriately prompted\nLLMs can achieve high performance on both the symptom delineation task and the\nsummarization task. This research contributes to the nascent field of applying\nLLMs to psychiatric interview and demonstrates their potential effectiveness in\naiding mental health practitioners.\n","authors":["Jae-hee So","Joonhwan Chang","Eunji Kim","Junho Na","JiYeon Choi","Jy-yong Sohn","Byung-Hoon Kim","Sang Hui Chu"],"pdf_url":"https://arxiv.org/pdf/2403.17428v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17413v1","updated":"2024-03-26T06:12:21Z","published":"2024-03-26T06:12:21Z","title":"LM-Combiner: A Contextual Rewriting Model for Chinese Grammatical Error\n  Correction","summary":"  Over-correction is a critical problem in Chinese grammatical error correction\n(CGEC) task. Recent work using model ensemble methods based on voting can\neffectively mitigate over-correction and improve the precision of the GEC\nsystem. However, these methods still require the output of several GEC systems\nand inevitably lead to reduced error recall. In this light, we propose the\nLM-Combiner, a rewriting model that can directly modify the over-correction of\nGEC system outputs without a model ensemble. Specifically, we train the model\non an over-correction dataset constructed through the proposed K-fold cross\ninference method, which allows it to directly generate filtered sentences by\ncombining the original and the over-corrected text. In the inference stage, we\ndirectly take the original sentences and the output results of other systems as\ninput and then obtain the filtered sentences through LM-Combiner. Experiments\non the FCGEC dataset show that our proposed method effectively alleviates the\nover-correction of the original system (+18.2 Precision) while ensuring the\nerror recall remains unchanged. Besides, we find that LM-Combiner still has a\ngood rewriting performance even with small parameters and few training data,\nand thus can cost-effectively mitigate the over-correction of black-box GEC\nsystems (e.g., ChatGPT).\n","authors":["Yixuan Wang","Baoxin Wang","Yijun Liu","Dayong Wu","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2403.17413v1.pdf","comment":"Accepted to COLING 2024"},{"id":"http://arxiv.org/abs/2403.17411v1","updated":"2024-03-26T06:11:07Z","published":"2024-03-26T06:11:07Z","title":"PCToolkit: A Unified Plug-and-Play Prompt Compression Toolkit of Large\n  Language Models","summary":"  Prompt compression is an innovative method for efficiently condensing input\nprompts while preserving essential information. To facilitate quick-start\nservices, user-friendly interfaces, and compatibility with common datasets and\nmetrics, we present the Prompt Compression Toolkit (PCToolkit). This toolkit is\na unified plug-and-play solution for compressing prompts in Large Language\nModels (LLMs), featuring cutting-edge prompt compressors, diverse datasets, and\nmetrics for comprehensive performance evaluation. PCToolkit boasts a modular\ndesign, allowing for easy integration of new datasets and metrics through\nportable and user-friendly interfaces. In this paper, we outline the key\ncomponents and functionalities of PCToolkit. We conducted evaluations of the\ncompressors within PCToolkit across various natural language tasks, including\nreconstruction, summarization, mathematical problem-solving, question\nanswering, few-shot learning, synthetic tasks, code completion, boolean\nexpressions, multiple choice questions, and lies recognition.\n","authors":["Jinyi Li","Yihuai Lan","Lei Wang","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2403.17411v1.pdf","comment":"For open-source repository, see\n  https://github.com/3DAgentWorld/Toolkit-for-Prompt-Compression"},{"id":"http://arxiv.org/abs/2403.17407v1","updated":"2024-03-26T05:55:21Z","published":"2024-03-26T05:55:21Z","title":"Transcribing Bengali Text with Regional Dialects to IPA using District\n  Guided Tokens","summary":"  Accurate transcription of Bengali text to the International Phonetic Alphabet\n(IPA) is a challenging task due to the complex phonology of the language and\ncontext-dependent sound changes. This challenge is even more for regional\nBengali dialects due to unavailability of standardized spelling conventions for\nthese dialects, presence of local and foreign words popular in those regions\nand phonological diversity across different regions. This paper presents an\napproach to this sequence-to-sequence problem by introducing the District\nGuided Tokens (DGT) technique on a new dataset spanning six districts of\nBangladesh. The key idea is to provide the model with explicit information\nabout the regional dialect or \"district\" of the input text before generating\nthe IPA transcription. This is achieved by prepending a district token to the\ninput sequence, effectively guiding the model to understand the unique phonetic\npatterns associated with each district. The DGT technique is applied to\nfine-tune several transformer-based models, on this new dataset. Experimental\nresults demonstrate the effectiveness of DGT, with the ByT5 model achieving\nsuperior performance over word-based models like mT5, BanglaT5, and umT5. This\nis attributed to ByT5's ability to handle a high percentage of\nout-of-vocabulary words in the test set. The proposed approach highlights the\nimportance of incorporating regional dialect information into ubiquitous\nnatural language processing systems for languages with diverse phonological\nvariations. The following work was a result of the \"Bhashamul\" challenge, which\nis dedicated to solving the problem of Bengali text with regional dialects to\nIPA transcription https://www.kaggle.com/competitions/regipa/. The training and\ninference notebooks are available through the competition link.\n","authors":["S M Jishanul Islam","Sadia Ahmmed","Sahid Hossain Mustakim"],"pdf_url":"https://arxiv.org/pdf/2403.17407v1.pdf","comment":"This work became the champion of the Bhashamul challenge"},{"id":"http://arxiv.org/abs/2403.17385v1","updated":"2024-03-26T05:11:51Z","published":"2024-03-26T05:11:51Z","title":"ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity\n  Recognition","summary":"  In this work, we revisit the problem of semi-supervised named entity\nrecognition (NER) focusing on extremely light supervision, consisting of a\nlexicon containing only 10 examples per class. We introduce ELLEN, a simple,\nfully modular, neuro-symbolic method that blends fine-tuned language models\nwith linguistic rules. These rules include insights such as ''One Sense Per\nDiscourse'', using a Masked Language Model as an unsupervised NER, leveraging\npart-of-speech tags to identify and eliminate unlabeled entities as false\nnegatives, and other intuitions about classifier confidence scores in local and\nglobal context. ELLEN achieves very strong performance on the CoNLL-2003\ndataset when using the minimal supervision from the lexicon above. It also\noutperforms most existing (and considerably more complex) semi-supervised NER\nmethods under the same supervision settings commonly used in the literature\n(i.e., 5% of the training data). Further, we evaluate our CoNLL-2003 model in a\nzero-shot scenario on WNUT-17 where we find that it outperforms GPT-3.5 and\nachieves comparable performance to GPT-4. In a zero-shot setting, ELLEN also\nachieves over 75% of the performance of a strong, fully supervised model\ntrained on gold data. Our code is available at:\nhttps://github.com/hriaz17/ELLEN.\n","authors":["Haris Riaz","Razvan-Gabriel Dumitru","Mihai Surdeanu"],"pdf_url":"https://arxiv.org/pdf/2403.17385v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2112.06166v2","updated":"2024-03-26T04:26:18Z","published":"2021-12-12T06:25:15Z","title":"Topic Detection and Tracking with Time-Aware Document Embeddings","summary":"  The time at which a message is communicated is a vital piece of metadata in\nmany real-world natural language processing tasks such as Topic Detection and\nTracking (TDT). TDT systems aim to cluster a corpus of news articles by event,\nand in that context, stories that describe the same event are likely to have\nbeen written at around the same time. Prior work on time modeling for TDT takes\nthis into account, but does not well capture how time interacts with the\nsemantic nature of the event. For example, stories about a tropical storm are\nlikely to be written within a short time interval, while stories about a movie\nrelease may appear over weeks or months. In our work, we design a neural method\nthat fuses temporal and textual information into a single representation of\nnews documents for event detection. We fine-tune these time-aware document\nembeddings with a triplet loss architecture, integrate the model into\ndownstream TDT systems, and evaluate the systems on two benchmark TDT data sets\nin English. In the retrospective setting, we apply clustering algorithms to the\ntime-aware embeddings and show substantial improvements over baselines on the\nNews2013 data set. In the online streaming setting, we add our document encoder\nto an existing state-of-the-art TDT pipeline and demonstrate that it can\nbenefit the overall performance. We conduct ablation studies on the time\nrepresentation and fusion algorithm strategies, showing that our proposed model\noutperforms alternative strategies. Finally, we probe the model to examine how\nit handles recurring events more effectively than previous TDT systems.\n","authors":["Hang Jiang","Doug Beeferman","Weiquan Mao","Deb Roy"],"pdf_url":"https://arxiv.org/pdf/2112.06166v2.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2308.06463v2","updated":"2024-03-26T04:23:12Z","published":"2023-08-12T04:05:57Z","title":"GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher","summary":"  Safety lies at the core of the development of Large Language Models (LLMs).\nThere is ample work on aligning LLMs with human ethics and preferences,\nincluding data filtering in pretraining, supervised fine-tuning, reinforcement\nlearning from human feedback, and red teaming, etc. In this study, we discover\nthat chat in cipher can bypass the safety alignment techniques of LLMs, which\nare mainly conducted in natural languages. We propose a novel framework\nCipherChat to systematically examine the generalizability of safety alignment\nto non-natural languages -- ciphers. CipherChat enables humans to chat with\nLLMs through cipher prompts topped with system role descriptions and few-shot\nenciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs,\nincluding ChatGPT and GPT-4 for different representative human ciphers across\n11 safety domains in both English and Chinese. Experimental results show that\ncertain ciphers succeed almost 100% of the time to bypass the safety alignment\nof GPT-4 in several safety domains, demonstrating the necessity of developing\nsafety alignment for non-natural languages. Notably, we identify that LLMs seem\nto have a ''secret cipher'', and propose a novel SelfCipher that uses only role\nplay and several demonstrations in natural language to evoke this capability.\nSelfCipher surprisingly outperforms existing human ciphers in almost all cases.\nOur code and data will be released at https://github.com/RobustNLP/CipherChat.\n","authors":["Youliang Yuan","Wenxiang Jiao","Wenxuan Wang","Jen-tse Huang","Pinjia He","Shuming Shi","Zhaopeng Tu"],"pdf_url":"https://arxiv.org/pdf/2308.06463v2.pdf","comment":"Accepted by ICLR 2024. 21 pages, 3 figures, 13 tables"},{"id":"http://arxiv.org/abs/2403.09963v2","updated":"2024-03-26T04:08:47Z","published":"2024-03-15T02:04:35Z","title":"Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias\n  in Factual Knowledge Extraction","summary":"  Recent research shows that pre-trained language models (PLMs) suffer from\n\"prompt bias\" in factual knowledge extraction, i.e., prompts tend to introduce\nbiases toward specific labels. Prompt bias presents a significant challenge in\nassessing the factual knowledge within PLMs. Therefore, this paper aims to\nimprove the reliability of existing benchmarks by thoroughly investigating and\nmitigating prompt bias. We show that: 1) all prompts in the experiments exhibit\nnon-negligible bias, with gradient-based prompts like AutoPrompt and OptiPrompt\ndisplaying significantly higher levels of bias; 2) prompt bias can amplify\nbenchmark accuracy unreasonably by overfitting the test datasets, especially on\nimbalanced datasets like LAMA. Based on these findings, we propose a\nrepresentation-based approach to mitigate the prompt bias during inference\ntime. Specifically, we first estimate the biased representation using\nprompt-only querying, and then remove it from the model's internal\nrepresentations to generate the debiased representations, which are used to\nproduce the final debiased outputs. Experiments across various prompts, PLMs,\nand benchmarks show that our approach can not only correct the overfitted\nperformance caused by prompt bias, but also significantly improve the prompt\nretrieval capability (up to 10% absolute performance gain). These results\nindicate that our approach effectively alleviates prompt bias in knowledge\nevaluation, thereby enhancing the reliability of benchmark assessments.\nHopefully, our plug-and-play approach can be a golden standard to strengthen\nPLMs toward reliable knowledge bases. Code and data are released in\nhttps://github.com/FelliYang/PromptBias.\n","authors":["Ziyang Xu","Keqin Peng","Liang Ding","Dacheng Tao","Xiliang Lu"],"pdf_url":"https://arxiv.org/pdf/2403.09963v2.pdf","comment":"Accepted by COLING 2024"},{"id":"http://arxiv.org/abs/2403.17368v1","updated":"2024-03-26T04:07:08Z","published":"2024-03-26T04:07:08Z","title":"ChatGPT Rates Natural Language Explanation Quality Like Humans: But on\n  Which Scales?","summary":"  As AI becomes more integral in our lives, the need for transparency and\nresponsibility grows. While natural language explanations (NLEs) are vital for\nclarifying the reasoning behind AI decisions, evaluating them through human\njudgments is complex and resource-intensive due to subjectivity and the need\nfor fine-grained ratings. This study explores the alignment between ChatGPT and\nhuman assessments across multiple scales (i.e., binary, ternary, and 7-Likert\nscale). We sample 300 data instances from three NLE datasets and collect 900\nhuman annotations for both informativeness and clarity scores as the text\nquality measurement. We further conduct paired comparison experiments under\ndifferent ranges of subjectivity scores, where the baseline comes from 8,346\nhuman annotations. Our results show that ChatGPT aligns better with humans in\nmore coarse-grained scales. Also, paired comparisons and dynamic prompting\n(i.e., providing semantically similar examples in the prompt) improve the\nalignment. This research advances our understanding of large language models'\ncapabilities to assess the text explanation quality in different configurations\nfor responsible AI development.\n","authors":["Fan Huang","Haewoon Kwak","Kunwoo Park","Jisun An"],"pdf_url":"https://arxiv.org/pdf/2403.17368v1.pdf","comment":"Accpeted by LREC-COLING 2024 main conference, long paper"},{"id":"http://arxiv.org/abs/2403.17363v1","updated":"2024-03-26T03:58:52Z","published":"2024-03-26T03:58:52Z","title":"Extracting Biomedical Entities from Noisy Audio Transcripts","summary":"  Automatic Speech Recognition (ASR) technology is fundamental in transcribing\nspoken language into text, with considerable applications in the clinical\nrealm, including streamlining medical transcription and integrating with\nElectronic Health Record (EHR) systems. Nevertheless, challenges persist,\nespecially when transcriptions contain noise, leading to significant drops in\nperformance when Natural Language Processing (NLP) models are applied. Named\nEntity Recognition (NER), an essential clinical task, is particularly affected\nby such noise, often termed the ASR-NLP gap. Prior works have primarily studied\nASR's efficiency in clean recordings, leaving a research gap concerning the\nperformance in noisy environments. This paper introduces a novel dataset,\nBioASR-NER, designed to bridge the ASR-NLP gap in the biomedical domain,\nfocusing on extracting adverse drug reactions and mentions of entities from the\nBrief Test of Adult Cognition by Telephone (BTACT) exam. Our dataset offers a\ncomprehensive collection of almost 2,000 clean and noisy recordings. In\naddressing the noise challenge, we present an innovative transcript-cleaning\nmethod using GPT4, investigating both zero-shot and few-shot methodologies. Our\nstudy further delves into an error analysis, shedding light on the types of\nerrors in transcription software, corrections by GPT4, and the challenges GPT4\nfaces. This paper aims to foster improved understanding and potential solutions\nfor the ASR-NLP gap, ultimately supporting enhanced healthcare documentation\npractices.\n","authors":["Nima Ebadi","Kellen Morgan","Adrian Tan","Billy Linares","Sheri Osborn","Emma Majors","Jeremy Davis","Anthony Rios"],"pdf_url":"https://arxiv.org/pdf/2403.17363v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17361v1","updated":"2024-03-26T03:54:25Z","published":"2024-03-26T03:54:25Z","title":"Bridging Textual and Tabular Worlds for Fact Verification: A\n  Lightweight, Attention-Based Model","summary":"  FEVEROUS is a benchmark and research initiative focused on fact extraction\nand verification tasks involving unstructured text and structured tabular data.\nIn FEVEROUS, existing works often rely on extensive preprocessing and utilize\nrule-based transformations of data, leading to potential context loss or\nmisleading encodings. This paper introduces a simple yet powerful model that\nnullifies the need for modality conversion, thereby preserving the original\nevidence's context. By leveraging pre-trained models on diverse text and\ntabular datasets and by incorporating a lightweight attention-based mechanism,\nour approach efficiently exploits latent connections between different data\ntypes, thereby yielding comprehensive and reliable verdict predictions. The\nmodel's modular structure adeptly manages multi-modal information, ensuring the\nintegrity and authenticity of the original evidence are uncompromised.\nComparative analyses reveal that our approach exhibits competitive performance,\naligning itself closely with top-tier models on the FEVEROUS benchmark.\n","authors":["Shirin Dabbaghi Varnosfaderani","Canasai Kruengkrai","Ramin Yahyapour","Junichi Yamagishi"],"pdf_url":"https://arxiv.org/pdf/2403.17361v1.pdf","comment":"Accepted for a presentation at LREC-COLING 2024 - The 2024 Joint\n  International Conference on Computational Linguistics, Language Resources and\n  Evaluation"},{"id":"http://arxiv.org/abs/2403.17359v1","updated":"2024-03-26T03:51:01Z","published":"2024-03-26T03:51:01Z","title":"Chain-of-Action: Faithful and Multimodal Question Answering through\n  Large Language Models","summary":"  We present a Chain-of-Action (CoA) framework for multimodal and\nretrieval-augmented Question-Answering (QA). Compared to the literature, CoA\novercomes two major challenges of current QA applications: (i) unfaithful\nhallucination that is inconsistent with real-time or domain facts and (ii) weak\nreasoning performance over compositional information. Our key contribution is a\nnovel reasoning-retrieval mechanism that decomposes a complex question into a\nreasoning chain via systematic prompting and pre-designed actions.\nMethodologically, we propose three types of domain-adaptable `Plug-and-Play'\nactions for retrieving real-time information from heterogeneous sources. We\nalso propose a multi-reference faith score (MRFS) to verify and resolve\nconflicts in the answers. Empirically, we exploit both public benchmarks and a\nWeb3 case study to demonstrate the capability of CoA over other methods.\n","authors":["Zhenyu Pan","Haozheng Luo","Manling Li","Han Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17359v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06578v3","updated":"2024-03-26T03:33:45Z","published":"2023-09-07T04:15:17Z","title":"Can Large Language Models Discern Evidence for Scientific Hypotheses?\n  Case Studies in the Social Sciences","summary":"  Hypothesis formulation and testing are central to empirical research. A\nstrong hypothesis is a best guess based on existing evidence and informed by a\ncomprehensive view of relevant literature. However, with exponential increase\nin the number of scientific articles published annually, manual aggregation and\nsynthesis of evidence related to a given hypothesis is a challenge. Our work\nexplores the ability of current large language models (LLMs) to discern\nevidence in support or refute of specific hypotheses based on the text of\nscientific abstracts. We share a novel dataset for the task of scientific\nhypothesis evidencing using community-driven annotations of studies in the\nsocial sciences. We compare the performance of LLMs to several state-of-the-art\nbenchmarks and highlight opportunities for future research in this area. The\ndataset is available at\nhttps://github.com/Sai90000/ScientificHypothesisEvidencing.git\n","authors":["Sai Koneru","Jian Wu","Sarah Rajtmajer"],"pdf_url":"https://arxiv.org/pdf/2309.06578v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15230v2","updated":"2024-03-26T03:07:56Z","published":"2023-03-27T14:10:26Z","title":"Troika: Multi-Path Cross-Modal Traction for Compositional Zero-Shot\n  Learning","summary":"  Recent compositional zero-shot learning (CZSL) methods adapt pre-trained\nvision-language models (VLMs) by constructing trainable prompts only for\ncomposed state-object pairs. Relying on learning the joint representation of\nseen compositions, these methods ignore the explicit modeling of the state and\nobject, thus limiting the exploitation of pre-trained knowledge and\ngeneralization to unseen compositions. With a particular focus on the\nuniversality of the solution, in this work, we propose a novel paradigm for\nCZSL models that establishes three identification branches (i.e., Multi-Path)\nto jointly model the state, object, and composition. The presented Troika is\nour implementation that aligns the branch-specific prompt representations with\ndecomposed visual features. To calibrate the bias between semantically similar\nmulti-modal representations, we further devise a Cross-Modal Traction module\ninto Troika that shifts the prompt representation towards the current visual\ncontent. We conduct extensive experiments on three popular benchmarks, where\nour method significantly outperforms existing methods in both closed-world and\nopen-world settings. The code will be available at\nhttps://github.com/bighuang624/Troika.\n","authors":["Siteng Huang","Biao Gong","Yutong Feng","Min Zhang","Yiliang Lv","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2303.15230v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17344v1","updated":"2024-03-26T03:07:32Z","published":"2024-03-26T03:07:32Z","title":"Disambiguate Entity Matching through Relation Discovery with Large\n  Language Models","summary":"  Entity matching is a critical challenge in data integration and cleaning,\ncentral to tasks like fuzzy joins and deduplication. Traditional approaches\nhave focused on overcoming fuzzy term representations through methods such as\nedit distance, Jaccard similarity, and more recently, embeddings and deep\nneural networks, including advancements from large language models (LLMs) like\nGPT. However, the core challenge in entity matching extends beyond term\nfuzziness to the ambiguity in defining what constitutes a \"match,\" especially\nwhen integrating with external databases. This ambiguity arises due to varying\nlevels of detail and granularity among entities, complicating exact matches. We\npropose a novel approach that shifts focus from purely identifying semantic\nsimilarities to understanding and defining the \"relations\" between entities as\ncrucial for resolving ambiguities in matching. By predefining a set of\nrelations relevant to the task at hand, our method allows analysts to navigate\nthe spectrum of similarity more effectively, from exact matches to conceptually\nrelated entities.\n","authors":["Zezhou Huang"],"pdf_url":"https://arxiv.org/pdf/2403.17344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17343v1","updated":"2024-03-26T03:05:20Z","published":"2024-03-26T03:05:20Z","title":"Language Models are Free Boosters for Biomedical Imaging Tasks","summary":"  In this study, we uncover the unexpected efficacy of residual-based large\nlanguage models (LLMs) as part of encoders for biomedical imaging tasks, a\ndomain traditionally devoid of language or textual data. The approach diverges\nfrom established methodologies by utilizing a frozen transformer block,\nextracted from pre-trained LLMs, as an innovative encoder layer for the direct\nprocessing of visual tokens. This strategy represents a significant departure\nfrom the standard multi-modal vision-language frameworks, which typically hinge\non language-driven prompts and inputs. We found that these LLMs could boost\nperformance across a spectrum of biomedical imaging applications, including\nboth 2D and 3D visual classification tasks, serving as plug-and-play boosters.\nMore interestingly, as a byproduct, we found that the proposed framework\nachieved superior performance, setting new state-of-the-art results on\nextensive, standardized datasets in MedMNIST-2D and 3D. Through this work, we\naim to open new avenues for employing LLMs in biomedical imaging and enriching\nthe understanding of their potential in this specialized domain.\n","authors":["Zhixin Lai","Jing Wu","Suiyao Chen","Yucheng Zhou","Anna Hovakimyan","Naira Hovakimyan"],"pdf_url":"https://arxiv.org/pdf/2403.17343v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17336v1","updated":"2024-03-26T02:47:42Z","published":"2024-03-26T02:47:42Z","title":"Don't Listen To Me: Understanding and Exploring Jailbreak Prompts of\n  Large Language Models","summary":"  Recent advancements in generative AI have enabled ubiquitous access to large\nlanguage models (LLMs). Empowered by their exceptional capabilities to\nunderstand and generate human-like text, these models are being increasingly\nintegrated into our society. At the same time, there are also concerns on the\npotential misuse of this powerful technology, prompting defensive measures from\nservice providers. To overcome such protection, jailbreaking prompts have\nrecently emerged as one of the most effective mechanisms to circumvent security\nrestrictions and elicit harmful content originally designed to be prohibited.\n  Due to the rapid development of LLMs and their ease of access via natural\nlanguages, the frontline of jailbreak prompts is largely seen in online forums\nand among hobbyists. To gain a better understanding of the threat landscape of\nsemantically meaningful jailbreak prompts, we systemized existing prompts and\nmeasured their jailbreak effectiveness empirically. Further, we conducted a\nuser study involving 92 participants with diverse backgrounds to unveil the\nprocess of manually creating jailbreak prompts. We observed that users often\nsucceeded in jailbreak prompts generation regardless of their expertise in\nLLMs. Building on the insights from the user study, we also developed a system\nusing AI as the assistant to automate the process of jailbreak prompt\ngeneration.\n","authors":["Zhiyuan Yu","Xiaogeng Liu","Shunning Liang","Zach Cameron","Chaowei Xiao","Ning Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.17336v1.pdf","comment":"Accepted by USENIX Security 2024"},{"id":"http://arxiv.org/abs/2403.15729v2","updated":"2024-03-26T02:42:08Z","published":"2024-03-23T05:32:46Z","title":"Towards a RAG-based Summarization Agent for the Electron-Ion Collider","summary":"  The complexity and sheer volume of information encompassing documents,\npapers, data, and other resources from large-scale experiments demand\nsignificant time and effort to navigate, making the task of accessing and\nutilizing these varied forms of information daunting, particularly for new\ncollaborators and early-career scientists. To tackle this issue, a Retrieval\nAugmented Generation (RAG)--based Summarization AI for EIC (RAGS4EIC) is under\ndevelopment. This AI-Agent not only condenses information but also effectively\nreferences relevant responses, offering substantial advantages for\ncollaborators. Our project involves a two-step approach: first, querying a\ncomprehensive vector database containing all pertinent experiment information;\nsecond, utilizing a Large Language Model (LLM) to generate concise summaries\nenriched with citations based on user queries and retrieved data. We describe\nthe evaluation methods that use RAG assessments (RAGAs) scoring mechanisms to\nassess the effectiveness of responses. Furthermore, we describe the concept of\nprompt template-based instruction-tuning which provides flexibility and\naccuracy in summarization. Importantly, the implementation relies on LangChain,\nwhich serves as the foundation of our entire workflow. This integration ensures\nefficiency and scalability, facilitating smooth deployment and accessibility\nfor various user groups within the Electron Ion Collider (EIC) community. This\ninnovative AI-driven framework not only simplifies the understanding of vast\ndatasets but also encourages collaborative participation, thereby empowering\nresearchers. As a demonstration, a web application has been developed to\nexplain each stage of the RAG Agent development in detail.\n","authors":["Karthik Suresh","Neeltje Kackar","Luke Schleck","Cristiano Fanelli"],"pdf_url":"https://arxiv.org/pdf/2403.15729v2.pdf","comment":"updated title to have no latex formatting"},{"id":"http://arxiv.org/abs/2403.16971v2","updated":"2024-03-26T02:35:07Z","published":"2024-03-25T17:32:23Z","title":"AIOS: LLM Agent Operating System","summary":"  The integration and deployment of large language model (LLM)-based\nintelligent agents have been fraught with challenges that compromise their\nefficiency and efficacy. Among these issues are sub-optimal scheduling and\nresource allocation of agent requests over the LLM, the difficulties in\nmaintaining context during interactions between agent and LLM, and the\ncomplexities inherent in integrating heterogeneous agents with different\ncapabilities and specializations. The rapid increase of agent quantity and\ncomplexity further exacerbates these issues, often leading to bottlenecks and\nsub-optimal utilization of resources. Inspired by these challenges, this paper\npresents AIOS, an LLM agent operating system, which embeds large language model\ninto operating systems (OS) as the brain of the OS, enabling an operating\nsystem \"with soul\" -- an important step towards AGI. Specifically, AIOS is\ndesigned to optimize resource allocation, facilitate context switch across\nagents, enable concurrent execution of agents, provide tool service for agents,\nand maintain access control for agents. We present the architecture of such an\noperating system, outline the core challenges it aims to resolve, and provide\nthe basic design and implementation of the AIOS. Our experiments on concurrent\nexecution of multiple agents demonstrate the reliability and efficiency of our\nAIOS modules. Through this, we aim to not only improve the performance and\nefficiency of LLM agents but also to pioneer for better development and\ndeployment of the AIOS ecosystem in the future. The project is open-source at\nhttps://github.com/agiresearch/AIOS.\n","authors":["Kai Mei","Zelong Li","Shuyuan Xu","Ruosong Ye","Yingqiang Ge","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16971v2.pdf","comment":"14 pages, 5 figures, 5 tables; comments and suggestions are\n  appreciated"},{"id":"http://arxiv.org/abs/2403.16950v2","updated":"2024-03-26T02:28:42Z","published":"2024-03-25T17:11:28Z","title":"Aligning with Human Judgement: The Role of Pairwise Preference in Large\n  Language Model Evaluators","summary":"  Large Language Models (LLMs) have demonstrated promising capabilities as\nautomatic evaluators in assessing the quality of generated natural language.\nHowever, LLMs still exhibit biases in evaluation and often struggle to generate\ncoherent evaluations that align with human assessments. In this work, we first\nconduct a systematic study of the misalignment between LLM evaluators and human\njudgement, revealing that existing calibration methods aimed at mitigating\nbiases are insufficient for effectively aligning LLM evaluators. Inspired by\nthe use of preference data in RLHF, we formulate the evaluation as a ranking\nproblem and introduce Pairwise-preference Search (PairS), an uncertainty-guided\nsearch method that employs LLMs to conduct pairwise comparisons and efficiently\nranks candidate texts. PairS achieves state-of-the-art performance on\nrepresentative evaluation tasks and demonstrates significant improvements over\ndirect scoring. Furthermore, we provide insights into the role of pairwise\npreference in quantifying the transitivity of LLMs and demonstrate how PairS\nbenefits from calibration.\n","authors":["Yinhong Liu","Han Zhou","Zhijiang Guo","Ehsan Shareghi","Ivan Vulić","Anna Korhonen","Nigel Collier"],"pdf_url":"https://arxiv.org/pdf/2403.16950v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16303v2","updated":"2024-03-26T02:24:36Z","published":"2024-03-24T21:29:39Z","title":"Large Language Models in Biomedical and Health Informatics: A\n  Bibliometric Review","summary":"  Large Language Models (LLMs) have rapidly become important tools in\nBiomedical and Health Informatics (BHI), enabling new ways to analyze data,\ntreat patients, and conduct research. This bibliometric review aims to provide\na panoramic view of how LLMs have been used in BHI by examining research\narticles and collaboration networks from 2022 to 2023. It further explores how\nLLMs can improve Natural Language Processing (NLP) applications in various BHI\nareas like medical diagnosis, patient engagement, electronic health record\nmanagement, and personalized medicine. To do this, our bibliometric review\nidentifies key trends, maps out research networks, and highlights major\ndevelopments in this fast-moving field. Lastly, it discusses the ethical\nconcerns and practical challenges of using LLMs in BHI, such as data privacy\nand reliable medical recommendations. Looking ahead, we consider how LLMs could\nfurther transform biomedical research as well as healthcare delivery and\npatient outcomes. This bibliometric review serves as a resource for\nstakeholders in healthcare, including researchers, clinicians, and\npolicymakers, to understand the current state and future potential of LLMs in\nBHI.\n","authors":["Huizi Yu","Lizhou Fan","Lingyao Li","Jiayan Zhou","Zihui Ma","Lu Xian","Wenyue Hua","Sijia He","Mingyu Jin","Yongfeng Zhang","Ashvin Gandhi","Xin Ma"],"pdf_url":"https://arxiv.org/pdf/2403.16303v2.pdf","comment":"50 pages, 7 figures, 4 tables"},{"id":"http://arxiv.org/abs/2311.05020v2","updated":"2024-03-26T02:23:27Z","published":"2023-11-08T21:13:38Z","title":"First Tragedy, then Parse: History Repeats Itself in the New Era of\n  Large Language Models","summary":"  Many NLP researchers are experiencing an existential crisis triggered by the\nastonishing success of ChatGPT and other systems based on large language models\n(LLMs). After such a disruptive change to our understanding of the field, what\nis left to do? Taking a historical lens, we look for guidance from the first\nera of LLMs, which began in 2005 with large $n$-gram models for machine\ntranslation (MT). We identify durable lessons from the first era, and more\nimportantly, we identify evergreen problems where NLP researchers can continue\nto make meaningful contributions in areas where LLMs are ascendant. We argue\nthat disparities in scale are transient and researchers can work to reduce\nthem; that data, rather than hardware, is still a bottleneck for many\napplications; that meaningful realistic evaluation is still an open problem;\nand that there is still room for speculative approaches.\n","authors":["Naomi Saphra","Eve Fleisig","Kyunghyun Cho","Adam Lopez"],"pdf_url":"https://arxiv.org/pdf/2311.05020v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17319v1","updated":"2024-03-26T02:01:18Z","published":"2024-03-26T02:01:18Z","title":"JMultiWOZ: A Large-Scale Japanese Multi-Domain Task-Oriented Dialogue\n  Dataset","summary":"  Dialogue datasets are crucial for deep learning-based task-oriented dialogue\nsystem research. While numerous English language multi-domain task-oriented\ndialogue datasets have been developed and contributed to significant\nadvancements in task-oriented dialogue systems, such a dataset does not exist\nin Japanese, and research in this area is limited compared to that in English.\nIn this study, towards the advancement of research and development of\ntask-oriented dialogue systems in Japanese, we constructed JMultiWOZ, the first\nJapanese language large-scale multi-domain task-oriented dialogue dataset.\nUsing JMultiWOZ, we evaluated the dialogue state tracking and response\ngeneration capabilities of the state-of-the-art methods on the existing major\nEnglish benchmark dataset MultiWOZ2.2 and the latest large language model\n(LLM)-based methods. Our evaluation results demonstrated that JMultiWOZ\nprovides a benchmark that is on par with MultiWOZ2.2. In addition, through\nevaluation experiments of interactive dialogues with the models and human\nparticipants, we identified limitations in the task completion capabilities of\nLLMs in Japanese.\n","authors":["Atsumoto Ohashi","Ryu Hirai","Shinya Iizuka","Ryuichiro Higashinaka"],"pdf_url":"https://arxiv.org/pdf/2403.17319v1.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2309.13339v4","updated":"2024-03-26T01:53:30Z","published":"2023-09-23T11:21:12Z","title":"Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models\n  through Logic","summary":"  Recent advancements in large language models have showcased their remarkable\ngeneralizability across various domains. However, their reasoning abilities\nstill have significant room for improvement, especially when confronted with\nscenarios requiring multi-step reasoning. Although large language models\npossess extensive knowledge, their reasoning often fails to effectively utilize\nthis knowledge to establish a coherent thinking paradigm. These models\nsometimes show hallucinations as their reasoning procedures are unconstrained\nby logical principles. Aiming at improving the zero-shot chain-of-thought\nreasoning ability of large language models, we propose LoT (Logical Thoughts),\na self-improvement prompting framework that leverages principles rooted in\nsymbolic logic, particularly Reductio ad Absurdum, to systematically verify and\nrectify the reasoning processes step by step. Experimental evaluations\nconducted on language tasks in diverse domains, including arithmetic,\ncommonsense, symbolic, causal inference, and social problems, demonstrate the\nefficacy of enhanced reasoning by logic. The implementation code for LoT can be\naccessed at: https://github.com/xf-zhao/LoT.\n","authors":["Xufeng Zhao","Mengdi Li","Wenhao Lu","Cornelius Weber","Jae Hee Lee","Kun Chu","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2309.13339v4.pdf","comment":"Accepted in COLING 2024. Code see https://github.com/xf-zhao/LoT"},{"id":"http://arxiv.org/abs/2403.17314v1","updated":"2024-03-26T01:52:59Z","published":"2024-03-26T01:52:59Z","title":"Project MOSLA: Recording Every Moment of Second Language Acquisition","summary":"  Second language acquisition (SLA) is a complex and dynamic process. Many SLA\nstudies that have attempted to record and analyze this process have typically\nfocused on a single modality (e.g., textual output of learners), covered only a\nshort period of time, and/or lacked control (e.g., failed to capture every\naspect of the learning process). In Project MOSLA (Moments of Second Language\nAcquisition), we have created a longitudinal, multimodal, multilingual, and\ncontrolled dataset by inviting participants to learn one of three target\nlanguages (Arabic, Spanish, and Chinese) from scratch over a span of two years,\nexclusively through online instruction, and recording every lesson using Zoom.\nThe dataset is semi-automatically annotated with speaker/language IDs and\ntranscripts by both human annotators and fine-tuned state-of-the-art speech\nmodels. Our experiments reveal linguistic insights into learners' proficiency\ndevelopment over time, as well as the potential for automatically detecting the\nareas of focus on the screen purely from the unannotated multimodal data. Our\ndataset is freely available for research purposes and can serve as a valuable\nresource for a wide range of applications, including but not limited to SLA,\nproficiency assessment, language and speech processing, pedagogy, and\nmultimodal learning analytics.\n","authors":["Masato Hagiwara","Joshua Tanner"],"pdf_url":"https://arxiv.org/pdf/2403.17314v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2303.08014v2","updated":"2024-03-26T01:46:50Z","published":"2023-03-10T10:47:59Z","title":"Do large language models resemble humans in language use?","summary":"  Large language models (LLMs) such as ChatGPT and Vicuna have shown remarkable\ncapacities in comprehending and producing language. However, their internal\nworkings remain a black box, and it is unclear whether LLMs and chatbots can\ndevelop humanlike characteristics in language use. Cognitive scientists have\ndevised many experiments that probe, and have made great progress in\nexplaining, how people comprehend and produce language. We subjected ChatGPT\nand Vicuna to 12 of these experiments ranging from sounds to dialogue,\npreregistered and with 1000 runs (i.e., iterations) per experiment. ChatGPT and\nVicuna replicated the human pattern of language use in 10 and 7 out of the 12\nexperiments, respectively. The models associated unfamiliar words with\ndifferent meanings depending on their forms, continued to access recently\nencountered meanings of ambiguous words, reused recent sentence structures,\nattributed causality as a function of verb semantics, and accessed different\nmeanings and retrieved different words depending on an interlocutor's identity.\nIn addition, ChatGPT, but not Vicuna, nonliterally interpreted implausible\nsentences that were likely to have been corrupted by noise, drew reasonable\ninferences, and overlooked semantic fallacies in a sentence. Finally, unlike\nhumans, neither model preferred using shorter words to convey less informative\ncontent, nor did they use context to resolve syntactic ambiguities. We discuss\nhow these convergences and divergences may result from the transformer\narchitecture. Overall, these experiments demonstrate that LLMs such as ChatGPT\n(and Vicuna to a lesser extent) are humanlike in many aspects of human language\nprocessing.\n","authors":["Zhenguang G. Cai","Xufeng Duan","David A. Haslett","Shuqi Wang","Martin J. Pickering"],"pdf_url":"https://arxiv.org/pdf/2303.08014v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17308v1","updated":"2024-03-26T01:29:46Z","published":"2024-03-26T01:29:46Z","title":"Neural Multimodal Topic Modeling: A Comprehensive Evaluation","summary":"  Neural topic models can successfully find coherent and diverse topics in\ntextual data. However, they are limited in dealing with multimodal datasets\n(e.g., images and text). This paper presents the first systematic and\ncomprehensive evaluation of multimodal topic modeling of documents containing\nboth text and images. In the process, we propose two novel topic modeling\nsolutions and two novel evaluation metrics. Overall, our evaluation on an\nunprecedented rich and diverse collection of datasets indicates that both of\nour models generate coherent and diverse topics. Nevertheless, the extent to\nwhich one method outperforms the other depends on the metrics and dataset\ncombinations, which suggests further exploration of hybrid solutions in the\nfuture. Notably, our succinct human evaluation aligns with the outcomes\ndetermined by our proposed metrics. This alignment not only reinforces the\ncredibility of our metrics but also highlights the potential for their\napplication in guiding future multimodal topic modeling endeavors.\n","authors":["Felipe González-Pizarro","Giuseppe Carenini"],"pdf_url":"https://arxiv.org/pdf/2403.17308v1.pdf","comment":"Camera-Ready for LREC-COLING 2024 (Long Paper)"},{"id":"http://arxiv.org/abs/2403.17307v1","updated":"2024-03-26T01:29:17Z","published":"2024-03-26T01:29:17Z","title":"HILL: Hierarchy-aware Information Lossless Contrastive Learning for\n  Hierarchical Text Classification","summary":"  Existing self-supervised methods in natural language processing (NLP),\nespecially hierarchical text classification (HTC), mainly focus on\nself-supervised contrastive learning, extremely relying on human-designed\naugmentation rules to generate contrastive samples, which can potentially\ncorrupt or distort the original information. In this paper, we tend to\ninvestigate the feasibility of a contrastive learning scheme in which the\nsemantic and syntactic information inherent in the input sample is adequately\nreserved in the contrastive samples and fused during the learning process.\nSpecifically, we propose an information lossless contrastive learning strategy\nfor HTC, namely \\textbf{H}ierarchy-aware \\textbf{I}nformation \\textbf{L}ossless\ncontrastive \\textbf{L}earning (HILL), which consists of a text encoder\nrepresenting the input document, and a structure encoder directly generating\nthe positive sample. The structure encoder takes the document embedding as\ninput, extracts the essential syntactic information inherent in the label\nhierarchy with the principle of structural entropy minimization, and injects\nthe syntactic information into the text representation via hierarchical\nrepresentation learning. Experiments on three common datasets are conducted to\nverify the superiority of HILL.\n","authors":["He Zhu","Junran Wu","Ruomei Liu","Yue Hou","Ze Yuan","Shangzhe Li","Yicheng Pan","Ke Xu"],"pdf_url":"https://arxiv.org/pdf/2403.17307v1.pdf","comment":"Accepted by NAACL 2024"},{"id":"http://arxiv.org/abs/2403.10949v2","updated":"2024-03-26T01:15:09Z","published":"2024-03-16T15:30:34Z","title":"SelfIE: Self-Interpretation of Large Language Model Embeddings","summary":"  How do large language models (LLMs) obtain their answers? The ability to\nexplain and control an LLM's reasoning process is key for reliability,\ntransparency, and future model developments. We propose SelfIE\n(Self-Interpretation of Embeddings), a framework that enables LLMs to interpret\ntheir own embeddings in natural language by leveraging their ability to respond\nto inquiries about a given passage. Capable of interpreting open-world concepts\nin the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such\nas making ethical decisions, internalizing prompt injection, and recalling\nharmful knowledge. SelfIE's text descriptions on hidden embeddings also open up\nnew avenues to control LLM reasoning. We propose Supervised Control, which\nallows editing open-ended concepts while only requiring gradient computation of\nindividual layer. We extend RLHF to hidden embeddings and propose Reinforcement\nControl that erases harmful knowledge in LLM without supervision targets.\n","authors":["Haozhe Chen","Carl Vondrick","Chengzhi Mao"],"pdf_url":"https://arxiv.org/pdf/2403.10949v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17299v1","updated":"2024-03-26T00:56:06Z","published":"2024-03-26T00:56:06Z","title":"Decoding Probing: Revealing Internal Linguistic Structures in Neural\n  Language Models using Minimal Pairs","summary":"  Inspired by cognitive neuroscience studies, we introduce a novel `decoding\nprobing' method that uses minimal pairs benchmark (BLiMP) to probe internal\nlinguistic characteristics in neural language models layer by layer. By\ntreating the language model as the `brain' and its representations as `neural\nactivations', we decode grammaticality labels of minimal pairs from the\nintermediate layers' representations. This approach reveals: 1) Self-supervised\nlanguage models capture abstract linguistic structures in intermediate layers\nthat GloVe and RNN language models cannot learn. 2) Information about syntactic\ngrammaticality is robustly captured through the first third layers of GPT-2 and\nalso distributed in later layers. As sentence complexity increases, more layers\nare required for learning grammatical capabilities. 3) Morphological and\nsemantics/syntax interface-related features are harder to capture than syntax.\n4) For Transformer-based models, both embeddings and attentions capture\ngrammatical features but show distinct patterns. Different attention heads\nexhibit similar tendencies toward various linguistic phenomena, but with varied\ncontributions.\n","authors":["Linyang He","Peili Chen","Ercong Nie","Yuanning Li","Jonathan R. Brennan"],"pdf_url":"https://arxiv.org/pdf/2403.17299v1.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17297v1","updated":"2024-03-26T00:53:24Z","published":"2024-03-26T00:53:24Z","title":"InternLM2 Technical Report","summary":"  The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has\nsparked discussions on the advent of Artificial General Intelligence (AGI).\nHowever, replicating such advancements in open-source models has been\nchallenging. This paper introduces InternLM2, an open-source LLM that\noutperforms its predecessors in comprehensive evaluations across 6 dimensions\nand 30 benchmarks, long-context modeling, and open-ended subjective evaluations\nthrough innovative pre-training and optimization techniques. The pre-training\nprocess of InternLM2 is meticulously detailed, highlighting the preparation of\ndiverse data types including text, code, and long-context data. InternLM2\nefficiently captures long-term dependencies, initially trained on 4k tokens\nbefore advancing to 32k tokens in pre-training and fine-tuning stages,\nexhibiting remarkable performance on the 200k ``Needle-in-a-Haystack\" test.\nInternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel\nConditional Online Reinforcement Learning from Human Feedback (COOL RLHF)\nstrategy that addresses conflicting human preferences and reward hacking. By\nreleasing InternLM2 models in different training stages and model sizes, we\nprovide the community with insights into the model's evolution.\n","authors":["Zheng Cai","Maosong Cao","Haojiong Chen","Kai Chen","Keyu Chen","Xin Chen","Xun Chen","Zehui Chen","Zhi Chen","Pei Chu","Xiaoyi Dong","Haodong Duan","Qi Fan","Zhaoye Fei","Yang Gao","Jiaye Ge","Chenya Gu","Yuzhe Gu","Tao Gui","Aijia Guo","Qipeng Guo","Conghui He","Yingfan Hu","Ting Huang","Tao Jiang","Penglong Jiao","Zhenjiang Jin","Zhikai Lei","Jiaxing Li","Jingwen Li","Linyang Li","Shuaibin Li","Wei Li","Yining Li","Hongwei Liu","Jiangning Liu","Jiawei Hong","Kaiwen Liu","Kuikun Liu","Xiaoran Liu","Chengqi Lv","Haijun Lv","Kai Lv","Li Ma","Runyuan Ma","Zerun Ma","Wenchang Ning","Linke Ouyang","Jiantao Qiu","Yuan Qu","Fukai Shang","Yunfan Shao","Demin Song","Zifan Song","Zhihao Sui","Peng Sun","Yu Sun","Huanze Tang","Bin Wang","Guoteng Wang","Jiaqi Wang","Jiayu Wang","Rui Wang","Yudong Wang","Ziyi Wang","Xingjian Wei","Qizhen Weng","Fan Wu","Yingtong Xiong","Chao Xu","Ruiliang Xu","Hang Yan","Yirong Yan","Xiaogui Yang","Haochen Ye","Huaiyuan Ying","Jia Yu","Jing Yu","Yuhang Zang","Chuyu Zhang","Li Zhang","Pan Zhang","Peng Zhang","Ruijie Zhang","Shuo Zhang","Songyang Zhang","Wenjian Zhang","Wenwei Zhang","Xingcheng Zhang","Xinyue Zhang","Hui Zhao","Qian Zhao","Xiaomeng Zhao","Fengzhe Zhou","Zaida Zhou","Jingming Zhuo","Yicheng Zou","Xipeng Qiu","Yu Qiao","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2403.17297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17284v1","updated":"2024-03-26T00:25:01Z","published":"2024-03-26T00:25:01Z","title":"Common Ground Tracking in Multimodal Dialogue","summary":"  Within Dialogue Modeling research in AI and NLP, considerable attention has\nbeen spent on ``dialogue state tracking'' (DST), which is the ability to update\nthe representations of the speaker's needs at each turn in the dialogue by\ntaking into account the past dialogue moves and history. Less studied but just\nas important to dialogue modeling, however, is ``common ground tracking''\n(CGT), which identifies the shared belief space held by all of the participants\nin a task-oriented dialogue: the task-relevant propositions all participants\naccept as true. In this paper we present a method for automatically identifying\nthe current set of shared beliefs and ``questions under discussion'' (QUDs) of\na group with a shared goal. We annotate a dataset of multimodal interactions in\na shared physical space with speech transcriptions, prosodic features,\ngestures, actions, and facets of collaboration, and operationalize these\nfeatures for use in a deep neural model to predict moves toward construction of\ncommon ground. Model outputs cascade into a set of formal closure rules derived\nfrom situated evidence and belief axioms and update operations. We empirically\nassess the contribution of each feature type toward successful construction of\ncommon ground relative to ground truth, establishing a benchmark in this novel,\nchallenging task.\n","authors":["Ibrahim Khebour","Kenneth Lai","Mariah Bradford","Yifan Zhu","Richard Brutti","Christopher Tam","Jingxuan Tu","Benjamin Ibarra","Nathaniel Blanchard","Nikhil Krishnaswamy","James Pustejovsky"],"pdf_url":"https://arxiv.org/pdf/2403.17284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17281v1","updated":"2024-03-26T00:09:38Z","published":"2024-03-26T00:09:38Z","title":"Automate Knowledge Concept Tagging on Math Questions with LLMs","summary":"  Knowledge concept tagging for questions plays a crucial role in contemporary\nintelligent educational applications, including learning progress diagnosis,\npractice question recommendations, and course content organization.\nTraditionally, these annotations have been conducted manually with help from\npedagogical experts, as the task requires not only a strong semantic\nunderstanding of both question stems and knowledge definitions but also deep\ninsights into connecting question-solving logic with corresponding knowledge\nconcepts. In this paper, we explore automating the tagging task using Large\nLanguage Models (LLMs), in response to the inability of prior manual methods to\nmeet the rapidly growing demand for concept tagging in questions posed by\nadvanced educational applications. Moreover, the zero/few-shot learning\ncapability of LLMs makes them well-suited for application in educational\nscenarios, which often face challenges in collecting large-scale,\nexpertise-annotated datasets. By conducting extensive experiments with a\nvariety of representative LLMs, we demonstrate that LLMs are a promising tool\nfor concept tagging in math questions. Furthermore, through case studies\nexamining the results from different LLMs, we draw some empirical conclusions\nabout the key factors for success in applying LLMs to the automatic concept\ntagging task.\n","authors":["Hang Li","Tianlong Xu","Jiliang Tang","Qingsong Wen"],"pdf_url":"https://arxiv.org/pdf/2403.17281v1.pdf","comment":"7 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.09887v2","updated":"2024-03-26T23:52:35Z","published":"2024-03-14T21:44:48Z","title":"Sabiá-2: A New Generation of Portuguese Large Language Models","summary":"  We introduce Sabi\\'a-2, a family of large language models trained on\nPortuguese texts. The models are evaluated on a diverse range of exams,\nincluding entry-level tests for Brazilian universities, professional\ncertification exams, and graduate-level exams for various disciplines such as\naccounting, economics, engineering, law and medicine. Our results reveal that\nour best model so far, Sabi\\'a-2 Medium, matches or surpasses GPT-4's\nperformance in 23 out of 64 exams and outperforms GPT-3.5 in 58 out of 64\nexams. Notably, specialization has a significant impact on a model's\nperformance without the need to increase its size, allowing us to offer\nSabi\\'a-2 Medium at a price per token that is 10 times cheaper than GPT-4.\nFinally, we identified that math and coding are key abilities that need\nimprovement.\n","authors":["Thales Sales Almeida","Hugo Abonizio","Rodrigo Nogueira","Ramon Pires"],"pdf_url":"https://arxiv.org/pdf/2403.09887v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18159v1","updated":"2024-03-26T23:51:44Z","published":"2024-03-26T23:51:44Z","title":"Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal\n  Propagation Analysis for Large Language Models","summary":"  Large generative models, such as large language models (LLMs) and diffusion\nmodels have as revolutionized the fields of NLP and computer vision\nrespectively. However, their slow inference, high computation and memory\nrequirement makes it challenging to deploy them on edge devices. In this study,\nwe propose a light-weight quantization aware fine tuning technique using\nknowledge distillation (KD-QAT) to improve the performance of 4-bit weight\nquantized LLMs using commonly available datasets to realize a popular language\nuse case, on device chat applications. To improve this paradigm of finetuning,\nas main contributions, we provide insights into stability of KD-QAT by\nempirically studying the gradient propagation during training to better\nunderstand the vulnerabilities of KD-QAT based approaches to low-bit\nquantization errors. Based on our insights, we propose ov-freeze, a simple\ntechnique to stabilize the KD-QAT process. Finally, we experiment with the\npopular 7B LLaMAv2-Chat model at 4-bit quantization level and demonstrate that\nov-freeze results in near float-point precision performance, i.e., less than\n0.7% loss of accuracy on Commonsense Reasoning benchmarks.\n","authors":["Kartikeya Bhardwaj","Nilesh Prasad Pandey","Sweta Priyadarshi","Kyunggeun Lee","Jun Ma","Harris Teague"],"pdf_url":"https://arxiv.org/pdf/2403.18159v1.pdf","comment":"Accepted at Practical ML for Low Resource Settings Workshop at ICLR\n  2024"},{"id":"http://arxiv.org/abs/2403.18152v1","updated":"2024-03-26T23:32:52Z","published":"2024-03-26T23:32:52Z","title":"Large Language Models as Financial Data Annotators: A Study on\n  Effectiveness and Efficiency","summary":"  Collecting labeled datasets in finance is challenging due to scarcity of\ndomain experts and higher cost of employing them. While Large Language Models\n(LLMs) have demonstrated remarkable performance in data annotation tasks on\ngeneral domain datasets, their effectiveness on domain specific datasets\nremains underexplored. To address this gap, we investigate the potential of\nLLMs as efficient data annotators for extracting relations in financial\ndocuments. We compare the annotations produced by three LLMs (GPT-4, PaLM 2,\nand MPT Instruct) against expert annotators and crowdworkers. We demonstrate\nthat the current state-of-the-art LLMs can be sufficient alternatives to\nnon-expert crowdworkers. We analyze models using various prompts and parameter\nsettings and find that customizing the prompts for each relation group by\nproviding specific examples belonging to those groups is paramount.\nFurthermore, we introduce a reliability index (LLM-RelIndex) used to identify\noutputs that may require expert attention. Finally, we perform an extensive\ntime, cost and error analysis and provide recommendations for the collection\nand usage of automated annotations in domain-specific settings.\n","authors":["Toyin Aguda","Suchetha Siddagangappa","Elena Kochkina","Simerjot Kaur","Dongsheng Wang","Charese Smiley","Sameena Shah"],"pdf_url":"https://arxiv.org/pdf/2403.18152v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18148v1","updated":"2024-03-26T23:14:34Z","published":"2024-03-26T23:14:34Z","title":"Large Language Models Produce Responses Perceived to be Empathic","summary":"  Large Language Models (LLMs) have demonstrated surprising performance on many\ntasks, including writing supportive messages that display empathy. Here, we had\nthese models generate empathic messages in response to posts describing common\nlife experiences, such as workplace situations, parenting, relationships, and\nother anxiety- and anger-eliciting situations. Across two studies (N=192, 202),\nwe showed human raters a variety of responses written by several models (GPT4\nTurbo, Llama2, and Mistral), and had people rate these responses on how\nempathic they seemed to be. We found that LLM-generated responses were\nconsistently rated as more empathic than human-written responses. Linguistic\nanalyses also show that these models write in distinct, predictable ``styles\",\nin terms of their use of punctuation, emojis, and certain words. These results\nhighlight the potential of using LLMs to enhance human peer support in contexts\nwhere empathy is important.\n","authors":["Yoon Kyung Lee","Jina Suh","Hongli Zhan","Junyi Jessy Li","Desmond C. Ong"],"pdf_url":"https://arxiv.org/pdf/2403.18148v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09618v2","updated":"2024-03-26T22:59:52Z","published":"2023-03-16T19:47:41Z","title":"HIVE: Harnessing Human Feedback for Instructional Visual Editing","summary":"  Incorporating human feedback has been shown to be crucial to align text\ngenerated by large language models to human preferences. We hypothesize that\nstate-of-the-art instructional image editing models, where outputs are\ngenerated based on an input image and an editing instruction, could similarly\nbenefit from human feedback, as their outputs may not adhere to the correct\ninstructions and preferences of users. In this paper, we present a novel\nframework to harness human feedback for instructional visual editing (HIVE).\nSpecifically, we collect human feedback on the edited images and learn a reward\nfunction to capture the underlying user preferences. We then introduce scalable\ndiffusion model fine-tuning methods that can incorporate human preferences\nbased on the estimated reward. Besides, to mitigate the bias brought by the\nlimitation of data, we contribute a new 1M training dataset, a 3.6K reward\ndataset for rewards learning, and a 1K evaluation dataset to boost the\nperformance of instructional image editing. We conduct extensive empirical\nexperiments quantitatively and qualitatively, showing that HIVE is favored over\nprevious state-of-the-art instructional image editing approaches by a large\nmargin.\n","authors":["Shu Zhang","Xinyi Yang","Yihao Feng","Can Qin","Chia-Chih Chen","Ning Yu","Zeyuan Chen","Huan Wang","Silvio Savarese","Stefano Ermon","Caiming Xiong","Ran Xu"],"pdf_url":"https://arxiv.org/pdf/2303.09618v2.pdf","comment":"In CVPR, 2024"},{"id":"http://arxiv.org/abs/2310.19055v2","updated":"2024-03-26T22:59:36Z","published":"2023-10-29T16:02:46Z","title":"A Few-Shot Learning Focused Survey on Recent Named Entity Recognition\n  and Relation Classification Methods","summary":"  Named Entity Recognition (NER) and Relation Classification (RC) are important\nsteps in extracting information from unstructured text and formatting it into a\nmachine-readable format. We present a survey of recent deep learning models\nthat address named entity recognition and relation classification, with focus\non few-shot learning performance. Our survey is helpful for researchers in\nknowing the recent techniques in text mining and extracting structured\ninformation from raw text.\n","authors":["Sakher Khalil Alqaaidi","Elika Bozorgi","Afsaneh Shams","Krzysztof Kochut"],"pdf_url":"https://arxiv.org/pdf/2310.19055v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.05882v2","updated":"2024-03-26T22:54:48Z","published":"2023-06-09T13:24:27Z","title":"Good, but not always Fair: An Evaluation of Gender Bias for three\n  commercial Machine Translation Systems","summary":"  Machine Translation (MT) continues to make significant strides in quality and\nis increasingly adopted on a larger scale. Consequently, analyses have been\nredirected to more nuanced aspects, intricate phenomena, as well as potential\nrisks that may arise from the widespread use of MT tools. Along this line, this\npaper offers a meticulous assessment of three commercial MT systems - Google\nTranslate, DeepL, and Modern MT - with a specific focus on gender translation\nand bias. For three language pairs (English/Spanish, English/Italian, and\nEnglish/French), we scrutinize the behavior of such systems at several levels\nof granularity and on a variety of naturally occurring gender phenomena in\ntranslation. Our study takes stock of the current state of online MT tools, by\nrevealing significant discrepancies in the gender translation of the three\nsystems, with each system displaying varying degrees of bias despite their\noverall translation quality.\n","authors":["Silvia Alma Piazzolla","Beatrice Savoldi","Luisa Bentivogli"],"pdf_url":"https://arxiv.org/pdf/2306.05882v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18140v1","updated":"2024-03-26T22:54:12Z","published":"2024-03-26T22:54:12Z","title":"Juru: Legal Brazilian Large Language Model from Reputable Sources","summary":"  The high computational cost associated with pretraining large language models\nlimits their research. Two strategies have emerged to address this issue:\ndomain specialization and pretraining with high-quality data. To explore these\nstrategies, we specialized the Sabi\\'a-2 Small model with 1.9 billion unique\ntokens from reputable Brazilian legal sources and conducted few-shot\nevaluations on legal and general knowledge exams. Our model, Juru, demonstrates\nthe benefits of domain specialization with a reduced amount of pretraining\ndata. However, this specialization comes at the expense of degrading\nperformance in other knowledge areas within the same language. This study\ncontributes to the growing body of scientific evidence showing that pretraining\ndata selection may enhance the performance of large language models, enabling\nthe exploration of these models at a lower cost.\n","authors":["Roseval Malaquias Junior","Ramon Pires","Roseli Romero","Rodrigo Nogueira"],"pdf_url":"https://arxiv.org/pdf/2403.18140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05677v2","updated":"2024-03-26T22:53:56Z","published":"2023-12-09T20:51:48Z","title":"Batched Low-Rank Adaptation of Foundation Models","summary":"  Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning\nfoundation models by incorporating trainable low-rank matrices, thereby\nreducing the number of trainable parameters. While LoRA offers numerous\nadvantages, its applicability for real-time serving to a diverse and global\nuser base is constrained by its incapability to handle multiple task-specific\nadapters efficiently. This imposes a performance bottleneck in scenarios\nrequiring personalized, task-specific adaptations for each incoming request. To\nmitigate this constraint, we introduce Fast LoRA (FLoRA), a framework in which\neach input example in a minibatch can be associated with its unique low-rank\nadaptation weights, allowing for efficient batching of heterogeneous requests.\nWe empirically demonstrate that FLoRA retains the performance merits of LoRA,\nshowcasing competitive results on the MultiPL-E code generation benchmark\nspanning over 8 languages and a multilingual speech recognition task across 6\nlanguages.\n","authors":["Yeming Wen","Swarat Chaudhuri"],"pdf_url":"https://arxiv.org/pdf/2312.05677v2.pdf","comment":"16 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.18125v1","updated":"2024-03-26T22:08:33Z","published":"2024-03-26T22:08:33Z","title":"For those who don't know (how) to ask: Building a dataset of technology\n  questions for digital newcomers","summary":"  While the rise of large language models (LLMs) has created rich new\nopportunities to learn about digital technology, many on the margins of this\ntechnology struggle to gain and maintain competency due to lexical or\nconceptual barriers that prevent them from asking appropriate questions.\nAlthough there have been many efforts to understand factuality of LLM-created\ncontent and ability of LLMs to answer questions, it is not well understood how\nunclear or nonstandard language queries affect the model outputs. We propose\nthe creation of a dataset that captures questions of digital newcomers and\noutsiders, utilizing data we have compiled from a decade's worth of one-on-one\ntutoring. In this paper we lay out our planned efforts and some potential uses\nof this dataset.\n","authors":["Evan Lucas","Kelly S. Steelman","Leo C. Ureel","Charles Wallace"],"pdf_url":"https://arxiv.org/pdf/2403.18125v1.pdf","comment":"Presented at the AI4ED workshop at AAAI 2024"},{"id":"http://arxiv.org/abs/2403.18120v1","updated":"2024-03-26T22:01:13Z","published":"2024-03-26T22:01:13Z","title":"Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with\n  Autoformalization","summary":"  Large language models (LLM), such as Google's Minerva and OpenAI's GPT\nfamilies, are becoming increasingly capable of solving mathematical\nquantitative reasoning problems. However, they still make unjustified logical\nand computational errors in their reasoning steps and answers. In this paper,\nwe leverage the fact that if the training corpus of LLMs contained sufficiently\nmany examples of formal mathematics (e.g. in Isabelle, a formal theorem proving\nenvironment), they can be prompted to translate i.e. autoformalize informal\nmathematical statements into formal Isabelle code -- which can be verified\nautomatically for internal consistency. This provides a mechanism to\nautomatically reject solutions whose formalized versions are inconsistent\nwithin themselves or with the formalized problem statement. We evaluate our\nmethod on GSM8K, MATH and MultiArith datasets and demonstrate that our approach\nprovides a consistently better heuristic than vanilla majority voting -- the\npreviously best method to identify correct answers, by more than 12% on GSM8K.\nIn our experiments it improves results consistently across all datasets and LLM\nmodel sizes. The code can be found at https://github.com/jinpz/dtv.\n","authors":["Jin Peng Zhou","Charles Staats","Wenda Li","Christian Szegedy","Kilian Q. Weinberger","Yuhuai Wu"],"pdf_url":"https://arxiv.org/pdf/2403.18120v1.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.18121v1","updated":"2024-03-26T22:01:13Z","published":"2024-03-26T22:01:13Z","title":"ChatGPT Role-play Dataset: Analysis of User Motives and Model\n  Naturalness","summary":"  Recent advances in interactive large language models like ChatGPT have\nrevolutionized various domains; however, their behavior in natural and\nrole-play conversation settings remains underexplored. In our study, we address\nthis gap by deeply investigating how ChatGPT behaves during conversations in\ndifferent settings by analyzing its interactions in both a normal way and a\nrole-play setting. We introduce a novel dataset of broad range of human-AI\nconversations annotated with user motives and model naturalness to examine (i)\nhow humans engage with the conversational AI model, and (ii) how natural are AI\nmodel responses. Our study highlights the diversity of user motives when\ninteracting with ChatGPT and variable AI naturalness, showing not only the\nnuanced dynamics of natural conversations between humans and AI, but also\nproviding new avenues for improving the effectiveness of human-AI\ncommunication.\n","authors":["Yufei Tao","Ameeta Agrawal","Judit Dombi","Tetyana Sydorenko","Jung In Lee"],"pdf_url":"https://arxiv.org/pdf/2403.18121v1.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2306.03997v3","updated":"2024-03-26T21:32:51Z","published":"2023-06-06T20:19:33Z","title":"Sentiment Analysis in Finance: From Transformers Back to eXplainable\n  Lexicons (XLex)","summary":"  Lexicon-based sentiment analysis (SA) in finance leverages specialized,\nmanually annotated lexicons created by human experts to extract sentiment from\nfinancial texts. Although lexicon-based methods are simple to implement and\nfast to operate on textual data, they require considerable manual annotation\nefforts to create, maintain, and update the lexicons. These methods are also\nconsidered inferior to the deep learning-based approaches, such as transformer\nmodels, which have become dominant in various NLP tasks due to their remarkable\nperformance. However, transformers require extensive data and computational\nresources for both training and testing. Additionally, they involve significant\nprediction times, making them unsuitable for real-time production environments\nor systems with limited processing capabilities. In this paper, we introduce a\nnovel methodology named eXplainable Lexicons (XLex) that combines the\nadvantages of both lexicon-based methods and transformer models. We propose an\napproach that utilizes transformers and SHapley Additive exPlanations (SHAP)\nfor explainability to learn financial lexicons. Our study presents four main\ncontributions. Firstly, we demonstrate that transformer-aided explainable\nlexicons can enhance the vocabulary coverage of the benchmark Loughran-McDonald\n(LM) lexicon, reducing the human involvement in annotating, maintaining, and\nupdating the lexicons. Secondly, we show that the resulting lexicon outperforms\nthe standard LM lexicon in SA of financial datasets. Thirdly, we illustrate\nthat the lexicon-based approach is significantly more efficient in terms of\nmodel speed and size compared to transformers. Lastly, the XLex approach is\ninherently more interpretable than transformer models as lexicon models rely on\npredefined rules, allowing for better insights into the results of SA and\nmaking the XLex approach a viable tool for financial decision-making.\n","authors":["Maryan Rizinski","Hristijan Peshov","Kostadin Mishev","Milos Jovanovik","Dimitar Trajanov"],"pdf_url":"https://arxiv.org/pdf/2306.03997v3.pdf","comment":"Published by IEEE Access DOI: 10.1109/ACCESS.2024.3349970 Link:\n  https://ieeexplore.ieee.org/document/10380556"},{"id":"http://arxiv.org/abs/2403.18105v1","updated":"2024-03-26T21:04:29Z","published":"2024-03-26T21:04:29Z","title":"Large Language Models for Education: A Survey and Outlook","summary":"  The advent of Large Language Models (LLMs) has brought in a new era of\npossibilities in the realm of education. This survey paper summarizes the\nvarious technologies of LLMs in educational settings from multifaceted\nperspectives, encompassing student and teacher assistance, adaptive learning,\nand commercial tools. We systematically review the technological advancements\nin each perspective, organize related datasets and benchmarks, and identify the\nrisks and challenges associated with deploying LLMs in education. Furthermore,\nwe outline future research opportunities, highlighting the potential promising\ndirections. Our survey aims to provide a comprehensive technological picture\nfor educators, researchers, and policymakers to harness the power of LLMs to\nrevolutionize educational practices and foster a more effective personalized\nlearning environment.\n","authors":["Shen Wang","Tianlong Xu","Hang Li","Chaoli Zhang","Joleen Liang","Jiliang Tang","Philip S. Yu","Qingsong Wen"],"pdf_url":"https://arxiv.org/pdf/2403.18105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18098v1","updated":"2024-03-26T20:47:32Z","published":"2024-03-26T20:47:32Z","title":"GPTs and Language Barrier: A Cross-Lingual Legal QA Examination","summary":"  In this paper, we explore the application of Generative Pre-trained\nTransformers (GPTs) in cross-lingual legal Question-Answering (QA) systems\nusing the COLIEE Task 4 dataset. In the COLIEE Task 4, given a statement and a\nset of related legal articles that serve as context, the objective is to\ndetermine whether the statement is legally valid, i.e., if it can be inferred\nfrom the provided contextual articles or not, which is also known as an\nentailment task. By benchmarking four different combinations of English and\nJapanese prompts and data, we provide valuable insights into GPTs' performance\nin multilingual legal QA scenarios, contributing to the development of more\nefficient and accurate cross-lingual QA solutions in the legal domain.\n","authors":["Ha-Thanh Nguyen","Hiroaki Yamada","Ken Satoh"],"pdf_url":"https://arxiv.org/pdf/2403.18098v1.pdf","comment":"NLP 2024, Kobe, Japan"},{"id":"http://arxiv.org/abs/2403.18093v1","updated":"2024-03-26T20:25:53Z","published":"2024-03-26T20:25:53Z","title":"Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large\n  Language Models","summary":"  Large language models with billions of parameters, such as GPT-3.5, GPT-4,\nand LLaMA, are increasingly prevalent. Numerous studies have explored effective\nprompting techniques to harness the power of these LLMs for various research\nproblems. Retrieval, specifically in the legal data domain, poses a challenging\ntask for the direct application of Prompting techniques due to the large number\nand substantial length of legal articles. This research focuses on maximizing\nthe potential of prompting by placing it as the final phase of the retrieval\nsystem, preceded by the support of two phases: BM25 Pre-ranking and BERT-based\nRe-ranking. Experiments on the COLIEE 2023 dataset demonstrate that integrating\nprompting techniques on LLMs into the retrieval system significantly improves\nretrieval accuracy. However, error analysis reveals several existing issues in\nthe retrieval system that still need resolution.\n","authors":["Hai-Long Nguyen","Duc-Minh Nguyen","Tan-Minh Nguyen","Ha-Thanh Nguyen","Thi-Hai-Yen Vuong","Ken Satoh"],"pdf_url":"https://arxiv.org/pdf/2403.18093v1.pdf","comment":"JURISIN 2024"},{"id":"http://arxiv.org/abs/2402.09654v2","updated":"2024-03-26T20:12:18Z","published":"2024-02-15T01:38:50Z","title":"GPT-4's assessment of its performance in a USMLE-based case study","summary":"  This study investigates GPT-4's assessment of its performance in healthcare\napplications. A simple prompting technique was used to prompt the LLM with\nquestions taken from the United States Medical Licensing Examination (USMLE)\nquestionnaire and it was tasked to evaluate its confidence score before posing\nthe question and after asking the question. The questionnaire was categorized\ninto two groups-questions with feedback (WF) and questions with no feedback(NF)\npost-question. The model was asked to provide absolute and relative confidence\nscores before and after each question. The experimental findings were analyzed\nusing statistical tools to study the variability of confidence in WF and NF\ngroups. Additionally, a sequential analysis was conducted to observe the\nperformance variation for the WF and NF groups. Results indicate that feedback\ninfluences relative confidence but doesn't consistently increase or decrease\nit. Understanding the performance of LLM is paramount in exploring its utility\nin sensitive areas like healthcare. This study contributes to the ongoing\ndiscourse on the reliability of AI, particularly of LLMs like GPT-4, within\nhealthcare, offering insights into how feedback mechanisms might be optimized\nto enhance AI-assisted medical education and decision support.\n","authors":["Uttam Dhakal","Aniket Kumar Singh","Suman Devkota","Yogesh Sapkota","Bishal Lamichhane","Suprinsa Paudyal","Chandra Dhakal"],"pdf_url":"https://arxiv.org/pdf/2402.09654v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18063v1","updated":"2024-03-26T19:29:21Z","published":"2024-03-26T19:29:21Z","title":"Spectral Convolutional Transformer: Harmonizing Real vs. Complex\n  Multi-View Spectral Operators for Vision Transformer","summary":"  Transformers used in vision have been investigated through diverse\narchitectures - ViT, PVT, and Swin. These have worked to improve the attention\nmechanism and make it more efficient. Differently, the need for including local\ninformation was felt, leading to incorporating convolutions in transformers\nsuch as CPVT and CvT. Global information is captured using a complex Fourier\nbasis to achieve global token mixing through various methods, such as AFNO,\nGFNet, and Spectformer. We advocate combining three diverse views of data -\nlocal, global, and long-range dependence. We also investigate the simplest\nglobal representation using only the real domain spectral representation -\nobtained through the Hartley transform. We use a convolutional operator in the\ninitial layers to capture local information. Through these two contributions,\nwe are able to optimize and obtain a spectral convolution transformer (SCT)\nthat provides improved performance over the state-of-the-art methods while\nreducing the number of parameters. Through extensive experiments, we show that\nSCT-C-small gives state-of-the-art performance on the ImageNet dataset and\nreaches 84.5\\% top-1 accuracy, while SCT-C-Large reaches 85.9\\% and SCT-C-Huge\nreaches 86.4\\%. We evaluate SCT on transfer learning on datasets such as\nCIFAR-10, CIFAR-100, Oxford Flower, and Stanford Car. We also evaluate SCT on\ndownstream tasks i.e. instance segmentation on the MSCOCO dataset. The project\npage is available on this webpage.\\url{https://github.com/badripatro/sct}\n","authors":["Badri N. Patro","Vinay P. Namboodiri","Vijay S. Agneeswaran"],"pdf_url":"https://arxiv.org/pdf/2403.18063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.12492v2","updated":"2024-03-26T19:28:15Z","published":"2024-01-23T05:20:35Z","title":"Comparing Pre-trained Human Language Models: Is it Better with Human\n  Context as Groups, Individual Traits, or Both?","summary":"  Incorporating human context into language models is the next frontier for\nhuman-centered natural language processing. Currently, two pre-training methods\nexist: group-wise attributes (e.g., over-45-year-olds) or individual traits.\nGroup attributes are coarse -- not all 45-year-olds write the same way -- while\nmodeling individual traits allows for a more personalized representation, but\nrequires more complex modeling and data. So far, it is unclear which\npre-training approach benefits what tasks. We compare pre-training models with\nhuman context via 1) group attributes, 2) individual users, and 3) a combined\napproach on 5 user- and document-level tasks. We find that pre-training with\nboth group and individual features significantly improves the two user-level\nregression tasks like age estimation and personality assessment. Pre-training\non individual users significantly improves the three document-level\nclassification tasks like stance and topic detection. It even does well for\ndownstream tasks without historical user data. Our results suggest both\napproaches have specific use cases, opening new avenues for human-centered\nlanguage modeling.\n","authors":["Nikita Soni","Niranjan Balasubramanian","H. Andrew Schwartz","Dirk Hovy"],"pdf_url":"https://arxiv.org/pdf/2401.12492v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18058v1","updated":"2024-03-26T19:24:18Z","published":"2024-03-26T19:24:18Z","title":"COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning","summary":"  Recently, there have been significant advancements in large language models\n(LLMs), particularly focused on the English language. These advancements have\nenabled these LLMs to understand and execute complex instructions with\nunprecedented accuracy and fluency. However, despite these advancements, there\nremains a noticeable gap in the development of Chinese instruction tuning. The\nunique linguistic features and cultural depth of the Chinese language pose\nchallenges for instruction tuning tasks. Existing datasets are either derived\nfrom English-centric LLMs or are ill-suited for aligning with the interaction\npatterns of real-world Chinese users. To bridge this gap, we introduce\nCOIG-CQIA, a high-quality Chinese instruction tuning dataset. Our aim is to\nbuild a diverse, wide-ranging instruction-tuning dataset to better align model\nbehavior with human interactions. To this end, we collect a high-quality\nhuman-written corpus from various sources on the Chinese Internet, including\nQ&A communities, Wikis, examinations, and existing NLP datasets. This corpus\nwas rigorously filtered and carefully processed to form the COIG-CQIA dataset.\nFurthermore, we train models of various scales on different subsets of CQIA,\nfollowing in-depth evaluation and analyses. The findings from our experiments\noffer valuable insights for selecting and developing Chinese instruction-tuning\ndatasets. We also find that models trained on CQIA-Subset achieve competitive\nresults in human assessment as well as knowledge and security benchmarks. Data\nare available at https://huggingface.co/datasets/m-a-p/COIG-CQIA\n","authors":["Yuelin Bai","Xinrun Du","Yiming Liang","Yonggang Jin","Ziqiang Liu","Junting Zhou","Tianyu Zheng","Xincheng Zhang","Nuo Ma","Zekun Wang","Ruibin Yuan","Haihong Wu","Hongquan Lin","Wenhao Huang","Jiajun Zhang","Wenhu Chen","Chenghua Lin","Jie Fu","Min Yang","Shiwen Ni","Ge Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.18058v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18051v1","updated":"2024-03-26T19:08:20Z","published":"2024-03-26T19:08:20Z","title":"Supervisory Prompt Training","summary":"  The performance of Large Language Models (LLMs) relies heavily on the quality\nof prompts, which are often manually engineered and task-specific, making them\ncostly and non-scalable. We propose a novel approach, Supervisory Prompt\nTraining (SPT). SPT automates the generation of highly effective prompts using\na dual LLM system. In this system, one LLM, the generator, performs a task\nwhile the other, the corrector, provides feedback and generates improved\nprompts. In contrast to earlier techniques, both the generator and corrector\ncollaboratively and continuously improve their prompts over time. We also\nintroduce the concept of \\textit{impact scores} to measure the sentence-level\neffectiveness of the prompts. Our method was tested on four benchmarks, testing\nthe level of hallucinations in LLMs. Notably, we were able to increase the\naccuracy of GPT-4 on GSM8K from 65.8\\% to 94.1\\% (28.3\\% increase). SPT\nadvances LLMs by refining prompts to enhance performance and reduce\nhallucinations, offering an efficient and scalable alternative to traditional\nmodel fine-tuning.\n","authors":["Jean Ghislain Billa","Min Oh","Liang Du"],"pdf_url":"https://arxiv.org/pdf/2403.18051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18031v1","updated":"2024-03-26T18:38:14Z","published":"2024-03-26T18:38:14Z","title":"The Impact of Syntactic and Semantic Proximity on Machine Translation\n  with Back-Translation","summary":"  Unsupervised on-the-fly back-translation, in conjunction with multilingual\npretraining, is the dominant method for unsupervised neural machine\ntranslation. Theoretically, however, the method should not work in general. We\ntherefore conduct controlled experiments with artificial languages to determine\nwhat properties of languages make back-translation an effective training\nmethod, covering lexical, syntactic, and semantic properties. We find, contrary\nto popular belief, that (i) parallel word frequency distributions, (ii)\npartially shared vocabulary, and (iii) similar syntactic structure across\nlanguages are not sufficient to explain the success of back-translation. We\nshow however that even crude semantic signal (similar lexical fields across\nlanguages) does improve alignment of two languages through back-translation. We\nconjecture that rich semantic dependencies, parallel across languages, are at\nthe root of the success of unsupervised methods based on back-translation.\nOverall, the success of unsupervised machine translation was far from being\nanalytically guaranteed. Instead, it is another proof that languages of the\nworld share deep similarities, and we hope to show how to identify which of\nthese similarities can serve the development of unsupervised, cross-linguistic\ntools.\n","authors":["Nicolas Guerin","Shane Steinert-Threlkeld","Emmanuel Chemla"],"pdf_url":"https://arxiv.org/pdf/2403.18031v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10438v6","updated":"2024-03-26T18:36:31Z","published":"2022-11-18T18:59:33Z","title":"SmoothQuant: Accurate and Efficient Post-Training Quantization for Large\n  Language Models","summary":"  Large language models (LLMs) show excellent performance but are compute- and\nmemory-intensive. Quantization can reduce memory and accelerate inference.\nHowever, existing methods cannot maintain accuracy and hardware efficiency at\nthe same time. We propose SmoothQuant, a training-free, accuracy-preserving,\nand general-purpose post-training quantization (PTQ) solution to enable 8-bit\nweight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that\nweights are easy to quantize while activations are not, SmoothQuant smooths the\nactivation outliers by offline migrating the quantization difficulty from\nactivations to weights with a mathematically equivalent transformation.\nSmoothQuant enables an INT8 quantization of both weights and activations for\nall the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG,\nLlama-1/2, Falcon, Mistral, and Mixtral models. We demonstrate up to 1.56x\nspeedup and 2x memory reduction for LLMs with negligible loss in accuracy.\nSmoothQuant enables serving 530B LLM within a single node. Our work offers a\nturn-key solution that reduces hardware costs and democratizes LLMs. Code is\navailable at https://github.com/mit-han-lab/smoothquant.\n","authors":["Guangxuan Xiao","Ji Lin","Mickael Seznec","Hao Wu","Julien Demouth","Song Han"],"pdf_url":"https://arxiv.org/pdf/2211.10438v6.pdf","comment":"ICML 2023. First two authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2403.18025v1","updated":"2024-03-26T18:23:16Z","published":"2024-03-26T18:23:16Z","title":"Improving Pre-trained Language Model Sensitivity via Mask Specific\n  losses: A case study on Biomedical NER","summary":"  Adapting language models (LMs) to novel domains is often achieved through\nfine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning\nintroduces new knowledge into an LM, enabling it to comprehend and efficiently\nperform a target domain task. Fine-tuning can however be inadvertently\ninsensitive if it ignores the wide array of disparities (e.g in word meaning)\nbetween source and target domains. For instance, words such as chronic and\npressure may be treated lightly in social conversations, however, clinically,\nthese words are usually an expression of concern. To address insensitive\nfine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach\nthat efficiently acquires target domain knowledge by appropriately weighting\nthe importance of domain-specific terms (DS-terms) during fine-tuning. MSLM\njointly masks DS-terms and generic words, then learns mask-specific losses by\nensuring LMs incur larger penalties for inaccurately predicting DS-terms\ncompared to generic words. Results of our analysis show that MSLM improves LMs\nsensitivity and detection of DS-terms. We empirically show that an optimal\nmasking rate not only depends on the LM, but also on the dataset and the length\nof sequences. Our proposed masking strategy outperforms advanced masking\nstrategies such as span- and PMI-based masking.\n","authors":["Micheal Abaho","Danushka Bollegala","Gary Leeming","Dan Joyce","Iain E Buchan"],"pdf_url":"https://arxiv.org/pdf/2403.18025v1.pdf","comment":"Paper alrerady accepted for publishing by the NAACL 2024 conference\n  (main conference paper)"},{"id":"http://arxiv.org/abs/2403.18024v1","updated":"2024-03-26T18:22:05Z","published":"2024-03-26T18:22:05Z","title":"Enriching Word Usage Graphs with Cluster Definitions","summary":"  We present a dataset of word usage graphs (WUGs), where the existing WUGs for\nmultiple languages are enriched with cluster labels functioning as sense\ndefinitions. They are generated from scratch by fine-tuned encoder-decoder\nlanguage models. The conducted human evaluation has shown that these\ndefinitions match the existing clusters in WUGs better than the definitions\nchosen from WordNet by two baseline systems. At the same time, the method is\nstraightforward to use and easy to extend to new languages. The resulting\nenriched datasets can be extremely helpful for moving on to explainable\nsemantic change modeling.\n","authors":["Mariia Fedorova","Andrey Kutuzov","Nikolay Arefyev","Dominik Schlechtweg"],"pdf_url":"https://arxiv.org/pdf/2403.18024v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.14814v2","updated":"2024-03-26T18:10:10Z","published":"2024-03-21T19:59:52Z","title":"The opportunities and risks of large language models in mental health","summary":"  Global rates of mental health concerns are rising and there is increasing\nrealization that existing models of mental healthcare will not adequately\nexpand to meet the demand. With the emergence of large language models (LLMs)\nhas come great optimism regarding their promise to create novel, large-scale\nsolutions to support mental health. Despite their nascence, LLMs have already\nbeen applied to mental health-related tasks. In this review, we summarize the\nextant literature on efforts to use LLMs to provide mental health education,\nassessment, and intervention and highlight key opportunities for positive\nimpact in each area. We then highlight risks associated with LLMs application\nto mental health and encourage adoption of strategies to mitigate these risks.\nThe urgent need for mental health support must be balanced with responsible\ndevelopment, testing, and deployment of mental health LLMs. Especially critical\nis ensuring that mental health LLMs are fine-tuned for mental health, enhance\nmental health equity, adhere to ethical standards, and that people, including\nthose with lived experience with mental health concerns, are involved in all\nstages from development through deployment. Prioritizing these efforts will\nminimize potential harms to mental health and maximize the likelihood that LLMs\nwill positively impact mental health globally.\n","authors":["Hannah R. Lawrence","Renee A. Schneider","Susan B. Rubin","Maja J. Mataric","Daniel J. McDuff","Megan Jones Bell"],"pdf_url":"https://arxiv.org/pdf/2403.14814v2.pdf","comment":"12 pages, 2 tables, 4 figures"},{"id":"http://arxiv.org/abs/2403.18018v1","updated":"2024-03-26T18:07:10Z","published":"2024-03-26T18:07:10Z","title":"DORE: A Dataset For Portuguese Definition Generation","summary":"  Definition modelling (DM) is the task of automatically generating a\ndictionary definition for a specific word. Computational systems that are\ncapable of DM can have numerous applications benefiting a wide range of\naudiences. As DM is considered a supervised natural language generation\nproblem, these systems require large annotated datasets to train the machine\nlearning (ML) models. Several DM datasets have been released for English and\nother high-resource languages. While Portuguese is considered a\nmid/high-resource language in most natural language processing tasks and is\nspoken by more than 200 million native speakers, there is no DM dataset\navailable for Portuguese. In this research, we fill this gap by introducing\nDORE; the first dataset for Definition MOdelling for PoRtuguEse containing more\nthan 100,000 definitions. We also evaluate several deep learning based DM\nmodels on DORE and report the results. The dataset and the findings of this\npaper will facilitate research and study of Portuguese in wider contexts.\n","authors":["Anna Beatriz Dimas Furtado","Tharindu Ranasinghe","Frédéric Blain","Ruslan Mitkov"],"pdf_url":"https://arxiv.org/pdf/2403.18018v1.pdf","comment":"Accepted to LREC-COLING 2024 (The 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation)"},{"id":"http://arxiv.org/abs/2305.14718v4","updated":"2024-03-26T18:07:01Z","published":"2023-05-24T04:42:17Z","title":"Leftover-Lunch: Advantage-based Offline Reinforcement Learning for\n  Language Models","summary":"  Reinforcement Learning with Human Feedback (RLHF) is the most prominent\nmethod for Language Model (LM) alignment. However, RLHF is an unstable and\ndata-hungry process that continually requires new high-quality LM-generated\ndata for finetuning. We introduce Advantage-Leftover Lunch RL (A-LoL), a new\nclass of offline policy gradient algorithms that enable RL training on any\npre-existing data. By assuming the entire LM output sequence as a single\naction, A-LoL allows incorporating sequence-level classifiers or human-designed\nscoring functions as rewards. Subsequently, by using LM's value estimate, A-LoL\nonly trains on positive advantage (leftover) data points, making it resilient\nto noise. Overall, A-LoL is an easy-to-implement, sample-efficient, and stable\nLM training recipe.\n  We demonstrate the effectiveness of A-LoL and its variants with a set of four\ndifferent language generation tasks. We compare against both online RL (PPO)\nand recent preference-based (DPO, PRO) and reward-based (GOLD) offline RL\nbaselines. On the commonly-used RLHF benchmark, Helpful and Harmless Assistant\n(HHA), LMs trained with A-LoL methods achieve the highest diversity while also\nbeing rated more safe and helpful than the baselines according to humans.\nAdditionally, in the remaining three tasks, A-LoL could optimize multiple\ndistinct reward functions even when using noisy or suboptimal training data.\n  We also release our experimental code. https://github.com/abaheti95/LoL-RL\n","authors":["Ashutosh Baheti","Ximing Lu","Faeze Brahman","Ronan Le Bras","Maarten Sap","Mark Riedl"],"pdf_url":"https://arxiv.org/pdf/2305.14718v4.pdf","comment":"published at ICLR 2024"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2403.17937v1","updated":"2024-03-26T17:59:58Z","published":"2024-03-26T17:59:58Z","title":"Efficient Video Object Segmentation via Modulated Cross-Attention Memory","summary":"  Recently, transformer-based approaches have shown promising results for\nsemi-supervised video object segmentation. However, these approaches typically\nstruggle on long videos due to increased GPU memory demands, as they frequently\nexpand the memory bank every few frames. We propose a transformer-based\napproach, named MAVOS, that introduces an optimized and dynamic long-term\nmodulated cross-attention (MCA) memory to model temporal smoothness without\nrequiring frequent memory expansion. The proposed MCA effectively encodes both\nlocal and global features at various levels of granularity while efficiently\nmaintaining consistent speed regardless of the video length. Extensive\nexperiments on multiple benchmarks, LVOS, Long-Time Video, and DAVIS 2017,\ndemonstrate the effectiveness of our proposed contributions leading to\nreal-time inference and markedly reduced memory demands without any degradation\nin segmentation accuracy on long videos. Compared to the best existing\ntransformer-based approach, our MAVOS increases the speed by 7.6x, while\nsignificantly reducing the GPU memory by 87% with comparable segmentation\nperformance on short and long video datasets. Notably on the LVOS dataset, our\nMAVOS achieves a J&F score of 63.3% while operating at 37 frames per second\n(FPS) on a single V100 GPU. Our code and models will be publicly available at:\nhttps://github.com/Amshaker/MAVOS.\n","authors":["Abdelrahman Shaker","Syed Talal Wasim","Martin Danelljan","Salman Khan","Ming-Hsuan Yang","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2403.17937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17936v1","updated":"2024-03-26T17:59:52Z","published":"2024-03-26T17:59:52Z","title":"ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture\n  Synthesis","summary":"  Gestures play a key role in human communication. Recent methods for co-speech\ngesture generation, while managing to generate beat-aligned motions, struggle\ngenerating gestures that are semantically aligned with the utterance. Compared\nto beat gestures that align naturally to the audio signal, semantically\ncoherent gestures require modeling the complex interactions between the\nlanguage and human motion, and can be controlled by focusing on certain words.\nTherefore, we present ConvoFusion, a diffusion-based approach for multi-modal\ngesture synthesis, which can not only generate gestures based on multi-modal\nspeech inputs, but can also facilitate controllability in gesture synthesis.\nOur method proposes two guidance objectives that allow the users to modulate\nthe impact of different conditioning modalities (e.g. audio vs text) as well as\nto choose certain words to be emphasized during gesturing. Our method is\nversatile in that it can be trained either for generating monologue gestures or\neven the conversational gestures. To further advance the research on\nmulti-party interactive gestures, the DnD Group Gesture dataset is released,\nwhich contains 6 hours of gesture data showing 5 people interacting with one\nanother. We compare our method with several recent works and demonstrate\neffectiveness of our method on a variety of tasks. We urge the reader to watch\nour supplementary video at our website.\n","authors":["Muhammad Hamza Mughal","Rishabh Dabral","Ikhsanul Habibie","Lucia Donatelli","Marc Habermann","Christian Theobalt"],"pdf_url":"https://arxiv.org/pdf/2403.17936v1.pdf","comment":"CVPR 2024. Project Page:\n  https://vcai.mpi-inf.mpg.de/projects/ConvoFusion/"},{"id":"http://arxiv.org/abs/2403.17935v1","updated":"2024-03-26T17:59:24Z","published":"2024-03-26T17:59:24Z","title":"OmniVid: A Generative Framework for Universal Video Understanding","summary":"  The core of video understanding tasks, such as recognition, captioning, and\ntracking, is to automatically detect objects or actions in a video and analyze\ntheir temporal evolution. Despite sharing a common goal, different tasks often\nrely on distinct model architectures and annotation formats. In contrast,\nnatural language processing benefits from a unified output space, i.e., text\nsequences, which simplifies the training of powerful foundational language\nmodels, such as GPT-3, with extensive training corpora. Inspired by this, we\nseek to unify the output space of video understanding tasks by using languages\nas labels and additionally introducing time and box tokens. In this way, a\nvariety of video tasks could be formulated as video-grounded token generation.\nThis enables us to address various types of video tasks, including\nclassification (such as action recognition), captioning (covering clip\ncaptioning, video question answering, and dense video captioning), and\nlocalization tasks (such as visual object tracking) within a fully shared\nencoder-decoder architecture, following a generative framework. Through\ncomprehensive experiments, we demonstrate such a simple and straightforward\nidea is quite effective and can achieve state-of-the-art or competitive results\non seven video benchmarks, providing a novel perspective for more universal\nvideo understanding. Code is available at https://github.com/wangjk666/OmniVid.\n","authors":["Junke Wang","Dongdong Chen","Chong Luo","Bo He","Lu Yuan","Zuxuan Wu","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.17935v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17934v1","updated":"2024-03-26T17:59:23Z","published":"2024-03-26T17:59:23Z","title":"AiOS: All-in-One-Stage Expressive Human Pose and Shape Estimation","summary":"  Expressive human pose and shape estimation (a.k.a. 3D whole-body mesh\nrecovery) involves the human body, hand, and expression estimation. Most\nexisting methods have tackled this task in a two-stage manner, first detecting\nthe human body part with an off-the-shelf detection model and inferring the\ndifferent human body parts individually. Despite the impressive results\nachieved, these methods suffer from 1) loss of valuable contextual information\nvia cropping, 2) introducing distractions, and 3) lacking inter-association\namong different persons and body parts, inevitably causing performance\ndegradation, especially for crowded scenes. To address these issues, we\nintroduce a novel all-in-one-stage framework, AiOS, for multiple expressive\nhuman pose and shape recovery without an additional human detection step.\nSpecifically, our method is built upon DETR, which treats multi-person\nwhole-body mesh recovery task as a progressive set prediction problem with\nvarious sequential detection. We devise the decoder tokens and extend them to\nour task. Specifically, we first employ a human token to probe a human location\nin the image and encode global features for each instance, which provides a\ncoarse location for the later transformer block. Then, we introduce a\njoint-related token to probe the human joint in the image and encoder a\nfine-grained local feature, which collaborates with the global feature to\nregress the whole-body mesh. This straightforward but effective model\noutperforms previous state-of-the-art methods by a 9% reduction in NMVE on\nAGORA, a 30% reduction in PVE on EHF, a 10% reduction in PVE on ARCTIC, and a\n3% reduction in PVE on EgoBody.\n","authors":["Qingping Sun","Yanjun Wang","Ailing Zeng","Wanqi Yin","Chen Wei","Wenjia Wang","Haiyi Mei","Chi Sing Leung","Ziwei Liu","Lei Yang","Zhongang Cai"],"pdf_url":"https://arxiv.org/pdf/2403.17934v1.pdf","comment":"Homepage: https://ttxskk.github.io/AiOS/"},{"id":"http://arxiv.org/abs/2403.17933v1","updated":"2024-03-26T17:58:29Z","published":"2024-03-26T17:58:29Z","title":"SLEDGE: Synthesizing Simulation Environments for Driving Agents with\n  Generative Models","summary":"  SLEDGE is the first generative simulator for vehicle motion planning trained\non real-world driving logs. Its core component is a learned model that is able\nto generate agent bounding boxes and lane graphs. The model's outputs serve as\nan initial state for traffic simulation. The unique properties of the entities\nto be generated for SLEDGE, such as their connectivity and variable count per\nscene, render the naive application of most modern generative models to this\ntask non-trivial. Therefore, together with a systematic study of existing lane\ngraph representations, we introduce a novel raster-to-vector autoencoder\n(RVAE). It encodes agents and the lane graph into distinct channels in a\nrasterized latent map. This facilitates both lane-conditioned agent generation\nand combined generation of lanes and agents with a Diffusion Transformer. Using\ngenerated entities in SLEDGE enables greater control over the simulation, e.g.\nupsampling turns or increasing traffic density. Further, SLEDGE can support\n500m long routes, a capability not found in existing data-driven simulators\nlike nuPlan. It presents new challenges for planning algorithms, evidenced by\nfailure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge,\nwhen tested on hard routes and dense traffic generated by our model. Compared\nto nuPlan, SLEDGE requires 500$\\times$ less storage to set up (<4GB), making it\na more accessible option and helping with democratizing future research in this\nfield.\n","authors":["Kashyap Chitta","Daniel Dauner","Andreas Geiger"],"pdf_url":"https://arxiv.org/pdf/2403.17933v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17931v1","updated":"2024-03-26T17:58:22Z","published":"2024-03-26T17:58:22Z","title":"Track Everything Everywhere Fast and Robustly","summary":"  We propose a novel test-time optimization approach for efficiently and\nrobustly tracking any pixel at any time in a video. The latest state-of-the-art\noptimization-based tracking technique, OmniMotion, requires a prohibitively\nlong optimization time, rendering it impractical for downstream applications.\nOmniMotion is sensitive to the choice of random seeds, leading to unstable\nconvergence. To improve efficiency and robustness, we introduce a novel\ninvertible deformation network, CaDeX++, which factorizes the function\nrepresentation into a local spatial-temporal feature grid and enhances the\nexpressivity of the coupling blocks with non-linear functions. While CaDeX++\nincorporates a stronger geometric bias within its architectural design, it also\ntakes advantage of the inductive bias provided by the vision foundation models.\nOur system utilizes monocular depth estimation to represent scene geometry and\nenhances the objective by incorporating DINOv2 long-term semantics to regulate\nthe optimization process. Our experiments demonstrate a substantial improvement\nin training speed (more than \\textbf{10 times} faster), robustness, and\naccuracy in tracking over the SoTA optimization-based method OmniMotion.\n","authors":["Yunzhou Song","Jiahui Lei","Ziyun Wang","Lingjie Liu","Kostas Daniilidis"],"pdf_url":"https://arxiv.org/pdf/2403.17931v1.pdf","comment":"project page: https://timsong412.github.io/FastOmniTrack/"},{"id":"http://arxiv.org/abs/2403.17929v1","updated":"2024-03-26T17:58:07Z","published":"2024-03-26T17:58:07Z","title":"Towards Explaining Hypercomplex Neural Networks","summary":"  Hypercomplex neural networks are gaining increasing interest in the deep\nlearning community. The attention directed towards hypercomplex models\noriginates from several aspects, spanning from purely theoretical and\nmathematical characteristics to the practical advantage of lightweight models\nover conventional networks, and their unique properties to capture both global\nand local relations. In particular, a branch of these architectures,\nparameterized hypercomplex neural networks (PHNNs), has also gained popularity\ndue to their versatility across a multitude of application domains.\nNonetheless, only few attempts have been made to explain or interpret their\nintricacies. In this paper, we propose inherently interpretable PHNNs and\nquaternion-like networks, thus without the need for any post-hoc method. To\nachieve this, we define a type of cosine-similarity transform within the\nparameterized hypercomplex domain. This PHB-cos transform induces weight\nalignment with relevant input features and allows to reduce the model into a\nsingle linear transform, rendering it directly interpretable. In this work, we\nstart to draw insights into how this unique branch of neural models operates.\nWe observe that hypercomplex networks exhibit a tendency to concentrate on the\nshape around the main object of interest, in addition to the shape of the\nobject itself. We provide a thorough analysis, studying single neurons of\ndifferent layers and comparing them against how real-valued networks learn. The\ncode of the paper is available at https://github.com/ispamm/HxAI.\n","authors":["Eleonora Lopez","Eleonora Grassucci","Debora Capriotti","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2403.17929v1.pdf","comment":"The paper has been accepted at IEEE WCCI 2024"},{"id":"http://arxiv.org/abs/2403.17926v1","updated":"2024-03-26T17:57:20Z","published":"2024-03-26T17:57:20Z","title":"FastCAR: Fast Classification And Regression Multi-Task Learning via Task\n  Consolidation for Modelling a Continuous Property Variable of Object Classes","summary":"  FastCAR is a novel task consolidation approach in Multi-Task Learning (MTL)\nfor a classification and a regression task, despite task heterogeneity with\nonly subtle correlation. It addresses object classification and continuous\nproperty variable regression, a crucial use case in science and engineering.\nFastCAR involves a labeling transformation approach that can be used with a\nsingle-task regression network architecture. FastCAR outperforms traditional\nMTL model families, parametrized in the landscape of architecture and loss\nweighting schemes, when learning of both tasks are collectively considered\n(classification accuracy of 99.54%, regression mean absolute percentage error\nof 2.3%). The experiments performed used an Advanced Steel Property dataset\ncontributed by us. The dataset comprises 4536 images of 224x224 pixels,\nannotated with object classes and hardness properties that take continuous\nvalues. With the labeling transformation and single-task regression network\narchitecture, FastCAR achieves reduced latency and time efficiency.\n","authors":["Anoop Kini","Andreas Jansche","Timo Bernthaler","Gerhard Schneider"],"pdf_url":"https://arxiv.org/pdf/2403.17926v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17924v1","updated":"2024-03-26T17:57:05Z","published":"2024-03-26T17:57:05Z","title":"AID: Attention Interpolation of Text-to-Image Diffusion","summary":"  Conditional diffusion models can create unseen images in various settings,\naiding image interpolation. Interpolation in latent spaces is well-studied, but\ninterpolation with specific conditions like text or poses is less understood.\nSimple approaches, such as linear interpolation in the space of conditions,\noften result in images that lack consistency, smoothness, and fidelity. To that\nend, we introduce a novel training-free technique named Attention Interpolation\nvia Diffusion (AID). Our key contributions include 1) proposing an inner/outer\ninterpolated attention layer; 2) fusing the interpolated attention with\nself-attention to boost fidelity; and 3) applying beta distribution to\nselection to increase smoothness. We also present a variant, Prompt-guided\nAttention Interpolation via Diffusion (PAID), that considers interpolation as a\ncondition-dependent generative process. This method enables the creation of new\nimages with greater consistency, smoothness, and efficiency, and offers control\nover the exact path of interpolation. Our approach demonstrates effectiveness\nfor conceptual and spatial interpolation. Code and demo are available at\nhttps://github.com/QY-H00/attention-interpolation-diffusion.\n","authors":["Qiyuan He","Jinghao Wang","Ziwei Liu","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2403.17924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17920v1","updated":"2024-03-26T17:55:11Z","published":"2024-03-26T17:55:11Z","title":"TC4D: Trajectory-Conditioned Text-to-4D Generation","summary":"  Recent techniques for text-to-4D generation synthesize dynamic 3D scenes\nusing supervision from pre-trained text-to-video models. However, existing\nrepresentations for motion, such as deformation models or time-dependent neural\nrepresentations, are limited in the amount of motion they can generate-they\ncannot synthesize motion extending far beyond the bounding box used for volume\nrendering. The lack of a more flexible motion model contributes to the gap in\nrealism between 4D generation methods and recent, near-photorealistic video\ngeneration models. Here, we propose TC4D: trajectory-conditioned text-to-4D\ngeneration, which factors motion into global and local components. We represent\nthe global motion of a scene's bounding box using rigid transformation along a\ntrajectory parameterized by a spline. We learn local deformations that conform\nto the global trajectory using supervision from a text-to-video model. Our\napproach enables the synthesis of scenes animated along arbitrary trajectories,\ncompositional scene generation, and significant improvements to the realism and\namount of generated motion, which we evaluate qualitatively and through a user\nstudy. Video results can be viewed on our website:\nhttps://sherwinbahmani.github.io/tc4d.\n","authors":["Sherwin Bahmani","Xian Liu","Yifan Wang","Ivan Skorokhodov","Victor Rong","Ziwei Liu","Xihui Liu","Jeong Joon Park","Sergey Tulyakov","Gordon Wetzstein","Andrea Tagliasacchi","David B. Lindell"],"pdf_url":"https://arxiv.org/pdf/2403.17920v1.pdf","comment":"Project Page: https://sherwinbahmani.github.io/tc4d"},{"id":"http://arxiv.org/abs/2403.17916v1","updated":"2024-03-26T17:53:27Z","published":"2024-03-26T17:53:27Z","title":"CMP: Cooperative Motion Prediction with Multi-Agent Communication","summary":"  The confluence of the advancement of Autonomous Vehicles (AVs) and the\nmaturity of Vehicle-to-Everything (V2X) communication has enabled the\ncapability of cooperative connected and automated vehicles (CAVs). Building on\ntop of cooperative perception, this paper explores the feasibility and\neffectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR\nsignals as input to enhance tracking and prediction capabilities. Unlike\nprevious work that focuses separately on either cooperative perception or\nmotion prediction, our framework, to the best of our knowledge, is the first to\naddress the unified problem where CAVs share information in both perception and\nprediction modules. Incorporated into our design is the unique capability to\ntolerate realistic V2X bandwidth limitations and transmission delays, while\ndealing with bulky perception representations. We also propose a prediction\naggregation module, which unifies the predictions obtained by different CAVs\nand generates the final prediction. Through extensive experiments and ablation\nstudies, we demonstrate the effectiveness of our method in cooperative\nperception, tracking, and motion prediction tasks. In particular, CMP reduces\nthe average prediction error by 17.2\\% with fewer missing detections compared\nwith the no cooperation setting. Our work marks a significant step forward in\nthe cooperative capabilities of CAVs, showcasing enhanced performance in\ncomplex scenarios.\n","authors":["Zhuoyuan Wu","Yuping Wang","Hengbo Ma","Zhaowei Li","Hang Qiu","Jiachen Li"],"pdf_url":"https://arxiv.org/pdf/2403.17916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17915v1","updated":"2024-03-26T17:52:23Z","published":"2024-03-26T17:52:23Z","title":"Leveraging Near-Field Lighting for Monocular Depth Estimation from\n  Endoscopy Videos","summary":"  Monocular depth estimation in endoscopy videos can enable assistive and\nrobotic surgery to obtain better coverage of the organ and detection of various\nhealth issues. Despite promising progress on mainstream, natural image depth\nestimation, techniques perform poorly on endoscopy images due to a lack of\nstrong geometric features and challenging illumination effects. In this paper,\nwe utilize the photometric cues, i.e., the light emitted from an endoscope and\nreflected by the surface, to improve monocular depth estimation. We first\ncreate two novel loss functions with supervised and self-supervised variants\nthat utilize a per-pixel shading representation. We then propose a novel depth\nrefinement network (PPSNet) that leverages the same per-pixel shading\nrepresentation. Finally, we introduce teacher-student transfer learning to\nproduce better depth maps from both synthetic data with supervision and\nclinical data with self-supervision. We achieve state-of-the-art results on the\nC3VD dataset while estimating high-quality depth maps from clinical data. Our\ncode, pre-trained models, and supplementary materials can be found on our\nproject page: https://ppsnet.github.io/\n","authors":["Akshay Paruchuri","Samuel Ehrenstein","Shuxian Wang","Inbar Fried","Stephen M. Pizer","Marc Niethammer","Roni Sengupta"],"pdf_url":"https://arxiv.org/pdf/2403.17915v1.pdf","comment":"26 pages, 7 tables, 7 figures"},{"id":"http://arxiv.org/abs/2403.17909v1","updated":"2024-03-26T17:46:25Z","published":"2024-03-26T17:46:25Z","title":"ELGC-Net: Efficient Local-Global Context Aggregation for Remote Sensing\n  Change Detection","summary":"  Deep learning has shown remarkable success in remote sensing change detection\n(CD), aiming to identify semantic change regions between co-registered\nsatellite image pairs acquired at distinct time stamps. However, existing\nconvolutional neural network and transformer-based frameworks often struggle to\naccurately segment semantic change regions. Moreover, transformers-based\nmethods with standard self-attention suffer from quadratic computational\ncomplexity with respect to the image resolution, making them less practical for\nCD tasks with limited training data. To address these issues, we propose an\nefficient change detection framework, ELGC-Net, which leverages rich contextual\ninformation to precisely estimate change regions while reducing the model size.\nOur ELGC-Net comprises a Siamese encoder, fusion modules, and a decoder. The\nfocus of our design is the introduction of an Efficient Local-Global Context\nAggregator module within the encoder, capturing enhanced global context and\nlocal spatial information through a novel pooled-transpose (PT) attention and\ndepthwise convolution, respectively. The PT attention employs pooling\noperations for robust feature extraction and minimizes computational cost with\ntransposed attention. Extensive experiments on three challenging CD datasets\ndemonstrate that ELGC-Net outperforms existing methods. Compared to the recent\ntransformer-based CD approach (ChangeFormer), ELGC-Net achieves a 1.4% gain in\nintersection over union metric on the LEVIR-CD dataset, while significantly\nreducing trainable parameters. Our proposed ELGC-Net sets a new\nstate-of-the-art performance in remote sensing change detection benchmarks.\nFinally, we also introduce ELGC-Net-LW, a lighter variant with significantly\nreduced computational complexity, suitable for resource-constrained settings,\nwhile achieving comparable performance. Project url\nhttps://github.com/techmn/elgcnet.\n","authors":["Mubashir Noman","Mustansar Fiaz","Hisham Cholakkal","Salman Khan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2403.17909v1.pdf","comment":"accepted at IEEE TGRS"},{"id":"http://arxiv.org/abs/2403.17905v1","updated":"2024-03-26T17:45:06Z","published":"2024-03-26T17:45:06Z","title":"Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2","summary":"  We propose a new approach for non-Cartesian magnetic resonance image\nreconstruction. While unrolled architectures provide robustness via\ndata-consistency layers, embedding measurement operators in Deep Neural Network\n(DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP)\napproaches, where the denoising DNNs are blind to the measurement setting, are\nnot affected by this limitation and have also proven effective, but their\nhighly iterative nature also affects scalability. To address this scalability\nchallenge, we leverage the \"Residual-to-Residual DNN series for high-Dynamic\nrange imaging (R2D2)\" approach recently introduced in astronomical imaging.\nR2D2's reconstruction is formed as a series of residual images, iteratively\nestimated as outputs of DNNs taking the previous iteration's image estimate and\nassociated data residual as inputs. The method can be interpreted as a learned\nversion of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation,\nconsidering radial k-space sampling acquisition sequences. Our preliminary\nresults suggest that R2D2 achieves: (i) suboptimal performance compared to its\nunrolled incarnation R2D2-Net, which is however non-scalable due to the\nnecessary embedding of NUFFT-based data-consistency layers; (ii) superior\nreconstruction quality to a scalable version of R2D2-Net embedding an FFT-based\napproximation for data consistency; (iii) superior reconstruction quality to\nPnP, while only requiring few iterations.\n","authors":["Chen Yiwei","Tang Chao","Aghabiglou Amir","Chu Chung San","Wiaux Yves"],"pdf_url":"https://arxiv.org/pdf/2403.17905v1.pdf","comment":"submitted to IEEE EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2403.17902v1","updated":"2024-03-26T17:43:15Z","published":"2024-03-26T17:43:15Z","title":"Serpent: Scalable and Efficient Image Restoration via Multi-scale\n  Structured State Space Models","summary":"  The landscape of computational building blocks of efficient image restoration\narchitectures is dominated by a combination of convolutional processing and\nvarious attention mechanisms. However, convolutional filters are inherently\nlocal and therefore struggle at modeling long-range dependencies in images. On\nthe other hand, attention excels at capturing global interactions between\narbitrary image regions, however at a quadratic cost in image dimension. In\nthis work, we propose Serpent, an architecture that leverages recent advances\nin state space models (SSMs) in its core computational block. SSMs, originally\nintroduced for sequence modeling, can maintain a global receptive field with a\nfavorable linear scaling in input size. Our preliminary results demonstrate\nthat Serpent can achieve reconstruction quality on par with state-of-the-art\ntechniques, while requiring orders of magnitude less compute (up to $150$ fold\nreduction in FLOPS) and a factor of up to $5\\times$ less GPU memory while\nmaintaining a compact model size.\n","authors":["Mohammad Shahab Sepehri","Zalan Fabian","Mahdi Soltanolkotabi"],"pdf_url":"https://arxiv.org/pdf/2403.17902v1.pdf","comment":"7 pages, 5 figures, preliminary workshop submission of a\n  comprehensive work to be released soon"},{"id":"http://arxiv.org/abs/2307.16897v2","updated":"2024-03-26T17:40:47Z","published":"2023-07-31T17:59:48Z","title":"DiVa-360: The Dynamic Visual Dataset for Immersive Neural Fields","summary":"  Advances in neural fields are enabling high-fidelity capture of the shape and\nappearance of dynamic 3D scenes. However, their capabilities lag behind those\noffered by conventional representations such as 2D videos because of\nalgorithmic challenges and the lack of large-scale multi-view real-world\ndatasets. We address the dataset limitation with DiVa-360, a real-world 360\ndynamic visual dataset that contains synchronized high-resolution and\nlong-duration multi-view video sequences of table-scale scenes captured using a\ncustomized low-cost system with 53 cameras. It contains 21 object-centric\nsequences categorized by different motion types, 25 intricate hand-object\ninteraction sequences, and 8 long-duration sequences for a total of 17.4 M\nimage frames. In addition, we provide foreground-background segmentation masks,\nsynchronized audio, and text descriptions. We benchmark the state-of-the-art\ndynamic neural field methods on DiVa-360 and provide insights about existing\nmethods and future challenges on long-duration neural field capture.\n","authors":["Cheng-You Lu","Peisen Zhou","Angela Xing","Chandradeep Pokhariya","Arnab Dey","Ishaan Shah","Rugved Mavidipalli","Dylan Hu","Andrew Comport","Kefan Chen","Srinath Sridhar"],"pdf_url":"https://arxiv.org/pdf/2307.16897v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17898v1","updated":"2024-03-26T17:39:36Z","published":"2024-03-26T17:39:36Z","title":"Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D\n  Gaussians","summary":"  The recent 3D Gaussian splatting (3D-GS) has shown remarkable rendering\nfidelity and efficiency compared to NeRF-based neural scene representations.\nWhile demonstrating the potential for real-time rendering, 3D-GS encounters\nrendering bottlenecks in large scenes with complex details due to an excessive\nnumber of Gaussian primitives located within the viewing frustum. This\nlimitation is particularly noticeable in zoom-out views and can lead to\ninconsistent rendering speeds in scenes with varying details. Moreover, it\noften struggles to capture the corresponding level of details at different\nscales with its heuristic density control operation. Inspired by the\nLevel-of-Detail (LOD) techniques, we introduce Octree-GS, featuring an\nLOD-structured 3D Gaussian approach supporting level-of-detail decomposition\nfor scene representation that contributes to the final rendering results. Our\nmodel dynamically selects the appropriate level from the set of\nmulti-resolution anchor points, ensuring consistent rendering performance with\nadaptive LOD adjustments while maintaining high-fidelity rendering results.\n","authors":["Kerui Ren","Lihan Jiang","Tao Lu","Mulin Yu","Linning Xu","Zhangkai Ni","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2403.17898v1.pdf","comment":"Project page: https://city-super.github.io/octree-gs/"},{"id":"http://arxiv.org/abs/2403.17893v1","updated":"2024-03-26T17:29:26Z","published":"2024-03-26T17:29:26Z","title":"A Survey on 3D Egocentric Human Pose Estimation","summary":"  Egocentric human pose estimation aims to estimate human body poses and\ndevelop body representations from a first-person camera perspective. It has\ngained vast popularity in recent years because of its wide range of\napplications in sectors like XR-technologies, human-computer interaction, and\nfitness tracking. However, to the best of our knowledge, there is no systematic\nliterature review based on the proposed solutions regarding egocentric 3D human\npose estimation. To that end, the aim of this survey paper is to provide an\nextensive overview of the current state of egocentric pose estimation research.\nIn this paper, we categorize and discuss the popular datasets and the different\npose estimation models, highlighting the strengths and weaknesses of different\nmethods by comparative analysis. This survey can be a valuable resource for\nboth researchers and practitioners in the field, offering insights into key\nconcepts and cutting-edge solutions in egocentric pose estimation, its\nwide-ranging applications, as well as the open problems with future scope.\n","authors":["Md Mushfiqur Azam","Kevin Desai"],"pdf_url":"https://arxiv.org/pdf/2403.17893v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17888v1","updated":"2024-03-26T17:21:24Z","published":"2024-03-26T17:21:24Z","title":"2D Gaussian Splatting for Geometrically Accurate Radiance Fields","summary":"  3D Gaussian Splatting (3DGS) has recently revolutionized radiance field\nreconstruction, achieving high quality novel view synthesis and fast rendering\nspeed without baking. However, 3DGS fails to accurately represent surfaces due\nto the multi-view inconsistent nature of 3D Gaussians. We present 2D Gaussian\nSplatting (2DGS), a novel approach to model and reconstruct geometrically\naccurate radiance fields from multi-view images. Our key idea is to collapse\nthe 3D volume into a set of 2D oriented planar Gaussian disks. Unlike 3D\nGaussians, 2D Gaussians provide view-consistent geometry while modeling\nsurfaces intrinsically. To accurately recover thin surfaces and achieve stable\noptimization, we introduce a perspective-accurate 2D splatting process\nutilizing ray-splat intersection and rasterization. Additionally, we\nincorporate depth distortion and normal consistency terms to further enhance\nthe quality of the reconstructions. We demonstrate that our differentiable\nrenderer allows for noise-free and detailed geometry reconstruction while\nmaintaining competitive appearance quality, fast training speed, and real-time\nrendering. Our code will be made publicly available.\n","authors":["Binbin Huang","Zehao Yu","Anpei Chen","Andreas Geiger","Shenghua Gao"],"pdf_url":"https://arxiv.org/pdf/2403.17888v1.pdf","comment":"12 pages, 12 figures"},{"id":"http://arxiv.org/abs/2403.17884v1","updated":"2024-03-26T17:16:04Z","published":"2024-03-26T17:16:04Z","title":"Sen2Fire: A Challenging Benchmark Dataset for Wildfire Detection using\n  Sentinel Data","summary":"  Utilizing satellite imagery for wildfire detection presents substantial\npotential for practical applications. To advance the development of machine\nlearning algorithms in this domain, our study introduces the \\textit{Sen2Fire}\ndataset--a challenging satellite remote sensing dataset tailored for wildfire\ndetection. This dataset is curated from Sentinel-2 multi-spectral data and\nSentinel-5P aerosol product, comprising a total of 2466 image patches. Each\npatch has a size of 512$\\times$512 pixels with 13 bands. Given the distinctive\nsensitivities of various wavebands to wildfire responses, our research focuses\non optimizing wildfire detection by evaluating different wavebands and\nemploying a combination of spectral indices, such as normalized burn ratio\n(NBR) and normalized difference vegetation index (NDVI). The results suggest\nthat, in contrast to using all bands for wildfire detection, selecting specific\nband combinations yields superior performance. Additionally, our study\nunderscores the positive impact of integrating Sentinel-5 aerosol data for\nwildfire detection. The code and dataset are available online\n(https://zenodo.org/records/10881058).\n","authors":["Yonghao Xu","Amanda Berg","Leif Haglund"],"pdf_url":"https://arxiv.org/pdf/2403.17884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02640v3","updated":"2024-03-26T17:14:14Z","published":"2024-03-05T04:08:19Z","title":"HoloVIC: Large-scale Dataset and Benchmark for Multi-Sensor Holographic\n  Intersection and Vehicle-Infrastructure Cooperative","summary":"  Vehicle-to-everything (V2X) is a popular topic in the field of Autonomous\nDriving in recent years. Vehicle-infrastructure cooperation (VIC) becomes one\nof the important research area. Due to the complexity of traffic conditions\nsuch as blind spots and occlusion, it greatly limits the perception\ncapabilities of single-view roadside sensing systems. To further enhance the\naccuracy of roadside perception and provide better information to the vehicle\nside, in this paper, we constructed holographic intersections with various\nlayouts to build a large-scale multi-sensor holographic vehicle-infrastructure\ncooperation dataset, called HoloVIC. Our dataset includes 3 different types of\nsensors (Camera, Lidar, Fisheye) and employs 4 sensor-layouts based on the\ndifferent intersections. Each intersection is equipped with 6-18 sensors to\ncapture synchronous data. While autonomous vehicles pass through these\nintersections for collecting VIC data. HoloVIC contains in total on 100k+\nsynchronous frames from different sensors. Additionally, we annotated 3D\nbounding boxes based on Camera, Fisheye, and Lidar. We also associate the IDs\nof the same objects across different devices and consecutive frames in\nsequence. Based on HoloVIC, we formulated four tasks to facilitate the\ndevelopment of related research. We also provide benchmarks for these tasks.\n","authors":["Cong Ma","Lei Qiao","Chengkai Zhu","Kai Liu","Zelong Kong","Qing Li","Xueqi Zhou","Yuheng Kan","Wei Wu"],"pdf_url":"https://arxiv.org/pdf/2403.02640v3.pdf","comment":"Accept to CVPR 2024, Benchmark Website: https://holovic.net"},{"id":"http://arxiv.org/abs/2403.17883v1","updated":"2024-03-26T17:13:17Z","published":"2024-03-26T17:13:17Z","title":"Superior and Pragmatic Talking Face Generation with Teacher-Student\n  Framework","summary":"  Talking face generation technology creates talking videos from arbitrary\nappearance and motion signal, with the \"arbitrary\" offering ease of use but\nalso introducing challenges in practical applications. Existing methods work\nwell with standard inputs but suffer serious performance degradation with\nintricate real-world ones. Moreover, efficiency is also an important concern in\ndeployment. To comprehensively address these issues, we introduce SuperFace, a\nteacher-student framework that balances quality, robustness, cost and\neditability. We first propose a simple but effective teacher model capable of\nhandling inputs of varying qualities to generate high-quality results. Building\non this, we devise an efficient distillation strategy to acquire an\nidentity-specific student model that maintains quality with significantly\nreduced computational load. Our experiments validate that SuperFace offers a\nmore comprehensive solution than existing methods for the four mentioned\nobjectives, especially in reducing FLOPs by 99\\% with the student model.\nSuperFace can be driven by both video and audio and allows for localized facial\nattributes editing.\n","authors":["Chao Liang","Jianwen Jiang","Tianyun Zhong","Gaojie Lin","Zhengkun Rong","Jiaqi Yang","Yongming Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.17883v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17881v1","updated":"2024-03-26T17:12:34Z","published":"2024-03-26T17:12:34Z","title":"Deepfake Generation and Detection: A Benchmark and Survey","summary":"  In addition to the advancements in deepfake generation, corresponding\ndetection technologies need to continuously evolve to regulate the potential\nmisuse of deepfakes, such as for privacy invasion and phishing attacks. This\nsurvey comprehensively reviews the latest developments in deepfake generation\nand detection, summarizing and analyzing the current state of the art in this\nrapidly evolving field. We first unify task definitions, comprehensively\nintroduce datasets and metrics, and discuss the development of generation and\ndetection technology frameworks. Then, we discuss the development of several\nrelated sub-fields and focus on researching four mainstream deepfake fields:\npopular face swap, face reenactment, talking face generation, and facial\nattribute editing, as well as foreign detection. Subsequently, we\ncomprehensively benchmark representative methods on popular datasets for each\nfield, fully evaluating the latest and influential works published in top\nconferences/journals. Finally, we analyze the challenges and future research\ndirections of the discussed fields. We closely follow the latest developments\nin https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection.\n","authors":["Gan Pei","Jiangning Zhang","Menghan Hu","Guangtao Zhai","Chengjie Wang","Zhenyu Zhang","Jian Yang","Chunhua Shen","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2403.17881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17879v1","updated":"2024-03-26T17:11:51Z","published":"2024-03-26T17:11:51Z","title":"Low-Latency Neural Stereo Streaming","summary":"  The rise of new video modalities like virtual reality or autonomous driving\nhas increased the demand for efficient multi-view video compression methods,\nboth in terms of rate-distortion (R-D) performance and in terms of delay and\nruntime. While most recent stereo video compression approaches have shown\npromising performance, they compress left and right views sequentially, leading\nto poor parallelization and runtime performance. This work presents Low-Latency\nneural codec for Stereo video Streaming (LLSS), a novel parallel stereo video\ncoding method designed for fast and efficient low-latency stereo video\nstreaming. Instead of using a sequential cross-view motion compensation like\nexisting methods, LLSS introduces a bidirectional feature shifting module to\ndirectly exploit mutual information among views and encode them effectively\nwith a joint cross-view prior model for entropy coding. Thanks to this design,\nLLSS processes left and right views in parallel, minimizing latency; all while\nsubstantially improving R-D performance compared to both existing neural and\nconventional codecs.\n","authors":["Qiqi Hou","Farzad Farhadzadeh","Amir Said","Guillaume Sautiere","Hoang Le"],"pdf_url":"https://arxiv.org/pdf/2403.17879v1.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.17870v1","updated":"2024-03-26T16:57:55Z","published":"2024-03-26T16:57:55Z","title":"Boosting Diffusion Models with Moving Average Sampling in Frequency\n  Domain","summary":"  Diffusion models have recently brought a powerful revolution in image\ngeneration. Despite showing impressive generative capabilities, most of these\nmodels rely on the current sample to denoise the next one, possibly resulting\nin denoising instability. In this paper, we reinterpret the iterative denoising\nprocess as model optimization and leverage a moving average mechanism to\nensemble all the prior samples. Instead of simply applying moving average to\nthe denoised samples at different timesteps, we first map the denoised samples\nto data space and then perform moving average to avoid distribution shift\nacross timesteps. In view that diffusion models evolve the recovery from\nlow-frequency components to high-frequency details, we further decompose the\nsamples into different frequency components and execute moving average\nseparately on each component. We name the complete approach \"Moving Average\nSampling in Frequency domain (MASF)\". MASF could be seamlessly integrated into\nmainstream pre-trained diffusion models and sampling schedules. Extensive\nexperiments on both unconditional and conditional diffusion models demonstrate\nthat our MASF leads to superior performances compared to the baselines, with\nalmost negligible additional complexity cost.\n","authors":["Yurui Qian","Qi Cai","Yingwei Pan","Yehao Li","Ting Yao","Qibin Sun","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2403.17870v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17869v1","updated":"2024-03-26T16:57:33Z","published":"2024-03-26T16:57:33Z","title":"To Supervise or Not to Supervise: Understanding and Addressing the Key\n  Challenges of 3D Transfer Learning","summary":"  Transfer learning has long been a key factor in the advancement of many\nfields including 2D image analysis. Unfortunately, its applicability in 3D data\nprocessing has been relatively limited. While several approaches for 3D\ntransfer learning have been proposed in recent literature, with contrastive\nlearning gaining particular prominence, most existing methods in this domain\nhave only been studied and evaluated in limited scenarios. Most importantly,\nthere is currently a lack of principled understanding of both when and why 3D\ntransfer learning methods are applicable. Remarkably, even the applicability of\nstandard supervised pre-training is poorly understood. In this work, we conduct\nthe first in-depth quantitative and qualitative investigation of supervised and\ncontrastive pre-training strategies and their utility in downstream 3D tasks.\nWe demonstrate that layer-wise analysis of learned features provides\nsignificant insight into the downstream utility of trained networks. Informed\nby this analysis, we propose a simple geometric regularization strategy, which\nimproves the transferability of supervised pre-training. Our work thus sheds\nlight onto both the specific challenges of 3D transfer learning, as well as\nstrategies to overcome them.\n","authors":["Souhail Hadgi","Lei Li","Maks Ovsjanikov"],"pdf_url":"https://arxiv.org/pdf/2403.17869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17846v1","updated":"2024-03-26T16:36:43Z","published":"2024-03-26T16:36:43Z","title":"Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot\n  Navigation","summary":"  Recent open-vocabulary robot mapping methods enrich dense geometric maps with\npre-trained visual-language features. While these maps allow for the prediction\nof point-wise saliency maps when queried for a certain language concept,\nlarge-scale environments and abstract queries beyond the object level still\npose a considerable hurdle, ultimately limiting language-grounded robotic\nnavigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D\nscene graph mapping approach for language-grounded robot navigation. Leveraging\nopen-vocabulary vision foundation models, we first obtain state-of-the-art\nopen-vocabulary segment-level maps in 3D and subsequently construct a 3D scene\ngraph hierarchy consisting of floor, room, and object concepts, each enriched\nwith open-vocabulary features. Our approach is able to represent multi-story\nbuildings and allows robotic traversal of those using a cross-floor Voronoi\ngraph. HOV-SG is evaluated on three distinct datasets and surpasses previous\nbaselines in open-vocabulary semantic accuracy on the object, room, and floor\nlevel while producing a 75% reduction in representation size compared to dense\nopen-vocabulary maps. In order to prove the efficacy and generalization\ncapabilities of HOV-SG, we showcase successful long-horizon\nlanguage-conditioned robot navigation within real-world multi-storage\nenvironments. We provide code and trial video data at http://hovsg.github.io/.\n","authors":["Abdelrhman Werby","Chenguang Huang","Martin Büchner","Abhinav Valada","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2403.17846v1.pdf","comment":"Code and video are available at http://hovsg.github.io/"},{"id":"http://arxiv.org/abs/2401.06003v2","updated":"2024-03-26T16:30:20Z","published":"2024-01-11T16:06:36Z","title":"TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering","summary":"  Point-based radiance field rendering has demonstrated impressive results for\nnovel view synthesis, offering a compelling blend of rendering quality and\ncomputational efficiency. However, also latest approaches in this domain are\nnot without their shortcomings. 3D Gaussian Splatting [Kerbl and Kopanas et al.\n2023] struggles when tasked with rendering highly detailed scenes, due to\nblurring and cloudy artifacts. On the other hand, ADOP [R\\\"uckert et al. 2022]\ncan accommodate crisper images, but the neural reconstruction network decreases\nperformance, it grapples with temporal instability and it is unable to\neffectively address large gaps in the point cloud.\n  In this paper, we present TRIPS (Trilinear Point Splatting), an approach that\ncombines ideas from both Gaussian Splatting and ADOP. The fundamental concept\nbehind our novel technique involves rasterizing points into a screen-space\nimage pyramid, with the selection of the pyramid layer determined by the\nprojected point size. This approach allows rendering arbitrarily large points\nusing a single trilinear write. A lightweight neural network is then used to\nreconstruct a hole-free image including detail beyond splat resolution.\nImportantly, our render pipeline is entirely differentiable, allowing for\nautomatic optimization of both point sizes and positions.\n  Our evaluation demonstrate that TRIPS surpasses existing state-of-the-art\nmethods in terms of rendering quality while maintaining a real-time frame rate\nof 60 frames per second on readily available hardware. This performance extends\nto challenging scenarios, such as scenes featuring intricate geometry,\nexpansive landscapes, and auto-exposed footage.\n  The project page is located at: https://lfranke.github.io/trips/\n","authors":["Linus Franke","Darius Rückert","Laura Fink","Marc Stamminger"],"pdf_url":"https://arxiv.org/pdf/2401.06003v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17839v1","updated":"2024-03-26T16:27:37Z","published":"2024-03-26T16:27:37Z","title":"ReMamber: Referring Image Segmentation with Mamba Twister","summary":"  Referring Image Segmentation (RIS) leveraging transformers has achieved great\nsuccess on the interpretation of complex visual-language tasks. However, the\nquadratic computation cost makes it resource-consuming in capturing long-range\nvisual-language dependencies. Fortunately, Mamba addresses this with efficient\nlinear complexity in processing. However, directly applying Mamba to\nmulti-modal interactions presents challenges, primarily due to inadequate\nchannel interactions for the effective fusion of multi-modal data. In this\npaper, we propose ReMamber, a novel RIS architecture that integrates the power\nof Mamba with a multi-modal Mamba Twister block. The Mamba Twister explicitly\nmodels image-text interaction, and fuses textual and visual features through\nits unique channel and spatial twisting mechanism. We achieve the\nstate-of-the-art on three challenging benchmarks. Moreover, we conduct thorough\nanalyses of ReMamber and discuss other fusion designs using Mamba. These\nprovide valuable perspectives for future research.\n","authors":["Yuhuan Yang","Chaofan Ma","Jiangchao Yao","Zhun Zhong","Ya Zhang","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.17839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17837v1","updated":"2024-03-26T16:24:42Z","published":"2024-03-26T16:24:42Z","title":"GTA-HDR: A Large-Scale Synthetic Dataset for HDR Image Reconstruction","summary":"  High Dynamic Range (HDR) content (i.e., images and videos) has a broad range\nof applications. However, capturing HDR content from real-world scenes is\nexpensive and time- consuming. Therefore, the challenging task of\nreconstructing visually accurate HDR images from their Low Dynamic Range (LDR)\ncounterparts is gaining attention in the vision research community. A major\nchallenge in this research problem is the lack of datasets, which capture\ndiverse scene conditions (e.g., lighting, shadows, weather, locations,\nlandscapes, objects, humans, buildings) and various image features (e.g.,\ncolor, contrast, saturation, hue, luminance, brightness, radiance). To address\nthis gap, in this paper, we introduce GTA-HDR, a large-scale synthetic dataset\nof photo-realistic HDR images sampled from the GTA-V video game. We perform\nthorough evaluation of the proposed dataset, which demonstrates significant\nqualitative and quantitative improvements of the state-of-the-art HDR image\nreconstruction methods. Furthermore, we demonstrate the effectiveness of the\nproposed dataset and its impact on additional computer vision tasks including\n3D human pose estimation, human body part segmentation, and holistic scene\nsegmentation. The dataset, data collection pipeline, and evaluation code are\navailable at: https://github.com/HrishavBakulBarua/GTA-HDR.\n","authors":["Hrishav Bakul Barua","Kalin Stefanov","KokSheik Wong","Abhinav Dhall","Ganesh Krishnasamy"],"pdf_url":"https://arxiv.org/pdf/2403.17837v1.pdf","comment":"Submitted to IEEE"},{"id":"http://arxiv.org/abs/2403.17834v1","updated":"2024-03-26T16:19:56Z","published":"2024-03-26T16:19:56Z","title":"A foundation model utilizing chest CT volumes and radiology reports for\n  supervised-level zero-shot detection of abnormalities","summary":"  A major challenge in computational research in 3D medical imaging is the lack\nof comprehensive datasets. Addressing this issue, our study introduces CT-RATE,\nthe first 3D medical imaging dataset that pairs images with textual reports.\nCT-RATE consists of 25,692 non-contrast chest CT volumes, expanded to 50,188\nthrough various reconstructions, from 21,304 unique patients, along with\ncorresponding radiology text reports. Leveraging CT-RATE, we developed CT-CLIP,\na CT-focused contrastive language-image pre-training framework. As a versatile,\nself-supervised model, CT-CLIP is designed for broad application and does not\nrequire task-specific training. Remarkably, CT-CLIP outperforms\nstate-of-the-art, fully supervised methods in multi-abnormality detection\nacross all key metrics, thus eliminating the need for manual annotation. We\nalso demonstrate its utility in case retrieval, whether using imagery or\ntextual queries, thereby advancing knowledge dissemination. The open-source\nrelease of CT-RATE and CT-CLIP marks a significant advancement in medical AI,\nenhancing 3D imaging analysis and fostering innovation in healthcare.\n","authors":["Ibrahim Ethem Hamamci","Sezgin Er","Furkan Almas","Ayse Gulnihan Simsek","Sevval Nil Esirgun","Irem Dogan","Muhammed Furkan Dasdelen","Bastian Wittmann","Enis Simsar","Mehmet Simsar","Emine Bensu Erdemir","Abdullah Alanbay","Anjany Sekuboyina","Berkan Lafci","Mehmet K. Ozdemir","Bjoern Menze"],"pdf_url":"https://arxiv.org/pdf/2403.17834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.13969v3","updated":"2024-03-26T16:13:26Z","published":"2021-08-31T16:51:00Z","title":"Semi-Supervised Crowd Counting from Unlabeled Data","summary":"  Automatic Crowd behavior analysis can be applied to effectively help the\ndaily transportation statistics and planning, which helps the smart city\nconstruction. As one of the most important keys, crowd counting has drawn\nincreasing attention. Recent works achieved promising performance but relied on\nthe supervised paradigm with expensive crowd annotations. To alleviate the\nannotation cost in real-world transportation scenarios, in this work we\nproposed a semi-supervised learning framework $S^{4}\\textit{Crowd}$, which can\nleverage both unlabeled/labeled data for robust crowd counting. In the\nunsupervised pathway, two \\textit{self-supervised losses} were proposed to\nsimulate the crowd variations such as scale, illumination, based on which\nsupervised information pseudo labels were generated and gradually refined. We\nalso proposed a crowd-driven recurrent unit \\textit{Gated-Crowd-Recurrent-Unit\n(GCRU)}, which can preserve discriminant crowd information by extracting\nsecond-order statistics, yielding pseudo labels with improved quality. A joint\nloss including both unsupervised/supervised information was proposed, and a\ndynamic weighting strategy was employed to balance the importance of the\nunsupervised loss and supervised loss at different training stages. We\nconducted extensive experiments on four popular crowd counting datasets in\nsemi-supervised settings. Experimental results supported the effectiveness of\neach proposed component in our $S^{4}$Crowd framework. Our method achieved\ncompetitive performance in semi-supervised learning approaches on these crowd\ncounting datasets.\n","authors":["Haoran Duan","Fan Wan","Rui Sun","Zeyu Wang","Varun Ojha","Yu Guan","Hubert P. H. Shum","Bingzhang Hu","Yang Long"],"pdf_url":"https://arxiv.org/pdf/2108.13969v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17830v1","updated":"2024-03-26T16:10:21Z","published":"2024-03-26T16:10:21Z","title":"Assessment of Multimodal Large Language Models in Alignment with Human\n  Values","summary":"  Large Language Models (LLMs) aim to serve as versatile assistants aligned\nwith human values, as defined by the principles of being helpful, honest, and\nharmless (hhh). However, in terms of Multimodal Large Language Models (MLLMs),\ndespite their commendable performance in perception and reasoning tasks, their\nalignment with human values remains largely unexplored, given the complexity of\ndefining hhh dimensions in the visual world and the difficulty in collecting\nrelevant data that accurately mirrors real-world situations. To address this\ngap, we introduce Ch3Ef, a Compreh3ensive Evaluation dataset and strategy for\nassessing alignment with human expectations. Ch3Ef dataset contains 1002\nhuman-annotated data samples, covering 12 domains and 46 tasks based on the hhh\nprinciple. We also present a unified evaluation strategy supporting assessment\nacross various scenarios and different perspectives. Based on the evaluation\nresults, we summarize over 10 key findings that deepen the understanding of\nMLLM capabilities, limitations, and the dynamic relationships between\nevaluation levels, guiding future advancements in the field.\n","authors":["Zhelun Shi","Zhipin Wang","Hongxing Fan","Zaibin Zhang","Lijun Li","Yongting Zhang","Zhenfei Yin","Lu Sheng","Yu Qiao","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2403.17830v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2311.02692"},{"id":"http://arxiv.org/abs/2403.17827v1","updated":"2024-03-26T16:06:42Z","published":"2024-03-26T16:06:42Z","title":"DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from\n  Textual Descriptions","summary":"  Generating natural hand-object interactions in 3D is challenging as the\nresulting hand and object motions are expected to be physically plausible and\nsemantically meaningful. Furthermore, generalization to unseen objects is\nhindered by the limited scale of available hand-object interaction datasets. We\npropose DiffH2O, a novel method to synthesize realistic, one or two-handed\nobject interactions from provided text prompts and geometry of the object. The\nmethod introduces three techniques that enable effective learning from limited\ndata. First, we decompose the task into a grasping stage and a text-based\ninteraction stage and use separate diffusion models for each. In the grasping\nstage, the model only generates hand motions, whereas in the interaction phase\nboth hand and object poses are synthesized. Second, we propose a compact\nrepresentation that tightly couples hand and object poses. Third, we propose\ntwo different guidance schemes to allow more control of the generated motions:\ngrasp guidance and detailed textual guidance. Grasp guidance takes a single\ntarget grasping pose and guides the diffusion model to reach this grasp at the\nend of the grasping stage, which provides control over the grasping pose. Given\na grasping motion from this stage, multiple different actions can be prompted\nin the interaction phase. For textual guidance, we contribute comprehensive\ntext descriptions to the GRAB dataset and show that they enable our method to\nhave more fine-grained control over hand-object interactions. Our quantitative\nand qualitative evaluation demonstrates that the proposed method outperforms\nbaseline methods and leads to natural hand-object motions. Moreover, we\ndemonstrate the practicality of our framework by utilizing a hand pose estimate\nfrom an off-the-shelf pose estimator for guidance, and then sampling multiple\ndifferent actions in the interaction stage.\n","authors":["Sammy Christen","Shreyas Hampali","Fadime Sener","Edoardo Remelli","Tomas Hodan","Eric Sauser","Shugao Ma","Bugra Tekin"],"pdf_url":"https://arxiv.org/pdf/2403.17827v1.pdf","comment":"Project Page: https://diffh2o.github.io/"},{"id":"http://arxiv.org/abs/2403.17823v1","updated":"2024-03-26T16:04:19Z","published":"2024-03-26T16:04:19Z","title":"Efficient Image Pre-Training with Siamese Cropped Masked Autoencoders","summary":"  Self-supervised pre-training of image encoders is omnipresent in the\nliterature, particularly following the introduction of Masked autoencoders\n(MAE). Current efforts attempt to learn object-centric representations from\nmotion in videos. In particular, SiamMAE recently introduced a Siamese network,\ntraining a shared-weight encoder from two frames of a video with a high\nasymmetric masking ratio (95%). In this work, we propose CropMAE, an\nalternative approach to the Siamese pre-training introduced by SiamMAE. Our\nmethod specifically differs by exclusively considering pairs of cropped images\nsourced from the same image but cropped differently, deviating from the\nconventional pairs of frames extracted from a video. CropMAE therefore\nalleviates the need for video datasets, while maintaining competitive\nperformances and drastically reducing pre-training time. Furthermore, we\ndemonstrate that CropMAE learns similar object-centric representations without\nexplicit motion, showing that current self-supervised learning methods do not\nlearn objects from motion, but rather thanks to the Siamese architecture.\nFinally, CropMAE achieves the highest masking ratio to date (98.5%), enabling\nthe reconstruction of images using only two visible patches. Our code is\navailable at https://github.com/alexandre-eymael/CropMAE.\n","authors":["Alexandre Eymaël","Renaud Vandeghen","Anthony Cioppa","Silvio Giancola","Bernard Ghanem","Marc Van Droogenbroeck"],"pdf_url":"https://arxiv.org/pdf/2403.17823v1.pdf","comment":"19 pages, 6 figures, 3 tables, 1 page of supplementary material"},{"id":"http://arxiv.org/abs/2403.17822v1","updated":"2024-03-26T16:00:31Z","published":"2024-03-26T16:00:31Z","title":"DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing","summary":"  3D Gaussian splatting, a novel differentiable rendering technique, has\nachieved state-of-the-art novel view synthesis results with high rendering\nspeeds and relatively low training times. However, its performance on scenes\ncommonly seen in indoor datasets is poor due to the lack of geometric\nconstraints during optimization. We extend 3D Gaussian splatting with depth and\nnormal cues to tackle challenging indoor datasets and showcase techniques for\nefficient mesh extraction, an important downstream application. Specifically,\nwe regularize the optimization procedure with depth information, enforce local\nsmoothness of nearby Gaussians, and use the geometry of the 3D Gaussians\nsupervised by normal cues to achieve better alignment with the true scene\ngeometry. We improve depth estimation and novel view synthesis results over\nbaselines and show how this simple yet effective regularization technique can\nbe used to directly extract meshes from the Gaussian representation yielding\nmore physically accurate reconstructions on indoor scenes. Our code will be\nreleased in https://github.com/maturk/dn-splatter.\n","authors":["Matias Turkulainen","Xuqian Ren","Iaroslav Melekhov","Otto Seiskari","Esa Rahtu","Juho Kannala"],"pdf_url":"https://arxiv.org/pdf/2403.17822v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15964v2","updated":"2024-03-26T15:58:26Z","published":"2023-11-27T16:07:37Z","title":"Efficient Pre-training for Localized Instruction Generation of Videos","summary":"  Procedural videos show step-by-step demonstrations of tasks like recipe\npreparation. Understanding such videos is challenging, involving the precise\nlocalization of steps and the generation of textual instructions. Manually\nannotating steps and writing instructions is costly, which limits the size of\ncurrent datasets and hinders effective learning. Leveraging large but noisy\nvideo-transcript datasets for pre-training can boost performance, but demands\nsignificant computational resources. Furthermore, transcripts contain\nirrelevant content and exhibit style variation compared to instructions written\nby human annotators. To mitigate both issues, we propose a technique,\nSieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters\nirrelevant transcripts and (ii) Swap enhances the quality of the text\ninstruction by automatically replacing the transcripts with human-written\ninstructions from a text-only recipe dataset. The curated dataset, three orders\nof magnitude smaller than current web-scale datasets, enables efficient\ntraining of large-scale models with competitive performance. We complement our\nSieve-\\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step\nlocalization and instruction generation for procedural videos. When this model\nis pre-trained on our curated dataset, it achieves state-of-the-art performance\nin zero-shot and finetuning settings on YouCook2 and Tasty, while using a\nfraction of the computational resources.\n","authors":["Anil Batra","Davide Moltisanti","Laura Sevilla-Lara","Marcus Rohrbach","Frank Keller"],"pdf_url":"https://arxiv.org/pdf/2311.15964v2.pdf","comment":"This version has some missing experiments and elaborative technical\n  details"},{"id":"http://arxiv.org/abs/2403.17808v1","updated":"2024-03-26T15:45:29Z","published":"2024-03-26T15:45:29Z","title":"Annotated Biomedical Video Generation using Denoising Diffusion\n  Probabilistic Models and Flow Fields","summary":"  The segmentation and tracking of living cells play a vital role within the\nbiomedical domain, particularly in cancer research, drug development, and\ndevelopmental biology. These are usually tedious and time-consuming tasks that\nare traditionally done by biomedical experts. Recently, to automatize these\nprocesses, deep learning based segmentation and tracking methods have been\nproposed. These methods require large-scale datasets and their full potential\nis constrained by the scarcity of annotated data in the biomedical imaging\ndomain. To address this limitation, we propose Biomedical Video Diffusion Model\n(BVDM), capable of generating realistic-looking synthetic microscopy videos.\nTrained only on a single real video, BVDM can generate videos of arbitrary\nlength with pixel-level annotations that can be used for training data-hungry\nmodels. It is composed of a denoising diffusion probabilistic model (DDPM)\ngenerating high-fidelity synthetic cell microscopy images and a flow prediction\nmodel (FPM) predicting the non-rigid transformation between consecutive video\nframes. During inference, initially, the DDPM imposes realistic cell textures\non synthetic cell masks which are generated based on real data statistics. The\nflow prediction model predicts the flow field between consecutive masks and\napplies that to the DDPM output from the previous time frame to create the next\none while keeping temporal consistency. BVDM outperforms state-of-the-art\nsynthetic live cell microscopy video generation models. Furthermore, we\ndemonstrate that a sufficiently large synthetic dataset enhances the\nperformance of cell segmentation and tracking models compared to using a\nlimited amount of available real data.\n","authors":["Rüveyda Yilmaz","Dennis Eschweiler","Johannes Stegmaier"],"pdf_url":"https://arxiv.org/pdf/2403.17808v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17804v1","updated":"2024-03-26T15:42:01Z","published":"2024-03-26T15:42:01Z","title":"Improving Text-to-Image Consistency via Automatic Prompt Optimization","summary":"  Impressive advances in text-to-image (T2I) generative models have yielded a\nplethora of high performing models which are able to generate aesthetically\nappealing, photorealistic images. Despite the progress, these models still\nstruggle to produce images that are consistent with the input prompt,\noftentimes failing to capture object quantities, relations and attributes\nproperly. Existing solutions to improve prompt-image consistency suffer from\nthe following challenges: (1) they oftentimes require model fine-tuning, (2)\nthey only focus on nearby prompt samples, and (3) they are affected by\nunfavorable trade-offs among image quality, representation diversity, and\nprompt-image consistency. In this paper, we address these challenges and\nintroduce a T2I optimization-by-prompting framework, OPT2I, which leverages a\nlarge language model (LLM) to improve prompt-image consistency in T2I models.\nOur framework starts from a user prompt and iteratively generates revised\nprompts with the goal of maximizing a consistency score. Our extensive\nvalidation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost\nthe initial consistency score by up to 24.9% in terms of DSG score while\npreserving the FID and increasing the recall between generated and real data.\nOur work paves the way toward building more reliable and robust T2I systems by\nharnessing the power of LLMs.\n","authors":["Oscar Mañas","Pietro Astolfi","Melissa Hall","Candace Ross","Jack Urbanek","Adina Williams","Aishwarya Agrawal","Adriana Romero-Soriano","Michal Drozdzal"],"pdf_url":"https://arxiv.org/pdf/2403.17804v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00454v3","updated":"2024-03-26T15:41:17Z","published":"2023-09-30T18:13:41Z","title":"SimLVSeg: Simplifying Left Ventricular Segmentation in 2D+Time\n  Echocardiograms with Self- and Weakly-Supervised Learning","summary":"  Echocardiography has become an indispensable clinical imaging modality for\ngeneral heart health assessment. From calculating biomarkers such as ejection\nfraction to the probability of a patient's heart failure, accurate segmentation\nof the heart structures allows doctors to assess the heart's condition and\ndevise treatments with greater precision and accuracy. However, achieving\naccurate and reliable left ventricle segmentation is time-consuming and\nchallenging due to different reasons. Hence, clinicians often rely on\nsegmenting the left ventricular (LV) in two specific echocardiogram frames to\nmake a diagnosis. This limited coverage in manual LV segmentation poses a\nchallenge for developing automatic LV segmentation with high temporal\nconsistency, as the resulting dataset is typically annotated sparsely. In\nresponse to this challenge, this work introduces SimLVSeg, a novel paradigm\nthat enables video-based networks for consistent LV segmentation from sparsely\nannotated echocardiogram videos. SimLVSeg consists of self-supervised\npre-training with temporal masking, followed by weakly supervised learning\ntailored for LV segmentation from sparse annotations. We demonstrate how\nSimLVSeg outperforms the state-of-the-art solutions by achieving a 93.32%\n(95%CI 93.21-93.43%) dice score on the largest 2D+time echocardiography dataset\n(EchoNet-Dynamic) while being more efficient. SimLVSeg is compatible with two\ntypes of video segmentation networks: 2D super image and 3D segmentation. To\nshow the effectiveness of our approach, we provide extensive ablation studies,\nincluding pre-training settings and various deep learning backbones. We further\nconduct an out-of-distribution test to showcase SimLVSeg's generalizability on\nunseen distribution (CAMUS dataset). The code is publicly available at\nhttps://github.com/fadamsyah/SimLVSeg.\n","authors":["Fadillah Maani","Asim Ukaye","Nada Saadi","Numan Saeed","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2310.00454v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08639v2","updated":"2024-03-26T15:40:20Z","published":"2024-03-13T15:51:23Z","title":"HIMap: HybrId Representation Learning for End-to-end Vectorized HD Map\n  Construction","summary":"  Vectorized High-Definition (HD) map construction requires predictions of the\ncategory and point coordinates of map elements (e.g. road boundary, lane\ndivider, pedestrian crossing, etc.). State-of-the-art methods are mainly based\non point-level representation learning for regressing accurate point\ncoordinates. However, this pipeline has limitations in obtaining element-level\ninformation and handling element-level failures, e.g. erroneous element shape\nor entanglement between elements. To tackle the above issues, we propose a\nsimple yet effective HybrId framework named HIMap to sufficiently learn and\ninteract both point-level and element-level information. Concretely, we\nintroduce a hybrid representation called HIQuery to represent all map elements,\nand propose a point-element interactor to interactively extract and encode the\nhybrid information of elements, e.g. point position and element shape, into the\nHIQuery. Additionally, we present a point-element consistency constraint to\nenhance the consistency between the point-level and element-level information.\nFinally, the output point-element integrated HIQuery can be directly converted\ninto map elements' class, point coordinates, and mask. We conduct extensive\nexperiments and consistently outperform previous methods on both nuScenes and\nArgoverse2 datasets. Notably, our method achieves $77.8$ mAP on the nuScenes\ndataset, remarkably superior to previous SOTAs by $8.3$ mAP at least.\n","authors":["Yi Zhou","Hui Zhang","Jiaqian Yu","Yifan Yang","Sangil Jung","Seung-In Park","ByungIn Yoo"],"pdf_url":"https://arxiv.org/pdf/2403.08639v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17801v1","updated":"2024-03-26T15:40:05Z","published":"2024-03-26T15:40:05Z","title":"Towards 3D Vision with Low-Cost Single-Photon Cameras","summary":"  We present a method for reconstructing 3D shape of arbitrary Lambertian\nobjects based on measurements by miniature, energy-efficient, low-cost\nsingle-photon cameras. These cameras, operating as time resolved image sensors,\nilluminate the scene with a very fast pulse of diffuse light and record the\nshape of that pulse as it returns back from the scene at a high temporal\nresolution. We propose to model this image formation process, account for its\nnon-idealities, and adapt neural rendering to reconstruct 3D geometry from a\nset of spatially distributed sensors with known poses. We show that our\napproach can successfully recover complex 3D shapes from simulated data. We\nfurther demonstrate 3D object reconstruction from real-world captures,\nutilizing measurements from a commodity proximity sensor. Our work draws a\nconnection between image-based modeling and active range scanning and is a step\ntowards 3D vision with single-photon cameras.\n","authors":["Fangzhou Mu","Carter Sifferman","Sacha Jungerman","Yiquan Li","Mark Han","Michael Gleicher","Mohit Gupta","Yin Li"],"pdf_url":"https://arxiv.org/pdf/2403.17801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17787v1","updated":"2024-03-26T15:20:49Z","published":"2024-03-26T15:20:49Z","title":"Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models\n  Versus Fine-Tuned Vision Transformers in Image-Based Security Applications","summary":"  The success of Large Language Models (LLMs) has led to a parallel rise in the\ndevelopment of Large Multimodal Models (LMMs), such as Gemini-pro, which have\nbegun to transform a variety of applications. These sophisticated multimodal\nmodels are designed to interpret and analyze complex data, integrating both\ntextual and visual information on a scale previously unattainable, opening new\navenues for a range of applications. This paper investigates the applicability\nand effectiveness of prompt-engineered Gemini-pro LMMs versus fine-tuned Vision\nTransformer (ViT) models in addressing critical security challenges. We focus\non two distinct tasks: a visually evident task of detecting simple triggers,\nsuch as small squares in images, indicative of potential backdoors, and a\nnon-visually evident task of malware classification through visual\nrepresentations. Our results highlight a significant divergence in performance,\nwith Gemini-pro falling short in accuracy and reliability when compared to\nfine-tuned ViT models. The ViT models, on the other hand, demonstrate\nexceptional accuracy, achieving near-perfect performance on both tasks. This\nstudy not only showcases the strengths and limitations of prompt-engineered\nLMMs in cybersecurity applications but also emphasizes the unmatched efficacy\nof fine-tuned ViT models for precise and dependable tasks.\n","authors":["Fouad Trad","Ali Chehab"],"pdf_url":"https://arxiv.org/pdf/2403.17787v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17782v1","updated":"2024-03-26T15:15:15Z","published":"2024-03-26T15:15:15Z","title":"GenesisTex: Adapting Image Denoising Diffusion to Texture Space","summary":"  We present GenesisTex, a novel method for synthesizing textures for 3D\ngeometries from text descriptions. GenesisTex adapts the pretrained image\ndiffusion model to texture space by texture space sampling. Specifically, we\nmaintain a latent texture map for each viewpoint, which is updated with\npredicted noise on the rendering of the corresponding viewpoint. The sampled\nlatent texture maps are then decoded into a final texture map. During the\nsampling process, we focus on both global and local consistency across multiple\nviewpoints: global consistency is achieved through the integration of style\nconsistency mechanisms within the noise prediction network, and low-level\nconsistency is achieved by dynamically aligning latent textures. Finally, we\napply reference-based inpainting and img2img on denser views for texture\nrefinement. Our approach overcomes the limitations of slow optimization in\ndistillation-based methods and instability in inpainting-based methods.\nExperiments on meshes from various sources demonstrate that our method\nsurpasses the baseline methods quantitatively and qualitatively.\n","authors":["Chenjian Gao","Boyan Jiang","Xinghui Li","Yingpeng Zhang","Qian Yu"],"pdf_url":"https://arxiv.org/pdf/2403.17782v1.pdf","comment":"12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.16167v2","updated":"2024-03-26T15:14:25Z","published":"2024-03-24T14:21:06Z","title":"Exploiting Semantic Reconstruction to Mitigate Hallucinations in\n  Vision-Language Models","summary":"  Hallucinations in vision-language models pose a significant challenge to\ntheir reliability, particularly in the generation of long captions. Current\nmethods fall short of accurately identifying and mitigating these\nhallucinations. To address this issue, we introduce ESREAL, a novel\nunsupervised learning framework designed to suppress the generation of\nhallucinations through accurate localization and penalization of hallucinated\ntokens. Initially, ESREAL creates a reconstructed image based on the generated\ncaption and aligns its corresponding regions with those of the original image.\nThis semantic reconstruction aids in identifying both the presence and type of\ntoken-level hallucinations within the generated caption. Subsequently, ESREAL\ncomputes token-level hallucination scores by assessing the semantic similarity\nof aligned regions based on the type of hallucination. Finally, ESREAL employs\na proximal policy optimization algorithm, where it selectively penalizes\nhallucinated tokens according to their token-level hallucination scores. Our\nframework notably reduces hallucinations in LLaVA, InstructBLIP, and mPLUG-Owl2\nby 32.81%, 27.08%, and 7.46% on the CHAIR metric. This improvement is achieved\nsolely through signals derived from the image itself, without the need for any\nimage-text pairs.\n","authors":["Minchan Kim","Minyeong Kim","Junik Bae","Suhwan Choi","Sungkyung Kim","Buru Chang"],"pdf_url":"https://arxiv.org/pdf/2403.16167v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12225v2","updated":"2024-03-26T15:06:00Z","published":"2024-02-19T15:33:09Z","title":"Pushing Auto-regressive Models for 3D Shape Generation at Capacity and\n  Scalability","summary":"  Auto-regressive models have achieved impressive results in 2D image\ngeneration by modeling joint distributions in grid space. In this paper, we\nextend auto-regressive models to 3D domains, and seek a stronger ability of 3D\nshape generation by improving auto-regressive models at capacity and\nscalability simultaneously. Firstly, we leverage an ensemble of publicly\navailable 3D datasets to facilitate the training of large-scale models. It\nconsists of a comprehensive collection of approximately 900,000 objects, with\nmultiple properties of meshes, points, voxels, rendered images, and text\ncaptions. This diverse labeled dataset, termed Objaverse-Mix, empowers our\nmodel to learn from a wide range of object variations. However, directly\napplying 3D auto-regression encounters critical challenges of high\ncomputational demands on volumetric grids and ambiguous auto-regressive order\nalong grid dimensions, resulting in inferior quality of 3D shapes. To this end,\nwe then present a novel framework Argus3D in terms of capacity. Concretely, our\napproach introduces discrete representation learning based on a latent vector\ninstead of volumetric grids, which not only reduces computational costs but\nalso preserves essential geometric details by learning the joint distributions\nin a more tractable order. The capacity of conditional generation can thus be\nrealized by simply concatenating various conditioning inputs to the latent\nvector, such as point clouds, categories, images, and texts. In addition,\nthanks to the simplicity of our model architecture, we naturally scale up our\napproach to a larger model with an impressive 3.6 billion parameters, further\nenhancing the quality of versatile 3D generation. Extensive experiments on four\ngeneration tasks demonstrate that Argus3D can synthesize diverse and faithful\nshapes across multiple categories, achieving remarkable performance.\n","authors":["Xuelin Qian","Yu Wang","Simian Luo","Yinda Zhang","Ying Tai","Zhenyu Zhang","Chengjie Wang","Xiangyang Xue","Bo Zhao","Tiejun Huang","Yunsheng Wu","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2402.12225v2.pdf","comment":"Project page: https://argus-3d.github.io/ . Datasets:\n  https://huggingface.co/datasets/BAAI/Objaverse-MIX. arXiv admin note:\n  substantial text overlap with arXiv:2303.14700"},{"id":"http://arxiv.org/abs/2403.17770v1","updated":"2024-03-26T14:59:11Z","published":"2024-03-26T14:59:11Z","title":"CT Synthesis with Conditional Diffusion Models for Abdominal Lymph Node\n  Segmentation","summary":"  Despite the significant success achieved by deep learning methods in medical\nimage segmentation, researchers still struggle in the computer-aided diagnosis\nof abdominal lymph nodes due to the complex abdominal environment, small and\nindistinguishable lesions, and limited annotated data. To address these\nproblems, we present a pipeline that integrates the conditional diffusion model\nfor lymph node generation and the nnU-Net model for lymph node segmentation to\nimprove the segmentation performance of abdominal lymph nodes through\nsynthesizing a diversity of realistic abdominal lymph node data. We propose\nLN-DDPM, a conditional denoising diffusion probabilistic model (DDPM) for lymph\nnode (LN) generation. LN-DDPM utilizes lymph node masks and anatomical\nstructure masks as model conditions. These conditions work in two conditioning\nmechanisms: global structure conditioning and local detail conditioning, to\ndistinguish between lymph nodes and their surroundings and better capture lymph\nnode characteristics. The obtained paired abdominal lymph node images and masks\nare used for the downstream segmentation task. Experimental results on the\nabdominal lymph node datasets demonstrate that LN-DDPM outperforms other\ngenerative methods in the abdominal lymph node image synthesis and better\nassists the downstream abdominal lymph node segmentation task.\n","authors":["Yongrui Yu","Hanyu Chen","Zitian Zhang","Qiong Xiao","Wenhui Lei","Linrui Dai","Yu Fu","Hui Tan","Guan Wang","Peng Gao","Xiaofan Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.17770v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17057v2","updated":"2024-03-26T14:54:04Z","published":"2023-11-28T18:59:52Z","title":"ReMoS: 3D Motion-Conditioned Reaction Synthesis for Two-Person\n  Interactions","summary":"  Current approaches for 3D human motion synthesis generate high-quality\nanimations of digital humans performing a wide variety of actions and gestures.\nHowever, a notable technological gap exists in addressing the complex dynamics\nof multi-human interactions within this paradigm. In this work, we present\nReMoS, a denoising diffusion-based model that synthesizes full-body reactive\nmotion of a person in a two-person interaction scenario. Assuming the motion of\none person is given, we employ a combined spatio-temporal cross-attention\nmechanism to synthesize the reactive body and hand motion of the second person,\nthereby completing the interactions between the two. We demonstrate ReMoS\nacross challenging two-person scenarios such as pair-dancing, Ninjutsu,\nkickboxing, and acrobatics, where one person's movements have complex and\ndiverse influences on the other. We also contribute the ReMoCap dataset for\ntwo-person interactions containing full-body and finger motions. We evaluate\nReMoS through multiple quantitative metrics, qualitative visualizations, and a\nuser study, and also indicate usability in interactive motion editing\napplications.\n","authors":["Anindita Ghosh","Rishabh Dabral","Vladislav Golyanik","Christian Theobalt","Philipp Slusallek"],"pdf_url":"https://arxiv.org/pdf/2311.17057v2.pdf","comment":"17 pages, 7 figures, 5 tables"},{"id":"http://arxiv.org/abs/2403.17765v1","updated":"2024-03-26T14:53:24Z","published":"2024-03-26T14:53:24Z","title":"MUTE-SLAM: Real-Time Neural SLAM with Multiple Tri-Plane Hash\n  Representations","summary":"  We introduce MUTE-SLAM, a real-time neural RGB-D SLAM system employing\nmultiple tri-plane hash-encodings for efficient scene representation. MUTE-SLAM\neffectively tracks camera positions and incrementally builds a scalable\nmulti-map representation for both small and large indoor environments. It\ndynamically allocates sub-maps for newly observed local regions, enabling\nconstraint-free mapping without prior scene information. Unlike traditional\ngrid-based methods, we use three orthogonal axis-aligned planes for\nhash-encoding scene properties, significantly reducing hash collisions and the\nnumber of trainable parameters. This hybrid approach not only speeds up\nconvergence but also enhances the fidelity of surface reconstruction.\nFurthermore, our optimization strategy concurrently optimizes all sub-maps\nintersecting with the current camera frustum, ensuring global consistency.\nExtensive testing on both real-world and synthetic datasets has shown that\nMUTE-SLAM delivers state-of-the-art surface reconstruction quality and\ncompetitive tracking performance across diverse indoor settings. The code will\nbe made public upon acceptance of the paper.\n","authors":["Yifan Yan","Ruomin He","Zhenghua Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15585v2","updated":"2024-03-26T14:51:57Z","published":"2024-03-22T19:19:51Z","title":"MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis","summary":"  Chest X-ray images are commonly used for predicting acute and chronic\ncardiopulmonary conditions, but efforts to integrate them with structured\nclinical data face challenges due to incomplete electronic health records\n(EHR). This paper introduces \\textbf{MedPromptX}, the first model to integrate\nmultimodal large language models (MLLMs), few-shot prompting (FP) and visual\ngrounding (VG) to combine imagery with EHR data for chest X-ray diagnosis. A\npre-trained MLLM is utilized to complement the missing EHR information,\nproviding a comprehensive understanding of patients' medical history.\nAdditionally, FP reduces the necessity for extensive training of MLLMs while\neffectively tackling the issue of hallucination. Nevertheless, the process of\ndetermining the optimal number of few-shot examples and selecting high-quality\ncandidates can be burdensome, yet it profoundly influences model performance.\nHence, we propose a new technique that dynamically refines few-shot data for\nreal-time adjustment to new patient scenarios. Moreover, VG aids in focusing\nthe model's attention on relevant regions of interest in X-ray images,\nenhancing the identification of abnormalities. We release MedPromptX-VQA, a new\nin-context visual question answering dataset encompassing interleaved image and\nEHR data derived from MIMIC-IV and MIMIC-CXR databases. Results demonstrate the\nSOTA performance of MedPromptX, achieving an 11% improvement in F1-score\ncompared to the baselines. Code and data are available at\nhttps://github.com/BioMedIA-MBZUAI/MedPromptX\n","authors":["Mai A. Shaaban","Adnan Khan","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2403.15585v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17761v1","updated":"2024-03-26T14:51:53Z","published":"2024-03-26T14:51:53Z","title":"Makeup Prior Models for 3D Facial Makeup Estimation and Applications","summary":"  In this work, we introduce two types of makeup prior models to extend\nexisting 3D face prior models: PCA-based and StyleGAN2-based priors. The\nPCA-based prior model is a linear model that is easy to construct and is\ncomputationally efficient. However, it retains only low-frequency information.\nConversely, the StyleGAN2-based model can represent high-frequency information\nwith relatively higher computational cost than the PCA-based model. Although\nthere is a trade-off between the two models, both are applicable to 3D facial\nmakeup estimation and related applications. By leveraging makeup prior models\nand designing a makeup consistency module, we effectively address the\nchallenges that previous methods faced in robustly estimating makeup,\nparticularly in the context of handling self-occluded faces. In experiments, we\ndemonstrate that our approach reduces computational costs by several orders of\nmagnitude, achieving speeds up to 180 times faster. In addition, by improving\nthe accuracy of the estimated makeup, we confirm that our methods are highly\nadvantageous for various 3D facial makeup applications such as 3D makeup face\nreconstruction, user-friendly makeup editing, makeup transfer, and\ninterpolation.\n","authors":["Xingchao Yang","Takafumi Taketomi","Yuki Endo","Yoshihiro Kanamori"],"pdf_url":"https://arxiv.org/pdf/2403.17761v1.pdf","comment":"CVPR2024. Project: https://yangxingchao.github.io/makeup-priors-page"},{"id":"http://arxiv.org/abs/2403.17757v1","updated":"2024-03-26T14:49:22Z","published":"2024-03-26T14:49:22Z","title":"Noise2Noise Denoising of CRISM Hyperspectral Data","summary":"  Hyperspectral data acquired by the Compact Reconnaissance Imaging\nSpectrometer for Mars (CRISM) have allowed for unparalleled mapping of the\nsurface mineralogy of Mars. Due to sensor degradation over time, a significant\nportion of the recently acquired data is considered unusable. Here a new\ndata-driven model architecture, Noise2Noise4Mars (N2N4M), is introduced to\nremove noise from CRISM images. Our model is self-supervised and does not\nrequire zero-noise target data, making it well suited for use in Planetary\nScience applications where high quality labelled data is scarce. We demonstrate\nits strong performance on synthetic-noise data and CRISM images, and its impact\non downstream classification performance, outperforming benchmark methods on\nmost metrics. This allows for detailed analysis for critical sites of interest\non the Martian surface, including proposed lander sites.\n","authors":["Robert Platt","Rossella Arcucci","Cédric John"],"pdf_url":"https://arxiv.org/pdf/2403.17757v1.pdf","comment":"5 pages, 3 figures. Accepted as a conference paper at the ICLR 2024\n  ML4RS Workshop"},{"id":"http://arxiv.org/abs/2403.17755v1","updated":"2024-03-26T14:44:51Z","published":"2024-03-26T14:44:51Z","title":"DataCook: Crafting Anti-Adversarial Examples for Healthcare Data\n  Copyright Protection","summary":"  In the realm of healthcare, the challenges of copyright protection and\nunauthorized third-party misuse are increasingly significant. Traditional\nmethods for data copyright protection are applied prior to data distribution,\nimplying that models trained on these data become uncontrollable. This paper\nintroduces a novel approach, named DataCook, designed to safeguard the\ncopyright of healthcare data during the deployment phase. DataCook operates by\n\"cooking\" the raw data before distribution, enabling the development of models\nthat perform normally on this processed data. However, during the deployment\nphase, the original test data must be also \"cooked\" through DataCook to ensure\nnormal model performance. This process grants copyright holders control over\nauthorization during the deployment phase. The mechanism behind DataCook is by\ncrafting anti-adversarial examples (AntiAdv), which are designed to enhance\nmodel confidence, as opposed to standard adversarial examples (Adv) that aim to\nconfuse models. Similar to Adv, AntiAdv introduces imperceptible perturbations,\nensuring that the data processed by DataCook remains easily understandable. We\nconducted extensive experiments on MedMNIST datasets, encompassing both 2D/3D\ndata and the high-resolution variants. The outcomes indicate that DataCook\neffectively meets its objectives, preventing models trained on AntiAdv from\nanalyzing unauthorized data effectively, without compromising the validity and\naccuracy of the data in legitimate scenarios. Code and data are available at\nhttps://github.com/MedMNIST/DataCook.\n","authors":["Sihan Shang","Jiancheng Yang","Zhenglong Sun","Pascal Fua"],"pdf_url":"https://arxiv.org/pdf/2403.17755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06247v2","updated":"2024-03-26T14:42:21Z","published":"2024-03-10T16:11:17Z","title":"Text-Guided Variational Image Generation for Industrial Anomaly\n  Detection and Segmentation","summary":"  We propose a text-guided variational image generation method to address the\nchallenge of getting clean data for anomaly detection in industrial\nmanufacturing. Our method utilizes text information about the target object,\nlearned from extensive text library documents, to generate non-defective data\nimages resembling the input image. The proposed framework ensures that the\ngenerated non-defective images align with anticipated distributions derived\nfrom textual and image-based knowledge, ensuring stability and generality.\nExperimental results demonstrate the effectiveness of our approach, surpassing\nprevious methods even with limited non-defective data. Our approach is\nvalidated through generalization tests across four baseline models and three\ndistinct datasets. We present an additional analysis to enhance the\neffectiveness of anomaly detection models by utilizing the generated images.\n","authors":["Mingyu Lee","Jongwon Choi"],"pdf_url":"https://arxiv.org/pdf/2403.06247v2.pdf","comment":"18 pages, Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17749v1","updated":"2024-03-26T14:40:17Z","published":"2024-03-26T14:40:17Z","title":"Multi-Task Dense Prediction via Mixture of Low-Rank Experts","summary":"  Previous multi-task dense prediction methods based on the Mixture of Experts\n(MoE) have received great performance but they neglect the importance of\nexplicitly modeling the global relations among all tasks. In this paper, we\npresent a novel decoder-focused method for multi-task dense prediction, called\nMixture-of-Low-Rank-Experts (MLoRE). To model the global task relationships,\nMLoRE adds a generic convolution path to the original MoE structure, where each\ntask feature can go through this path for explicit parameter sharing.\nFurthermore, to control the parameters and computational cost brought by the\nincrease in the number of experts, we take inspiration from LoRA and propose to\nleverage the low-rank format of a vanilla convolution in the expert network.\nSince the low-rank experts have fewer parameters and can be dynamically\nparameterized into the generic convolution, the parameters and computational\ncost do not change much with the increase of experts. Benefiting from this\ndesign, we increase the number of experts and its reception field to enlarge\nthe representation capacity, facilitating multiple dense tasks learning in a\nunified network. Extensive experiments on the PASCAL-Context and NYUD-v2\nbenchmarks show that our MLoRE achieves superior performance compared to\nprevious state-of-the-art methods on all metrics. Our code is available at\nhttps://github.com/YuqiYang213/MLoRE.\n","authors":["Yuqi Yang","Peng-Tao Jiang","Qibin Hou","Hao Zhang","Jinwei Chen","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2403.17749v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.08270v2","updated":"2024-03-26T14:39:43Z","published":"2024-03-13T05:46:36Z","title":"Identity-aware Dual-constraint Network for Cloth-Changing Person\n  Re-identification","summary":"  Cloth-Changing Person Re-Identification (CC-ReID) aims to accurately identify\nthe target person in more realistic surveillance scenarios, where pedestrians\nusually change their clothing. Despite great progress, limited cloth-changing\ntraining samples in existing CC-ReID datasets still prevent the model from\nadequately learning cloth-irrelevant features. In addition, due to the absence\nof explicit supervision to keep the model constantly focused on\ncloth-irrelevant areas, existing methods are still hampered by the disruption\nof clothing variations. To solve the above issues, we propose an Identity-aware\nDual-constraint Network (IDNet) for the CC-ReID task. Specifically, to help the\nmodel extract cloth-irrelevant clues, we propose a Clothes Diversity\nAugmentation (CDA), which generates more realistic cloth-changing samples by\nenriching the clothing color while preserving the texture. In addition, a\nMulti-scale Constraint Block (MCB) is designed, which extracts fine-grained\nidentity-related features and effectively transfers cloth-irrelevant knowledge.\nMoreover, a Counterfactual-guided Attention Module (CAM) is presented, which\nlearns cloth-irrelevant features from channel and space dimensions and utilizes\nthe counterfactual intervention for supervising the attention map to highlight\nidentity-related regions. Finally, a Semantic Alignment Constraint (SAC) is\ndesigned to facilitate high-level semantic feature interaction. Comprehensive\nexperiments on four CC-ReID datasets indicate that our method outperforms prior\nstate-of-the-art approaches.\n","authors":["Peini Guo","Mengyuan Liu","Hong Liu","Ruijia Fan","Guoquan Wang","Bin He"],"pdf_url":"https://arxiv.org/pdf/2403.08270v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02129v4","updated":"2024-03-26T14:38:23Z","published":"2023-10-03T15:10:46Z","title":"Unveiling the Pitfalls of Knowledge Editing for Large Language Models","summary":"  As the cost associated with fine-tuning Large Language Models (LLMs)\ncontinues to rise, recent research efforts have pivoted towards developing\nmethodologies to edit implicit knowledge embedded within LLMs. Yet, there's\nstill a dark cloud lingering overhead -- will knowledge editing trigger\nbutterfly effect? since it is still unclear whether knowledge editing might\nintroduce side effects that pose potential risks or not. This paper pioneers\nthe investigation into the potential pitfalls associated with knowledge editing\nfor LLMs. To achieve this, we introduce new benchmark datasets and propose\ninnovative evaluation metrics. Our results underline two pivotal concerns: (1)\nKnowledge Conflict: Editing groups of facts that logically clash can magnify\nthe inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)\nKnowledge Distortion: Altering parameters with the aim of editing factual\nknowledge can irrevocably warp the innate knowledge structure of LLMs.\nExperimental results vividly demonstrate that knowledge editing might\ninadvertently cast a shadow of unintended consequences on LLMs, which warrant\nattention and efforts for future works. Code and data are available at\nhttps://github.com/zjunlp/PitfallsKnowledgeEditing.\n","authors":["Zhoubo Li","Ningyu Zhang","Yunzhi Yao","Mengru Wang","Xi Chen","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2310.02129v4.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.17734v1","updated":"2024-03-26T14:21:49Z","published":"2024-03-26T14:21:49Z","title":"Paired Diffusion: Generation of related, synthetic PET-CT-Segmentation\n  scans using Linked Denoising Diffusion Probabilistic Models","summary":"  The rapid advancement of Artificial Intelligence (AI) in biomedical imaging\nand radiotherapy is hindered by the limited availability of large imaging data\nrepositories. With recent research and improvements in denoising diffusion\nprobabilistic models (DDPM), high quality synthetic medical scans are now\npossible. Despite this, there is currently no way of generating multiple\nrelated images, such as a corresponding ground truth which can be used to train\nmodels, so synthetic scans are often manually annotated before use. This\nresearch introduces a novel architecture that is able to generate multiple,\nrelated PET-CT-tumour mask pairs using paired networks and conditional\nencoders. Our approach includes innovative, time step-controlled mechanisms and\na `noise-seeding' strategy to improve DDPM sampling consistency. While our\nmodel requires a modified perceptual loss function to ensure accurate feature\nalignment we show generation of clearly aligned synthetic images and\nimprovement in segmentation accuracy with generated images.\n","authors":["Rowan Bradbury","Katherine A. Vallis","Bartlomiej W. Papiez"],"pdf_url":"https://arxiv.org/pdf/2403.17734v1.pdf","comment":"to be published in IEEE International Symposium on Biomedical Imaging\n  2024"},{"id":"http://arxiv.org/abs/2403.17727v1","updated":"2024-03-26T14:16:56Z","published":"2024-03-26T14:16:56Z","title":"FastPerson: Enhancing Video Learning through Effective Video\n  Summarization that Preserves Linguistic and Visual Contexts","summary":"  Quickly understanding lengthy lecture videos is essential for learners with\nlimited time and interest in various topics to improve their learning\nefficiency. To this end, video summarization has been actively researched to\nenable users to view only important scenes from a video. However, these studies\nfocus on either the visual or audio information of a video and extract\nimportant segments in the video. Therefore, there is a risk of missing\nimportant information when both the teacher's speech and visual information on\nthe blackboard or slides are important, such as in a lecture video. To tackle\nthis issue, we propose FastPerson, a video summarization approach that\nconsiders both the visual and auditory information in lecture videos.\nFastPerson creates summary videos by utilizing audio transcriptions along with\non-screen images and text, minimizing the risk of overlooking crucial\ninformation for learners. Further, it provides a feature that allows learners\nto switch between the summary and original videos for each chapter of the\nvideo, enabling them to adjust the pace of learning based on their interests\nand level of understanding. We conducted an evaluation with 40 participants to\nassess the effectiveness of our method and confirmed that it reduced viewing\ntime by 53\\% at the same level of comprehension as that when using traditional\nvideo playback methods.\n","authors":["Kazuki Kawamura","Jun Rekimoto"],"pdf_url":"https://arxiv.org/pdf/2403.17727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17464v2","updated":"2024-03-26T14:15:25Z","published":"2024-02-27T12:42:06Z","title":"Generative 3D Part Assembly via Part-Whole-Hierarchy Message Passing","summary":"  Generative 3D part assembly involves understanding part relationships and\npredicting their 6-DoF poses for assembling a realistic 3D shape. Prior work\noften focus on the geometry of individual parts, neglecting part-whole\nhierarchies of objects. Leveraging two key observations: 1) super-part poses\nprovide strong hints about part poses, and 2) predicting super-part poses is\neasier due to fewer superparts, we propose a part-whole-hierarchy message\npassing network for efficient 3D part assembly. We first introduce super-parts\nby grouping geometrically similar parts without any semantic labels. Then we\nemploy a part-whole hierarchical encoder, wherein a super-part encoder predicts\nlatent super-part poses based on input parts. Subsequently, we transform the\npoint cloud using the latent poses, feeding it to the part encoder for\naggregating super-part information and reasoning about part relationships to\npredict all part poses. In training, only ground-truth part poses are required.\nDuring inference, the predicted latent poses of super-parts enhance\ninterpretability. Experimental results on the PartNet dataset show that our\nmethod achieves state-of-the-art performance in part and connectivity accuracy\nand enables an interpretable hierarchical part assembly.\n","authors":["Bi'an Du","Xiang Gao","Wei Hu","Renjie Liao"],"pdf_url":"https://arxiv.org/pdf/2402.17464v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17725v1","updated":"2024-03-26T14:13:44Z","published":"2024-03-26T14:13:44Z","title":"Deep Learning for Segmentation of Cracks in High-Resolution Images of\n  Steel Bridges","summary":"  Automating the current bridge visual inspection practices using drones and\nimage processing techniques is a prominent way to make these inspections more\neffective, robust, and less expensive. In this paper, we investigate the\ndevelopment of a novel deep-learning method for the detection of fatigue cracks\nin high-resolution images of steel bridges. First, we present a novel and\nchallenging dataset comprising of images of cracks in steel bridges. Secondly,\nwe integrate the ConvNext neural network with a previous state- of-the-art\nencoder-decoder network for crack segmentation. We study and report, the\neffects of the use of background patches on the network performance when\napplied to high-resolution images of cracks in steel bridges. Finally, we\nintroduce a loss function that allows the use of more background patches for\nthe training process, which yields a significant reduction in false positive\nrates.\n","authors":["Andrii Kompanets","Gautam Pai","Remco Duits","Davide Leonetti","Bert Snijder"],"pdf_url":"https://arxiv.org/pdf/2403.17725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17712v1","updated":"2024-03-26T13:58:47Z","published":"2024-03-26T13:58:47Z","title":"Invisible Gas Detection: An RGB-Thermal Cross Attention Network and A\n  New Benchmark","summary":"  The widespread use of various chemical gases in industrial processes\nnecessitates effective measures to prevent their leakage during transportation\nand storage, given their high toxicity. Thermal infrared-based computer vision\ndetection techniques provide a straightforward approach to identify gas leakage\nareas. However, the development of high-quality algorithms has been challenging\ndue to the low texture in thermal images and the lack of open-source datasets.\nIn this paper, we present the RGB-Thermal Cross Attention Network (RT-CAN),\nwhich employs an RGB-assisted two-stream network architecture to integrate\ntexture information from RGB images and gas area information from thermal\nimages. Additionally, to facilitate the research of invisible gas detection, we\nintroduce Gas-DB, an extensive open-source gas detection database including\nabout 1.3K well-annotated RGB-thermal images with eight variant collection\nscenes. Experimental results demonstrate that our method successfully leverages\nthe advantages of both modalities, achieving state-of-the-art (SOTA)\nperformance among RGB-thermal methods, surpassing single-stream SOTA models in\nterms of accuracy, Intersection of Union (IoU), and F2 metrics by 4.86%, 5.65%,\nand 4.88%, respectively. The code and data will be made available soon.\n","authors":["Jue Wang","Yuxiang Lin","Qi Zhao","Dong Luo","Shuaibao Chen","Wei Chen","Xiaojiang Peng"],"pdf_url":"https://arxiv.org/pdf/2403.17712v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.15094v2","updated":"2024-03-26T13:57:26Z","published":"2023-05-24T12:22:23Z","title":"InNeRF360: Text-Guided 3D-Consistent Object Inpainting on 360-degree\n  Neural Radiance Fields","summary":"  We propose InNeRF360, an automatic system that accurately removes\ntext-specified objects from 360-degree Neural Radiance Fields (NeRF). The\nchallenge is to effectively remove objects while inpainting perceptually\nconsistent content for the missing regions, which is particularly demanding for\nexisting NeRF models due to their implicit volumetric representation. Moreover,\nunbounded scenes are more prone to floater artifacts in the inpainted region\nthan frontal-facing scenes, as the change of object appearance and background\nacross views is more sensitive to inaccurate segmentations and inconsistent\ninpainting. With a trained NeRF and a text description, our method efficiently\nremoves specified objects and inpaints visually consistent content without\nartifacts. We apply depth-space warping to enforce consistency across multiview\ntext-encoded segmentations, and then refine the inpainted NeRF model using\nperceptual priors and 3D diffusion-based geometric priors to ensure visual\nplausibility. Through extensive experiments in segmentation and inpainting on\n360-degree and frontal-facing NeRFs, we show that our approach is effective and\nenhances NeRF's editability. Project page: https://ivrl.github.io/InNeRF360.\n","authors":["Dongqing Wang","Tong Zhang","Alaa Abboud","Sabine Süsstrunk"],"pdf_url":"https://arxiv.org/pdf/2305.15094v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17709v1","updated":"2024-03-26T13:56:34Z","published":"2024-03-26T13:56:34Z","title":"Groupwise Query Specialization and Quality-Aware Multi-Assignment for\n  Transformer-based Visual Relationship Detection","summary":"  Visual Relationship Detection (VRD) has seen significant advancements with\nTransformer-based architectures recently. However, we identify two key\nlimitations in a conventional label assignment for training Transformer-based\nVRD models, which is a process of mapping a ground-truth (GT) to a prediction.\nUnder the conventional assignment, an unspecialized query is trained since a\nquery is expected to detect every relation, which makes it difficult for a\nquery to specialize in specific relations. Furthermore, a query is also\ninsufficiently trained since a GT is assigned only to a single prediction,\ntherefore near-correct or even correct predictions are suppressed by being\nassigned no relation as a GT. To address these issues, we propose Groupwise\nQuery Specialization and Quality-Aware Multi-Assignment (SpeaQ). Groupwise\nQuery Specialization trains a specialized query by dividing queries and\nrelations into disjoint groups and directing a query in a specific query group\nsolely toward relations in the corresponding relation group. Quality-Aware\nMulti-Assignment further facilitates the training by assigning a GT to multiple\npredictions that are significantly close to a GT in terms of a subject, an\nobject, and the relation in between. Experimental results and analyses show\nthat SpeaQ effectively trains specialized queries, which better utilize the\ncapacity of a model, resulting in consistent performance gains with zero\nadditional inference cost across multiple VRD models and benchmarks. Code is\navailable at https://github.com/mlvlab/SpeaQ.\n","authors":["Jongha Kim","Jihwan Park","Jinyoung Park","Jinyoung Kim","Sehyung Kim","Hyunwoo J. Kim"],"pdf_url":"https://arxiv.org/pdf/2403.17709v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2312.16014v2","updated":"2024-03-26T13:55:40Z","published":"2023-12-26T11:49:23Z","title":"Passive Non-Line-of-Sight Imaging with Light Transport Modulation","summary":"  Passive non-line-of-sight (NLOS) imaging has witnessed rapid development in\nrecent years, due to its ability to image objects that are out of sight. The\nlight transport condition plays an important role in this task since changing\nthe conditions will lead to different imaging models. Existing learning-based\nNLOS methods usually train independent models for different light transport\nconditions, which is computationally inefficient and impairs the practicality\nof the models. In this work, we propose NLOS-LTM, a novel passive NLOS imaging\nmethod that effectively handles multiple light transport conditions with a\nsingle network. We achieve this by inferring a latent light transport\nrepresentation from the projection image and using this representation to\nmodulate the network that reconstructs the hidden image from the projection\nimage. We train a light transport encoder together with a vector quantizer to\nobtain the light transport representation. To further regulate this\nrepresentation, we jointly learn both the reconstruction network and the\nreprojection network during training. A set of light transport modulation\nblocks is used to modulate the two jointly trained networks in a multi-scale\nway. Extensive experiments on a large-scale passive NLOS dataset demonstrate\nthe superiority of the proposed method. The code is available at\nhttps://github.com/JerryOctopus/NLOS-LTM.\n","authors":["Jiarui Zhang","Ruixu Geng","Xiaolong Du","Yan Chen","Houqiang Li","Yang Hu"],"pdf_url":"https://arxiv.org/pdf/2312.16014v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17708v1","updated":"2024-03-26T13:54:52Z","published":"2024-03-26T13:54:52Z","title":"Panonut360: A Head and Eye Tracking Dataset for Panoramic Video","summary":"  With the rapid development and widespread application of VR/AR technology,\nmaximizing the quality of immersive panoramic video services that match users'\npersonal preferences and habits has become a long-standing challenge.\nUnderstanding the saliency region where users focus, based on data collected\nwith HMDs, can promote multimedia encoding, transmission, and quality\nassessment. At the same time, large-scale datasets are essential for\nresearchers and developers to explore short/long-term user behavior patterns\nand train AI models related to panoramic videos. However, existing panoramic\nvideo datasets often include low-frequency user head or eye movement data\nthrough short-term videos only, lacking sufficient data for analyzing users'\nField of View (FoV) and generating video saliency regions.\n  Driven by these practical factors, in this paper, we present a head and eye\ntracking dataset involving 50 users (25 males and 25 females) watching 15\npanoramic videos. The dataset provides details on the viewport and gaze\nattention locations of users. Besides, we present some statistics samples\nextracted from the dataset. For example, the deviation between head and eye\nmovements challenges the widely held assumption that gaze attention decreases\nfrom the center of the FoV following a Gaussian distribution. Our analysis\nreveals a consistent downward offset in gaze fixations relative to the FoV in\nexperimental settings involving multiple users and videos. That's why we name\nthe dataset Panonut, a saliency weighting shaped like a donut. Finally, we also\nprovide a script that generates saliency distributions based on given head or\neye coordinates and pre-generated saliency distribution map sets of each video\nfrom the collected eye tracking data.\n  The dataset is available on website: https://dianvrlab.github.io/Panonut360/.\n","authors":["Yutong Xu","Junhao Du","Jiahe Wang","Yuwei Ning","Sihan Zhou Yang Cao"],"pdf_url":"https://arxiv.org/pdf/2403.17708v1.pdf","comment":"7 pages,ACM MMSys'24 accepted"},{"id":"http://arxiv.org/abs/2403.17702v1","updated":"2024-03-26T13:40:52Z","published":"2024-03-26T13:40:52Z","title":"The Solution for the CVPR 2023 1st foundation model challenge-Track2","summary":"  In this paper, we propose a solution for cross-modal transportation\nretrieval. Due to the cross-domain problem of traffic images, we divide the\nproblem into two sub-tasks of pedestrian retrieval and vehicle retrieval\nthrough a simple strategy. In pedestrian retrieval tasks, we use IRRA as the\nbase model and specifically design an Attribute Classification to mine the\nknowledge implied by attribute labels. More importantly, We use the strategy of\nInclusion Relation Matching to make the image-text pairs with inclusion\nrelation have similar representation in the feature space. For the vehicle\nretrieval task, we use BLIP as the base model. Since aligning the color\nattributes of vehicles is challenging, we introduce attribute-based object\ndetection techniques to add color patch blocks to vehicle images for color data\naugmentation. This serves as strong prior information, helping the model\nperform the image-text alignment. At the same time, we incorporate labeled\nattributes into the image-text alignment loss to learn fine-grained alignment\nand prevent similar images and texts from being incorrectly separated. Our\napproach ranked first in the final B-board test with a score of 70.9.\n","authors":["Haonan Xu","Yurui Huang","Sishun Pan","Zhihao Guan","Yi Xu","Yang Yang"],"pdf_url":"https://arxiv.org/pdf/2403.17702v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17701v1","updated":"2024-03-26T13:40:18Z","published":"2024-03-26T13:40:18Z","title":"Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical\n  Image Segmentation","summary":"  Image segmentation holds a vital position in the realms of diagnosis and\ntreatment within the medical domain. Traditional convolutional neural networks\n(CNNs) and Transformer models have made significant advancements in this realm,\nbut they still encounter challenges because of limited receptive field or high\ncomputing complexity. Recently, State Space Models (SSMs), particularly Mamba\nand its variants, have demonstrated notable performance in the field of vision.\nHowever, their feature extraction methods may not be sufficiently effective and\nretain some redundant structures, leaving room for parameter reduction.\nMotivated by previous spatial and channel attention methods, we propose Triplet\nMamba-UNet. The method leverages residual VSS Blocks to extract intensive\ncontextual features, while Triplet SSM is employed to fuse features across\nspatial and channel dimensions. We conducted experiments on ISIC17, ISIC18,\nCVC-300, CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB, and Kvasir-Instrument datasets,\ndemonstrating the superior segmentation performance of our proposed TM-UNet.\nAdditionally, compared to the previous VM-UNet, our model achieves a one-third\nreduction in parameters.\n","authors":["Hao Tang","Lianglun Cheng","Guoheng Huang","Zhengguang Tan","Junhao Lu","Kaihong Wu"],"pdf_url":"https://arxiv.org/pdf/2403.17701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17695v1","updated":"2024-03-26T13:35:10Z","published":"2024-03-26T13:35:10Z","title":"PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition","summary":"  We present PlainMamba: a simple non-hierarchical state space model (SSM)\ndesigned for general visual recognition. The recent Mamba model has shown how\nSSMs can be highly competitive with other architectures on sequential data and\ninitial attempts have been made to apply it to images. In this paper, we\nfurther adapt the selective scanning process of Mamba to the visual domain,\nenhancing its ability to learn features from two-dimensional images by (i) a\ncontinuous 2D scanning process that improves spatial continuity by ensuring\nadjacency of tokens in the scanning sequence, and (ii) direction-aware updating\nwhich enables the model to discern the spatial relations of tokens by encoding\ndirectional information. Our architecture is designed to be easy to use and\neasy to scale, formed by stacking identical PlainMamba blocks, resulting in a\nmodel with constant width throughout all layers. The architecture is further\nsimplified by removing the need for special tokens. We evaluate PlainMamba on a\nvariety of visual recognition tasks including image classification, semantic\nsegmentation, object detection, and instance segmentation. Our method achieves\nperformance gains over previous non-hierarchical models and is competitive with\nhierarchical alternatives. For tasks requiring high-resolution inputs, in\nparticular, PlainMamba requires much less computing while maintaining high\nperformance. Code and models are available at\nhttps://github.com/ChenhongyiYang/PlainMamba\n","authors":["Chenhongyi Yang","Zehui Chen","Miguel Espinosa","Linus Ericsson","Zhenyu Wang","Jiaming Liu","Elliot J. Crowley"],"pdf_url":"https://arxiv.org/pdf/2403.17695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17694v1","updated":"2024-03-26T13:35:02Z","published":"2024-03-26T13:35:02Z","title":"AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation","summary":"  In this study, we propose AniPortrait, a novel framework for generating\nhigh-quality animation driven by audio and a reference portrait image. Our\nmethodology is divided into two stages. Initially, we extract 3D intermediate\nrepresentations from audio and project them into a sequence of 2D facial\nlandmarks. Subsequently, we employ a robust diffusion model, coupled with a\nmotion module, to convert the landmark sequence into photorealistic and\ntemporally consistent portrait animation. Experimental results demonstrate the\nsuperiority of AniPortrait in terms of facial naturalness, pose diversity, and\nvisual quality, thereby offering an enhanced perceptual experience. Moreover,\nour methodology exhibits considerable potential in terms of flexibility and\ncontrollability, which can be effectively applied in areas such as facial\nmotion editing or face reenactment. We release code and model weights at\nhttps://github.com/scutzzj/AniPortrait\n","authors":["Huawei Wei","Zejun Yang","Zhisheng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.17694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17692v1","updated":"2024-03-26T13:33:16Z","published":"2024-03-26T13:33:16Z","title":"Manifold-Guided Lyapunov Control with Diffusion Models","summary":"  This paper presents a novel approach to generating stabilizing controllers\nfor a large class of dynamical systems using diffusion models. The core\nobjective is to develop stabilizing control functions by identifying the\nclosest asymptotically stable vector field relative to a predetermined manifold\nand adjusting the control function based on this finding. To achieve this, we\nemploy a diffusion model trained on pairs consisting of asymptotically stable\nvector fields and their corresponding Lyapunov functions. Our numerical results\ndemonstrate that this pre-trained model can achieve stabilization over\npreviously unseen systems efficiently and rapidly, showcasing the potential of\nour approach in fast zero-shot control and generalizability.\n","authors":["Amartya Mukherjee","Thanin Quartz","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17692v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2403.17691v1","updated":"2024-03-26T13:32:32Z","published":"2024-03-26T13:32:32Z","title":"Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to\n  Inform GenAI Copyright Disputes","summary":"  The advent of Generative Artificial Intelligence (GenAI) models, including\nGitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content\ncreation, enabling non-professionals to produce high-quality content across\nvarious domains. This transformative technology has led to a surge of synthetic\ncontent and sparked legal disputes over copyright infringement. To address\nthese challenges, this paper introduces a novel approach that leverages the\nlearning capacity of GenAI models for copyright legal analysis, demonstrated\nwith GPT2 and Stable Diffusion models. Copyright law distinguishes between\noriginal expressions and generic ones (Sc\\`enes \\`a faire), protecting the\nformer and permitting reproduction of the latter. However, this distinction has\nhistorically been challenging to make consistently, leading to over-protection\nof copyrighted works. GenAI offers an unprecedented opportunity to enhance this\nlegal analysis by revealing shared patterns in preexisting works. We propose a\ndata-driven approach to identify the genericity of works created by GenAI,\nemploying \"data-driven bias\" to assess the genericity of expressive\ncompositions. This approach aids in copyright scope determination by utilizing\nthe capabilities of GenAI to identify and prioritize expressive elements and\nrank them according to their frequency in the model's dataset. The potential\nimplications of measuring expressive genericity for copyright law are profound.\nSuch scoring could assist courts in determining copyright scope during\nlitigation, inform the registration practices of Copyright Offices, allowing\nregistration of only highly original synthetic works, and help copyright owners\nsignal the value of their works and facilitate fairer licensing deals. More\ngenerally, this approach offers valuable insights to policymakers grappling\nwith adapting copyright law to the challenges posed by the era of GenAI.\n","authors":["Uri Hacohen","Adi Haviv","Shahar Sarfaty","Bruria Friedman","Niva Elkin-Koren","Roi Livni","Amit H Bermano"],"pdf_url":"https://arxiv.org/pdf/2403.17691v1.pdf","comment":"Presented at ACM CSLAW 2024"},{"id":"http://arxiv.org/abs/2311.16081v2","updated":"2024-03-26T13:32:06Z","published":"2023-11-27T18:52:09Z","title":"ViT-Lens: Towards Omni-modal Representations","summary":"  Aiming to advance AI agents, large foundation models significantly improve\nreasoning and instruction execution, yet the current focus on vision and\nlanguage neglects the potential of perceiving diverse modalities in open-world\nenvironments. However, the success of data-driven vision and language models is\ncostly or even infeasible to be reproduced for rare modalities. In this paper,\nwe present ViT-Lens-2 that facilitates efficient omni-modal representation\nlearning by perceiving novel modalities with a pretrained ViT and aligning them\nto a pre-defined space. Specifically, the modality-specific lens is tuned to\nproject any-modal signals to an intermediate embedding space, which are then\nprocessed by a strong ViT with pre-trained visual knowledge. The encoded\nrepresentations are optimized toward aligning with the modal-independent space,\npre-defined by off-the-shelf foundation models. ViT-Lens-2 provides a unified\nsolution for representation learning of increasing modalities with two\nappealing advantages: (i) Unlocking the great potential of pretrained ViTs to\nnovel modalities effectively with efficient data regime; (ii) Enabling emergent\ndownstream capabilities through modality alignment and shared ViT parameters.\nWe tailor ViT-Lens-2 to learn representations for 3D point cloud, depth, audio,\ntactile and EEG, and set new state-of-the-art results across various\nunderstanding tasks, such as zero-shot classification. By seamlessly\nintegrating ViT-Lens-2 into Multimodal Foundation Models, we enable\nAny-modality to Text and Image Generation in a zero-shot manner. Code and\nmodels are available at https://github.com/TencentARC/ViT-Lens.\n","authors":["Weixian Lei","Yixiao Ge","Kun Yi","Jianfeng Zhang","Difei Gao","Dylan Sun","Yuying Ge","Ying Shan","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2311.16081v2.pdf","comment":"This work is a follow-up of arXiv:2308.10185. Accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2403.11708v3","updated":"2024-03-26T13:21:52Z","published":"2024-03-18T12:12:45Z","title":"Implicit Discriminative Knowledge Learning for Visible-Infrared Person\n  Re-Identification","summary":"  Visible-Infrared Person Re-identification (VI-ReID) is a challenging\ncross-modal pedestrian retrieval task, due to significant intra-class\nvariations and cross-modal discrepancies among different cameras. Existing\nworks mainly focus on embedding images of different modalities into a unified\nspace to mine modality-shared features. They only seek distinctive information\nwithin these shared features, while ignoring the identity-aware useful\ninformation that is implicit in the modality-specific features. To address this\nissue, we propose a novel Implicit Discriminative Knowledge Learning (IDKL)\nnetwork to uncover and leverage the implicit discriminative information\ncontained within the modality-specific. First, we extract modality-specific and\nmodality-shared features using a novel dual-stream network. Then, the\nmodality-specific features undergo purification to reduce their modality style\ndiscrepancies while preserving identity-aware discriminative knowledge.\nSubsequently, this kind of implicit knowledge is distilled into the\nmodality-shared feature to enhance its distinctiveness. Finally, an alignment\nloss is proposed to minimize modality discrepancy on enhanced modality-shared\nfeatures. Extensive experiments on multiple public datasets demonstrate the\nsuperiority of IDKL network over the state-of-the-art methods. Code is\navailable at https://github.com/1KK077/IDKL.\n","authors":["Kaijie Ren","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11708v3.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2311.17094v2","updated":"2024-03-26T13:21:43Z","published":"2023-11-28T06:17:49Z","title":"In Search of a Data Transformation That Accelerates Neural Field\n  Training","summary":"  Neural field is an emerging paradigm in data representation that trains a\nneural network to approximate the given signal. A key obstacle that prevents\nits widespread adoption is the encoding speed-generating neural fields requires\nan overfitting of a neural network, which can take a significant number of SGD\nsteps to reach the desired fidelity level. In this paper, we delve into the\nimpacts of data transformations on the speed of neural field training,\nspecifically focusing on how permuting pixel locations affect the convergence\nspeed of SGD. Counterintuitively, we find that randomly permuting the pixel\nlocations can considerably accelerate the training. To explain this phenomenon,\nwe examine the neural field training through the lens of PSNR curves, loss\nlandscapes, and error patterns. Our analyses suggest that the random pixel\npermutations remove the easy-to-fit patterns, which facilitate easy\noptimization in the early stage but hinder capturing fine details of the\nsignal.\n","authors":["Junwon Seo","Sangyoon Lee","Kwang In Kim","Jaeho Lee"],"pdf_url":"https://arxiv.org/pdf/2311.17094v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2312.02512v2","updated":"2024-03-26T13:21:28Z","published":"2023-12-05T05:36:44Z","title":"AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation\n  with Unified Audio-Visual Speech Representation","summary":"  This paper proposes a novel direct Audio-Visual Speech to Audio-Visual Speech\nTranslation (AV2AV) framework, where the input and output of the system are\nmultimodal (i.e., audio and visual speech). With the proposed AV2AV, two key\nadvantages can be brought: 1) We can perform real-like conversations with\nindividuals worldwide in a virtual meeting by utilizing our own primary\nlanguages. In contrast to Speech-to-Speech Translation (A2A), which solely\ntranslates between audio modalities, the proposed AV2AV directly translates\nbetween audio-visual speech. This capability enhances the dialogue experience\nby presenting synchronized lip movements along with the translated speech. 2)\nWe can improve the robustness of the spoken language translation system. By\nemploying the complementary information of audio-visual speech, the system can\neffectively translate spoken language even in the presence of acoustic noise,\nshowcasing robust performance. To mitigate the problem of the absence of a\nparallel AV2AV translation dataset, we propose to train our spoken language\ntranslation system with the audio-only dataset of A2A. This is done by learning\nunified audio-visual speech representations through self-supervised learning in\nadvance to train the translation system. Moreover, we propose an AV-Renderer\nthat can generate raw audio and video in parallel. It is designed with\nzero-shot speaker modeling, thus the speaker in source audio-visual speech can\nbe maintained at the target translated audio-visual speech. The effectiveness\nof AV2AV is evaluated with extensive experiments in a many-to-many language\ntranslation setting. Demo page is available on\nhttps://choijeongsoo.github.io/av2av.\n","authors":["Jeongsoo Choi","Se Jin Park","Minsu Kim","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2312.02512v2.pdf","comment":"CVPR 2024. Code & Demo: https://choijeongsoo.github.io/av2av"},{"id":"http://arxiv.org/abs/2304.10417v3","updated":"2024-03-26T13:16:02Z","published":"2023-04-20T16:01:55Z","title":"SINC: Spatial Composition of 3D Human Motions for Simultaneous Action\n  Generation","summary":"  Our goal is to synthesize 3D human motions given textual inputs describing\nsimultaneous actions, for example 'waving hand' while 'walking' at the same\ntime. We refer to generating such simultaneous movements as performing 'spatial\ncompositions'. In contrast to temporal compositions that seek to transition\nfrom one action to another, spatial compositing requires understanding which\nbody parts are involved in which action, to be able to move them\nsimultaneously. Motivated by the observation that the correspondence between\nactions and body parts is encoded in powerful language models, we extract this\nknowledge by prompting GPT-3 with text such as \"what are the body parts\ninvolved in the action <action name>?\", while also providing the parts list and\nfew-shot examples. Given this action-part mapping, we combine body parts from\ntwo motions together and establish the first automated method to spatially\ncompose two actions. However, training data with compositional actions is\nalways limited by the combinatorics. Hence, we further create synthetic data\nwith this approach, and use it to train a new state-of-the-art text-to-motion\ngeneration model, called SINC (\"SImultaneous actioN Compositions for 3D human\nmotions\"). In our experiments, that training with such GPT-guided synthetic\ndata improves spatial composition generation over baselines. Our code is\npublicly available at https://sinc.is.tue.mpg.de/.\n","authors":["Nikos Athanasiou","Mathis Petrovich","Michael J. Black","Gül Varol"],"pdf_url":"https://arxiv.org/pdf/2304.10417v3.pdf","comment":"Teaser Fixed"},{"id":"http://arxiv.org/abs/2403.14135v2","updated":"2024-03-26T13:15:12Z","published":"2024-03-21T05:10:26Z","title":"Powerful Lossy Compression for Noisy Images","summary":"  Image compression and denoising represent fundamental challenges in image\nprocessing with many real-world applications. To address practical demands,\ncurrent solutions can be categorized into two main strategies: 1) sequential\nmethod; and 2) joint method. However, sequential methods have the disadvantage\nof error accumulation as there is information loss between multiple individual\nmodels. Recently, the academic community began to make some attempts to tackle\nthis problem through end-to-end joint methods. Most of them ignore that\ndifferent regions of noisy images have different characteristics. To solve\nthese problems, in this paper, our proposed signal-to-noise ratio~(SNR) aware\njoint solution exploits local and non-local features for image compression and\ndenoising simultaneously. We design an end-to-end trainable network, which\nincludes the main encoder branch, the guidance branch, and the signal-to-noise\nratio~(SNR) aware branch. We conducted extensive experiments on both synthetic\nand real-world datasets, demonstrating that our joint solution outperforms\nexisting state-of-the-art methods.\n","authors":["Shilv Cai","Xiaoguo Liang","Shuning Cao","Luxin Yan","Sheng Zhong","Liqun Chen","Xu Zou"],"pdf_url":"https://arxiv.org/pdf/2403.14135v2.pdf","comment":"Accepted by ICME 2024"},{"id":"http://arxiv.org/abs/2308.10185v2","updated":"2024-03-26T13:11:07Z","published":"2023-08-20T07:26:51Z","title":"ViT-Lens: Initiating Omni-Modal Exploration through 3D Insights","summary":"  Though the success of CLIP-based training recipes in vision-language models,\ntheir scalability to more modalities (e.g., 3D, audio, etc.) is limited to\nlarge-scale data, which is expensive or even inapplicable for rare modalities.\nIn this paper, we present ViT-Lens that facilitates efficient omni-modal\nrepresentation learning by perceiving novel modalities with a pretrained ViT\nand aligning to a pre-defined space. Specifically, the modality-specific lens\nis tuned to project multimodal signals to the shared embedding space, which are\nthen processed by a strong ViT that carries pre-trained image knowledge. The\nencoded multimodal representations are optimized toward aligning with the\nmodal-independent space, pre-defined by off-the-shelf foundation models. A\nwell-trained lens with a ViT backbone has the potential to serve as one of\nthese foundation models, supervising the learning of subsequent modalities.\nViT-Lens provides a unified solution for representation learning of increasing\nmodalities with two appealing benefits: (i) Exploiting the pretrained ViT\nacross tasks and domains effectively with efficient data regime; (ii) Emergent\ndownstream capabilities of novel modalities are demonstrated due to the\nmodality alignment space. We evaluate ViT-Lens in the context of 3D as an\ninitial verification. In zero-shot 3D classification, ViT-Lens achieves\nsubstantial improvements over previous state-of-the-art, showing 52.0% accuracy\non Objaverse-LVIS, 87.4% on ModelNet40, and 60.6% on ScanObjectNN. Furthermore,\nwe enable zero-shot 3D question-answering by simply integrating the trained 3D\nlens into the InstructBLIP model without any adaptation. We will release the\nresults of ViT-Lens on more modalities in the near future.\n","authors":["Weixian Lei","Yixiao Ge","Jianfeng Zhang","Dylan Sun","Kun Yi","Ying Shan","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2308.10185v2.pdf","comment":"19 pages, 4 figures and 9 tables"},{"id":"http://arxiv.org/abs/2403.17678v1","updated":"2024-03-26T13:05:49Z","published":"2024-03-26T13:05:49Z","title":"Hierarchical Light Transformer Ensembles for Multimodal Trajectory\n  Forecasting","summary":"  Accurate trajectory forecasting is crucial for the performance of various\nsystems, such as advanced driver-assistance systems and self-driving vehicles.\nThese forecasts allow to anticipate events leading to collisions and,\ntherefore, to mitigate them. Deep Neural Networks have excelled in motion\nforecasting, but issues like overconfidence and uncertainty quantification\npersist. Deep Ensembles address these concerns, yet applying them to multimodal\ndistributions remains challenging. In this paper, we propose a novel approach\nnamed Hierarchical Light Transformer Ensembles (HLT-Ens), aimed at efficiently\ntraining an ensemble of Transformer architectures using a novel hierarchical\nloss function. HLT-Ens leverages grouped fully connected layers, inspired by\ngrouped convolution techniques, to capture multimodal distributions,\neffectively. Through extensive experimentation, we demonstrate that HLT-Ens\nachieves state-of-the-art performance levels, offering a promising avenue for\nimproving trajectory forecasting techniques.\n","authors":["Adrien Lafage","Mathieu Barbier","Gianni Franchi","David Filliat"],"pdf_url":"https://arxiv.org/pdf/2403.17678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17672v1","updated":"2024-03-26T13:02:38Z","published":"2024-03-26T13:02:38Z","title":"Predicting Perceived Gloss: Do Weak Labels Suffice?","summary":"  Estimating perceptual attributes of materials directly from images is a\nchallenging task due to their complex, not fully-understood interactions with\nexternal factors, such as geometry and lighting. Supervised deep learning\nmodels have recently been shown to outperform traditional approaches, but rely\non large datasets of human-annotated images for accurate perception\npredictions. Obtaining reliable annotations is a costly endeavor, aggravated by\nthe limited ability of these models to generalise to different aspects of\nappearance. In this work, we show how a much smaller set of human annotations\n(\"strong labels\") can be effectively augmented with automatically derived \"weak\nlabels\" in the context of learning a low-dimensional image-computable gloss\nmetric. We evaluate three alternative weak labels for predicting human gloss\nperception from limited annotated data. Incorporating weak labels enhances our\ngloss prediction beyond the current state of the art. Moreover, it enables a\nsubstantial reduction in human annotation costs without sacrificing accuracy,\nwhether working with rendered images or real photographs.\n","authors":["Julia Guerrero-Viu","J. Daniel Subias","Ana Serrano","Katherine R. Storrs","Roland W. Fleming","Belen Masia","Diego Gutierrez"],"pdf_url":"https://arxiv.org/pdf/2403.17672v1.pdf","comment":"Computer Graphics Forum (Eurographics 2024)"},{"id":"http://arxiv.org/abs/2310.01819v3","updated":"2024-03-26T12:59:39Z","published":"2023-10-03T06:16:38Z","title":"TP2O: Creative Text Pair-to-Object Generation using Balance\n  Swap-Sampling","summary":"  Generating creative combinatorial objects from two seemingly unrelated object\ntexts is a challenging task in text-to-image synthesis, often hindered by a\nfocus on emulating existing data distributions. In this paper, we develop a\nstraightforward yet highly effective method, called \\textbf{balance\nswap-sampling}. First, we propose a swapping mechanism that generates a novel\ncombinatorial object image set by randomly exchanging intrinsic elements of two\ntext embeddings through a cutting-edge diffusion model. Second, we introduce a\nbalance swapping region to efficiently sample a small subset from the newly\ngenerated image set by balancing CLIP distances between the new images and\ntheir original generations, increasing the likelihood of accepting the\nhigh-quality combinations. Last, we employ a segmentation method to compare\nCLIP distances among the segmented components, ultimately selecting the most\npromising object from the sampled subset. Extensive experiments demonstrate\nthat our approach outperforms recent SOTA T2I methods. Surprisingly, our\nresults even rival those of human artists, such as frog-broccoli.\n","authors":["Jun Li","Zedong Zhang","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2310.01819v3.pdf","comment":"Project page: https://tp2o.github.io/anon/"},{"id":"http://arxiv.org/abs/2312.00869v2","updated":"2024-03-26T12:56:55Z","published":"2023-12-01T19:00:17Z","title":"Segment and Caption Anything","summary":"  We propose a method to efficiently equip the Segment Anything Model (SAM)\nwith the ability to generate regional captions. SAM presents strong\ngeneralizability to segment anything while is short for semantic understanding.\nBy introducing a lightweight query-based feature mixer, we align the\nregion-specific features with the embedding space of language models for later\ncaption generation. As the number of trainable parameters is small (typically\nin the order of tens of millions), it costs less computation, less memory\nusage, and less communication bandwidth, resulting in both fast and scalable\ntraining. To address the scarcity problem of regional caption data, we propose\nto first pre-train our model on objection detection and segmentation tasks. We\ncall this step weak supervision pretraining since the pre-training data only\ncontains category names instead of full-sentence descriptions. The weak\nsupervision pretraining allows us to leverage many publicly available object\ndetection and segmentation datasets. We conduct extensive experiments to\ndemonstrate the superiority of our method and validate each design choice. This\nwork serves as a stepping stone towards scaling up regional captioning data and\nsheds light on exploring efficient ways to augment SAM with regional semantics.\nThe project page, along with the associated code, can be accessed via\nhttps://xk-huang.github.io/segment-caption-anything/.\n","authors":["Xiaoke Huang","Jianfeng Wang","Yansong Tang","Zheng Zhang","Han Hu","Jiwen Lu","Lijuan Wang","Zicheng Liu"],"pdf_url":"https://arxiv.org/pdf/2312.00869v2.pdf","comment":"The project page, along with the associated code, can be accessed via\n  https://xk-huang.github.io/segment-caption-anything/; Update author\n  information; Accepted by CVPR 24"},{"id":"http://arxiv.org/abs/2403.17664v1","updated":"2024-03-26T12:53:10Z","published":"2024-03-26T12:53:10Z","title":"DiffFAE: Advancing High-fidelity One-shot Facial Appearance Editing with\n  Space-sensitive Customization and Semantic Preservation","summary":"  Facial Appearance Editing (FAE) aims to modify physical attributes, such as\npose, expression and lighting, of human facial images while preserving\nattributes like identity and background, showing great importance in\nphotograph. In spite of the great progress in this area, current researches\ngenerally meet three challenges: low generation fidelity, poor attribute\npreservation, and inefficient inference. To overcome above challenges, this\npaper presents DiffFAE, a one-stage and highly-efficient diffusion-based\nframework tailored for high-fidelity FAE. For high-fidelity query attributes\ntransfer, we adopt Space-sensitive Physical Customization (SPC), which ensures\nthe fidelity and generalization ability by utilizing rendering texture derived\nfrom 3D Morphable Model (3DMM). In order to preserve source attributes, we\nintroduce the Region-responsive Semantic Composition (RSC). This module is\nguided to learn decoupled source-regarding features, thereby better preserving\nthe identity and alleviating artifacts from non-facial attributes such as hair,\nclothes, and background. We further introduce a consistency regularization for\nour pipeline to enhance editing controllability by leveraging prior knowledge\nin the attention matrices of diffusion model. Extensive experiments demonstrate\nthe superiority of DiffFAE over existing methods, achieving state-of-the-art\nperformance in facial appearance editing.\n","authors":["Qilin Wang","Jiangning Zhang","Chengming Xu","Weijian Cao","Ying Tai","Yue Han","Yanhao Ge","Hong Gu","Chengjie Wang","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2403.17664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14149v4","updated":"2024-03-26T12:47:12Z","published":"2023-12-21T18:59:06Z","title":"TagAlign: Improving Vision-Language Alignment with Multi-Tag\n  Classification","summary":"  The crux of learning vision-language models is to extract semantically\naligned information from visual and linguistic data. Existing attempts usually\nface the problem of coarse alignment, e.g., the vision encoder struggles in\nlocalizing an attribute-specified object. In this work, we propose an\nembarrassingly simple approach to better align image and text features with no\nneed of additional data formats other than image-text pairs. Concretely, given\nan image and its paired text, we manage to parse objects (e.g., cat) and\nattributes (e.g., black) from the description, which are highly likely to exist\nin the image. It is noteworthy that the parsing pipeline is fully automatic and\nthus enjoys good scalability. With these parsed semantics as supervision\nsignals, we can complement the commonly used image-text contrastive loss with\nthe multi-tag classification loss. Extensive experimental results on a broad\nsuite of semantic segmentation datasets substantiate the average 5.2\\%\nimprovement of our framework over existing alternatives. Furthermore, the\nvisualization results indicate that attribute supervision makes vision-language\nmodels accurately localize attribute-specified objects. Project page can be\nfound at https://qinying-liu.github.io/Tag-Align.\n","authors":["Qinying Liu","Wei Wu","Kecheng Zheng","Zhan Tong","Jiawei Liu","Yu Liu","Wei Chen","Zilei Wang","Yujun Shen"],"pdf_url":"https://arxiv.org/pdf/2312.14149v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03246v5","updated":"2024-03-26T12:35:03Z","published":"2024-02-05T18:03:53Z","title":"SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM","summary":"  We present SGS-SLAM, the first semantic visual SLAM system based on Gaussian\nSplatting. It incorporates appearance, geometry, and semantic features through\nmulti-channel optimization, addressing the oversmoothing limitations of neural\nimplicit SLAM systems in high-quality rendering, scene understanding, and\nobject-level geometry. We introduce a unique semantic feature loss that\neffectively compensates for the shortcomings of traditional depth and color\nlosses in object optimization. Through a semantic-guided keyframe selection\nstrategy, we prevent erroneous reconstructions caused by cumulative errors.\nExtensive experiments demonstrate that SGS-SLAM delivers state-of-the-art\nperformance in camera pose estimation, map reconstruction, precise semantic\nsegmentation, and object-level geometric accuracy, while ensuring real-time\nrendering capabilities.\n","authors":["Mingrui Li","Shuhong Liu","Heng Zhou","Guohao Zhu","Na Cheng","Tianchen Deng","Hongyu Wang"],"pdf_url":"https://arxiv.org/pdf/2402.03246v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17651v1","updated":"2024-03-26T12:31:58Z","published":"2024-03-26T12:31:58Z","title":"Exploring Dynamic Transformer for Efficient Object Tracking","summary":"  The speed-precision trade-off is a critical problem for visual object\ntracking which usually requires low latency and deployment on constrained\nresources. Existing solutions for efficient tracking mainly focus on adopting\nlight-weight backbones or modules, which nevertheless come at the cost of a\nsacrifice in precision. In this paper, inspired by dynamic network routing, we\npropose DyTrack, a dynamic transformer framework for efficient tracking.\nReal-world tracking scenarios exhibit diverse levels of complexity. We argue\nthat a simple network is sufficient for easy frames in video sequences, while\nmore computation could be assigned to difficult ones. DyTrack automatically\nlearns to configure proper reasoning routes for various inputs, gaining better\nutilization of the available computational budget. Thus, it can achieve higher\nperformance with the same running speed. We formulate instance-specific\ntracking as a sequential decision problem and attach terminating branches to\nintermediate layers of the entire model. Especially, to fully utilize the\ncomputations, we introduce the feature recycling mechanism to reuse the outputs\nof predecessors. Furthermore, a target-aware self-distillation strategy is\ndesigned to enhance the discriminating capabilities of early predictions by\neffectively mimicking the representation pattern of the deep model. Extensive\nexperiments on multiple benchmarks demonstrate that DyTrack achieves promising\nspeed-precision trade-offs with only a single model. For instance, DyTrack\nobtains 64.9% AUC on LaSOT with a speed of 256 fps.\n","authors":["Jiawen Zhu","Xin Chen","Haiwen Diao","Shuai Li","Jun-Yan He","Chenyang Li","Bin Luo","Dong Wang","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2403.17651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02109v2","updated":"2024-03-26T12:28:02Z","published":"2023-12-04T18:39:00Z","title":"ArtAdapter: Text-to-Image Style Transfer using Multi-Level Style Encoder\n  and Explicit Adaptation","summary":"  This work introduces ArtAdapter, a transformative text-to-image (T2I) style\ntransfer framework that transcends traditional limitations of color,\nbrushstrokes, and object shape, capturing high-level style elements such as\ncomposition and distinctive artistic expression. The integration of a\nmulti-level style encoder with our proposed explicit adaptation mechanism\nenables ArtAdapter to achieve unprecedented fidelity in style transfer,\nensuring close alignment with textual descriptions. Additionally, the\nincorporation of an Auxiliary Content Adapter (ACA) effectively separates\ncontent from style, alleviating the borrowing of content from style references.\nMoreover, our novel fast finetuning approach could further enhance zero-shot\nstyle representation while mitigating the risk of overfitting. Comprehensive\nevaluations confirm that ArtAdapter surpasses current state-of-the-art methods.\n","authors":["Dar-Yen Chen","Hamish Tennent","Ching-Wen Hsu"],"pdf_url":"https://arxiv.org/pdf/2312.02109v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17639v1","updated":"2024-03-26T12:21:47Z","published":"2024-03-26T12:21:47Z","title":"High-Resolution Image Translation Model Based on Grayscale Redefinition","summary":"  Image-to-image translation is a technique that focuses on transferring images\nfrom one domain to another while maintaining the essential content\nrepresentations. In recent years, image-to-image translation has gained\nsignificant attention and achieved remarkable advancements due to its diverse\napplications in computer vision and image processing tasks. In this work, we\npropose an innovative method for image translation between different domains.\nFor high-resolution image translation tasks, we use a grayscale adjustment\nmethod to achieve pixel-level translation. For other tasks, we utilize the\nPix2PixHD model with a coarse-to-fine generator, multi-scale discriminator, and\nimproved loss to enhance the image translation performance. On the other hand,\nto tackle the issue of sparse training data, we adopt model weight\ninitialization from other task to optimize the performance of the current task.\n","authors":["Xixian Wu","Dian Chao","Yang Yang"],"pdf_url":"https://arxiv.org/pdf/2403.17639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17638v1","updated":"2024-03-26T12:17:46Z","published":"2024-03-26T12:17:46Z","title":"Learning with Unreliability: Fast Few-shot Voxel Radiance Fields with\n  Relative Geometric Consistency","summary":"  We propose a voxel-based optimization framework, ReVoRF, for few-shot\nradiance fields that strategically address the unreliability in pseudo novel\nview synthesis. Our method pivots on the insight that relative depth\nrelationships within neighboring regions are more reliable than the absolute\ncolor values in disoccluded areas. Consequently, we devise a bilateral\ngeometric consistency loss that carefully navigates the trade-off between color\nfidelity and geometric accuracy in the context of depth consistency for\nuncertain regions. Moreover, we present a reliability-guided learning strategy\nto discern and utilize the variable quality across synthesized views,\ncomplemented by a reliability-aware voxel smoothing algorithm that smoothens\nthe transition between reliable and unreliable data patches. Our approach\nallows for a more nuanced use of all available data, promoting enhanced\nlearning from regions previously considered unsuitable for high-quality\nreconstruction. Extensive experiments across diverse datasets reveal that our\napproach attains significant gains in efficiency and accuracy, delivering\nrendering speeds of 3 FPS, 7 mins to train a $360^\\circ$ scene, and a 5\\%\nimprovement in PSNR over existing few-shot methods. Code is available at\nhttps://github.com/HKCLynn/ReVoRF.\n","authors":["Yingjie Xu","Bangzhen Liu","Hao Tang","Bailin Deng","Shengfeng He"],"pdf_url":"https://arxiv.org/pdf/2403.17638v1.pdf","comment":"CVPR 2024 final version"},{"id":"http://arxiv.org/abs/2403.15010v2","updated":"2024-03-26T12:16:14Z","published":"2024-03-22T07:47:13Z","title":"Clean-image Backdoor Attacks","summary":"  To gather a significant quantity of annotated training data for\nhigh-performance image classification models, numerous companies opt to enlist\nthird-party providers to label their unlabeled data. This practice is widely\nregarded as secure, even in cases where some annotated errors occur, as the\nimpact of these minor inaccuracies on the final performance of the models is\nnegligible and existing backdoor attacks require attacker's ability to poison\nthe training images. Nevertheless, in this paper, we propose clean-image\nbackdoor attacks which uncover that backdoors can still be injected via a\nfraction of incorrect labels without modifying the training images.\nSpecifically, in our attacks, the attacker first seeks a trigger feature to\ndivide the training images into two parts: those with the feature and those\nwithout it. Subsequently, the attacker falsifies the labels of the former part\nto a backdoor class. The backdoor will be finally implanted into the target\nmodel after it is trained on the poisoned data. During the inference phase, the\nattacker can activate the backdoor in two ways: slightly modifying the input\nimage to obtain the trigger feature, or taking an image that naturally has the\ntrigger feature as input. We conduct extensive experiments to demonstrate the\neffectiveness and practicality of our attacks. According to the experimental\nresults, we conclude that our attacks seriously jeopardize the fairness and\nrobustness of image classification models, and it is necessary to be vigilant\nabout the incorrect labels in outsourced labeling.\n","authors":["Dazhong Rong","Guoyao Yu","Shuheng Shen","Xinyi Fu","Peng Qian","Jianhai Chen","Qinming He","Xing Fu","Weiqiang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.15010v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06683v2","updated":"2024-03-26T12:10:13Z","published":"2024-03-11T12:57:51Z","title":"Transferring Relative Monocular Depth to Surgical Vision with Temporal\n  Consistency","summary":"  Relative monocular depth, inferring depth up to shift and scale from a single\nimage, is an active research topic. Recent deep learning models, trained on\nlarge and varied meta-datasets, now provide excellent performance in the domain\nof natural images. However, few datasets exist which provide ground truth depth\nfor endoscopic images, making training such models from scratch unfeasible.\nThis work investigates the transfer of these models into the surgical domain,\nand presents an effective and simple way to improve on standard supervision\nthrough the use of temporal consistency self-supervision. We show temporal\nconsistency significantly improves supervised training alone when transferring\nto the low-data regime of endoscopy, and outperforms the prevalent\nself-supervision technique for this task. In addition we show our method\ndrastically outperforms the state-of-the-art method from within the domain of\nendoscopy. We also release our code, model and ensembled meta-dataset,\nMeta-MED, establishing a strong benchmark for future work.\n","authors":["Charlie Budd","Tom Vercauteren"],"pdf_url":"https://arxiv.org/pdf/2403.06683v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17633v1","updated":"2024-03-26T12:08:14Z","published":"2024-03-26T12:08:14Z","title":"UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object\n  Detection with Sparse LiDAR and Large Domain Gaps","summary":"  In this study, we address a gap in existing unsupervised domain adaptation\napproaches on LiDAR-based 3D object detection, which have predominantly\nconcentrated on adapting between established, high-density autonomous driving\ndatasets. We focus on sparser point clouds, capturing scenarios from different\nperspectives: not just from vehicles on the road but also from mobile robots on\nsidewalks, which encounter significantly different environmental conditions and\nsensor configurations. We introduce Unsupervised Adversarial Domain Adaptation\nfor 3D Object Detection (UADA3D). UADA3D does not depend on pre-trained source\nmodels or teacher-student architectures. Instead, it uses an adversarial\napproach to directly learn domain-invariant features. We demonstrate its\nefficacy in various adaptation scenarios, showing significant improvements in\nboth self-driving car and mobile robot domains. Our code is open-source and\nwill be available soon.\n","authors":["Maciej K Wozniak","Mattias Hansson","Marko Thiel","Patric Jensfelt"],"pdf_url":"https://arxiv.org/pdf/2403.17633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17631v1","updated":"2024-03-26T12:08:04Z","published":"2024-03-26T12:08:04Z","title":"AniArtAvatar: Animatable 3D Art Avatar from a Single Image","summary":"  We present a novel approach for generating animatable 3D-aware art avatars\nfrom a single image, with controllable facial expressions, head poses, and\nshoulder movements. Unlike previous reenactment methods, our approach utilizes\na view-conditioned 2D diffusion model to synthesize multi-view images from a\nsingle art portrait with a neutral expression. With the generated colors and\nnormals, we synthesize a static avatar using an SDF-based neural surface. For\navatar animation, we extract control points, transfer the motion with these\npoints, and deform the implicit canonical space. Firstly, we render the front\nimage of the avatar, extract the 2D landmarks, and project them to the 3D space\nusing a trained SDF network. We extract 3D driving landmarks using 3DMM and\ntransfer the motion to the avatar landmarks. To animate the avatar pose, we\nmanually set the body height and bound the head and torso of an avatar with two\ncages. The head and torso can be animated by transforming the two cages. Our\napproach is a one-shot pipeline that can be applied to various styles.\nExperiments demonstrate that our method can generate high-quality 3D art\navatars with desired control over different motions.\n","authors":["Shaoxu Li"],"pdf_url":"https://arxiv.org/pdf/2403.17631v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.01598v3","updated":"2024-03-26T11:54:40Z","published":"2023-06-02T15:09:19Z","title":"Towards Source-free Domain Adaptive Semantic Segmentation via\n  Importance-aware and Prototype-contrast Learning","summary":"  Domain adaptive semantic segmentation enables robust pixel-wise understanding\nin real-world driving scenes. Source-free domain adaptation, as a more\npractical technique, addresses the concerns of data privacy and storage\nlimitations in typical unsupervised domain adaptation methods, making it\nespecially relevant in the context of intelligent vehicles. It utilizes a\nwell-trained source model and unlabeled target data to achieve adaptation in\nthe target domain. However, in the absence of source data and target labels,\ncurrent solutions cannot sufficiently reduce the impact of domain shift and\nfully leverage the information from the target data. In this paper, we propose\nan end-to-end source-free domain adaptation semantic segmentation method via\nImportance-Aware and Prototype-Contrast (IAPC) learning. The proposed IAPC\nframework effectively extracts domain-invariant knowledge from the well-trained\nsource model and learns domain-specific knowledge from the unlabeled target\ndomain. Specifically, considering the problem of domain shift in the prediction\nof the target domain by the source model, we put forward an importance-aware\nmechanism for the biased target prediction probability distribution to extract\ndomain-invariant knowledge from the source model. We further introduce a\nprototype-contrast strategy, which includes a prototype-symmetric cross-entropy\nloss and a prototype-enhanced cross-entropy loss, to learn target intra-domain\nknowledge without relying on labels. A comprehensive variety of experiments on\ntwo domain adaptive semantic segmentation benchmarks demonstrates that the\nproposed end-to-end IAPC solution outperforms existing state-of-the-art\nmethods. The source code is publicly available at\nhttps://github.com/yihong-97/Source-free-IAPC.\n","authors":["Yihong Cao","Hui Zhang","Xiao Lu","Zheng Xiao","Kailun Yang","Yaonan Wang"],"pdf_url":"https://arxiv.org/pdf/2306.01598v3.pdf","comment":"Accepted to IEEE Transactions on Intelligent Vehicles (T-IV). The\n  source code is publicly available at\n  https://github.com/yihong-97/Source-free-IAPC"},{"id":"http://arxiv.org/abs/2310.17569v2","updated":"2024-03-26T11:52:23Z","published":"2023-10-26T16:58:01Z","title":"SD4Match: Learning to Prompt Stable Diffusion Model for Semantic\n  Matching","summary":"  In this paper, we address the challenge of matching semantically similar\nkeypoints across image pairs. Existing research indicates that the intermediate\noutput of the UNet within the Stable Diffusion (SD) can serve as robust image\nfeature maps for such a matching task. We demonstrate that by employing a basic\nprompt tuning technique, the inherent potential of Stable Diffusion can be\nharnessed, resulting in a significant enhancement in accuracy over previous\napproaches. We further introduce a novel conditional prompting module that\nconditions the prompt on the local details of the input image pairs, leading to\na further improvement in performance. We designate our approach as SD4Match,\nshort for Stable Diffusion for Semantic Matching. Comprehensive evaluations of\nSD4Match on the PF-Pascal, PF-Willow, and SPair-71k datasets show that it sets\nnew benchmarks in accuracy across all these datasets. Particularly, SD4Match\noutperforms the previous state-of-the-art by a margin of 12 percentage points\non the challenging SPair-71k dataset.\n","authors":["Xinghui Li","Jingyi Lu","Kai Han","Victor Prisacariu"],"pdf_url":"https://arxiv.org/pdf/2310.17569v2.pdf","comment":"Accepted to CVPR 2024. Project website:\n  https://sd4match.active.vision/"},{"id":"http://arxiv.org/abs/2403.17615v1","updated":"2024-03-26T11:48:37Z","published":"2024-03-26T11:48:37Z","title":"Grad-CAMO: Learning Interpretable Single-Cell Morphological Profiles\n  from 3D Cell Painting Images","summary":"  Despite their black-box nature, deep learning models are extensively used in\nimage-based drug discovery to extract feature vectors from single cells in\nmicroscopy images. To better understand how these networks perform\nrepresentation learning, we employ visual explainability techniques (e.g.,\nGrad-CAM). Our analyses reveal several mechanisms by which supervised models\ncheat, exploiting biologically irrelevant pixels when extracting morphological\nfeatures from images, such as noise in the background. This raises doubts\nregarding the fidelity of learned single-cell representations and their\nrelevance when investigating downstream biological questions. To address this\nmisalignment between researcher expectations and machine behavior, we introduce\nGrad-CAMO, a novel single-cell interpretability score for supervised feature\nextractors. Grad-CAMO measures the proportion of a model's attention that is\nconcentrated on the cell of interest versus the background. This metric can be\nassessed per-cell or averaged across a validation set, offering a tool to audit\nindividual features vectors or guide the improved design of deep learning\narchitectures. Importantly, Grad-CAMO seamlessly integrates into existing\nworkflows, requiring no dataset or model modifications, and is compatible with\nboth 2D and 3D Cell Painting data. Additional results are available at\nhttps://github.com/eigenvivek/Grad-CAMO.\n","authors":["Vivek Gopalakrishnan","Jingzhe Ma","Zhiyong Xie"],"pdf_url":"https://arxiv.org/pdf/2403.17615v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17610v1","updated":"2024-03-26T11:43:05Z","published":"2024-03-26T11:43:05Z","title":"MMVP: A Multimodal MoCap Dataset with Vision and Pressure Sensors","summary":"  Foot contact is an important cue not only for human motion capture but also\nfor motion understanding and physically plausible motion generation. However,\nmost of the foot-contact annotations in existing datasets are estimated by\npurely visual matching and distance thresholding, which results in low accuracy\nand coarse granularity. Even though existing multimodal datasets\nsynergistically capture plantar pressure (foot contact) and visual signals,\nthey are specifically designed for small-range and slow motion such as Taiji\nQuan and Yoga. Therefore, there is still a lack of a vision-pressure multimodal\ndataset with large-range and fast human motion, as well as accurate and dense\nfoot-contact annotation. To fill this gap, we propose a Multimodal MoCap\nDataset with Vision and Pressure sensors, named MMVP. MMVP provides accurate\nand dense plantar pressure signals synchronized with RGBD observations, which\nis especially useful for both plausible shape estimation, robust pose fitting\nwithout foot drifting, and accurate global translation tracking. To validate\nthe dataset, we propose an RGBD-P SMPL fitting method and also a\nmonocular-video-based baseline framework, VP-MoCap, for human motion capture.\nExperiments demonstrate that our RGBD-P SMPL Fitting results significantly\noutperform pure visual motion capture. Moreover, VP-MoCap outperforms SOTA\nmethods in foot-contact and global translation estimation accuracy. We believe\nthe configuration of the dataset and the baseline frameworks will stimulate the\nresearch in this direction and also provide a good reference for MoCap\napplications in various domains. Project page:\nhttps://haolyuan.github.io/MMVP-Dataset/.\n","authors":["He Zhang","Shenghao Ren","Haolei Yuan","Jianhui Zhao","Fan Li","Shuangpeng Sun","Zhenghao Liang","Tao Yu","Qiu Shen","Xun Cao"],"pdf_url":"https://arxiv.org/pdf/2403.17610v1.pdf","comment":"CVPR2024"},{"id":"http://arxiv.org/abs/2403.17608v1","updated":"2024-03-26T11:39:00Z","published":"2024-03-26T11:39:00Z","title":"Fake or JPEG? Revealing Common Biases in Generated Image Detection\n  Datasets","summary":"  The widespread adoption of generative image models has highlighted the urgent\nneed to detect artificial content, which is a crucial step in combating\nwidespread manipulation and misinformation. Consequently, numerous detectors\nand associated datasets have emerged. However, many of these datasets\ninadvertently introduce undesirable biases, thereby impacting the effectiveness\nand evaluation of detectors. In this paper, we emphasize that many datasets for\nAI-generated image detection contain biases related to JPEG compression and\nimage size. Using the GenImage dataset, we demonstrate that detectors indeed\nlearn from these undesired factors. Furthermore, we show that removing the\nnamed biases substantially increases robustness to JPEG compression and\nsignificantly alters the cross-generator performance of evaluated detectors.\nSpecifically, it leads to more than 11 percentage points increase in\ncross-generator performance for ResNet50 and Swin-T detectors on the GenImage\ndataset, achieving state-of-the-art results.\n  We provide the dataset and source codes of this paper on the anonymous\nwebsite: https://www.unbiased-genimage.org\n","authors":["Patrick Grommelt","Louis Weiss","Franz-Josef Pfreundt","Janis Keuper"],"pdf_url":"https://arxiv.org/pdf/2403.17608v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04701v3","updated":"2024-03-26T11:26:17Z","published":"2024-03-07T17:48:48Z","title":"ObjectCompose: Evaluating Resilience of Vision-Based Models on\n  Object-to-Background Compositional Changes","summary":"  Given the large-scale multi-modal training of recent vision-based models and\ntheir generalization capabilities, understanding the extent of their robustness\nis critical for their real-world deployment. In this work, we evaluate the\nresilience of current vision-based models against diverse object-to-background\ncontext variations. The majority of robustness evaluation methods have\nintroduced synthetic datasets to induce changes to object characteristics\n(viewpoints, scale, color) or utilized image transformation techniques\n(adversarial changes, common corruptions) on real images to simulate shifts in\ndistributions. Recent works have explored leveraging large language models and\ndiffusion models to generate changes in the background. However, these methods\neither lack in offering control over the changes to be made or distort the\nobject semantics, making them unsuitable for the task. Our method, on the other\nhand, can induce diverse object-to-background changes while preserving the\noriginal semantics and appearance of the object. To achieve this goal, we\nharness the generative capabilities of text-to-image, image-to-text, and\nimage-to-segment models to automatically generate a broad spectrum of\nobject-to-background changes. We induce both natural and adversarial background\nchanges by either modifying the textual prompts or optimizing the latents and\ntextual embedding of text-to-image models. We produce various versions of\nstandard vision datasets (ImageNet, COCO), incorporating either diverse and\nrealistic backgrounds into the images or introducing color, texture, and\nadversarial changes in the background. We conduct extensive experiment to\nanalyze the robustness of vision-based models against object-to-background\ncontext variations across diverse tasks. Code\nhttps://github.com/Muhammad-Huzaifaa/ObjectCompose.git\n","authors":["Hashmat Shadab Malik","Muhammad Huzaifa","Muzammal Naseer","Salman Khan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2403.04701v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13518v2","updated":"2024-03-26T11:16:47Z","published":"2024-03-20T11:38:30Z","title":"Motion Generation from Fine-grained Textual Descriptions","summary":"  The task of text2motion is to generate human motion sequences from given\ntextual descriptions, where the model explores diverse mappings from natural\nlanguage instructions to human body movements. While most existing works are\nconfined to coarse-grained motion descriptions, e.g., \"A man squats.\",\nfine-grained descriptions specifying movements of relevant body parts are\nbarely explored. Models trained with coarse-grained texts may not be able to\nlearn mappings from fine-grained motion-related words to motion primitives,\nresulting in the failure to generate motions from unseen descriptions. In this\npaper, we build a large-scale language-motion dataset specializing in\nfine-grained textual descriptions, FineHumanML3D, by feeding GPT-3.5-turbo with\nstep-by-step instructions with pseudo-code compulsory checks. Accordingly, we\ndesign a new text2motion model, FineMotionDiffuse, making full use of\nfine-grained textual information. Our quantitative evaluation shows that\nFineMotionDiffuse trained on FineHumanML3D improves FID by a large margin of\n0.38, compared with competitive baselines. According to the qualitative\nevaluation and case study, our model outperforms MotionDiffuse in generating\nspatially or chronologically composite motions, by learning the implicit\nmappings from fine-grained descriptions to the corresponding basic motions. We\nrelease our data at https://github.com/KunhangL/finemotiondiffuse.\n","authors":["Kunhang Li","Yansong Feng"],"pdf_url":"https://arxiv.org/pdf/2403.13518v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15905v2","updated":"2024-03-26T11:11:49Z","published":"2024-03-23T18:19:02Z","title":"Towards Low-Energy Adaptive Personalization for Resource-Constrained\n  Devices","summary":"  The personalization of machine learning (ML) models to address data drift is\na significant challenge in the context of Internet of Things (IoT)\napplications. Presently, most approaches focus on fine-tuning either the full\nbase model or its last few layers to adapt to new data, while often neglecting\nenergy costs. However, various types of data drift exist, and fine-tuning the\nfull base model or the last few layers may not result in optimal performance in\ncertain scenarios. We propose Target Block Fine-Tuning (TBFT), a low-energy\nadaptive personalization framework designed for resource-constrained devices.\nWe categorize data drift and personalization into three types: input-level,\nfeature-level, and output-level. For each type, we fine-tune different blocks\nof the model to achieve optimal performance with reduced energy costs.\nSpecifically, input-, feature-, and output-level correspond to fine-tuning the\nfront, middle, and rear blocks of the model. We evaluate TBFT on a ResNet\nmodel, three datasets, three different training sizes, and a Raspberry Pi.\nCompared with the $Block Avg$, where each block is fine-tuned individually and\ntheir performance improvements are averaged, TBFT exhibits an improvement in\nmodel accuracy by an average of 15.30% whilst saving 41.57% energy consumption\non average compared with full fine-tuning.\n","authors":["Yushan Huang","Josh Millar","Yuxuan Long","Yuchen Zhao","Hamed Hadaddi"],"pdf_url":"https://arxiv.org/pdf/2403.15905v2.pdf","comment":"Accepetd to The 4th Workshop on Machine Learning and Systems\n  (EuroMLSys '24)"},{"id":"http://arxiv.org/abs/2403.07576v2","updated":"2024-03-26T10:55:51Z","published":"2024-03-12T12:05:43Z","title":"FPT: Fine-grained Prompt Tuning for Parameter and Memory Efficient Fine\n  Tuning in High-resolution Medical Image Classification","summary":"  Parameter-efficient fine-tuning (PEFT) is proposed as a cost-effective way to\ntransfer pre-trained models to downstream tasks, avoiding the high cost of\nupdating entire large-scale pre-trained models (LPMs). In this work, we present\nFine-grained Prompt Tuning (FPT), a novel PEFT method for medical image\nclassification. FPT significantly reduces memory consumption compared to other\nPEFT methods, especially in high-resolution contexts. To achieve this, we first\nfreeze the weights of the LPM and construct a learnable lightweight side\nnetwork. The frozen LPM takes high-resolution images as input to extract\nfine-grained features, while the side network is fed low-resolution images to\nreduce memory usage. To allow the side network to access pre-trained knowledge,\nwe introduce fine-grained prompts that summarize information from the LPM\nthrough a fusion module. Important tokens selection and preloading techniques\nare employed to further reduce training cost and memory requirements. We\nevaluate FPT on four medical datasets with varying sizes, modalities, and\ncomplexities. Experimental results demonstrate that FPT achieves comparable\nperformance to fine-tuning the entire LPM while using only 1.8% of the\nlearnable parameters and 13% of the memory costs of an encoder ViT-B model with\na 512 x 512 input resolution.\n","authors":["Yijin Huang","Pujin Cheng","Roger Tam","Xiaoying Tang"],"pdf_url":"https://arxiv.org/pdf/2403.07576v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17589v1","updated":"2024-03-26T10:54:07Z","published":"2024-03-26T10:54:07Z","title":"Dual Memory Networks: A Versatile Adaptation Approach for\n  Vision-Language Models","summary":"  With the emergence of pre-trained vision-language models like CLIP, how to\nadapt them to various downstream classification tasks has garnered significant\nattention in recent research. The adaptation strategies can be typically\ncategorized into three paradigms: zero-shot adaptation, few-shot adaptation,\nand the recently-proposed training-free few-shot adaptation. Most existing\napproaches are tailored for a specific setting and can only cater to one or two\nof these paradigms. In this paper, we introduce a versatile adaptation approach\nthat can effectively work under all three settings. Specifically, we propose\nthe dual memory networks that comprise dynamic and static memory components.\nThe static memory caches training data knowledge, enabling training-free\nfew-shot adaptation, while the dynamic memory preserves historical test\nfeatures online during the testing process, allowing for the exploration of\nadditional data insights beyond the training set. This novel capability\nenhances model performance in the few-shot setting and enables model usability\nin the absence of training data. The two memory networks employ the same\nflexible memory interactive strategy, which can operate in a training-free mode\nand can be further enhanced by incorporating learnable projection layers. Our\napproach is tested across 11 datasets under the three task settings.\nRemarkably, in the zero-shot scenario, it outperforms existing methods by over\n3\\% and even shows superior results against methods utilizing external training\ndata. Additionally, our method exhibits robust performance against natural\ndistribution shifts. Codes are available at \\url{https://github.com/YBZh/DMN}.\n","authors":["Yabin Zhang","Wenjie Zhu","Hui Tang","Zhiyuan Ma","Kaiyang Zhou","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.17589v1.pdf","comment":"CVPR2024; Codes are available at \\url{https://github.com/YBZh/DMN}"},{"id":"http://arxiv.org/abs/2311.13385v3","updated":"2024-03-26T10:21:46Z","published":"2023-11-22T13:27:36Z","title":"SegVol: Universal and Interactive Volumetric Medical Image Segmentation","summary":"  Precise image segmentation provides clinical study with instructive\ninformation. Despite the remarkable progress achieved in medical image\nsegmentation, there is still an absence of 3D foundation segmentation model\nthat can segment a wide range of anatomical categories with easy user\ninteraction. In this paper, we propose a 3D foundation segmentation model,\nnamed SegVol, supporting universal and interactive volumetric medical image\nsegmentation. By scaling up training data to 90K unlabeled Computed Tomography\n(CT) volumes and 6K labeled CT volumes, this foundation model supports the\nsegmentation of over 200 anatomical categories using semantic and spatial\nprompts. Extensive experiments on 10 internal validation tasks and 18 external\nvalidation tasks verify that SegVol outperforms the state of the art by a large\nmargin. Through its capacity to provide precise volumetric segmentation across\nvarious anatomical categories, SegVol has the potential to accelerate\nadvancements in medical imaging diagnosis and facilitate treatment\noptimization. The model and code are publicly available at:\nhttps://github.com/BAAI-DCAI/SegVol.\n","authors":["Yuxin Du","Fan Bai","Tiejun Huang","Bo Zhao"],"pdf_url":"https://arxiv.org/pdf/2311.13385v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03611v2","updated":"2024-03-26T10:13:11Z","published":"2023-12-06T16:55:53Z","title":"DreamComposer: Controllable 3D Object Generation via Multi-View\n  Conditions","summary":"  Utilizing pre-trained 2D large-scale generative models, recent works are\ncapable of generating high-quality novel views from a single in-the-wild image.\nHowever, due to the lack of information from multiple views, these works\nencounter difficulties in generating controllable novel views. In this paper,\nwe present DreamComposer, a flexible and scalable framework that can enhance\nexisting view-aware diffusion models by injecting multi-view conditions.\nSpecifically, DreamComposer first uses a view-aware 3D lifting module to obtain\n3D representations of an object from multiple views. Then, it renders the\nlatent features of the target view from 3D representations with the multi-view\nfeature fusion module. Finally the target view features extracted from\nmulti-view inputs are injected into a pre-trained diffusion model. Experiments\nshow that DreamComposer is compatible with state-of-the-art diffusion models\nfor zero-shot novel view synthesis, further enhancing them to generate\nhigh-fidelity novel view images with multi-view conditions, ready for\ncontrollable 3D object reconstruction and various other applications.\n","authors":["Yunhan Yang","Yukun Huang","Xiaoyang Wu","Yuan-Chen Guo","Song-Hai Zhang","Hengshuang Zhao","Tong He","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2312.03611v2.pdf","comment":"Project Page: https://yhyang-myron.github.io/DreamComposer/"},{"id":"http://arxiv.org/abs/2312.08879v2","updated":"2024-03-26T10:04:11Z","published":"2023-12-12T11:00:39Z","title":"Regularizing Self-supervised 3D Scene Flows with Surface Awareness and\n  Cyclic Consistency","summary":"  Learning without supervision how to predict 3D scene flows from point clouds\nis essential to many perception systems. We propose a novel learning framework\nfor this task which improves the necessary regularization. Relying on the\nassumption that scene elements are mostly rigid, current smoothness losses are\nbuilt on the definition of ``rigid clusters\" in the input point clouds. The\ndefinition of these clusters is challenging and has a significant impact on the\nquality of predicted flows. We introduce two new consistency losses that\nenlarge clusters while preventing them from spreading over distinct objects. In\nparticular, we enforce \\emph{temporal} consistency with a forward-backward\ncyclic loss and \\emph{spatial} consistency by considering surface orientation\nsimilarity in addition to spatial proximity. The proposed losses are\nmodel-independent and can thus be used in a plug-and-play fashion to\nsignificantly improve the performance of existing models, as demonstrated on\ntwo most widely used architectures. We also showcase the effectiveness and\ngeneralization capability of our framework on four standard sensor-unique\ndriving datasets, achieving state-of-the-art performance in 3D scene flow\nestimation. Our codes are available on https://github.com/ctu-vras/sac-flow.\n","authors":["Patrik Vacek","David Hurych","Karel Zimmermann","Patrick Perez","Tomas Svoboda"],"pdf_url":"https://arxiv.org/pdf/2312.08879v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17550v1","updated":"2024-03-26T09:58:06Z","published":"2024-03-26T09:58:06Z","title":"DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping","summary":"  Recently, significant progress has been achieved in sensing real large-scale\noutdoor 3D environments, particularly by using modern acquisition equipment\nsuch as LiDAR sensors. Unfortunately, they are fundamentally limited in their\nability to produce dense, complete 3D scenes. To address this issue, recent\nlearning-based methods integrate neural implicit representations and\noptimizable feature grids to approximate surfaces of 3D scenes. However,\nnaively fitting samples along raw LiDAR rays leads to noisy 3D mapping results\ndue to the nature of sparse, conflicting LiDAR measurements. Instead, in this\nwork we depart from fitting LiDAR data exactly, instead letting the network\noptimize a non-metric monotonic implicit field defined in 3D space. To fit our\nfield, we design a learning system integrating a monotonicity loss that enables\noptimizing neural monotonic fields and leverages recent progress in large-scale\n3D mapping. Our algorithm achieves high-quality dense 3D mapping performance as\ncaptured by multiple quantitative and perceptual measures and visual results\nobtained for Mai City, Newer College, and KITTI benchmarks. The code of our\napproach will be made publicly available.\n","authors":["Kutay Yılmaz","Matthias Nießner","Anastasiia Kornilova","Alexey Artemov"],"pdf_url":"https://arxiv.org/pdf/2403.17550v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.17549v1","updated":"2024-03-26T09:55:49Z","published":"2024-03-26T09:55:49Z","title":"Practical Applications of Advanced Cloud Services and Generative AI\n  Systems in Medical Image Analysis","summary":"  The medical field is one of the important fields in the application of\nartificial intelligence technology. With the explosive growth and\ndiversification of medical data, as well as the continuous improvement of\nmedical needs and challenges, artificial intelligence technology is playing an\nincreasingly important role in the medical field. Artificial intelligence\ntechnologies represented by computer vision, natural language processing, and\nmachine learning have been widely penetrated into diverse scenarios such as\nmedical imaging, health management, medical information, and drug research and\ndevelopment, and have become an important driving force for improving the level\nand quality of medical services.The article explores the transformative\npotential of generative AI in medical imaging, emphasizing its ability to\ngenerate syntheticACM-2 data, enhance images, aid in anomaly detection, and\nfacilitate image-to-image translation. Despite challenges like model\ncomplexity, the applications of generative models in healthcare, including\nMed-PaLM 2 technology, show promising results. By addressing limitations in\ndataset size and diversity, these models contribute to more accurate diagnoses\nand improved patient outcomes. However, ethical considerations and\ncollaboration among stakeholders are essential for responsible implementation.\nThrough experiments leveraging GANs to augment brain tumor MRI datasets, the\nstudy demonstrates how generative AI can enhance image quality and diversity,\nultimately advancing medical diagnostics and patient care.\n","authors":["Jingyu Xu","Binbin Wu","Jiaxin Huang","Yulu Gong","Yifan Zhang","Bo Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17545v1","updated":"2024-03-26T09:49:35Z","published":"2024-03-26T09:49:35Z","title":"A Gaze-grounded Visual Question Answering Dataset for Clarifying\n  Ambiguous Japanese Questions","summary":"  Situated conversations, which refer to visual information as visual question\nanswering (VQA), often contain ambiguities caused by reliance on directive\ninformation. This problem is exacerbated because some languages, such as\nJapanese, often omit subjective or objective terms. Such ambiguities in\nquestions are often clarified by the contexts in conversational situations,\nsuch as joint attention with a user or user gaze information. In this study, we\npropose the Gaze-grounded VQA dataset (GazeVQA) that clarifies ambiguous\nquestions using gaze information by focusing on a clarification process\ncomplemented by gaze information. We also propose a method that utilizes gaze\ntarget estimation results to improve the accuracy of GazeVQA tasks. Our\nexperimental results showed that the proposed method improved the performance\nin some cases of a VQA system on GazeVQA and identified some typical problems\nof GazeVQA tasks that need to be improved.\n","authors":["Shun Inadumi","Seiya Kawano","Akishige Yuguchi","Yasutomo Kawanishi","Koichiro Yoshino"],"pdf_url":"https://arxiv.org/pdf/2403.17545v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17541v1","updated":"2024-03-26T09:44:34Z","published":"2024-03-26T09:44:34Z","title":"WordRobe: Text-Guided Generation of Textured 3D Garments","summary":"  In this paper, we tackle a new and challenging problem of text-driven\ngeneration of 3D garments with high-quality textures. We propose \"WordRobe\", a\nnovel framework for the generation of unposed & textured 3D garment meshes from\nuser-friendly text prompts. We achieve this by first learning a latent\nrepresentation of 3D garments using a novel coarse-to-fine training strategy\nand a loss for latent disentanglement, promoting better latent interpolation.\nSubsequently, we align the garment latent space to the CLIP embedding space in\na weakly supervised manner, enabling text-driven 3D garment generation and\nediting. For appearance modeling, we leverage the zero-shot generation\ncapability of ControlNet to synthesize view-consistent texture maps in a single\nfeed-forward inference step, thereby drastically decreasing the generation time\nas compared to existing methods. We demonstrate superior performance over\ncurrent SOTAs for learning 3D garment latent space, garment interpolation, and\ntext-driven texture synthesis, supported by quantitative evaluation and\nqualitative user study. The unposed 3D garment meshes generated using WordRobe\ncan be directly fed to standard cloth simulation & animation pipelines without\nany post-processing.\n","authors":["Astitva Srivastava","Pranav Manu","Amit Raj","Varun Jampani","Avinash Sharma"],"pdf_url":"https://arxiv.org/pdf/2403.17541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17537v1","updated":"2024-03-26T09:42:28Z","published":"2024-03-26T09:42:28Z","title":"NeRF-HuGS: Improved Neural Radiance Fields in Non-static Scenes Using\n  Heuristics-Guided Segmentation","summary":"  Neural Radiance Field (NeRF) has been widely recognized for its excellence in\nnovel view synthesis and 3D scene reconstruction. However, their effectiveness\nis inherently tied to the assumption of static scenes, rendering them\nsusceptible to undesirable artifacts when confronted with transient distractors\nsuch as moving objects or shadows. In this work, we propose a novel paradigm,\nnamely \"Heuristics-Guided Segmentation\" (HuGS), which significantly enhances\nthe separation of static scenes from transient distractors by harmoniously\ncombining the strengths of hand-crafted heuristics and state-of-the-art\nsegmentation models, thus significantly transcending the limitations of\nprevious solutions. Furthermore, we delve into the meticulous design of\nheuristics, introducing a seamless fusion of Structure-from-Motion (SfM)-based\nheuristics and color residual heuristics, catering to a diverse range of\ntexture profiles. Extensive experiments demonstrate the superiority and\nrobustness of our method in mitigating transient distractors for NeRFs trained\nin non-static scenes. Project page: https://cnhaox.github.io/NeRF-HuGS/.\n","authors":["Jiahao Chen","Yipeng Qin","Lingjie Liu","Jiangbo Lu","Guanbin Li"],"pdf_url":"https://arxiv.org/pdf/2403.17537v1.pdf","comment":"To appear in CVPR2024"},{"id":"http://arxiv.org/abs/2403.17530v1","updated":"2024-03-26T09:36:20Z","published":"2024-03-26T09:36:20Z","title":"Boosting Few-Shot Learning with Disentangled Self-Supervised Learning\n  and Meta-Learning for Medical Image Classification","summary":"  Background and objective: Employing deep learning models in critical domains\nsuch as medical imaging poses challenges associated with the limited\navailability of training data. We present a strategy for improving the\nperformance and generalization capabilities of models trained in low-data\nregimes. Methods: The proposed method starts with a pre-training phase, where\nfeatures learned in a self-supervised learning setting are disentangled to\nimprove the robustness of the representations for downstream tasks. We then\nintroduce a meta-fine-tuning step, leveraging related classes between\nmeta-training and meta-testing phases but varying the granularity level. This\napproach aims to enhance the model's generalization capabilities by exposing it\nto more challenging classification tasks during meta-training and evaluating it\non easier tasks but holding greater clinical relevance during meta-testing. We\ndemonstrate the effectiveness of the proposed approach through a series of\nexperiments exploring several backbones, as well as diverse pre-training and\nfine-tuning schemes, on two distinct medical tasks, i.e., classification of\nprostate cancer aggressiveness from MRI data and classification of breast\ncancer malignity from microscopic images. Results: Our results indicate that\nthe proposed approach consistently yields superior performance w.r.t. ablation\nexperiments, maintaining competitiveness even when a distribution shift between\ntraining and evaluation data occurs. Conclusion: Extensive experiments\ndemonstrate the effectiveness and wide applicability of the proposed approach.\nWe hope that this work will add another solution to the arsenal of addressing\nlearning issues in data-scarce imaging domains.\n","authors":["Eva Pachetti","Sotirios A. Tsaftaris","Sara Colantonio"],"pdf_url":"https://arxiv.org/pdf/2403.17530v1.pdf","comment":"20 pages, 4 figures, 4 tables. Submitted to Elsevier on 25 March 2024"},{"id":"http://arxiv.org/abs/2207.12730v2","updated":"2024-03-26T09:35:03Z","published":"2022-07-26T08:34:17Z","title":"P2ANet: A Dataset and Benchmark for Dense Action Detection from Table\n  Tennis Match Broadcasting Videos","summary":"  While deep learning has been widely used for video analytics, such as video\nclassification and action detection, dense action detection with fast-moving\nsubjects from sports videos is still challenging. In this work, we release yet\nanother sports video benchmark \\TheName{} for \\emph{\\underline{P}}ing\n\\emph{\\underline{P}}ong-\\emph{\\underline{A}}ction detection, which consists of\n2,721 video clips collected from the broadcasting videos of professional table\ntennis matches in World Table Tennis Championships and Olympiads. We work with\na crew of table tennis professionals and referees on a specially designed\nannotation toolbox to obtain fine-grained action labels (in 14 classes) for\nevery ping-pong action that appeared in the dataset, and formulate two sets of\naction detection problems -- \\emph{action localization} and \\emph{action\nrecognition}. We evaluate a number of commonly-seen action recognition (e.g.,\nTSM, TSN, Video SwinTransformer, and Slowfast) and action localization models\n(e.g., BSN, BSN++, BMN, TCANet), using \\TheName{} for both problems, under\nvarious settings. These models can only achieve 48\\% area under the AR-AN curve\nfor localization and 82\\% top-one accuracy for recognition since the ping-pong\nactions are dense with fast-moving subjects but broadcasting videos are with\nonly 25 FPS. The results confirm that \\TheName{} is still a challenging task\nand can be used as a special benchmark for dense action detection from videos.\n","authors":["Jiang Bian","Xuhong Li","Tao Wang","Qingzhong Wang","Jun Huang","Chen Liu","Jun Zhao","Feixiang Lu","Dejing Dou","Haoyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2207.12730v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12378v2","updated":"2024-03-26T09:31:28Z","published":"2023-09-21T11:47:01Z","title":"Unsupervised Semantic Segmentation Through Depth-Guided Feature\n  Correlation and Sampling","summary":"  Traditionally, training neural networks to perform semantic segmentation\nrequired expensive human-made annotations. But more recently, advances in the\nfield of unsupervised learning have made significant progress on this issue and\ntowards closing the gap to supervised algorithms. To achieve this, semantic\nknowledge is distilled by learning to correlate randomly sampled features from\nimages across an entire dataset. In this work, we build upon these advances by\nincorporating information about the structure of the scene into the training\nprocess through the use of depth information. We achieve this by (1) learning\ndepth-feature correlation by spatially correlate the feature maps with the\ndepth maps to induce knowledge about the structure of the scene and (2)\nimplementing farthest-point sampling to more effectively select relevant\nfeatures by utilizing 3D sampling techniques on depth information of the scene.\nFinally, we demonstrate the effectiveness of our technical contributions\nthrough extensive experimentation and present significant improvements in\nperformance across multiple benchmark datasets.\n","authors":["Leon Sick","Dominik Engel","Pedro Hermosilla","Timo Ropinski"],"pdf_url":"https://arxiv.org/pdf/2309.12378v2.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17525v1","updated":"2024-03-26T09:26:12Z","published":"2024-03-26T09:26:12Z","title":"Equipping Sketch Patches with Context-Aware Positional Encoding for\n  Graphic Sketch Representation","summary":"  The drawing order of a sketch records how it is created stroke-by-stroke by a\nhuman being. For graphic sketch representation learning, recent studies have\ninjected sketch drawing orders into graph edge construction by linking each\npatch to another in accordance to a temporal-based nearest neighboring\nstrategy. However, such constructed graph edges may be unreliable, since a\nsketch could have variants of drawings. In this paper, we propose a\nvariant-drawing-protected method by equipping sketch patches with context-aware\npositional encoding (PE) to make better use of drawing orders for learning\ngraphic sketch representation. Instead of injecting sketch drawings into graph\nedges, we embed these sequential information into graph nodes only. More\nspecifically, each patch embedding is equipped with a sinusoidal absolute PE to\nhighlight the sequential position in the drawing order. And its neighboring\npatches, ranked by the values of self-attention scores between patch\nembeddings, are equipped with learnable relative PEs to restore the contextual\npositions within a neighborhood. During message aggregation via graph\nconvolutional networks, a node receives both semantic contents from patch\nembeddings and contextual patterns from PEs by its neighbors, arriving at\ndrawing-order-enhanced sketch representations. Experimental results indicate\nthat our method significantly improves sketch healing and controllable sketch\nsynthesis.\n","authors":["Sicong Zang","Zhijun Fang"],"pdf_url":"https://arxiv.org/pdf/2403.17525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17520v1","updated":"2024-03-26T09:22:37Z","published":"2024-03-26T09:22:37Z","title":"Boosting Adversarial Training via Fisher-Rao Norm-based Regularization","summary":"  Adversarial training is extensively utilized to improve the adversarial\nrobustness of deep neural networks. Yet, mitigating the degradation of standard\ngeneralization performance in adversarial-trained models remains an open\nproblem. This paper attempts to resolve this issue through the lens of model\ncomplexity. First, We leverage the Fisher-Rao norm, a geometrically invariant\nmetric for model complexity, to establish the non-trivial bounds of the\nCross-Entropy Loss-based Rademacher complexity for a ReLU-activated Multi-Layer\nPerceptron. Then we generalize a complexity-related variable, which is\nsensitive to the changes in model width and the trade-off factors in\nadversarial training. Moreover, intensive empirical evidence validates that\nthis variable highly correlates with the generalization gap of Cross-Entropy\nloss between adversarial-trained and standard-trained models, especially during\nthe initial and final phases of the training process. Building upon this\nobservation, we propose a novel regularization framework, called Logit-Oriented\nAdversarial Training (LOAT), which can mitigate the trade-off between\nrobustness and accuracy while imposing only a negligible increase in\ncomputational overhead. Our extensive experiments demonstrate that the proposed\nregularization strategy can boost the performance of the prevalent adversarial\ntraining algorithms, including PGD-AT, TRADES, TRADES (LSE), MART, and DM-AT,\nacross various network architectures. Our code will be available at\nhttps://github.com/TrustAI/LOAT.\n","authors":["Xiangyu Yin","Wenjie Ruan"],"pdf_url":"https://arxiv.org/pdf/2403.17520v1.pdf","comment":"This paper has been accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2312.04529v2","updated":"2024-03-26T09:21:29Z","published":"2023-12-07T18:50:00Z","title":"Diffusion Reflectance Map: Single-Image Stochastic Inverse Rendering of\n  Illumination and Reflectance","summary":"  Reflectance bounds the frequency spectrum of illumination in the object\nappearance. In this paper, we introduce the first stochastic inverse rendering\nmethod, which recovers the attenuated frequency spectrum of an illumination\njointly with the reflectance of an object of known geometry from a single\nimage. Our key idea is to solve this blind inverse problem in the reflectance\nmap, an appearance representation invariant to the underlying geometry, by\nlearning to reverse the image formation with a novel diffusion model which we\nrefer to as the Diffusion Reflectance Map Network (DRMNet). Given an observed\nreflectance map converted and completed from the single input image, DRMNet\ngenerates a reflectance map corresponding to a perfect mirror sphere while\njointly estimating the reflectance. The forward process can be understood as\ngradually filtering a natural illumination with lower and lower frequency\nreflectance and additive Gaussian noise. DRMNet learns to invert this process\nwith two subnetworks, IllNet and RefNet, which work in concert towards this\njoint estimation. The network is trained on an extensive synthetic dataset and\nis demonstrated to generalize to real images, showing state-of-the-art accuracy\non established datasets.\n","authors":["Yuto Enyo","Ko Nishino"],"pdf_url":"https://arxiv.org/pdf/2312.04529v2.pdf","comment":"to be published in CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17512v1","updated":"2024-03-26T09:13:06Z","published":"2024-03-26T09:13:06Z","title":"Random-coupled Neural Network","summary":"  Improving the efficiency of current neural networks and modeling them in\nbiological neural systems have become popular research directions in recent\nyears. Pulse-coupled neural network (PCNN) is a well applicated model for\nimitating the computation characteristics of the human brain in computer vision\nand neural network fields. However, differences between the PCNN and biological\nneural systems remain: limited neural connection, high computational cost, and\nlack of stochastic property. In this study, random-coupled neural network\n(RCNN) is proposed. It overcomes these difficulties in PCNN's neuromorphic\ncomputing via a random inactivation process. This process randomly closes some\nneural connections in the RCNN model, realized by the random inactivation\nweight matrix of link input. This releases the computational burden of PCNN,\nmaking it affordable to achieve vast neural connections. Furthermore, the image\nand video processing mechanisms of RCNN are researched. It encodes constant\nstimuli as periodic spike trains and periodic stimuli as chaotic spike trains,\nthe same as biological neural information encoding characteristics. Finally,\nthe RCNN is applicated to image segmentation, fusion, and pulse shape\ndiscrimination subtasks. It is demonstrated to be robust, efficient, and highly\nanti-noised, with outstanding performance in all applications mentioned above.\n","authors":["Haoran Liu","Mingzhe Liu","Peng Li","Jiahui Wu","Xin Jiang","Zhuo Zuo","Bingqi Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13660v2","updated":"2024-03-26T09:09:15Z","published":"2024-03-20T15:08:57Z","title":"ProMamba: Prompt-Mamba for polyp segmentation","summary":"  Detecting polyps through colonoscopy is an important task in medical image\nsegmentation, which provides significant assistance and reference value for\nclinical surgery. However, accurate segmentation of polyps is a challenging\ntask due to two main reasons. Firstly, polyps exhibit various shapes and\ncolors. Secondly, the boundaries between polyps and their normal surroundings\nare often unclear. Additionally, significant differences between different\ndatasets lead to limited generalization capabilities of existing methods. To\naddress these issues, we propose a segmentation model based on Prompt-Mamba,\nwhich incorporates the latest Vision-Mamba and prompt technologies. Compared to\nprevious models trained on the same dataset, our model not only maintains high\nsegmentation accuracy on the validation part of the same dataset but also\ndemonstrates superior accuracy on unseen datasets, exhibiting excellent\ngeneralization capabilities. Notably, we are the first to apply the\nVision-Mamba architecture to polyp segmentation and the first to utilize prompt\ntechnology in a polyp segmentation model. Our model efficiently accomplishes\nsegmentation tasks, surpassing previous state-of-the-art methods by an average\nof 5% across six datasets. Furthermore, we have developed multiple versions of\nour model with scaled parameter counts, achieving better performance than\nprevious models even with fewer parameters. Our code and trained weights will\nbe released soon.\n","authors":["Jianhao Xie","Ruofan Liao","Ziang Zhang","Sida Yi","Yuesheng Zhu","Guibo Luo"],"pdf_url":"https://arxiv.org/pdf/2403.13660v2.pdf","comment":"10 pages, 2 figures,3 tabels"},{"id":"http://arxiv.org/abs/2403.17503v1","updated":"2024-03-26T09:04:18Z","published":"2024-03-26T09:04:18Z","title":"DS-AL: A Dual-Stream Analytic Learning for Exemplar-Free\n  Class-Incremental Learning","summary":"  Class-incremental learning (CIL) under an exemplar-free constraint has\npresented a significant challenge. Existing methods adhering to this constraint\nare prone to catastrophic forgetting, far more so than replay-based techniques\nthat retain access to past samples. In this paper, to solve the exemplar-free\nCIL problem, we propose a Dual-Stream Analytic Learning (DS-AL) approach. The\nDS-AL contains a main stream offering an analytical (i.e., closed-form) linear\nsolution, and a compensation stream improving the inherent under-fitting\nlimitation due to adopting linear mapping. The main stream redefines the CIL\nproblem into a Concatenated Recursive Least Squares (C-RLS) task, allowing an\nequivalence between the CIL and its joint-learning counterpart. The\ncompensation stream is governed by a Dual-Activation Compensation (DAC) module.\nThis module re-activates the embedding with a different activation function\nfrom the main stream one, and seeks fitting compensation by projecting the\nembedding to the null space of the main stream's linear mapping. Empirical\nresults demonstrate that the DS-AL, despite being an exemplar-free technique,\ndelivers performance comparable with or better than that of replay-based\nmethods across various datasets, including CIFAR-100, ImageNet-100 and\nImageNet-Full. Additionally, the C-RLS' equivalent property allows the DS-AL to\nexecute CIL in a phase-invariant manner. This is evidenced by a\nnever-before-seen 500-phase CIL ImageNet task, which performs on a level\nidentical to a 5-phase one. Our codes are available at\nhttps://github.com/ZHUANGHP/Analytic-continual-learning.\n","authors":["Huiping Zhuang","Run He","Kai Tong","Ziqian Zeng","Cen Chen","Zhiping Lin"],"pdf_url":"https://arxiv.org/pdf/2403.17503v1.pdf","comment":"Accepted in AAAI 2024"},{"id":"http://arxiv.org/abs/2403.17502v1","updated":"2024-03-26T09:03:40Z","published":"2024-03-26T09:03:40Z","title":"SeNM-VAE: Semi-Supervised Noise Modeling with Hierarchical Variational\n  Autoencoder","summary":"  The data bottleneck has emerged as a fundamental challenge in learning based\nimage restoration methods. Researchers have attempted to generate synthesized\ntraining data using paired or unpaired samples to address this challenge. This\nstudy proposes SeNM-VAE, a semi-supervised noise modeling method that leverages\nboth paired and unpaired datasets to generate realistic degraded data. Our\napproach is based on modeling the conditional distribution of degraded and\nclean images with a specially designed graphical model. Under the variational\ninference framework, we develop an objective function for handling both paired\nand unpaired data. We employ our method to generate paired training samples for\nreal-world image denoising and super-resolution tasks. Our approach excels in\nthe quality of synthetic degraded images compared to other unpaired and paired\nnoise modeling methods. Furthermore, our approach demonstrates remarkable\nperformance in downstream image restoration tasks, even with limited paired\ndata. With more paired data, our method achieves the best performance on the\nSIDD dataset.\n","authors":["Dihan Zheng","Yihang Zou","Xiaowen Zhang","Chenglong Bao"],"pdf_url":"https://arxiv.org/pdf/2403.17502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17497v1","updated":"2024-03-26T08:58:28Z","published":"2024-03-26T08:58:28Z","title":"Sharing the Cost of Success: A Game for Evaluating and Learning\n  Collaborative Multi-Agent Instruction Giving and Following Policies","summary":"  In collaborative goal-oriented settings, the participants are not only\ninterested in achieving a successful outcome, but do also implicitly negotiate\nthe effort they put into the interaction (by adapting to each other). In this\nwork, we propose a challenging interactive reference game that requires two\nplayers to coordinate on vision and language observations. The learning signal\nin this game is a score (given after playing) that takes into account the\nachieved goal and the players' assumed efforts during the interaction. We show\nthat a standard Proximal Policy Optimization (PPO) setup achieves a high\nsuccess rate when bootstrapped with heuristic partner behaviors that implement\ninsights from the analysis of human-human interactions. And we find that a\npairing of neural partners indeed reduces the measured joint effort when\nplaying together repeatedly. However, we observe that in comparison to a\nreasonable heuristic pairing there is still room for improvement -- which\ninvites further research in the direction of cost-sharing in collaborative\ninteractions.\n","authors":["Philipp Sadler","Sherzod Hakimov","David Schlangen"],"pdf_url":"https://arxiv.org/pdf/2403.17497v1.pdf","comment":"9 pages, Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2310.05370v2","updated":"2024-03-26T08:54:49Z","published":"2023-10-09T02:59:21Z","title":"SocialCircle: Learning the Angle-based Social Interaction Representation\n  for Pedestrian Trajectory Prediction","summary":"  Analyzing and forecasting trajectories of agents like pedestrians and cars in\ncomplex scenes has become more and more significant in many intelligent systems\nand applications. The diversity and uncertainty in socially interactive\nbehaviors among a rich variety of agents make this task more challenging than\nother deterministic computer vision tasks. Researchers have made a lot of\nefforts to quantify the effects of these interactions on future trajectories\nthrough different mathematical models and network structures, but this problem\nhas not been well solved. Inspired by marine animals that localize the\npositions of their companions underwater through echoes, we build a new\nanglebased trainable social interaction representation, named SocialCircle, for\ncontinuously reflecting the context of social interactions at different angular\norientations relative to the target agent. We validate the effect of the\nproposed SocialCircle by training it along with several newly released\ntrajectory prediction models, and experiments show that the SocialCircle not\nonly quantitatively improves the prediction performance, but also qualitatively\nhelps better simulate social interactions when forecasting pedestrian\ntrajectories in a way that is consistent with human intuitions.\n","authors":["Conghao Wong","Beihao Xia","Ziqian Zou","Yulong Wang","Xinge You"],"pdf_url":"https://arxiv.org/pdf/2310.05370v2.pdf","comment":"CVPR 2024 accepted"},{"id":"http://arxiv.org/abs/2403.17496v1","updated":"2024-03-26T08:53:25Z","published":"2024-03-26T08:53:25Z","title":"Dr.Hair: Reconstructing Scalp-Connected Hair Strands without\n  Pre-training via Differentiable Rendering of Line Segments","summary":"  In the film and gaming industries, achieving a realistic hair appearance\ntypically involves the use of strands originating from the scalp. However,\nreconstructing these strands from observed surface images of hair presents\nsignificant challenges. The difficulty in acquiring Ground Truth (GT) data has\nled state-of-the-art learning-based methods to rely on pre-training with\nmanually prepared synthetic CG data. This process is not only labor-intensive\nand costly but also introduces complications due to the domain gap when\ncompared to real-world data. In this study, we propose an optimization-based\napproach that eliminates the need for pre-training. Our method represents hair\nstrands as line segments growing from the scalp and optimizes them using a\nnovel differentiable rendering algorithm. To robustly optimize a substantial\nnumber of slender explicit geometries, we introduce 3D orientation estimation\nutilizing global optimization, strand initialization based on Laplace's\nequation, and reparameterization that leverages geometric connectivity and\nspatial proximity. Unlike existing optimization-based methods, our method is\ncapable of reconstructing internal hair flow in an absolute direction. Our\nmethod exhibits robust and accurate inverse rendering, surpassing the quality\nof existing methods and significantly improving processing speed.\n","authors":["Yusuke Takimoto","Hikari Takehara","Hiroyuki Sato","Zihao Zhu","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.17496v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.13039v2","updated":"2024-03-26T08:52:05Z","published":"2024-03-19T16:21:47Z","title":"Emotic Masked Autoencoder with Attention Fusion for Facial Expression\n  Recognition","summary":"  Facial Expression Recognition (FER) is a critical task within computer vision\nwith diverse applications across various domains. Addressing the challenge of\nlimited FER datasets, which hampers the generalization capability of expression\nrecognition models, is imperative for enhancing performance. Our paper presents\nan innovative approach integrating the MAE-Face self-supervised learning (SSL)\nmethod and Fusion Attention mechanism for expression classification,\nparticularly showcased in the 6th Affective Behavior 32 pages harvmac; added\nreferences for section 5Analysis in-the-wild (ABAW) competition. Additionally,\nwe propose preprocessing techniques to emphasize essential facial features,\nthereby enhancing model performance on both training and validation sets,\nnotably demonstrated on the Aff-wild2 dataset.\n","authors":["Bach Nguyen-Xuan","Thien Nguyen-Hoang","Nhu Tai-Do"],"pdf_url":"https://arxiv.org/pdf/2403.13039v2.pdf","comment":"6 pages; added references for section 1; corrected typo for email\n  author"},{"id":"http://arxiv.org/abs/2403.13653v2","updated":"2024-03-26T08:45:09Z","published":"2024-03-20T14:58:40Z","title":"Learning User Embeddings from Human Gaze for Personalised Saliency\n  Prediction","summary":"  Reusable embeddings of user behaviour have shown significant performance\nimprovements for the personalised saliency prediction task. However, prior\nworks require explicit user characteristics and preferences as input, which are\noften difficult to obtain. We present a novel method to extract user embeddings\nfrom pairs of natural images and corresponding saliency maps generated from a\nsmall amount of user-specific eye tracking data. At the core of our method is a\nSiamese convolutional neural encoder that learns the user embeddings by\ncontrasting the image and personal saliency map pairs of different users.\nEvaluations on two public saliency datasets show that the generated embeddings\nhave high discriminative power, are effective at refining universal saliency\nmaps to the individual users, and generalise well across users and images.\nFinally, based on our model's ability to encode individual user\ncharacteristics, our work points towards other applications that can benefit\nfrom reusable embeddings of gaze behaviour.\n","authors":["Florian Strohm","Mihai Bâce","Andreas Bulling"],"pdf_url":"https://arxiv.org/pdf/2403.13653v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17726v2","updated":"2024-03-26T08:38:52Z","published":"2024-02-27T17:58:09Z","title":"VRP-SAM: SAM with Visual Reference Prompt","summary":"  In this paper, we propose a novel Visual Reference Prompt (VRP) encoder that\nempowers the Segment Anything Model (SAM) to utilize annotated reference images\nas prompts for segmentation, creating the VRP-SAM model. In essence, VRP-SAM\ncan utilize annotated reference images to comprehend specific objects and\nperform segmentation of specific objects in target image. It is note that the\nVRP encoder can support a variety of annotation formats for reference images,\nincluding \\textbf{point}, \\textbf{box}, \\textbf{scribble}, and \\textbf{mask}.\nVRP-SAM achieves a breakthrough within the SAM framework by extending its\nversatility and applicability while preserving SAM's inherent strengths, thus\nenhancing user-friendliness. To enhance the generalization ability of VRP-SAM,\nthe VRP encoder adopts a meta-learning strategy. To validate the effectiveness\nof VRP-SAM, we conducted extensive empirical studies on the Pascal and COCO\ndatasets. Remarkably, VRP-SAM achieved state-of-the-art performance in visual\nreference segmentation with minimal learnable parameters. Furthermore, VRP-SAM\ndemonstrates strong generalization capabilities, allowing it to perform\nsegmentation of unseen objects and enabling cross-domain segmentation. The\nsource code and models will be available at\n\\url{https://github.com/syp2ysy/VRP-SAM}\n","authors":["Yanpeng Sun","Jiahui Chen","Shan Zhang","Xinyu Zhang","Qiang Chen","Gang Zhang","Errui Ding","Jingdong Wang","Zechao Li"],"pdf_url":"https://arxiv.org/pdf/2402.17726v2.pdf","comment":"Accepted by CVPR 2024; The camera-ready version"},{"id":"http://arxiv.org/abs/2403.13972v2","updated":"2024-03-26T08:34:16Z","published":"2024-03-20T20:47:53Z","title":"SeFFeC: Semantic Facial Feature Control for Fine-grained Face Editing","summary":"  We propose Semantic Facial Feature Control (SeFFeC) - a novel method for\nfine-grained face shape editing. Our method enables the manipulation of\nhuman-understandable, semantic face features, such as nose length or mouth\nwidth, which are defined by different groups of facial landmarks. In contrast\nto existing methods, the use of facial landmarks enables precise measurement of\nthe facial features, which then enables training SeFFeC without any manually\nannotated labels. SeFFeC consists of a transformer-based encoder network that\ntakes a latent vector of a pre-trained generative model and a facial feature\nembedding as input, and learns to modify the latent vector to perform the\ndesired face edit operation. To ensure that the desired feature measurement is\nchanged towards the target value without altering uncorrelated features, we\nintroduced a novel semantic face feature loss. Qualitative and quantitative\nresults show that SeFFeC enables precise and fine-grained control of 23 facial\nfeatures, some of which could not previously be controlled by other methods,\nwithout requiring manual annotations. Unlike existing methods, SeFFeC also\nprovides deterministic control over the exact values of the facial features and\nmore localised and disentangled face edits.\n","authors":["Florian Strohm","Mihai Bâce","Markus Kaltenecker","Andreas Bulling"],"pdf_url":"https://arxiv.org/pdf/2403.13972v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17477v1","updated":"2024-03-26T08:13:02Z","published":"2024-03-26T08:13:02Z","title":"DiffGaze: A Diffusion Model for Continuous Gaze Sequence Generation on\n  360° Images","summary":"  We present DiffGaze, a novel method for generating realistic and diverse\ncontinuous human gaze sequences on 360{\\deg} images based on a conditional\nscore-based denoising diffusion model. Generating human gaze on 360{\\deg}\nimages is important for various human-computer interaction and computer\ngraphics applications, e.g. for creating large-scale eye tracking datasets or\nfor realistic animation of virtual humans. However, existing methods are\nlimited to predicting discrete fixation sequences or aggregated saliency maps,\nthereby neglecting crucial parts of natural gaze behaviour. Our method uses\nfeatures extracted from 360{\\deg} images as condition and uses two transformers\nto model the temporal and spatial dependencies of continuous human gaze. We\nevaluate DiffGaze on two 360{\\deg} image benchmarks for gaze sequence\ngeneration as well as scanpath prediction and saliency prediction. Our\nevaluations show that DiffGaze outperforms state-of-the-art methods on all\ntasks on both benchmarks. We also report a 21-participant user study showing\nthat our method generates gaze sequences that are indistinguishable from real\nhuman sequences.\n","authors":["Chuhan Jiao","Yao Wang","Guanhua Zhang","Mihai Bâce","Zhiming Hu","Andreas Bulling"],"pdf_url":"https://arxiv.org/pdf/2403.17477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12036v3","updated":"2024-03-26T08:09:43Z","published":"2022-11-22T06:19:17Z","title":"Dual Prototype Attention for Unsupervised Video Object Segmentation","summary":"  Unsupervised video object segmentation (VOS) aims to detect and segment the\nmost salient object in videos. The primary techniques used in unsupervised VOS\nare 1) the collaboration of appearance and motion information; and 2) temporal\nfusion between different frames. This paper proposes two novel prototype-based\nattention mechanisms, inter-modality attention (IMA) and inter-frame attention\n(IFA), to incorporate these techniques via dense propagation across different\nmodalities and frames. IMA densely integrates context information from\ndifferent modalities based on a mutual refinement. IFA injects global context\nof a video to the query frame, enabling a full utilization of useful properties\nfrom multiple frames. Experimental results on public benchmark datasets\ndemonstrate that our proposed approach outperforms all existing methods by a\nsubstantial margin. The proposed two components are also thoroughly validated\nvia ablative study.\n","authors":["Suhwan Cho","Minhyeok Lee","Seunghoon Lee","Dogyoon Lee","Heeseung Choi","Ig-Jae Kim","Sangyoun Lee"],"pdf_url":"https://arxiv.org/pdf/2211.12036v3.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2311.09974v2","updated":"2024-03-26T08:04:00Z","published":"2023-11-16T15:47:49Z","title":"From Pretext to Purpose: Batch-Adaptive Self-Supervised Learning","summary":"  In recent years, self-supervised contrastive learning has emerged as a\ndistinguished paradigm in the artificial intelligence landscape. It facilitates\nunsupervised feature learning through contrastive delineations at the instance\nlevel. However, crafting an effective self-supervised paradigm remains a\npivotal challenge within this field. This paper delves into two crucial factors\nimpacting self-supervised contrastive learning-bach size and pretext tasks, and\nfrom a data processing standpoint, proposes an adaptive technique of batch\nfusion. The proposed method, via dimensionality reduction and reconstruction of\nbatch data, enables formerly isolated individual data to partake in intra-batch\ncommunication through the Embedding Layer. Moreover, it adaptively amplifies\nthe self-supervised feature encoding capability as the training progresses. We\nconducted a linear classification test of this method based on the classic\ncontrastive learning framework on ImageNet-1k. The empirical findings\nillustrate that our approach achieves state-of-the-art performance under\nequitable comparisons. Benefiting from its \"plug-and-play\" characteristics, we\nfurther explored other contrastive learning methods. On the ImageNet-100,\ncompared to the original performance, the top1 has seen a maximum increase of\n1.25%. We suggest that the proposed method may contribute to the advancement of\ndata-driven self-supervised learning research, bringing a fresh perspective to\nthis community.\n","authors":["Jiansong Zhang","Linlin Shen","Peizhong Liu"],"pdf_url":"https://arxiv.org/pdf/2311.09974v2.pdf","comment":"14 pages, 2 figures, the code of this paper will be released soon"},{"id":"http://arxiv.org/abs/2311.16926v4","updated":"2024-03-26T07:55:24Z","published":"2023-11-28T16:31:27Z","title":"LLaFS: When Large Language Models Meet Few-Shot Segmentation","summary":"  This paper proposes LLaFS, the first attempt to leverage large language\nmodels (LLMs) in few-shot segmentation. In contrast to the conventional\nfew-shot segmentation methods that only rely on the limited and biased\ninformation from the annotated support images, LLaFS leverages the vast prior\nknowledge gained by LLM as an effective supplement and directly uses the LLM to\nsegment images in a few-shot manner. To enable the text-based LLM to handle\nimage-related tasks, we carefully design an input instruction that allows the\nLLM to produce segmentation results represented as polygons, and propose a\nregion-attribute table to simulate the human visual mechanism and provide\nmulti-modal guidance. We also synthesize pseudo samples and use curriculum\nlearning for pretraining to augment data and achieve better optimization. LLaFS\nachieves state-of-the-art results on multiple datasets, showing the potential\nof using LLMs for few-shot computer vision tasks.\n","authors":["Lanyun Zhu","Tianrun Chen","Deyi Ji","Jieping Ye","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2311.16926v4.pdf","comment":"Accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2403.17465v1","updated":"2024-03-26T07:55:16Z","published":"2024-03-26T07:55:16Z","title":"LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated\n  Image Detection","summary":"  The evolution of Diffusion Models has dramatically improved image generation\nquality, making it increasingly difficult to differentiate between real and\ngenerated images. This development, while impressive, also raises significant\nprivacy and security concerns. In response to this, we propose a novel Latent\nREconstruction error guided feature REfinement method (LaRE^2) for detecting\nthe diffusion-generated images. We come up with the Latent Reconstruction Error\n(LaRE), the first reconstruction-error based feature in the latent space for\ngenerated image detection. LaRE surpasses existing methods in terms of feature\nextraction efficiency while preserving crucial cues required to differentiate\nbetween the real and the fake. To exploit LaRE, we propose an Error-Guided\nfeature REfinement module (EGRE), which can refine the image feature guided by\nLaRE to enhance the discriminativeness of the feature. Our EGRE utilizes an\nalign-then-refine mechanism, which effectively refines the image feature for\ngenerated-image detection from both spatial and channel perspectives. Extensive\nexperiments on the large-scale GenImage benchmark demonstrate the superiority\nof our LaRE^2, which surpasses the best SoTA method by up to 11.9%/12.1%\naverage ACC/AP across 8 different image generators. LaRE also surpasses\nexisting methods in terms of feature extraction cost, delivering an impressive\nspeed enhancement of 8 times.\n","authors":["Yunpeng Luo","Junlong Du","Ke Yan","Shouhong Ding"],"pdf_url":"https://arxiv.org/pdf/2403.17465v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17460v1","updated":"2024-03-26T07:48:49Z","published":"2024-03-26T07:48:49Z","title":"Building Bridges across Spatial and Temporal Resolutions:\n  Reference-Based Super-Resolution via Change Priors and Conditional Diffusion\n  Model","summary":"  Reference-based super-resolution (RefSR) has the potential to build bridges\nacross spatial and temporal resolutions of remote sensing images. However,\nexisting RefSR methods are limited by the faithfulness of content\nreconstruction and the effectiveness of texture transfer in large scaling\nfactors. Conditional diffusion models have opened up new opportunities for\ngenerating realistic high-resolution images, but effectively utilizing\nreference images within these models remains an area for further exploration.\nFurthermore, content fidelity is difficult to guarantee in areas without\nrelevant reference information. To solve these issues, we propose a\nchange-aware diffusion model named Ref-Diff for RefSR, using the land cover\nchange priors to guide the denoising process explicitly. Specifically, we\ninject the priors into the denoising model to improve the utilization of\nreference information in unchanged areas and regulate the reconstruction of\nsemantically relevant content in changed areas. With this powerful guidance, we\ndecouple the semantics-guided denoising and reference texture-guided denoising\nprocesses to improve the model performance. Extensive experiments demonstrate\nthe superior effectiveness and robustness of the proposed method compared with\nstate-of-the-art RefSR methods in both quantitative and qualitative\nevaluations. The code and data are available at\nhttps://github.com/dongrunmin/RefDiff.\n","authors":["Runmin Dong","Shuai Yuan","Bin Luo","Mengxuan Chen","Jinxiao Zhang","Lixian Zhang","Weijia Li","Juepeng Zheng","Haohuan Fu"],"pdf_url":"https://arxiv.org/pdf/2403.17460v1.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.14027v2","updated":"2024-03-26T07:47:20Z","published":"2024-03-20T22:52:34Z","title":"EcoSense: Energy-Efficient Intelligent Sensing for In-Shore Ship\n  Detection through Edge-Cloud Collaboration","summary":"  Detecting marine objects inshore presents challenges owing to algorithmic\nintricacies and complexities in system deployment. We propose a\ndifficulty-aware edge-cloud collaborative sensing system that splits the task\ninto object localization and fine-grained classification. Objects are\nclassified either at the edge or within the cloud, based on their estimated\ndifficulty. The framework comprises a low-power device-tailored front-end model\nfor object localization, classification, and difficulty estimation, along with\na transformer-graph convolutional network-based back-end model for fine-grained\nclassification. Our system demonstrates superior performance (mAP@0.5 +4.3%})\non widely used marine object detection datasets, significantly reducing both\ndata transmission volume (by 95.43%) and energy consumption (by 72.7%}) at the\nsystem level. We validate the proposed system across various embedded system\nplatforms and in real-world scenarios involving drone deployment.\n","authors":["Wenjun Huang","Hanning Chen","Yang Ni","Arghavan Rezvani","Sanggeon Yun","Sungheon Jeon","Eric Pedley","Mohsen Imani"],"pdf_url":"https://arxiv.org/pdf/2403.14027v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.03180v5","updated":"2024-03-26T07:44:45Z","published":"2021-06-06T17:01:13Z","title":"Vision Transformers with Hierarchical Attention","summary":"  This paper tackles the high computational/space complexity associated with\nMulti-Head Self-Attention (MHSA) in vanilla vision transformers. To this end,\nwe propose Hierarchical MHSA (H-MHSA), a novel approach that computes\nself-attention in a hierarchical fashion. Specifically, we first divide the\ninput image into patches as commonly done, and each patch is viewed as a token.\nThen, the proposed H-MHSA learns token relationships within local patches,\nserving as local relationship modeling. Then, the small patches are merged into\nlarger ones, and H-MHSA models the global dependencies for the small number of\nthe merged tokens. At last, the local and global attentive features are\naggregated to obtain features with powerful representation capacity. Since we\nonly calculate attention for a limited number of tokens at each step, the\ncomputational load is reduced dramatically. Hence, H-MHSA can efficiently model\nglobal relationships among tokens without sacrificing fine-grained information.\nWith the H-MHSA module incorporated, we build a family of\nHierarchical-Attention-based Transformer Networks, namely HAT-Net. To\ndemonstrate the superiority of HAT-Net in scene understanding, we conduct\nextensive experiments on fundamental vision tasks, including image\nclassification, semantic segmentation, object detection, and instance\nsegmentation. Therefore, HAT-Net provides a new perspective for vision\ntransformers. Code and pretrained models are available at\nhttps://github.com/yun-liu/HAT-Net.\n","authors":["Yun Liu","Yu-Huan Wu","Guolei Sun","Le Zhang","Ajad Chhatkuli","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2106.03180v5.pdf","comment":"Machine Intelligence Research (MIR), DOI: 10.1007/s11633-024-1393-8"},{"id":"http://arxiv.org/abs/2308.07728v5","updated":"2024-03-26T07:43:08Z","published":"2023-08-15T12:08:43Z","title":"Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability","summary":"  Fine-tuning pre-trained neural network models has become a widely adopted\napproach across various domains. However, it can lead to the distortion of\npre-trained feature extractors that already possess strong generalization\ncapabilities. Mitigating feature distortion during adaptation to new target\ndomains is crucial. Recent studies have shown promising results in handling\nfeature distortion by aligning the head layer on in-distribution datasets\nbefore performing fine-tuning. Nonetheless, a significant limitation arises\nfrom the treatment of batch normalization layers during fine-tuning, leading to\nsuboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning\n(DAFT), a novel approach that incorporates batch normalization conversion and\nthe integration of linear probing and fine-tuning. Our batch normalization\nconversion method effectively mitigates feature distortion by reducing\nmodifications to the neural network during fine-tuning. Additionally, we\nintroduce the integration of linear probing and fine-tuning to optimize the\nhead layer with gradual adaptation of the feature extractor. By leveraging\nbatch normalization layers and integrating linear probing and fine-tuning, our\nDAFT significantly mitigates feature distortion and achieves improved model\nperformance on both in-distribution and out-of-distribution datasets. Extensive\nexperiments demonstrate that our method outperforms other baseline methods,\ndemonstrating its effectiveness in not only improving performance but also\nmitigating feature distortion.\n","authors":["Seokhyeon Ha","Sunbeom Jung","Jungwoo Lee"],"pdf_url":"https://arxiv.org/pdf/2308.07728v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17447v1","updated":"2024-03-26T07:26:00Z","published":"2024-03-26T07:26:00Z","title":"Chain of Compression: A Systematic Approach to Combinationally Compress\n  Convolutional Neural Networks","summary":"  Convolutional neural networks (CNNs) have achieved significant popularity,\nbut their computational and memory intensity poses challenges for\nresource-constrained computing systems, particularly with the prerequisite of\nreal-time performance. To release this burden, model compression has become an\nimportant research focus. Many approaches like quantization, pruning, early\nexit, and knowledge distillation have demonstrated the effect of reducing\nredundancy in neural networks. Upon closer examination, it becomes apparent\nthat each approach capitalizes on its unique features to compress the neural\nnetwork, and they can also exhibit complementary behavior when combined. To\nexplore the interactions and reap the benefits from the complementary features,\nwe propose the Chain of Compression, which works on the combinational sequence\nto apply these common techniques to compress the neural network. Validated on\nthe image-based regression and classification networks across different data\nsets, our proposed Chain of Compression can significantly compress the\ncomputation cost by 100-1000 times with ignorable accuracy loss compared with\nthe baseline model.\n","authors":["Yingtao Shen","Minqing Sun","Jie Zhao","An Zou"],"pdf_url":"https://arxiv.org/pdf/2403.17447v1.pdf","comment":"10 pages, 15 figures"},{"id":"http://arxiv.org/abs/2306.07632v3","updated":"2024-03-26T07:00:27Z","published":"2023-06-13T09:02:57Z","title":"NeuS-PIR: Learning Relightable Neural Surface using Pre-Integrated\n  Rendering","summary":"  This paper presents a method, namely NeuS-PIR, for recovering relightable\nneural surfaces using pre-integrated rendering from multi-view images or video.\nUnlike methods based on NeRF and discrete meshes, our method utilizes implicit\nneural surface representation to reconstruct high-quality geometry, which\nfacilitates the factorization of the radiance field into two components: a\nspatially varying material field and an all-frequency lighting representation.\nThis factorization, jointly optimized using an adapted differentiable\npre-integrated rendering framework with material encoding regularization, in\nturn addresses the ambiguity of geometry reconstruction and leads to better\ndisentanglement and refinement of each scene property. Additionally, we\nintroduced a method to distil indirect illumination fields from the learned\nrepresentations, further recovering the complex illumination effect like\ninter-reflection. Consequently, our method enables advanced applications such\nas relighting, which can be seamlessly integrated with modern graphics engines.\nQualitative and quantitative experiments have shown that NeuS-PIR outperforms\nexisting methods across various tasks on both synthetic and real datasets.\nSource code is available at https://github.com/Sheldonmao/NeuSPIR\n","authors":["Shi Mao","Chenming Wu","Zhelun Shen","Yifan Wang","Dayan Wu","Liangjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2306.07632v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17432v1","updated":"2024-03-26T06:57:50Z","published":"2024-03-26T06:57:50Z","title":"Integrating Mamba Sequence Model and Hierarchical Upsampling Network for\n  Accurate Semantic Segmentation of Multiple Sclerosis Legion","summary":"  Integrating components from convolutional neural networks and state space\nmodels in medical image segmentation presents a compelling approach to enhance\naccuracy and efficiency. We introduce Mamba HUNet, a novel architecture\ntailored for robust and efficient segmentation tasks. Leveraging strengths from\nMamba UNet and the lighter version of Hierarchical Upsampling Network (HUNet),\nMamba HUNet combines convolutional neural networks local feature extraction\npower with state space models long range dependency modeling capabilities. We\nfirst converted HUNet into a lighter version, maintaining performance parity\nand then integrated this lighter HUNet into Mamba HUNet, further enhancing its\nefficiency. The architecture partitions input grayscale images into patches,\ntransforming them into 1D sequences for processing efficiency akin to Vision\nTransformers and Mamba models. Through Visual State Space blocks and patch\nmerging layers, hierarchical features are extracted while preserving spatial\ninformation. Experimental results on publicly available Magnetic Resonance\nImaging scans, notably in Multiple Sclerosis lesion segmentation, demonstrate\nMamba HUNet's effectiveness across diverse segmentation tasks. The model's\nrobustness and flexibility underscore its potential in handling complex\nanatomical structures. These findings establish Mamba HUNet as a promising\nsolution in advancing medical image segmentation, with implications for\nimproving clinical decision making processes.\n","authors":["Kazi Shahriar Sanjid","Md. Tanzim Hossain","Md. Shakib Shahariar Junayed","Dr. Mohammad Monir Uddin"],"pdf_url":"https://arxiv.org/pdf/2403.17432v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2403.17423v1","updated":"2024-03-26T06:40:03Z","published":"2024-03-26T06:40:03Z","title":"Test-time Adaptation Meets Image Enhancement: Improving Accuracy via\n  Uncertainty-aware Logit Switching","summary":"  Deep neural networks have achieved remarkable success in a variety of\ncomputer vision applications. However, there is a problem of degrading accuracy\nwhen the data distribution shifts between training and testing. As a solution\nof this problem, Test-time Adaptation~(TTA) has been well studied because of\nits practicality. Although TTA methods increase accuracy under distribution\nshift by updating the model at test time, using high-uncertainty predictions is\nknown to degrade accuracy. Since the input image is the root of the\ndistribution shift, we incorporate a new perspective on enhancing the input\nimage into TTA methods to reduce the prediction's uncertainty. We hypothesize\nthat enhancing the input image reduces prediction's uncertainty and increase\nthe accuracy of TTA methods. On the basis of our hypothesis, we propose a novel\nmethod: Test-time Enhancer and Classifier Adaptation~(TECA). In TECA, the\nclassification model is combined with the image enhancement model that\ntransforms input images into recognition-friendly ones, and these models are\nupdated by existing TTA methods. Furthermore, we found that the prediction from\nthe enhanced image does not always have lower uncertainty than the prediction\nfrom the original image. Thus, we propose logit switching, which compares the\nuncertainty measure of these predictions and outputs the lower one. In our\nexperiments, we evaluate TECA with various TTA methods and show that TECA\nreduces prediction's uncertainty and increases accuracy of TTA methods despite\nhaving no hyperparameters and little parameter overhead.\n","authors":["Shohei Enomoto","Naoya Hasegawa","Kazuki Adachi","Taku Sasaki","Shin'ya Yamaguchi","Satoshi Suzuki","Takeharu Eda"],"pdf_url":"https://arxiv.org/pdf/2403.17423v1.pdf","comment":"Accepted to IJCNN2024"},{"id":"http://arxiv.org/abs/2403.16169v2","updated":"2024-03-26T06:39:30Z","published":"2024-03-24T14:24:13Z","title":"Gaze-guided Hand-Object Interaction Synthesis: Benchmark and Method","summary":"  Gaze plays a crucial role in revealing human attention and intention,\nshedding light on the cognitive processes behind human actions. The integration\nof gaze guidance with the dynamics of hand-object interactions boosts the\naccuracy of human motion prediction. However, the lack of datasets that capture\nthe intricate relationship and consistency among gaze, hand, and object\nmovements remains a substantial hurdle. In this paper, we introduce the first\nGaze-guided Hand-Object Interaction dataset, GazeHOI, and present a novel task\nfor synthesizing gaze-guided hand-object interactions. Our dataset, GazeHOI,\nfeatures simultaneous 3D modeling of gaze, hand, and object interactions,\ncomprising 479 sequences with an average duration of 19.1 seconds, 812\nsub-sequences, and 33 objects of various sizes. We propose a hierarchical\nframework centered on a gaze-guided hand-object interaction diffusion model,\nnamed GHO-Diffusion. In the pre-diffusion phase, we separate gaze conditions\ninto spatial-temporal features and goal pose conditions at different levels of\ninformation granularity. During the diffusion phase, two gaze-conditioned\ndiffusion models are stacked to simplify the complex synthesis of hand-object\nmotions. Here, the object motion diffusion model generates sequences of object\nmotions based on gaze conditions, while the hand motion diffusion model\nproduces hand motions based on the generated object motion. To improve\nfine-grained goal pose alignment, we introduce a Spherical Gaussian constraint\nto guide the denoising step. In the subsequent post-diffusion phase, we\noptimize the generated hand motions using contact consistency. Our extensive\nexperiments highlight the uniqueness of our dataset and the effectiveness of\nour approach.\n","authors":["Jie Tian","Lingxiao Yang","Ran Ji","Yuexin Ma","Lan Xu","Jingyi Yu","Ye Shi","Jingya Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16169v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17422v1","updated":"2024-03-26T06:35:55Z","published":"2024-03-26T06:35:55Z","title":"InterHandGen: Two-Hand Interaction Generation via Cascaded Reverse\n  Diffusion","summary":"  We present InterHandGen, a novel framework that learns the generative prior\nof two-hand interaction. Sampling from our model yields plausible and diverse\ntwo-hand shapes in close interaction with or without an object. Our prior can\nbe incorporated into any optimization or learning methods to reduce ambiguity\nin an ill-posed setup. Our key observation is that directly modeling the joint\ndistribution of multiple instances imposes high learning complexity due to its\ncombinatorial nature. Thus, we propose to decompose the modeling of joint\ndistribution into the modeling of factored unconditional and conditional single\ninstance distribution. In particular, we introduce a diffusion model that\nlearns the single-hand distribution unconditional and conditional to another\nhand via conditioning dropout. For sampling, we combine anti-penetration and\nclassifier-free guidance to enable plausible generation. Furthermore, we\nestablish the rigorous evaluation protocol of two-hand synthesis, where our\nmethod significantly outperforms baseline generative models in terms of\nplausibility and diversity. We also demonstrate that our diffusion prior can\nboost the performance of two-hand reconstruction from monocular in-the-wild\nimages, achieving new state-of-the-art accuracy.\n","authors":["Jihyun Lee","Shunsuke Saito","Giljoo Nam","Minhyuk Sung","Tae-Kyun Kim"],"pdf_url":"https://arxiv.org/pdf/2403.17422v1.pdf","comment":"Accepted to CVPR 2024, project page:\n  https://jyunlee.github.io/projects/interhandgen/"},{"id":"http://arxiv.org/abs/2403.17420v1","updated":"2024-03-26T06:27:50Z","published":"2024-03-26T06:27:50Z","title":"Learning to Visually Localize Sound Sources from Mixtures without Prior\n  Source Knowledge","summary":"  The goal of the multi-sound source localization task is to localize sound\nsources from the mixture individually. While recent multi-sound source\nlocalization methods have shown improved performance, they face challenges due\nto their reliance on prior information about the number of objects to be\nseparated. In this paper, to overcome this limitation, we present a novel\nmulti-sound source localization method that can perform localization without\nprior knowledge of the number of sound sources. To achieve this goal, we\npropose an iterative object identification (IOI) module, which can recognize\nsound-making objects in an iterative manner. After finding the regions of\nsound-making objects, we devise object similarity-aware clustering (OSC) loss\nto guide the IOI module to effectively combine regions of the same object but\nalso distinguish between different objects and backgrounds. It enables our\nmethod to perform accurate localization of sound-making objects without any\nprior knowledge. Extensive experimental results on the MUSIC and VGGSound\nbenchmarks show the significant performance improvements of the proposed method\nover the existing methods for both single and multi-source. Our code is\navailable at: https://github.com/VisualAIKHU/NoPrior_MultiSSL\n","authors":["Dongjin Kim","Sung Jin Um","Sangmin Lee","Jung Uk Kim"],"pdf_url":"https://arxiv.org/pdf/2403.17420v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2312.09551v2","updated":"2024-03-26T06:05:17Z","published":"2023-12-15T06:04:42Z","title":"Learning-based Axial Video Motion Magnification","summary":"  Video motion magnification amplifies invisible small motions to be\nperceptible, which provides humans with a spatially dense and holistic\nunderstanding of small motions in the scene of interest. This is based on the\npremise that magnifying small motions enhances the legibility of motions. In\nthe real world, however, vibrating objects often possess convoluted systems\nthat have complex natural frequencies, modes, and directions. Existing motion\nmagnification often fails to improve legibility since the intricate motions\nstill retain complex characteristics even after being magnified, which may\ndistract us from analyzing them. In this work, we focus on improving legibility\nby proposing a new concept, axial motion magnification, which magnifies\ndecomposed motions along the user-specified direction. Axial motion\nmagnification can be applied to various applications where motions of specific\naxes are critical, by providing simplified and easily readable motion\ninformation. To achieve this, we propose a novel Motion Separation Module that\nenables to disentangle and magnify the motion representation along axes of\ninterest. Furthermore, we build a new synthetic training dataset for the axial\nmotion magnification task. Our proposed method improves the legibility of\nresulting motions along certain axes by adding a new feature: user\ncontrollability. Axial motion magnification is a more generalized concept;\nthus, our method can be directly adapted to the generic motion magnification\nand achieves favorable performance against competing methods.\n","authors":["Kwon Byung-Ki","Oh Hyun-Bin","Kim Jun-Seong","Hyunwoo Ha","Tae-Hyun Oh"],"pdf_url":"https://arxiv.org/pdf/2312.09551v2.pdf","comment":"main paper: 12 pages, supplementary: 10 pages, 20 figures, 1 table"},{"id":"http://arxiv.org/abs/2403.17409v1","updated":"2024-03-26T06:04:50Z","published":"2024-03-26T06:04:50Z","title":"Neural Clustering based Visual Representation Learning","summary":"  We investigate a fundamental aspect of machine vision: the measurement of\nfeatures, by revisiting clustering, one of the most classic approaches in\nmachine learning and data analysis. Existing visual feature extractors,\nincluding ConvNets, ViTs, and MLPs, represent an image as rectangular regions.\nThough prevalent, such a grid-style paradigm is built upon engineering practice\nand lacks explicit modeling of data distribution. In this work, we propose\nfeature extraction with clustering (FEC), a conceptually elegant yet\nsurprisingly ad-hoc interpretable neural clustering framework, which views\nfeature extraction as a process of selecting representatives from data and thus\nautomatically captures the underlying data distribution. Given an image, FEC\nalternates between grouping pixels into individual clusters to abstract\nrepresentatives and updating the deep features of pixels with current\nrepresentatives. Such an iterative working mechanism is implemented in the form\nof several neural layers and the final representatives can be used for\ndownstream tasks. The cluster assignments across layers, which can be viewed\nand inspected by humans, make the forward process of FEC fully transparent and\nempower it with promising ad-hoc interpretability. Extensive experiments on\nvarious visual recognition models and tasks verify the effectiveness,\ngenerality, and interpretability of FEC. We expect this work will provoke a\nrethink of the current de facto grid-style paradigm.\n","authors":["Guikun Chen","Xia Li","Yi Yang","Wenguan Wang"],"pdf_url":"https://arxiv.org/pdf/2403.17409v1.pdf","comment":"CVPR 2024. Code: https://github.com/guikunchen/FEC/"},{"id":"http://arxiv.org/abs/2403.17390v1","updated":"2024-03-26T05:19:15Z","published":"2024-03-26T05:19:15Z","title":"SSF3D: Strict Semi-Supervised 3D Object Detection with Switching Filter","summary":"  SSF3D modified the semi-supervised 3D object detection (SS3DOD) framework,\nwhich designed specifically for point cloud data. Leveraging the\ncharacteristics of non-coincidence and weak correlation of target objects in\npoint cloud, we adopt a strategy of retaining only the truth-determining pseudo\nlabels and trimming the other fuzzy labels with points, instead of pursuing a\nbalance between the quantity and quality of pseudo labels. Besides, we notice\nthat changing the filter will make the model meet different distributed\ntargets, which is beneficial to break the training bottleneck. Two mechanism\nare introduced to achieve above ideas: strict threshold and filter switching.\nThe experiments are conducted to analyze the effectiveness of above approaches\nand their impact on the overall performance of the system. Evaluating on the\nKITTI dataset, SSF3D exhibits superior performance compared to the current\nstate-of-the-art methods. The code will be released here.\n","authors":["Songbur Wong"],"pdf_url":"https://arxiv.org/pdf/2403.17390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17387v1","updated":"2024-03-26T05:12:18Z","published":"2024-03-26T05:12:18Z","title":"Decoupled Pseudo-labeling for Semi-Supervised Monocular 3D Object\n  Detection","summary":"  We delve into pseudo-labeling for semi-supervised monocular 3D object\ndetection (SSM3OD) and discover two primary issues: a misalignment between the\nprediction quality of 3D and 2D attributes and the tendency of depth\nsupervision derived from pseudo-labels to be noisy, leading to significant\noptimization conflicts with other reliable forms of supervision. We introduce a\nnovel decoupled pseudo-labeling (DPL) approach for SSM3OD. Our approach\nfeatures a Decoupled Pseudo-label Generation (DPG) module, designed to\nefficiently generate pseudo-labels by separately processing 2D and 3D\nattributes. This module incorporates a unique homography-based method for\nidentifying dependable pseudo-labels in BEV space, specifically for 3D\nattributes. Additionally, we present a DepthGradient Projection (DGP) module to\nmitigate optimization conflicts caused by noisy depth supervision of\npseudo-labels, effectively decoupling the depth gradient and removing\nconflicting gradients. This dual decoupling strategy-at both the pseudo-label\ngeneration and gradient levels-significantly improves the utilization of\npseudo-labels in SSM3OD. Our comprehensive experiments on the KITTI benchmark\ndemonstrate the superiority of our method over existing approaches.\n","authors":["Jiacheng Zhang","Jiaming Li","Xiangru Lin","Wei Zhang","Xiao Tan","Junyu Han","Errui Ding","Jingdong Wang","Guanbin Li"],"pdf_url":"https://arxiv.org/pdf/2403.17387v1.pdf","comment":"To appear in CVPR2024"},{"id":"http://arxiv.org/abs/2403.17377v1","updated":"2024-03-26T04:49:11Z","published":"2024-03-26T04:49:11Z","title":"Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance","summary":"  Recent studies have demonstrated that diffusion models are capable of\ngenerating high-quality samples, but their quality heavily depends on sampling\nguidance techniques, such as classifier guidance (CG) and classifier-free\nguidance (CFG). These techniques are often not applicable in unconditional\ngeneration or in various downstream tasks such as image restoration. In this\npaper, we propose a novel sampling guidance, called Perturbed-Attention\nGuidance (PAG), which improves diffusion sample quality across both\nunconditional and conditional settings, achieving this without requiring\nadditional training or the integration of external modules. PAG is designed to\nprogressively enhance the structure of samples throughout the denoising\nprocess. It involves generating intermediate samples with degraded structure by\nsubstituting selected self-attention maps in diffusion U-Net with an identity\nmatrix, by considering the self-attention mechanisms' ability to capture\nstructural information, and guiding the denoising process away from these\ndegraded samples. In both ADM and Stable Diffusion, PAG surprisingly improves\nsample quality in conditional and even unconditional scenarios. Moreover, PAG\nsignificantly improves the baseline performance in various downstream tasks\nwhere existing guidances such as CG or CFG cannot be fully utilized, including\nControlNet with empty prompts and image restoration such as inpainting and\ndeblurring.\n","authors":["Donghoon Ahn","Hyoungwon Cho","Jaewon Min","Wooseok Jang","Jungwoo Kim","SeonHwa Kim","Hyun Hee Park","Kyong Hwan Jin","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2403.17377v1.pdf","comment":"Project page is available at\n  https://ku-cvlab.github.io/Perturbed-Attention-Guidance"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2403.17901v1","updated":"2024-03-26T17:43:08Z","published":"2024-03-26T17:43:08Z","title":"Search and Society: Reimagining Information Access for Radical Futures","summary":"  Information retrieval (IR) technologies and research are undergoing\ntransformative changes. It is our perspective that the community should accept\nthis opportunity to re-center our research agendas on societal needs while\ndismantling the artificial separation between the work on fairness,\naccountability, transparency, and ethics in IR and the rest of IR research.\nInstead of adopting a reactionary strategy of trying to mitigate potential\nsocial harms from emerging technologies, the community should aim to\nproactively set the research agenda for the kinds of systems we should build\ninspired by diverse explicitly stated sociotechnical imaginaries. The\nsociotechnical imaginaries that underpin the design and development of\ninformation access technologies needs to be explicitly articulated, and we need\nto develop theories of change in context of these diverse perspectives. Our\nguiding future imaginaries must be informed by other academic fields, such as\ndemocratic theory and critical theory, and should be co-developed with social\nscience scholars, legal scholars, civil rights and social justice activists,\nand artists, among others. In this perspective paper, we motivate why the\ncommunity must consider this radical shift in how we do research and what we\nwork on, and sketch a path forward towards this transformation.\n","authors":["Bhaskar Mitra"],"pdf_url":"https://arxiv.org/pdf/2403.17901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17876v1","updated":"2024-03-26T17:06:56Z","published":"2024-03-26T17:06:56Z","title":"MIND Your Language: A Multilingual Dataset for Cross-lingual News\n  Recommendation","summary":"  Digital news platforms use news recommenders as the main instrument to cater\nto the individual information needs of readers. Despite an increasingly\nlanguage-diverse online community, in which many Internet users consume news in\nmultiple languages, the majority of news recommendation focuses on major,\nresource-rich languages, and English in particular. Moreover, nearly all news\nrecommendation efforts assume monolingual news consumption, whereas more and\nmore users tend to consume information in at least two languages. Accordingly,\nthe existing body of work on news recommendation suffers from a lack of\npublicly available multilingual benchmarks that would catalyze development of\nnews recommenders effective in multilingual settings and for low-resource\nlanguages. Aiming to fill this gap, we introduce xMIND, an open, multilingual\nnews recommendation dataset derived from the English MIND dataset using machine\ntranslation, covering a set of 14 linguistically and geographically diverse\nlanguages, with digital footprints of varying sizes. Using xMIND, we\nsystematically benchmark several state-of-the-art content-based neural news\nrecommenders (NNRs) in both zero-shot (ZS-XLT) and few-shot (FS-XLT)\ncross-lingual transfer scenarios, considering both monolingual and bilingual\nnews consumption patterns. Our findings reveal that (i) current NNRs, even when\nbased on a multilingual language model, suffer from substantial performance\nlosses under ZS-XLT and that (ii) inclusion of target-language data in FS-XLT\ntraining has limited benefits, particularly when combined with a bilingual news\nconsumption. Our findings thus warrant a broader research effort in\nmultilingual and cross-lingual news recommendation. The xMIND dataset is\navailable at https://github.com/andreeaiana/xMIND.\n","authors":["Andreea Iana","Goran Glavaš","Heiko Paulheim"],"pdf_url":"https://arxiv.org/pdf/2403.17876v1.pdf","comment":"Accepted at the 47th International ACM SIGIR Conference on Research\n  and Development in Information Retrieval (SIGIR 2024)"},{"id":"http://arxiv.org/abs/2403.17848v1","updated":"2024-03-26T16:37:54Z","published":"2024-03-26T16:37:54Z","title":"ArabicaQA: A Comprehensive Dataset for Arabic Question Answering","summary":"  In this paper, we address the significant gap in Arabic natural language\nprocessing (NLP) resources by introducing ArabicaQA, the first large-scale\ndataset for machine reading comprehension and open-domain question answering in\nArabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701\nunanswerable questions created by crowdworkers to look similar to answerable\nones, along with additional labels of open-domain questions marks a crucial\nadvancement in Arabic NLP resources. We also present AraDPR, the first dense\npassage retrieval model trained on the Arabic Wikipedia corpus, specifically\ndesigned to tackle the unique challenges of Arabic text retrieval. Furthermore,\nour study includes extensive benchmarking of large language models (LLMs) for\nArabic question answering, critically evaluating their performance in the\nArabic language context. In conclusion, ArabicaQA, AraDPR, and the benchmarking\nof LLMs in Arabic question answering offer significant advancements in the\nfield of Arabic NLP. The dataset and code are publicly accessible for further\nresearch https://github.com/DataScienceUIBK/ArabicaQA.\n","authors":["Abdelrahman Abdallah","Mahmoud Kasem","Mahmoud Abdalla","Mohamed Mahmoud","Mohamed Elkasaby","Yasser Elbendary","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2403.17848v1.pdf","comment":"Accepted at SIGIR 2024"},{"id":"http://arxiv.org/abs/2306.13186v3","updated":"2024-03-26T16:29:17Z","published":"2023-06-22T20:03:09Z","title":"A Decade of Scholarly Research on Open Knowledge Graphs","summary":"  The proliferation of open knowledge graphs has led to a surge in scholarly\nresearch on the topic over the past decade. This paper presents a bibliometric\nanalysis of the scholarly literature on open knowledge graphs published between\n2013 and 2023. The study aims to identify the trends, patterns, and impact of\nresearch in this field, as well as the key topics and research questions that\nhave emerged. The work uses bibliometric techniques to analyze a sample of 4445\nscholarly articles retrieved from Scopus. The findings reveal an\never-increasing number of publications on open knowledge graphs published every\nyear, particularly in developed countries (+50 per year). These outputs are\npublished in highly-referred scholarly journals and conferences. The study\nidentifies three main research themes: (1) knowledge graph construction and\nenrichment, (2) evaluation and reuse, and (3) fusion of knowledge graphs into\nNLP systems. Within these themes, the study identifies specific tasks that have\nreceived considerable attention, including entity linking, knowledge graph\nembedding, and graph neural networks.\n","authors":["Houcemeddine Turki","Abraham Toluwase Owodunni","Mohamed Ali Hadj Taieb","René Fabrice Bile","Mohamed Ben Aouicha"],"pdf_url":"https://arxiv.org/pdf/2306.13186v3.pdf","comment":"Camera-ready edition for LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17780v1","updated":"2024-03-26T15:13:16Z","published":"2024-03-26T15:13:16Z","title":"CaseLink: Inductive Graph Learning for Legal Case Retrieval","summary":"  In case law, the precedents are the relevant cases that are used to support\nthe decisions made by the judges and the opinions of lawyers towards a given\ncase. This relevance is referred to as the case-to-case reference relation. To\nefficiently find relevant cases from a large case pool, retrieval tools are\nwidely used by legal practitioners. Existing legal case retrieval models mainly\nwork by comparing the text representations of individual cases. Although they\nobtain a decent retrieval accuracy, the intrinsic case connectivity\nrelationships among cases have not been well exploited for case encoding,\ntherefore limiting the further improvement of retrieval performance. In a case\npool, there are three types of case connectivity relationships: the case\nreference relationship, the case semantic relationship, and the case legal\ncharge relationship. Due to the inductive manner in the task of legal case\nretrieval, using case reference as input is not applicable for testing. Thus,\nin this paper, a CaseLink model based on inductive graph learning is proposed\nto utilise the intrinsic case connectivity for legal case retrieval, a novel\nGlobal Case Graph is incorporated to represent both the case semantic\nrelationship and the case legal charge relationship. A novel contrastive\nobjective with a regularisation on the degree of case nodes is proposed to\nleverage the information carried by the case reference relationship to optimise\nthe model. Extensive experiments have been conducted on two benchmark datasets,\nwhich demonstrate the state-of-the-art performance of CaseLink. The code has\nbeen released on https://github.com/yanran-tang/CaseLink.\n","authors":["Yanran Tang","Ruihong Qiu","Hongzhi Yin","Xue Li","Zi Huang"],"pdf_url":"https://arxiv.org/pdf/2403.17780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17759v1","updated":"2024-03-26T14:51:03Z","published":"2024-03-26T14:51:03Z","title":"TWOLAR: a TWO-step LLM-Augmented distillation method for passage\n  Reranking","summary":"  In this paper, we present TWOLAR: a two-stage pipeline for passage reranking\nbased on the distillation of knowledge from Large Language Models (LLM). TWOLAR\nintroduces a new scoring strategy and a distillation process consisting in the\ncreation of a novel and diverse training dataset. The dataset consists of 20K\nqueries, each associated with a set of documents retrieved via four distinct\nretrieval methods to ensure diversity, and then reranked by exploiting the\nzero-shot reranking capabilities of an LLM. Our ablation studies demonstrate\nthe contribution of each new component we introduced. Our experimental results\nshow that TWOLAR significantly enhances the document reranking ability of the\nunderlying model, matching and in some cases even outperforming\nstate-of-the-art models with three orders of magnitude more parameters on the\nTREC-DL test sets and the zero-shot evaluation benchmark BEIR. To facilitate\nfuture work we release our data set, finetuned models, and code.\n","authors":["Davide Baldelli","Junfeng Jiang","Akiko Aizawa","Paolo Torroni"],"pdf_url":"https://arxiv.org/pdf/2403.17759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17740v1","updated":"2024-03-26T14:29:34Z","published":"2024-03-26T14:29:34Z","title":"All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating\n  Prediction","summary":"  Cold-start rating prediction is a fundamental problem in recommender systems\nthat has been extensively studied. Many methods have been proposed that exploit\nexplicit relations among existing data, such as collaborative filtering, social\nrecommendations and heterogeneous information network, to alleviate the data\ninsufficiency issue for cold-start users and items. However, the explicit\nrelations constructed based on data between different roles may be unreliable\nand irrelevant, which limits the performance ceiling of the specific\nrecommendation task. Motivated by this, in this paper, we propose a flexible\nframework dubbed heterogeneous interaction rating network (HIRE). HIRE dose not\nsolely rely on the pre-defined interaction pattern or the manually constructed\nheterogeneous information network. Instead, we devise a Heterogeneous\nInteraction Module (HIM) to jointly model the heterogeneous interactions and\ndirectly infer the important interactions via the observed data. In the\nexperiments, we evaluate our model under three cold-start settings on three\nreal-world datasets. The experimental results show that HIRE outperforms other\nbaselines by a large margin. Furthermore, we visualize the inferred\ninteractions of HIRE to confirm the contribution of our model.\n","authors":["Shuheng Fang","Kangfei Zhao","Yu Rong","Zhixun Li","Jeffrey Xu Yu"],"pdf_url":"https://arxiv.org/pdf/2403.17740v1.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2309.17078v2","updated":"2024-03-26T14:27:59Z","published":"2023-09-29T09:14:53Z","title":"Unsupervised Large Language Model Alignment for Information Retrieval\n  via Contrastive Feedback","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious research domains, including the field of Information Retrieval (IR).\nHowever, the responses generated by off-the-shelf LLMs tend to be generic,\ni.e., cannot capture the distinctiveness of each document with similar content.\nThis limits the performance of LLMs in IR because finding and distinguishing\nrelevant documents from substantial similar documents is a typical problem in\nmany IR tasks. To address this issue, we propose an unsupervised alignment\nmethod, namely Reinforcement Learning from Contrastive Feedback (RLCF),\nempowering LLMs to generate both high-quality and context-specific responses.\nOur approach constructs unsupervised contrastive feedback signals based on\nsimilar document groups, and adopts a reward function, named group-wise\nreciprocal rank, to optimize LLMs within a standard Proximal Policy\nOptimization. We conduct extensive experiments to evaluate the effectiveness of\nRLCF on LLMs built with different languages and parameter sizes on multiple\ndownstream IR applications. RLCF significantly outperforms existing alignment\nmethods, and RLCF-optimized LLMs demonstrate considerable improvement in\ngenerating responses with distinctiveness.\n","authors":["Qian Dong","Yiding Liu","Qingyao Ai","Zhijing Wu","Haitao Li","Yiqun Liu","Shuaiqiang Wang","Dawei Yin","Shaoping Ma"],"pdf_url":"https://arxiv.org/pdf/2309.17078v2.pdf","comment":"Accepted by SIGIR24"},{"id":"http://arxiv.org/abs/2403.17729v1","updated":"2024-03-26T14:18:43Z","published":"2024-03-26T14:18:43Z","title":"EulerFormer: Sequential User Behavior Modeling with Complex Vector\n  Attention","summary":"  To capture user preference, transformer models have been widely applied to\nmodel sequential user behavior data. The core of transformer architecture lies\nin the self-attention mechanism, which computes the pairwise attention scores\nin a sequence. Due to the permutation-equivariant nature, positional encoding\nis used to enhance the attention between token representations. In this\nsetting, the pairwise attention scores can be derived by both semantic\ndifference and positional difference. However, prior studies often model the\ntwo kinds of difference measurements in different ways, which potentially\nlimits the expressive capacity of sequence modeling. To address this issue,\nthis paper proposes a novel transformer variant with complex vector attention,\nnamed EulerFormer, which provides a unified theoretical framework to formulate\nboth semantic difference and positional difference. The EulerFormer involves\ntwo key technical improvements. First, it employs a new transformation function\nfor efficiently transforming the sequence tokens into polar-form complex\nvectors using Euler's formula, enabling the unified modeling of both semantic\nand positional information in a complex rotation form.Secondly, it develops a\ndifferential rotation mechanism, where the semantic rotation angles can be\ncontrolled by an adaptation function, enabling the adaptive integration of the\nsemantic and positional information according to the semantic\ncontexts.Furthermore, a phase contrastive learning task is proposed to improve\nthe anisotropy of contextual representations in EulerFormer. Our theoretical\nframework possesses a high degree of completeness and generality. It is more\nrobust to semantic variations and possesses moresuperior theoretical properties\nin principle. Extensive experiments conducted on four public datasets\ndemonstrate the effectiveness and efficiency of our approach.\n","authors":["Zhen Tian","Wayne Xin Zhao","Changwang Zhang","Xin Zhao","Zhongrui Ma","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2403.17729v1.pdf","comment":"Accepted for publication in SIGIR'24"},{"id":"http://arxiv.org/abs/2403.17688v1","updated":"2024-03-26T13:31:33Z","published":"2024-03-26T13:31:33Z","title":"Large Language Models Enhanced Collaborative Filtering","summary":"  Recent advancements in Large Language Models (LLMs) have attracted\nconsiderable interest among researchers to leverage these models to enhance\nRecommender Systems (RSs). Existing work predominantly utilizes LLMs to\ngenerate knowledge-rich texts or utilizes LLM-derived embeddings as features to\nimprove RSs. Al- though the extensive world knowledge embedded in LLMs\ngenerally benefits RSs, the application can only take limited number of users\nand items as inputs, without adequately exploiting collaborative filtering\ninformation. Considering its crucial role in RSs, one key challenge in\nenhancing RSs with LLMs lies in providing better collaborative filtering\ninformation through LLMs. In this paper, drawing inspiration from the\nin-context learning and chain of thought reasoning in LLMs, we propose the\nLarge Language Models enhanced Collaborative Filtering (LLM-CF) framework,\nwhich distils the world knowledge and reasoning capabilities of LLMs into\ncollaborative filtering. We also explored a concise and efficient\ninstruction-tuning method, which improves the recommendation capabilities of\nLLMs while preserving their general functionalities (e.g., not decreasing on\nthe LLM benchmark). Comprehensive experiments on three real-world datasets\ndemonstrate that LLM-CF significantly enhances several backbone recommendation\nmodels and consistently outperforms competitive baselines, showcasing its\neffectiveness in distilling the world knowledge and reasoning capabilities of\nLLM into collaborative filtering.\n","authors":["Zhongxiang Sun","Zihua Si","Xiaoxue Zang","Kai Zheng","Yang Song","Xiao Zhang","Jun Xu"],"pdf_url":"https://arxiv.org/pdf/2403.17688v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2403.16915v2","updated":"2024-03-26T13:11:44Z","published":"2024-03-25T16:32:50Z","title":"Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language\n  Models","summary":"  Fine-tuning in information retrieval systems using pre-trained language\nmodels (PLM-based IR) requires learning query representations and\nquery-document relations, in addition to downstream task-specific learning.\nThis study introduces coarse-tuning as an intermediate learning stage that\nbridges pre-training and fine-tuning. By learning query representations and\nquery-document relations in coarse-tuning, we aim to reduce the load of\nfine-tuning and improve the learning effect of downstream IR tasks. We propose\nQuery-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the\nappropriateness of query-document pairs. Evaluation experiments show that the\nproposed method significantly improves MRR and/or nDCG@5 in four ad-hoc\ndocument retrieval datasets. Furthermore, the results of the query prediction\ntask suggested that coarse-tuning facilitated learning of query representation\nand query-document relations.\n","authors":["Atsushi Keyaki","Ribeka Keyaki"],"pdf_url":"https://arxiv.org/pdf/2403.16915v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17643v1","updated":"2024-03-26T12:23:34Z","published":"2024-03-26T12:23:34Z","title":"S+t-SNE - Bringing dimensionality reduction to data streams","summary":"  We present S+t-SNE, an adaptation of the t-SNE algorithm designed to handle\ninfinite data streams. The core idea behind S+t-SNE is to update the t-SNE\nembedding incrementally as new data arrives, ensuring scalability and\nadaptability to handle streaming scenarios. By selecting the most important\npoints at each step, the algorithm ensures scalability while keeping\ninformative visualisations. Employing a blind method for drift management\nadjusts the embedding space, facilitating continuous visualisation of evolving\ndata dynamics. Our experimental evaluations demonstrate the effectiveness and\nefficiency of S+t-SNE. The results highlight its ability to capture patterns in\na streaming scenario. We hope our approach offers researchers and practitioners\na real-time tool for understanding and interpreting high-dimensional data.\n","authors":["Pedro C. Vieira","João P. Montrezol","João T. Vieira","João Gama"],"pdf_url":"https://arxiv.org/pdf/2403.17643v1.pdf","comment":"This preprint has not undergone peer review or any post-submission\n  improvements or corrections. We will soon add a link to the final version of\n  this contribution that underwent peer-review and post-acceptance improvements\n  and was presented at IDA2024 (https://ida2024.org/)"},{"id":"http://arxiv.org/abs/2403.17634v1","updated":"2024-03-26T12:08:58Z","published":"2024-03-26T12:08:58Z","title":"Retentive Decision Transformer with Adaptive Masking for Reinforcement\n  Learning based Recommendation Systems","summary":"  Reinforcement Learning-based Recommender Systems (RLRS) have shown promise\nacross a spectrum of applications, from e-commerce platforms to streaming\nservices. Yet, they grapple with challenges, notably in crafting reward\nfunctions and harnessing large pre-existing datasets within the RL framework.\nRecent advancements in offline RLRS provide a solution for how to address these\ntwo challenges. However, existing methods mainly rely on the transformer\narchitecture, which, as sequence lengths increase, can introduce challenges\nassociated with computational resources and training costs. Additionally, the\nprevalent methods employ fixed-length input trajectories, restricting their\ncapacity to capture evolving user preferences. In this study, we introduce a\nnew offline RLRS method to deal with the above problems. We reinterpret the\nRLRS challenge by modeling sequential decision-making as an inference task,\nleveraging adaptive masking configurations. This adaptive approach selectively\nmasks input tokens, transforming the recommendation task into an inference\nchallenge based on varying token subsets, thereby enhancing the agent's ability\nto infer across diverse trajectory lengths. Furthermore, we incorporate a\nmulti-scale segmented retention mechanism that facilitates efficient modeling\nof long sequences, significantly enhancing computational efficiency. Our\nexperimental analysis, conducted on both online simulator and offline datasets,\nclearly demonstrates the advantages of our proposed method.\n","authors":["Siyu Wang","Xiaocong Chen","Lina Yao"],"pdf_url":"https://arxiv.org/pdf/2403.17634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17603v1","updated":"2024-03-26T11:28:31Z","published":"2024-03-26T11:28:31Z","title":"END4Rec: Efficient Noise-Decoupling for Multi-Behavior Sequential\n  Recommendation","summary":"  In recommendation systems, users frequently engage in multiple types of\nbehaviors, such as clicking, adding to a cart, and purchasing. However, with\ndiversified behavior data, user behavior sequences will become very long in the\nshort term, which brings challenges to the efficiency of the sequence\nrecommendation model. Meanwhile, some behavior data will also bring inevitable\nnoise to the modeling of user interests. To address the aforementioned issues,\nfirstly, we develop the Efficient Behavior Sequence Miner (EBM) that\nefficiently captures intricate patterns in user behavior while maintaining low\ntime complexity and parameter count. Secondly, we design hard and soft\ndenoising modules for different noise types and fully explore the relationship\nbetween behaviors and noise. Finally, we introduce a contrastive loss function\nalong with a guided training strategy to compare the valid information in the\ndata with the noisy signal, and seamlessly integrate the two denoising\nprocesses to achieve a high degree of decoupling of the noisy signal.\nSufficient experiments on real-world datasets demonstrate the effectiveness and\nefficiency of our approach in dealing with multi-behavior sequential\nrecommendation.\n","authors":["Yongqiang Han","Hao Wang","Kefan Wang","Likang Wu","Zhi Li","Wei Guo","Yong Liu","Defu Lian","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2403.17603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.10909v2","updated":"2024-03-26T09:26:32Z","published":"2023-01-26T02:38:59Z","title":"Optimizing Feature Set for Click-Through Rate Prediction","summary":"  Click-through prediction (CTR) models transform features into latent vectors\nand enumerate possible feature interactions to improve performance based on the\ninput feature set. Therefore, when selecting an optimal feature set, we should\nconsider the influence of both feature and its interaction. However, most\nprevious works focus on either feature field selection or only select feature\ninteraction based on the fixed feature set to produce the feature set. The\nformer restricts search space to the feature field, which is too coarse to\ndetermine subtle features. They also do not filter useless feature\ninteractions, leading to higher computation costs and degraded model\nperformance. The latter identifies useful feature interaction from all\navailable features, resulting in many redundant features in the feature set. In\nthis paper, we propose a novel method named OptFS to address these problems. To\nunify the selection of feature and its interaction, we decompose the selection\nof each feature interaction into the selection of two correlated features. Such\na decomposition makes the model end-to-end trainable given various feature\ninteraction operations. By adopting feature-level search space, we set a\nlearnable gate to determine whether each feature should be within the feature\nset. Because of the large-scale search space, we develop a\nlearning-by-continuation training scheme to learn such gates. Hence, OptFS\ngenerates the feature set only containing features which improve the final\nprediction results. Experimentally, we evaluate OptFS on three public datasets,\ndemonstrating OptFS can optimize feature sets which enhance the model\nperformance and further reduce both the storage and computational cost.\n","authors":["Fuyuan Lyu","Xing Tang","Dugang Liu","Liang Chen","Xiuqiang He","Xue Liu"],"pdf_url":"https://arxiv.org/pdf/2301.10909v2.pdf","comment":"Accepted by WWW 2023 Research Tracks"},{"id":"http://arxiv.org/abs/2311.08744v2","updated":"2024-03-26T08:14:22Z","published":"2023-11-15T07:25:14Z","title":"Graph Signal Diffusion Model for Collaborative Filtering","summary":"  Collaborative filtering is a critical technique in recommender systems. Among\nvarious methods, an increasingly popular paradigm is to reconstruct user-item\ninteractions based on the historical observations. This can be viewed as a\nconditional generative task, where recently developed diffusion model\ndemonstrates great potential. However, existing studies on diffusion models\nlack effective solutions for modeling implicit feedback data. Particularly, the\nisotropic nature of the standard diffusion process fails to account for the\nheterogeneous dependencies among items, leading to a misalignment with the\ngraphical structure of the interaction space. Meanwhile, random noise\ndestroying personalized information in interaction vectors, causing difficulty\nin reverse reconstruction. In this paper, we make novel adaptions of diffusion\nmodel and propose Graph Signal Diffusion Model for Collaborative Filtering\n(named GiffCF). To better represent the high-dimensional and sparse\ndistribution of implicit feedback, we define a generalized form of denoising\ndiffusion using heat equation on the item-item similarity graph. Our forward\nprocess smooths interaction signals with an advanced family of graph filters.\nHence, instead of losing information, it involves item-item similarities as\nbeneficial prior knowledge for recommendation. To reconstruct high-quality\ninteractions, our reverse process iteratively refines and sharpens preference\nsignals in a deterministic manner, where the update direction is conditioned on\nthe user history and computed from a carefully designed two-stage denoiser.\nFinally, through extensive experiments, we show that GiffCF effectively\nleverages the advantages of both diffusion model and graph signal processing,\nand achieves state-of-the-art performance on three benchmark datasets.\n","authors":["Yunqin Zhu","Chao Wang","Qi Zhang","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2311.08744v2.pdf","comment":"11 pages, 8 figures, Accepted by SIGIR 2024"},{"id":"http://arxiv.org/abs/2403.17473v1","updated":"2024-03-26T08:07:33Z","published":"2024-03-26T08:07:33Z","title":"Document Set Expansion with Positive-Unlabelled Learning Using\n  Intractable Density Estimation","summary":"  The Document Set Expansion (DSE) task involves identifying relevant documents\nfrom large collections based on a limited set of example documents. Previous\nresearch has highlighted Positive and Unlabeled (PU) learning as a promising\napproach for this task. However, most PU methods rely on the unrealistic\nassumption of knowing the class prior for positive samples in the collection.\nTo address this limitation, this paper introduces a novel PU learning framework\nthat utilizes intractable density estimation models. Experiments conducted on\nPubMed and Covid datasets in a transductive setting showcase the effectiveness\nof the proposed method for DSE. Code is available from\nhttps://github.com/Beautifuldog01/Document-set-expansion-puDE.\n","authors":["Haiyang Zhang","Qiuyi Chen","Yuanjie Zou","Yushan Pan","Jia Wang","Mark Stevenson"],"pdf_url":"https://arxiv.org/pdf/2403.17473v1.pdf","comment":"Accepted at LREC-COLING 2024. arXiv admin note: text overlap with\n  arXiv:2401.11145"},{"id":"http://arxiv.org/abs/2403.17442v1","updated":"2024-03-26T07:19:26Z","published":"2024-03-26T07:19:26Z","title":"Touch the Core: Exploring Task Dependence Among Hybrid Targets for\n  Recommendation","summary":"  As user behaviors become complicated on business platforms, online\nrecommendations focus more on how to touch the core conversions, which are\nhighly related to the interests of platforms. These core conversions are\nusually continuous targets, such as \\textit{watch time}, \\textit{revenue}, and\nso on, whose predictions can be enhanced by previous discrete conversion\nactions. Therefore, multi-task learning (MTL) can be adopted as the paradigm to\nlearn these hybrid targets. However, existing works mainly emphasize\ninvestigating the sequential dependence among discrete conversion actions,\nwhich neglects the complexity of dependence between discrete conversions and\nthe final continuous conversion. Moreover, simultaneously optimizing hybrid\ntasks with stronger task dependence will suffer from volatile issues where the\ncore regression task might have a larger influence on other tasks. In this\npaper, we study the MTL problem with hybrid targets for the first time and\npropose the model named Hybrid Targets Learning Network (HTLNet) to explore\ntask dependence and enhance optimization. Specifically, we introduce label\nembedding for each task to explicitly transfer the label information among\nthese tasks, which can effectively explore logical task dependence. We also\nfurther design the gradient adjustment regime between the final regression task\nand other classification tasks to enhance the optimization. Extensive\nexperiments on two offline public datasets and one real-world industrial\ndataset are conducted to validate the effectiveness of HTLNet. Moreover, online\nA/B tests on the financial recommender system also show our model has superior\nimprovement.\n","authors":["Xing Tang","Yang Qiao","Fuyuan Lyu","Dugang Liu","Xiuqiang He"],"pdf_url":"https://arxiv.org/pdf/2403.17442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.01262v2","updated":"2024-03-26T06:54:43Z","published":"2022-07-04T08:54:43Z","title":"Understanding Performance of Long-Document Ranking Models through\n  Comprehensive Evaluation and Leaderboarding","summary":"  We evaluated 20+ Transformer models for ranking of long documents (including\nrecent LongP models trained with FlashAttention) and compared them with simple\nFirstP baselines (applying the same model to input truncated to the first 512\ntokens). We used MS MARCO Documents v1 as a primary training set and evaluated\nmodels in the zero-shot scenario as well as after fine-tuning on other\ncollections.\n  In our initial experiments with standard collections we found that\nlong-document models underperformed FirstP or outperformed it by at most 5% on\naverage in terms of MRR or NDCG. We then conjectured that this was not due to\nmodels inability to process long context but rather due to a positional bias of\nrelevant passages, which tended to be among the first 512 document tokens. We\nfound evidence that this bias was, indeed, present in at least two test sets,\nwhich motivated us to create a new collection MS MARCO FarRelevant where the\nrelevant passages were not present among the first 512 tokens.\n  Unlike standard collections where we observed both little benefit from\nincorporating longer contexts and limited variability in model performance\n(within a few %), experiments on MS MARCO FarRelevant uncovered dramatic\ndifferences among models. FirstP models performed roughly at the\nrandom-baseline level in both zero-shot and fine-tuning scenarios. Simple\naggregation models (e.g., MaxP) had good zero-shot accuracy but benefited\nlittle from fine-tuning. Most other models had poor zero-shot performance\n(sometimes at a random baseline level) but outstripped MaxP by as much 13-28\\%\nafter finetuning. Thus, positional bias not only diminishes benefits of\nprocessing longer document contexts but also leads to model overfitting to this\nbias and performing poorly in a zero-shot setting when a distribution of\nrelevant passages changes substantially.\n  We make our software and MS MARCO FarRelevant available.\n","authors":["Leonid Boytsov","David Akinpelu","Tianyi Lin","Fangwei Gao","Yutian Zhao","Jeffrey Huang","Eric Nyberg"],"pdf_url":"https://arxiv.org/pdf/2207.01262v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17425v1","updated":"2024-03-26T06:42:23Z","published":"2024-03-26T06:42:23Z","title":"Masked Multi-Domain Network: Multi-Type and Multi-Scenario Conversion\n  Rate Prediction with a Single Model","summary":"  In real-world advertising systems, conversions have different types in nature\nand ads can be shown in different display scenarios, both of which highly\nimpact the actual conversion rate (CVR). This results in the multi-type and\nmulti-scenario CVR prediction problem. A desired model for this problem should\nsatisfy the following requirements: 1) Accuracy: the model should achieve\nfine-grained accuracy with respect to any conversion type in any display\nscenario. 2) Scalability: the model parameter size should be affordable. 3)\nConvenience: the model should not require a large amount of effort in data\npartitioning, subset processing and separate storage. Existing approaches\ncannot simultaneously satisfy these requirements. For example, building a\nseparate model for each (conversion type, display scenario) pair is neither\nscalable nor convenient. Building a unified model trained on all the data with\nconversion type and display scenario included as two features is not accurate\nenough. In this paper, we propose the Masked Multi-domain Network (MMN) to\nsolve this problem. To achieve the accuracy requirement, we model\ndomain-specific parameters and propose a dynamically weighted loss to account\nfor the loss scale imbalance issue within each mini-batch. To achieve the\nscalability requirement, we propose a parameter sharing and composition\nstrategy to reduce model parameters from a product space to a sum space. To\nachieve the convenience requirement, we propose an auto-masking strategy which\ncan take mixed data from all the domains as input. It avoids the overhead\ncaused by data partitioning, individual processing and separate storage. Both\noffline and online experimental results validate the superiority of MMN for\nmulti-type and multi-scenario CVR prediction. MMN is now the serving model for\nreal-time CVR prediction in UC Toutiao.\n","authors":["Wentao Ouyang","Xiuwu Zhang","Chaofeng Guo","Shukui Ren","Yupei Sui","Kun Zhang","Jinmei Luo","Yunfeng Chen","Dongbo Xu","Xiangzheng Liu","Yanlong Du"],"pdf_url":"https://arxiv.org/pdf/2403.17425v1.pdf","comment":"CIKM 2023 (larger figures)"},{"id":"http://arxiv.org/abs/2403.17421v1","updated":"2024-03-26T06:34:23Z","published":"2024-03-26T06:34:23Z","title":"MA4DIV: Multi-Agent Reinforcement Learning for Search Result\n  Diversification","summary":"  The objective of search result diversification (SRD) is to ensure that\nselected documents cover as many different subtopics as possible. Existing\nmethods primarily utilize a paradigm of \"greedy selection\", i.e., selecting one\ndocument with the highest diversity score at a time. These approaches tend to\nbe inefficient and are easily trapped in a suboptimal state. In addition, some\nother methods aim to approximately optimize the diversity metric, such as\n$\\alpha$-NDCG, but the results still remain suboptimal. To address these\nchallenges, we introduce Multi-Agent reinforcement learning (MARL) for search\nresult DIVersity, which called MA4DIV. In this approach, each document is an\nagent and the search result diversification is modeled as a cooperative task\namong multiple agents. This approach allows for directly optimizing the\ndiversity metrics, such as $\\alpha$-NDCG, while achieving high training\nefficiency. We conducted preliminary experiments on public TREC datasets to\ndemonstrate the effectiveness and potential of MA4DIV. Considering the limited\nnumber of queries in public TREC datasets, we construct a large-scale dataset\nfrom industry sources and show that MA4DIV achieves substantial improvements in\nboth effectiveness and efficiency than existing baselines on a industrial scale\ndataset.\n","authors":["Yiqun Chen","Jiaxin Mao","Yi Zhang","Dehong MA","Long Xia","Jun Fan","Daiting Shi","Zhicong Cheng","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2403.17421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17416v1","updated":"2024-03-26T06:14:19Z","published":"2024-03-26T06:14:19Z","title":"AFDGCF: Adaptive Feature De-correlation Graph Collaborative Filtering\n  for Recommendations","summary":"  Collaborative filtering methods based on graph neural networks (GNNs) have\nwitnessed significant success in recommender systems (RS), capitalizing on\ntheir ability to capture collaborative signals within intricate user-item\nrelationships via message-passing mechanisms. However, these GNN-based RS\ninadvertently introduce excess linear correlation between user and item\nembeddings, contradicting the goal of providing personalized recommendations.\nWhile existing research predominantly ascribes this flaw to the over-smoothing\nproblem, this paper underscores the critical, often overlooked role of the\nover-correlation issue in diminishing the effectiveness of GNN representations\nand subsequent recommendation performance. Up to now, the over-correlation\nissue remains unexplored in RS. Meanwhile, how to mitigate the impact of\nover-correlation while preserving collaborative filtering signals is a\nsignificant challenge. To this end, this paper aims to address the\naforementioned gap by undertaking a comprehensive study of the over-correlation\nissue in graph collaborative filtering models. Firstly, we present empirical\nevidence to demonstrate the widespread prevalence of over-correlation in these\nmodels. Subsequently, we dive into a theoretical analysis which establishes a\npivotal connection between the over-correlation and over-smoothing issues.\nLeveraging these insights, we introduce the Adaptive Feature De-correlation\nGraph Collaborative Filtering (AFDGCF) framework, which dynamically applies\ncorrelation penalties to the feature dimensions of the representation matrix,\neffectively alleviating both over-correlation and over-smoothing issues. The\nefficacy of the proposed framework is corroborated through extensive\nexperiments conducted with four representative graph collaborative filtering\nmodels across four publicly available datasets.\n","authors":["Wei Wu","Chao Wang","Dazhong Shen","Chuan Qin","Liyi Chen","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2403.17416v1.pdf","comment":"Accepted by SIGIR2024"},{"id":"http://arxiv.org/abs/2403.17374v1","updated":"2024-03-26T04:30:40Z","published":"2024-03-26T04:30:40Z","title":"Multi-Domain Recommendation to Attract Users via Domain Preference\n  Modeling","summary":"  Recently, web platforms have been operating various service domains\nsimultaneously. Targeting a platform that operates multiple service domains, we\nintroduce a new task, Multi-Domain Recommendation to Attract Users (MDRAU),\nwhich recommends items from multiple ``unseen'' domains with which each user\nhas not interacted yet, by using knowledge from the user's ``seen'' domains. In\nthis paper, we point out two challenges of MDRAU task. First, there are\nnumerous possible combinations of mappings from seen to unseen domains because\nusers have usually interacted with a different subset of service domains.\nSecond, a user might have different preferences for each of the target unseen\ndomains, which requires that recommendations reflect the user's preferences on\ndomains as well as items. To tackle these challenges, we propose DRIP framework\nthat models users' preferences at two levels (i.e., domain and item) and learns\nvarious seen-unseen domain mappings in a unified way with masked domain\nmodeling. Our extensive experiments demonstrate the effectiveness of DRIP in\nMDRAU task and its ability to capture users' domain-level preferences.\n","authors":["Hyuunjun Ju","SeongKu Kang","Dongha Lee","Junyoung Hwang","Sanghwan Jang","Hwanjo Yu"],"pdf_url":"https://arxiv.org/pdf/2403.17374v1.pdf","comment":"Accepted to AAAI'24"},{"id":"http://arxiv.org/abs/2403.17372v1","updated":"2024-03-26T04:16:57Z","published":"2024-03-26T04:16:57Z","title":"An Empirical Study of Training ID-Agnostic Multi-modal Sequential\n  Recommenders","summary":"  Sequential Recommendation (SR) aims to predict future user-item interactions\nbased on historical interactions. While many SR approaches concentrate on user\nIDs and item IDs, the human perception of the world through multi-modal\nsignals, like text and images, has inspired researchers to delve into\nconstructing SR from multi-modal information without using IDs. However, the\ncomplexity of multi-modal learning manifests in diverse feature extractors,\nfusion methods, and pre-trained models. Consequently, designing a simple and\nuniversal \\textbf{M}ulti-\\textbf{M}odal \\textbf{S}equential\n\\textbf{R}ecommendation (\\textbf{MMSR}) framework remains a formidable\nchallenge. We systematically summarize the existing multi-modal related SR\nmethods and distill the essence into four core components: visual encoder, text\nencoder, multimodal fusion module, and sequential architecture. Along these\ndimensions, we dissect the model designs, and answer the following\nsub-questions: First, we explore how to construct MMSR from scratch, ensuring\nits performance either on par with or exceeds existing SR methods without\ncomplex techniques. Second, we examine if MMSR can benefit from existing\nmulti-modal pre-training paradigms. Third, we assess MMSR's capability in\ntackling common challenges like cold start and domain transferring. Our\nexperiment results across four real-world recommendation scenarios demonstrate\nthe great potential ID-agnostic multi-modal sequential recommendation. Our\nframework can be found at: https://github.com/MMSR23/MMSR.\n","authors":["Youhua Li","Hanwen Du","Yongxin Ni","Yuanqi He","Junchen Fu","Xiangyan Liu","Qi Guo"],"pdf_url":"https://arxiv.org/pdf/2403.17372v1.pdf","comment":"An Empirical Study of Training ID-Agnostic Multi-modal Sequential\n  Recommenders"},{"id":"http://arxiv.org/abs/2403.09963v2","updated":"2024-03-26T04:08:47Z","published":"2024-03-15T02:04:35Z","title":"Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias\n  in Factual Knowledge Extraction","summary":"  Recent research shows that pre-trained language models (PLMs) suffer from\n\"prompt bias\" in factual knowledge extraction, i.e., prompts tend to introduce\nbiases toward specific labels. Prompt bias presents a significant challenge in\nassessing the factual knowledge within PLMs. Therefore, this paper aims to\nimprove the reliability of existing benchmarks by thoroughly investigating and\nmitigating prompt bias. We show that: 1) all prompts in the experiments exhibit\nnon-negligible bias, with gradient-based prompts like AutoPrompt and OptiPrompt\ndisplaying significantly higher levels of bias; 2) prompt bias can amplify\nbenchmark accuracy unreasonably by overfitting the test datasets, especially on\nimbalanced datasets like LAMA. Based on these findings, we propose a\nrepresentation-based approach to mitigate the prompt bias during inference\ntime. Specifically, we first estimate the biased representation using\nprompt-only querying, and then remove it from the model's internal\nrepresentations to generate the debiased representations, which are used to\nproduce the final debiased outputs. Experiments across various prompts, PLMs,\nand benchmarks show that our approach can not only correct the overfitted\nperformance caused by prompt bias, but also significantly improve the prompt\nretrieval capability (up to 10% absolute performance gain). These results\nindicate that our approach effectively alleviates prompt bias in knowledge\nevaluation, thereby enhancing the reliability of benchmark assessments.\nHopefully, our plug-and-play approach can be a golden standard to strengthen\nPLMs toward reliable knowledge bases. Code and data are released in\nhttps://github.com/FelliYang/PromptBias.\n","authors":["Ziyang Xu","Keqin Peng","Liang Ding","Dacheng Tao","Xiliang Lu"],"pdf_url":"https://arxiv.org/pdf/2403.09963v2.pdf","comment":"Accepted by COLING 2024"},{"id":"http://arxiv.org/abs/2112.06460v5","updated":"2024-03-26T03:44:29Z","published":"2021-12-13T07:33:28Z","title":"Improving Sequential Recommendations via Bidirectional Temporal Data\n  Augmentation with Pre-training","summary":"  Sequential recommendation systems are integral to discerning temporal user\npreferences. Yet, the task of learning from abbreviated user interaction\nsequences poses a notable challenge. Data augmentation has been identified as a\npotent strategy to enhance the informational richness of these sequences.\nTraditional augmentation techniques, such as item randomization, may disrupt\nthe inherent temporal dynamics. Although recent advancements in reverse\nchronological pseudo-item generation have shown promise, they can introduce\ntemporal discrepancies when assessed in a natural chronological context. In\nresponse, we introduce a sophisticated approach, Bidirectional temporal data\nAugmentation with pre-training (BARec). Our approach leverages bidirectional\ntemporal augmentation and knowledge-enhanced fine-tuning to synthesize\nauthentic pseudo-prior items that \\emph{retain user preferences and capture\ndeeper item semantic correlations}, thus boosting the model's expressive power.\nOur comprehensive experimental analysis confirms the superiority of BARec\nacross both short and elongated sequence contexts. Moreover, theoretical\nexamination and visual representation of item embeddings offer further insight\ninto the model's logical processes and interpretability. The source code for\nour study is available at\n\\textcolor{blue}{\\href{https://github.com/juyongjiang/BARec}{https://github.com/juyongjiang/BARec}}.\n","authors":["Juyong Jiang","Peiyan Zhang","Yingtao Luo","Chaozhuo Li","Jaeboum Kim","Kai Zhang","Senzhang Wang","Sunghun Kim"],"pdf_url":"https://arxiv.org/pdf/2112.06460v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17286v1","updated":"2024-03-26T00:29:52Z","published":"2024-03-26T00:29:52Z","title":"Cognitively Biased Users Interacting with Algorithmically Biased Results\n  in Whole-Session Search on Controversial Topics","summary":"  When interacting with information retrieval (IR) systems, users, affected by\nconfirmation biases, tend to select search results that confirm their existing\nbeliefs on socially significant contentious issues. To understand the judgments\nand attitude changes of users searching online, our study examined how\ncognitively biased users interact with algorithmically biased search engine\nresult pages (SERPs). We designed three-query search sessions on debated topics\nunder various bias conditions. We recruited 1,321 crowdsourcing participants\nand explored their attitude changes, search interactions, and the effects of\nconfirmation bias. Three key findings emerged: 1) most attitude changes occur\nin the initial query of a search session; 2) confirmation bias and result\npresentation on SERPs affect search behaviors in the current query and\nperceived familiarity with clicked results in subsequent queries. The bias\nposition also affect attitude changes of users with lower perceived openness to\nconflicting opinions; 3) Interactions in the first query and and dwell time\nthroughout the session are associated with users' attitude changes in different\nforms. Our study goes beyond traditional simulation-based evaluation settings\nand simulated rational users, sheds light on the mixed effects of human biases\nand algorithmic biases in controversial information retrieval tasks, and can\ninform the design of bias-aware user models, human-centered bias mitigation\ntechniques, and socially responsible intelligent IR systems.\n","authors":["Ben Wang","Jiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18025v1","updated":"2024-03-26T18:23:16Z","published":"2024-03-26T18:23:16Z","title":"Improving Pre-trained Language Model Sensitivity via Mask Specific\n  losses: A case study on Biomedical NER","summary":"  Adapting language models (LMs) to novel domains is often achieved through\nfine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning\nintroduces new knowledge into an LM, enabling it to comprehend and efficiently\nperform a target domain task. Fine-tuning can however be inadvertently\ninsensitive if it ignores the wide array of disparities (e.g in word meaning)\nbetween source and target domains. For instance, words such as chronic and\npressure may be treated lightly in social conversations, however, clinically,\nthese words are usually an expression of concern. To address insensitive\nfine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach\nthat efficiently acquires target domain knowledge by appropriately weighting\nthe importance of domain-specific terms (DS-terms) during fine-tuning. MSLM\njointly masks DS-terms and generic words, then learns mask-specific losses by\nensuring LMs incur larger penalties for inaccurately predicting DS-terms\ncompared to generic words. Results of our analysis show that MSLM improves LMs\nsensitivity and detection of DS-terms. We empirically show that an optimal\nmasking rate not only depends on the LM, but also on the dataset and the length\nof sequences. Our proposed masking strategy outperforms advanced masking\nstrategies such as span- and PMI-based masking.\n","authors":["Micheal Abaho","Danushka Bollegala","Gary Leeming","Dan Joyce","Iain E Buchan"],"pdf_url":"https://arxiv.org/pdf/2403.18025v1.pdf","comment":"Paper alrerady accepted for publishing by the NAACL 2024 conference\n  (main conference paper)"},{"id":"http://arxiv.org/abs/2403.17688v1","updated":"2024-03-26T13:31:33Z","published":"2024-03-26T13:31:33Z","title":"Large Language Models Enhanced Collaborative Filtering","summary":"  Recent advancements in Large Language Models (LLMs) have attracted\nconsiderable interest among researchers to leverage these models to enhance\nRecommender Systems (RSs). Existing work predominantly utilizes LLMs to\ngenerate knowledge-rich texts or utilizes LLM-derived embeddings as features to\nimprove RSs. Although the extensive world knowledge embedded in LLMs generally\nbenefits RSs, the application can only take limited number of users and items\nas inputs, without adequately exploiting collaborative filtering information.\nConsidering its crucial role in RSs, one key challenge in enhancing RSs with\nLLMs lies in providing better collaborative filtering information through LLMs.\nIn this paper, drawing inspiration from the in-context learning and chain of\nthought reasoning in LLMs, we propose the Large Language Models enhanced\nCollaborative Filtering (LLM-CF) framework, which distils the world knowledge\nand reasoning capabilities of LLMs into collaborative filtering. We also\nexplored a concise and efficient instruction-tuning method, which improves the\nrecommendation capabilities of LLMs while preserving their general\nfunctionalities (e.g., not decreasing on the LLM benchmark). Comprehensive\nexperiments on three real-world datasets demonstrate that LLM-CF significantly\nenhances several backbone recommendation models and consistently outperforms\ncompetitive baselines, showcasing its effectiveness in distilling the world\nknowledge and reasoning capabilities of LLM into collaborative filtering.\n","authors":["Zhongxiang Sun","Zihua Si","Xiaoxue Zang","Kai Zheng","Yang Song","Xiao Zhang","Jun Xu"],"pdf_url":"https://arxiv.org/pdf/2403.17688v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2403.17643v1","updated":"2024-03-26T12:23:34Z","published":"2024-03-26T12:23:34Z","title":"S+t-SNE -- Bringing dimensionality reduction to data streams","summary":"  We present S+t-SNE, an adaptation of the t-SNE algorithm designed to handle\ninfinite data streams. The core idea behind S+t-SNE is to update the t-SNE\nembedding incrementally as new data arrives, ensuring scalability and\nadaptability to handle streaming scenarios. By selecting the most important\npoints at each step, the algorithm ensures scalability while keeping\ninformative visualisations. Employing a blind method for drift management\nadjusts the embedding space, facilitating continuous visualisation of evolving\ndata dynamics. Our experimental evaluations demonstrate the effectiveness and\nefficiency of S+t-SNE. The results highlight its ability to capture patterns in\na streaming scenario. We hope our approach offers researchers and practitioners\na real-time tool for understanding and interpreting high-dimensional data.\n","authors":["Pedro C. Vieira","João P. Montrezol","João T. Vieira","João Gama"],"pdf_url":"https://arxiv.org/pdf/2403.17643v1.pdf","comment":"This preprint has not undergone peer review or any post-submission\n  improvements or corrections. We will soon add a link to the final version of\n  this contribution that underwent peer-review and post-acceptance improvements\n  and was presented at IDA2024 (https://ida2024.org/)"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2402.13452v2","updated":"2024-03-26T17:59:14Z","published":"2024-02-21T01:11:28Z","title":"LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based\n  on Twitter Data","summary":"  Prior research on Twitter (now X) data has provided positive evidence of its\nutility in developing supplementary health surveillance systems. In this study,\nwe present a new framework to surveil public health, focusing on mental health\n(MH) outcomes. We hypothesize that locally posted tweets are indicative of\nlocal MH outcomes and collect tweets posted from 765 neighborhoods (census\nblock groups) in the USA. We pair these tweets from each neighborhood with the\ncorresponding MH outcome reported by the Center for Disease Control (CDC) to\ncreate a benchmark dataset, LocalTweets. With LocalTweets, we present the first\npopulation-level evaluation task for Twitter-based MH surveillance systems. We\nthen develop an efficient and effective method, LocalHealth, for predicting MH\noutcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the\nhighest F1-score and accuracy of 0.7429 and 79.78\\%, respectively, a 59\\%\nimprovement in F1-score over the GPT3.5 in zero-shot setting. We also utilize\nLocalHealth to extrapolate CDC's estimates to proxy unreported neighborhoods,\nachieving an F1-score of 0.7291. Our work suggests that Twitter data can be\neffectively leveraged to simulate neighborhood-level MH outcomes.\n","authors":["Vijeta Deshpande","Minhwa Lee","Zonghai Yao","Zihao Zhang","Jason Brian Gibbons","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2402.13452v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08763v3","updated":"2024-03-26T17:58:48Z","published":"2024-03-13T17:58:57Z","title":"Simple and Scalable Strategies to Continually Pre-train Large Language\n  Models","summary":"  Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by the final loss\nand the average score on several language model (LM) evaluation benchmarks.\nSpecifically, we show this for a weak but realistic distribution shift between\ntwo commonly used LLM pre-training datasets (English$\\rightarrow$English) and a\nstronger distribution shift (English$\\rightarrow$German) at the $405$M\nparameter model scale with large dataset sizes (hundreds of billions of\ntokens). Selecting the weak but realistic shift for larger-scale experiments,\nwe also find that our continual learning strategies match the re-training\nbaseline for a 10B parameter LLM. Our results demonstrate that LLMs can be\nsuccessfully updated via simple and scalable continual learning strategies,\nmatching the re-training baseline using only a fraction of the compute.\nFinally, inspired by previous work, we propose alternatives to the cosine\nlearning rate schedule that help circumvent forgetting induced by LR re-warming\nand that are not bound to a fixed token budget.\n","authors":["Adam Ibrahim","Benjamin Thérien","Kshitij Gupta","Mats L. Richter","Quentin Anthony","Timothée Lesort","Eugene Belilovsky","Irina Rish"],"pdf_url":"https://arxiv.org/pdf/2403.08763v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17933v1","updated":"2024-03-26T17:58:29Z","published":"2024-03-26T17:58:29Z","title":"SLEDGE: Synthesizing Simulation Environments for Driving Agents with\n  Generative Models","summary":"  SLEDGE is the first generative simulator for vehicle motion planning trained\non real-world driving logs. Its core component is a learned model that is able\nto generate agent bounding boxes and lane graphs. The model's outputs serve as\nan initial state for traffic simulation. The unique properties of the entities\nto be generated for SLEDGE, such as their connectivity and variable count per\nscene, render the naive application of most modern generative models to this\ntask non-trivial. Therefore, together with a systematic study of existing lane\ngraph representations, we introduce a novel raster-to-vector autoencoder\n(RVAE). It encodes agents and the lane graph into distinct channels in a\nrasterized latent map. This facilitates both lane-conditioned agent generation\nand combined generation of lanes and agents with a Diffusion Transformer. Using\ngenerated entities in SLEDGE enables greater control over the simulation, e.g.\nupsampling turns or increasing traffic density. Further, SLEDGE can support\n500m long routes, a capability not found in existing data-driven simulators\nlike nuPlan. It presents new challenges for planning algorithms, evidenced by\nfailure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge,\nwhen tested on hard routes and dense traffic generated by our model. Compared\nto nuPlan, SLEDGE requires 500$\\times$ less storage to set up (<4GB), making it\na more accessible option and helping with democratizing future research in this\nfield.\n","authors":["Kashyap Chitta","Daniel Dauner","Andreas Geiger"],"pdf_url":"https://arxiv.org/pdf/2403.17933v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17921v1","updated":"2024-03-26T17:55:58Z","published":"2024-03-26T17:55:58Z","title":"The Need for Speed: Pruning Transformers with One Recipe","summary":"  We introduce the $\\textbf{O}$ne-shot $\\textbf{P}$runing $\\textbf{T}$echnique\nfor $\\textbf{I}$nterchangeable $\\textbf{N}$etworks ($\\textbf{OPTIN}$) framework\nas a tool to increase the efficiency of pre-trained transformer architectures\n$\\textit{without requiring re-training}$. Recent works have explored improving\ntransformer efficiency, however often incur computationally expensive\nre-training procedures or depend on architecture-specific characteristics, thus\nimpeding practical wide-scale adoption. To address these shortcomings, the\nOPTIN framework leverages intermediate feature distillation, capturing the\nlong-range dependencies of model parameters (coined $\\textit{trajectory}$), to\nproduce state-of-the-art results on natural language, image classification,\ntransfer learning, and semantic segmentation tasks $\\textit{without\nre-training}$. Given a FLOP constraint, the OPTIN framework will compress the\nnetwork while maintaining competitive accuracy performance and improved\nthroughput. Particularly, we show a $\\leq 2$% accuracy degradation from NLP\nbaselines and a $0.5$% improvement from state-of-the-art methods on image\nclassification at competitive FLOPs reductions. We further demonstrate the\ngeneralization of tasks and architecture with comparative performance using\nMask2Former for semantic segmentation and cnn-style networks. OPTIN presents\none of the first one-shot efficient frameworks for compressing transformer\narchitectures that generalizes well across different class domains, in\nparticular: natural language and image-related tasks, without\n$\\textit{re-training}$.\n","authors":["Samir Khaki","Konstantinos N. Plataniotis"],"pdf_url":"https://arxiv.org/pdf/2403.17921v1.pdf","comment":"Accepted in the International Conference on Learning Representations\n  (ICLR) 2024"},{"id":"http://arxiv.org/abs/2403.17919v1","updated":"2024-03-26T17:55:02Z","published":"2024-03-26T17:55:02Z","title":"LISA: Layerwise Importance Sampling for Memory-Efficient Large Language\n  Model Fine-Tuning","summary":"  The machine learning community has witnessed impressive advancements since\nthe first appearance of large language models (LLMs), yet their huge memory\nconsumption has become a major roadblock to large-scale training. Parameter\nEfficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been\nproposed to alleviate this problem, but their performance still fails to match\nfull parameter training in most large-scale fine-tuning settings. Attempting to\ncomplement this deficiency, we investigate layerwise properties of LoRA on\nfine-tuning tasks and observe an uncommon skewness of weight norms across\ndifferent layers. Utilizing this key observation, a surprisingly simple\ntraining strategy is discovered, which outperforms both LoRA and full parameter\ntraining in a wide range of settings with memory costs as low as LoRA. We name\nit Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA,\nwhich applies the idea of importance sampling to different layers in LLMs and\nrandomly freeze most middle layers during optimization. Experimental results\nshow that with similar or less GPU memory consumption, LISA surpasses LoRA or\neven full parameter tuning in downstream fine-tuning tasks, where LISA\nconsistently outperforms LoRA by over $11\\%$-$37\\%$ in terms of MT-Bench\nscores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or\nbetter performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating\nits effectiveness across different domains.\n","authors":["Rui Pan","Xiang Liu","Shizhe Diao","Renjie Pi","Jipeng Zhang","Chi Han","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.17919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17916v1","updated":"2024-03-26T17:53:27Z","published":"2024-03-26T17:53:27Z","title":"CMP: Cooperative Motion Prediction with Multi-Agent Communication","summary":"  The confluence of the advancement of Autonomous Vehicles (AVs) and the\nmaturity of Vehicle-to-Everything (V2X) communication has enabled the\ncapability of cooperative connected and automated vehicles (CAVs). Building on\ntop of cooperative perception, this paper explores the feasibility and\neffectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR\nsignals as input to enhance tracking and prediction capabilities. Unlike\nprevious work that focuses separately on either cooperative perception or\nmotion prediction, our framework, to the best of our knowledge, is the first to\naddress the unified problem where CAVs share information in both perception and\nprediction modules. Incorporated into our design is the unique capability to\ntolerate realistic V2X bandwidth limitations and transmission delays, while\ndealing with bulky perception representations. We also propose a prediction\naggregation module, which unifies the predictions obtained by different CAVs\nand generates the final prediction. Through extensive experiments and ablation\nstudies, we demonstrate the effectiveness of our method in cooperative\nperception, tracking, and motion prediction tasks. In particular, CMP reduces\nthe average prediction error by 17.2\\% with fewer missing detections compared\nwith the no cooperation setting. Our work marks a significant step forward in\nthe cooperative capabilities of CAVs, showcasing enhanced performance in\ncomplex scenarios.\n","authors":["Zhuoyuan Wu","Yuping Wang","Hengbo Ma","Zhaowei Li","Hang Qiu","Jiachen Li"],"pdf_url":"https://arxiv.org/pdf/2403.17916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17905v1","updated":"2024-03-26T17:45:06Z","published":"2024-03-26T17:45:06Z","title":"Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2","summary":"  We propose a new approach for non-Cartesian magnetic resonance image\nreconstruction. While unrolled architectures provide robustness via\ndata-consistency layers, embedding measurement operators in Deep Neural Network\n(DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP)\napproaches, where the denoising DNNs are blind to the measurement setting, are\nnot affected by this limitation and have also proven effective, but their\nhighly iterative nature also affects scalability. To address this scalability\nchallenge, we leverage the \"Residual-to-Residual DNN series for high-Dynamic\nrange imaging (R2D2)\" approach recently introduced in astronomical imaging.\nR2D2's reconstruction is formed as a series of residual images, iteratively\nestimated as outputs of DNNs taking the previous iteration's image estimate and\nassociated data residual as inputs. The method can be interpreted as a learned\nversion of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation,\nconsidering radial k-space sampling acquisition sequences. Our preliminary\nresults suggest that R2D2 achieves: (i) suboptimal performance compared to its\nunrolled incarnation R2D2-Net, which is however non-scalable due to the\nnecessary embedding of NUFFT-based data-consistency layers; (ii) superior\nreconstruction quality to a scalable version of R2D2-Net embedding an FFT-based\napproximation for data consistency; (iii) superior reconstruction quality to\nPnP, while only requiring few iterations.\n","authors":["Chen Yiwei","Tang Chao","Aghabiglou Amir","Chu Chung San","Wiaux Yves"],"pdf_url":"https://arxiv.org/pdf/2403.17905v1.pdf","comment":"submitted to IEEE EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2211.01364v3","updated":"2024-03-26T17:45:01Z","published":"2022-11-02T17:59:09Z","title":"An optimal control perspective on diffusion-based generative modeling","summary":"  We establish a connection between stochastic optimal control and generative\nmodels based on stochastic differential equations (SDEs), such as recently\ndeveloped diffusion probabilistic models. In particular, we derive a\nHamilton-Jacobi-Bellman equation that governs the evolution of the\nlog-densities of the underlying SDE marginals. This perspective allows to\ntransfer methods from optimal control theory to generative modeling. First, we\nshow that the evidence lower bound is a direct consequence of the well-known\nverification theorem from control theory. Further, we can formulate\ndiffusion-based generative modeling as a minimization of the Kullback-Leibler\ndivergence between suitable measures in path space. Finally, we develop a novel\ndiffusion-based method for sampling from unnormalized densities -- a problem\nfrequently occurring in statistics and computational sciences. We demonstrate\nthat our time-reversed diffusion sampler (DIS) can outperform other\ndiffusion-based sampling approaches on multiple numerical examples.\n","authors":["Julius Berner","Lorenz Richter","Karen Ullrich"],"pdf_url":"https://arxiv.org/pdf/2211.01364v3.pdf","comment":"Accepted for oral presentation at NeurIPS 2022 Workshop on\n  Score-Based Methods"},{"id":"http://arxiv.org/abs/2401.15059v2","updated":"2024-03-26T17:44:45Z","published":"2024-01-26T18:42:01Z","title":"Fully Independent Communication in Multi-Agent Reinforcement Learning","summary":"  Multi-Agent Reinforcement Learning (MARL) comprises a broad area of research\nwithin the field of multi-agent systems. Several recent works have focused\nspecifically on the study of communication approaches in MARL. While multiple\ncommunication methods have been proposed, these might still be too complex and\nnot easily transferable to more practical contexts. One of the reasons for that\nis due to the use of the famous parameter sharing trick. In this paper, we\ninvestigate how independent learners in MARL that do not share parameters can\ncommunicate. We demonstrate that this setting might incur into some problems,\nto which we propose a new learning scheme as a solution. Our results show that,\ndespite the challenges, independent agents can still learn communication\nstrategies following our method. Additionally, we use this method to\ninvestigate how communication in MARL is affected by different network\ncapacities, both for sharing and not sharing parameters. We observe that\ncommunication may not always be needed and that the chosen agent network sizes\nneed to be considered when used together with communication in order to achieve\nefficient learning.\n","authors":["Rafael Pina","Varuna De Silva","Corentin Artaud","Xiaolan Liu"],"pdf_url":"https://arxiv.org/pdf/2401.15059v2.pdf","comment":"Extended version of the paper appearing on AAMAS 2024 with the same\n  title. 11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.17902v1","updated":"2024-03-26T17:43:15Z","published":"2024-03-26T17:43:15Z","title":"Serpent: Scalable and Efficient Image Restoration via Multi-scale\n  Structured State Space Models","summary":"  The landscape of computational building blocks of efficient image restoration\narchitectures is dominated by a combination of convolutional processing and\nvarious attention mechanisms. However, convolutional filters are inherently\nlocal and therefore struggle at modeling long-range dependencies in images. On\nthe other hand, attention excels at capturing global interactions between\narbitrary image regions, however at a quadratic cost in image dimension. In\nthis work, we propose Serpent, an architecture that leverages recent advances\nin state space models (SSMs) in its core computational block. SSMs, originally\nintroduced for sequence modeling, can maintain a global receptive field with a\nfavorable linear scaling in input size. Our preliminary results demonstrate\nthat Serpent can achieve reconstruction quality on par with state-of-the-art\ntechniques, while requiring orders of magnitude less compute (up to $150$ fold\nreduction in FLOPS) and a factor of up to $5\\times$ less GPU memory while\nmaintaining a compact model size.\n","authors":["Mohammad Shahab Sepehri","Zalan Fabian","Mahdi Soltanolkotabi"],"pdf_url":"https://arxiv.org/pdf/2403.17902v1.pdf","comment":"7 pages, 5 figures, preliminary workshop submission of a\n  comprehensive work to be released soon"},{"id":"http://arxiv.org/abs/2310.18841v2","updated":"2024-03-26T17:39:30Z","published":"2023-10-28T22:57:56Z","title":"A randomized algorithm for nonconvex minimization with inexact\n  evaluations and complexity guarantees","summary":"  We consider minimization of a smooth nonconvex function with inexact oracle\naccess to gradient and Hessian (without assuming access to the function value)\nto achieve approximate second-order optimality. A novel feature of our method\nis that if an approximate direction of negative curvature is chosen as the\nstep, we choose its sense to be positive or negative with equal probability. We\nallow gradients to be inexact in a relative sense and relax the coupling\nbetween inexactness thresholds for the first- and second-order optimality\nconditions. Our convergence analysis includes both an expectation bound based\non martingale analysis and a high-probability bound based on concentration\ninequalities. We apply our algorithm to empirical risk minimization problems\nand obtain improved gradient sample complexity over existing works.\n","authors":["Shuyao Li","Stephen J. Wright"],"pdf_url":"https://arxiv.org/pdf/2310.18841v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09428v2","updated":"2024-03-26T17:38:38Z","published":"2024-03-14T14:19:48Z","title":"Borrowing Treasures from Neighbors: In-Context Learning for Multimodal\n  Learning with Missing Modalities and Data Scarcity","summary":"  Multimodal machine learning with missing modalities is an increasingly\nrelevant challenge arising in various applications such as healthcare. This\npaper extends the current research into missing modalities to the low-data\nregime, i.e., a downstream task has both missing modalities and limited sample\nsize issues. This problem setting is particularly challenging and also\npractical as it is often expensive to get full-modality data and sufficient\nannotated training samples. We propose to use retrieval-augmented in-context\nlearning to address these two crucial issues by unleashing the potential of a\ntransformer's in-context learning ability. Diverging from existing methods,\nwhich primarily belong to the parametric paradigm and often require sufficient\ntraining samples, our work exploits the value of the available full-modality\ndata, offering a novel perspective on resolving the challenge. The proposed\ndata-dependent framework exhibits a higher degree of sample efficiency and is\nempirically demonstrated to enhance the classification model's performance on\nboth full- and missing-modality data in the low-data regime across various\nmultimodal learning tasks. When only 1% of the training data are available, our\nproposed method demonstrates an average improvement of 6.1% over a recent\nstrong baseline across various datasets and missing states. Notably, our method\nalso reduces the performance gap between full-modality and missing-modality\ndata compared with the baseline.\n","authors":["Zhuo Zhi","Ziquan Liu","Moe Elbadawi","Adam Daneshmend","Mine Orlu","Abdul Basit","Andreas Demosthenous","Miguel Rodrigues"],"pdf_url":"https://arxiv.org/pdf/2403.09428v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02156v4","updated":"2024-03-26T17:36:54Z","published":"2023-10-03T15:43:59Z","title":"Probabilistically Rewired Message-Passing Neural Networks","summary":"  Message-passing graph neural networks (MPNNs) emerged as powerful tools for\nprocessing graph-structured input. However, they operate on a fixed input graph\nstructure, ignoring potential noise and missing information. Furthermore, their\nlocal aggregation mechanism can lead to problems such as over-squashing and\nlimited expressive power in capturing relevant graph structures. Existing\nsolutions to these challenges have primarily relied on heuristic methods, often\ndisregarding the underlying data distribution. Hence, devising principled\napproaches for learning to infer graph structures relevant to the given\nprediction task remains an open challenge. In this work, leveraging recent\nprogress in exact and differentiable $k$-subset sampling, we devise\nprobabilistically rewired MPNNs (PR-MPNNs), which learn to add relevant edges\nwhile omitting less beneficial ones. For the first time, our theoretical\nanalysis explores how PR-MPNNs enhance expressive power, and we identify\nprecise conditions under which they outperform purely randomized approaches.\nEmpirically, we demonstrate that our approach effectively mitigates issues like\nover-squashing and under-reaching. In addition, on established real-world\ndatasets, our method exhibits competitive or superior predictive performance\ncompared to traditional MPNN models and recent graph transformer architectures.\n","authors":["Chendi Qian","Andrei Manolache","Kareem Ahmed","Zhe Zeng","Guy Van den Broeck","Mathias Niepert","Christopher Morris"],"pdf_url":"https://arxiv.org/pdf/2310.02156v4.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2401.07809v2","updated":"2024-03-26T17:29:07Z","published":"2024-01-15T16:30:12Z","title":"Optimal Data Splitting in Distributed Optimization for Machine Learning","summary":"  The distributed optimization problem has become increasingly relevant\nrecently. It has a lot of advantages such as processing a large amount of data\nin less time compared to non-distributed methods. However, most distributed\napproaches suffer from a significant bottleneck - the cost of communications.\nTherefore, a large amount of research has recently been directed at solving\nthis problem. One such approach uses local data similarity. In particular,\nthere exists an algorithm provably optimally exploiting the similarity\nproperty. But this result, as well as results from other works solve the\ncommunication bottleneck by focusing only on the fact that communication is\nsignificantly more expensive than local computing and does not take into\naccount the various capacities of network devices and the different\nrelationship between communication time and local computing expenses. We\nconsider this setup and the objective of this study is to achieve an optimal\nratio of distributed data between the server and local machines for any costs\nof communications and local computations. The running times of the network are\ncompared between uniform and optimal distributions. The superior theoretical\nperformance of our solutions is experimentally validated.\n","authors":["Daniil Medyakov","Gleb Molodtsov","Aleksandr Beznosikov","Alexander Gasnikov"],"pdf_url":"https://arxiv.org/pdf/2401.07809v2.pdf","comment":"17 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.17891v1","updated":"2024-03-26T17:22:29Z","published":"2024-03-26T17:22:29Z","title":"Image-based Novel Fault Detection with Deep Learning Classifiers using\n  Hierarchical Labels","summary":"  One important characteristic of modern fault classification systems is the\nability to flag the system when faced with previously unseen fault types. This\nwork considers the unknown fault detection capabilities of deep neural\nnetwork-based fault classifiers. Specifically, we propose a methodology on how,\nwhen available, labels regarding the fault taxonomy can be used to increase\nunknown fault detection performance without sacrificing model performance. To\nachieve this, we propose to utilize soft label techniques to improve the\nstate-of-the-art deep novel fault detection techniques during the training\nprocess and novel hierarchically consistent detection statistics for online\nnovel fault detection. Finally, we demonstrated increased detection performance\non novel fault detection in inspection images from the hot steel rolling\nprocess, with results well replicated across multiple scenarios and baseline\ndetection methods.\n","authors":["Nurettin Sergin","Jiayu Huang","Tzyy-Shuh Chang","Hao Yan"],"pdf_url":"https://arxiv.org/pdf/2403.17891v1.pdf","comment":"Accepted in IISE Transaction"},{"id":"http://arxiv.org/abs/2403.17889v1","updated":"2024-03-26T17:21:54Z","published":"2024-03-26T17:21:54Z","title":"Large scale paired antibody language models","summary":"  Antibodies are proteins produced by the immune system that can identify and\nneutralise a wide variety of antigens with high specificity and affinity, and\nconstitute the most successful class of biotherapeutics. With the advent of\nnext-generation sequencing, billions of antibody sequences have been collected\nin recent years, though their application in the design of better therapeutics\nhas been constrained by the sheer volume and complexity of the data. To address\nthis challenge, we present IgBert and IgT5, the best performing\nantibody-specific language models developed to date which can consistently\nhandle both paired and unpaired variable region sequences as input. These\nmodels are trained comprehensively using the more than two billion unpaired\nsequences and two million paired sequences of light and heavy chains present in\nthe Observed Antibody Space dataset. We show that our models outperform\nexisting antibody and protein language models on a diverse range of design and\nregression tasks relevant to antibody engineering. This advancement marks a\nsignificant leap forward in leveraging machine learning, large scale data sets\nand high-performance computing for enhancing antibody design for therapeutic\ndevelopment.\n","authors":["Henry Kenlay","Frédéric A. Dreyer","Aleksandr Kovaltsuk","Dom Miketa","Douglas Pires","Charlotte M. Deane"],"pdf_url":"https://arxiv.org/pdf/2403.17889v1.pdf","comment":"14 pages, 2 figures, 6 tables, model weights available at\n  https://zenodo.org/doi/10.5281/zenodo.10876908"},{"id":"http://arxiv.org/abs/2403.17887v1","updated":"2024-03-26T17:20:04Z","published":"2024-03-26T17:20:04Z","title":"The Unreasonable Ineffectiveness of the Deeper Layers","summary":"  We empirically study a simple layer-pruning strategy for popular families of\nopen-weight pretrained LLMs, finding minimal degradation of performance on\ndifferent question-answering benchmarks until after a large fraction (up to\nhalf) of the layers are removed. To prune these models, we identify the optimal\nblock of layers to prune by considering similarity across layers; then, to\n\"heal\" the damage, we perform a small amount of finetuning. In particular, we\nuse parameter-efficient finetuning (PEFT) methods, specifically quantization\nand Low Rank Adapters (QLoRA), such that each of our experiments can be\nperformed on a single A100 GPU. From a practical perspective, these results\nsuggest that layer pruning methods can complement other PEFT strategies to\nfurther reduce computational resources of finetuning on the one hand, and can\nimprove the memory and latency of inference on the other hand. From a\nscientific perspective, the robustness of these LLMs to the deletion of layers\nimplies either that current pretraining methods are not properly leveraging the\nparameters in the deeper layers of the network or that the shallow layers play\na critical role in storing knowledge.\n","authors":["Andrey Gromov","Kushal Tirumala","Hassan Shapourian","Paolo Glorioso","Daniel A. Roberts"],"pdf_url":"https://arxiv.org/pdf/2403.17887v1.pdf","comment":"12 + 10 pages, 5 + 4 figures"},{"id":"http://arxiv.org/abs/2403.17886v1","updated":"2024-03-26T17:19:23Z","published":"2024-03-26T17:19:23Z","title":"Compressed Multi-task embeddings for Data-Efficient Downstream training\n  and inference in Earth Observation","summary":"  As repositories of large scale data in earth observation (EO) have grown, so\nhave transfer and storage costs for model training and inference, expending\nsignificant resources. We introduce Neural Embedding Compression (NEC), based\non the transfer of compressed embeddings to data consumers instead of raw data.\nWe adapt foundation models (FM) through learned neural compression to generate\nmulti-task embeddings while navigating the tradeoff between compression rate\nand embedding utility. We update only a small fraction of the FM parameters\n(10%) for a short training period (1% of the iterations of pre-training). We\nevaluate NEC on two EO tasks: scene classification and semantic segmentation.\nCompared with applying traditional compression to the raw data, NEC achieves\nsimilar accuracy with a 75% to 90% reduction in data. Even at 99.7%\ncompression, performance drops by only 5% on the scene classification task.\nOverall, NEC is a data-efficient yet performant approach for multi-task EO\nmodelling.\n","authors":["Carlos Gomes","Thomas Brunschwiler"],"pdf_url":"https://arxiv.org/pdf/2403.17886v1.pdf","comment":"Published at IGARSS 2024"},{"id":"http://arxiv.org/abs/2403.04202v3","updated":"2024-03-26T17:18:33Z","published":"2024-03-07T04:12:24Z","title":"Dynamics of Moral Behavior in Heterogeneous Populations of Learning\n  Agents","summary":"  Growing concerns about safety and alignment of AI systems highlight the\nimportance of embedding moral capabilities in artificial agents. A promising\nsolution is the use of learning from experience, i.e., Reinforcement Learning.\nIn multi-agent (social) environments, complex population-level phenomena may\nemerge from interactions between individual learning agents. Many of the\nexisting studies rely on simulated social dilemma environments to study the\ninteractions of independent learning agents. However, they tend to ignore the\nmoral heterogeneity that is likely to be present in societies of agents in\npractice. For example, at different points in time a single learning agent may\nface opponents who are consequentialist (i.e., caring about maximizing some\noutcome over time) or norm-based (i.e., focusing on conforming to a specific\nnorm here and now). The extent to which agents' co-development may be impacted\nby such moral heterogeneity in populations is not well understood. In this\npaper, we present a study of the learning dynamics of morally heterogeneous\npopulations interacting in a social dilemma setting. Using a Prisoner's Dilemma\nenvironment with a partner selection mechanism, we investigate the extent to\nwhich the prevalence of diverse moral agents in populations affects individual\nagents' learning behaviors and emergent population-level outcomes. We observe\nseveral types of non-trivial interactions between pro-social and anti-social\nagents, and find that certain classes of moral agents are able to steer selfish\nagents towards more cooperative behavior.\n","authors":["Elizaveta Tennant","Stephen Hailes","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2403.04202v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17878v1","updated":"2024-03-26T17:10:15Z","published":"2024-03-26T17:10:15Z","title":"Empowering Data Mesh with Federated Learning","summary":"  The evolution of data architecture has seen the rise of data lakes, aiming to\nsolve the bottlenecks of data management and promote intelligent\ndecision-making. However, this centralized architecture is limited by the\nproliferation of data sources and the growing demand for timely analysis and\nprocessing. A new data paradigm, Data Mesh, is proposed to overcome these\nchallenges. Data Mesh treats domains as a first-class concern by distributing\nthe data ownership from the central team to each data domain, while keeping the\nfederated governance to monitor domains and their data products. Many\nmulti-million dollar organizations like Paypal, Netflix, and Zalando have\nalready transformed their data analysis pipelines based on this new\narchitecture. In this decentralized architecture where data is locally\npreserved by each domain team, traditional centralized machine learning is\nincapable of conducting effective analysis across multiple domains, especially\nfor security-sensitive organizations. To this end, we introduce a pioneering\napproach that incorporates Federated Learning into Data Mesh. To the best of\nour knowledge, this is the first open-source applied work that represents a\ncritical advancement toward the integration of federated learning methods into\nthe Data Mesh paradigm, underscoring the promising prospects for\nprivacy-preserving and decentralized data analysis strategies within Data Mesh\narchitecture.\n","authors":["Haoyuan Li","Salman Toor"],"pdf_url":"https://arxiv.org/pdf/2403.17878v1.pdf","comment":"In Proceedings of ACM Knowledge Discovery and Data Mining, Barcelona,\n  Spain, 25th - 29th August, 2024 (Conference acronym KDD), 9 pages"},{"id":"http://arxiv.org/abs/2402.04866v2","updated":"2024-03-26T16:57:46Z","published":"2024-02-01T21:16:40Z","title":"Room Transfer Function Reconstruction Using Complex-valued Neural\n  Networks and Irregularly Distributed Microphones","summary":"  Reconstructing the room transfer functions needed to calculate the complex\nsound field in a room has several impor- tant real-world applications. However,\nan unpractical number of microphones is often required. Recently, in addition\nto classical signal processing methods, deep learning techniques have been\napplied to reconstruct the room transfer function starting from a very limited\nset of measurements at scattered points in the room. In this paper, we employ\ncomplex-valued neural networks to estimate room transfer functions in the\nfrequency range of the first room resonances, using a few irregularly\ndistributed microphones. To the best of our knowledge, this is the first time\nthat complex-valued neural networks are used to estimate room transfer\nfunctions. To analyze the benefits of applying complex- valued optimization to\nthe considered task, we compare the proposed technique with a state-of-the-art\nkernel-based signal processing approach for sound field reconstruction, showing\nthat the proposed technique exhibits relevant advantages in terms of phase\naccuracy and overall quality of the reconstructed sound field. For informative\npurposes, we also compare the model with a similarly-structured data-driven\napproach that, however, applies a real-valued neural network to reconstruct\nonly the magnitude of the sound field.\n","authors":["Francesca Ronchini","Luca Comanducci","Mirco Pezzoli","Fabio Antonacci","Augusto Sarti"],"pdf_url":"https://arxiv.org/pdf/2402.04866v2.pdf","comment":"Submitted to EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2403.17868v1","updated":"2024-03-26T16:57:01Z","published":"2024-03-26T16:57:01Z","title":"Sample complexity of quantum hypothesis testing","summary":"  Quantum hypothesis testing has been traditionally studied from the\ninformation-theoretic perspective, wherein one is interested in the optimal\ndecay rate of error probabilities as a function of the number of samples of an\nunknown state. In this paper, we study the sample complexity of quantum\nhypothesis testing, wherein the goal is to determine the minimum number of\nsamples needed to reach a desired error probability. By making use of the\nwealth of knowledge that already exists in the literature on quantum hypothesis\ntesting, we characterize the sample complexity of binary quantum hypothesis\ntesting in the symmetric and asymmetric settings, and we provide bounds on the\nsample complexity of multiple quantum hypothesis testing. In more detail, we\nprove that the sample complexity of symmetric binary quantum hypothesis testing\ndepends logarithmically on the inverse error probability and inversely on the\nnegative logarithm of the fidelity. As a counterpart of the quantum Stein's\nlemma, we also find that the sample complexity of asymmetric binary quantum\nhypothesis testing depends logarithmically on the inverse type~II error\nprobability and inversely on the quantum relative entropy. Finally, we provide\nlower and upper bounds on the sample complexity of multiple quantum hypothesis\ntesting, with it remaining an intriguing open question to improve these bounds.\n","authors":["Hao-Chung Cheng","Nilanjana Datta","Nana Liu","Theshani Nuradha","Robert Salzmann","Mark M. Wilde"],"pdf_url":"https://arxiv.org/pdf/2403.17868v1.pdf","comment":"38 pages, 1 figure, preliminary version; see independent and\n  concurrent work of Pensia, Jog, Loh at arXiv:2403.16981"},{"id":"http://arxiv.org/abs/2401.07788v2","updated":"2024-03-26T16:49:44Z","published":"2024-01-15T15:54:54Z","title":"Activations and Gradients Compression for Model-Parallel Training","summary":"  Large neural networks require enormous computational clusters of machines.\nModel-parallel training, when the model architecture is partitioned\nsequentially between workers, is a popular approach for training modern models.\nInformation compression can be applied to decrease workers communication time,\nas it is often a bottleneck in such systems. This work explores how\nsimultaneous compression of activations and gradients in model-parallel\ndistributed training setup affects convergence. We analyze compression methods\nsuch as quantization and TopK compression, and also experiment with error\ncompensation techniques. Moreover, we employ TopK with AQ-SGD per-batch error\nfeedback approach. We conduct experiments on image classification and language\nmodel fine-tuning tasks. Our findings demonstrate that gradients require milder\ncompression rates than activations. We observe that $K=10\\%$ is the lowest TopK\ncompression level, which does not harm model convergence severely. Experiments\nalso show that models trained with TopK perform well only when compression is\nalso applied during inference. We find that error feedback techniques do not\nimprove model-parallel training compared to plain compression, but allow model\ninference without compression with almost no quality drop. Finally, when\napplied with the AQ-SGD approach, TopK stronger than with $ K=30\\%$ worsens\nmodel performance significantly.\n","authors":["Mikhail Rudakov","Aleksandr Beznosikov","Yaroslav Kholodov","Alexander Gasnikov"],"pdf_url":"https://arxiv.org/pdf/2401.07788v2.pdf","comment":"17 pages, 6 figures, 5 tables"},{"id":"http://arxiv.org/abs/2210.06459v2","updated":"2024-03-26T16:49:11Z","published":"2022-10-12T17:56:04Z","title":"Differentially private multivariate medians","summary":"  Statistical tools which satisfy rigorous privacy guarantees are necessary for\nmodern data analysis. It is well-known that robustness against contamination is\nlinked to differential privacy. Despite this fact, using multivariate medians\nfor differentially private and robust multivariate location estimation has not\nbeen systematically studied. We develop novel finite-sample performance\nguarantees for differentially private multivariate depth-based medians, which\nare essentially sharp. Our results cover commonly used depth functions, such as\nthe halfspace (or Tukey) depth, spatial depth, and the integrated dual depth.\nWe show that under Cauchy marginals, the cost of heavy-tailed location\nestimation outweighs the cost of privacy. We demonstrate our results\nnumerically using a Gaussian contamination model in dimensions up to d = 100,\nand compare them to a state-of-the-art private mean estimation algorithm. As a\nby-product of our investigation, we prove concentration inequalities for the\noutput of the exponential mechanism about the maximizer of the population\nobjective function. This bound applies to objective functions that satisfy a\nmild regularity condition.\n","authors":["Kelly Ramsay","Aukosh Jagannath","Shoja'eddin Chenouri"],"pdf_url":"https://arxiv.org/pdf/2210.06459v2.pdf","comment":"42 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2401.06795v2","updated":"2024-03-26T16:44:34Z","published":"2024-01-08T18:42:55Z","title":"AI and Generative AI for Research Discovery and Summarization","summary":"  AI and generative AI tools, including chatbots like ChatGPT that rely on\nlarge language models (LLMs), have burst onto the scene this year, creating\nincredible opportunities to increase work productivity and improve our lives.\nStatisticians and data scientists have begun experiencing the benefits from the\navailability of these tools in numerous ways, such as the generation of\nprogramming code from text prompts to analyze data or fit statistical models.\nOne area that these tools can make a substantial impact is in research\ndiscovery and summarization. Standalone tools and plugins to chatbots are being\ndeveloped that allow researchers to more quickly find relevant literature than\npre-2023 search tools. Furthermore, generative AI tools have improved to the\npoint where they can summarize and extract the key points from research\narticles in succinct language. Finally, chatbots based on highly parameterized\nLLMs can be used to simulate abductive reasoning, which provides researchers\nthe ability to make connections among related technical topics, which can also\nbe used for research discovery. We review the developments in AI and generative\nAI for research discovery and summarization, and propose directions where these\ntypes of tools are likely to head in the future that may be of interest to\nstatistician and data scientists.\n","authors":["Mark Glickman","Yi Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.06795v2.pdf","comment":"29 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.17853v1","updated":"2024-03-26T16:42:30Z","published":"2024-03-26T16:42:30Z","title":"Using Domain Knowledge to Guide Dialog Structure Induction via Neural\n  Probabilistic Soft Logic","summary":"  Dialog Structure Induction (DSI) is the task of inferring the latent dialog\nstructure (i.e., a set of dialog states and their temporal transitions) of a\ngiven goal-oriented dialog. It is a critical component for modern dialog system\ndesign and discourse analysis. Existing DSI approaches are often purely\ndata-driven, deploy models that infer latent states without access to domain\nknowledge, underperform when the training corpus is limited/noisy, or have\ndifficulty when test dialogs exhibit distributional shifts from the training\ndomain. This work explores a neural-symbolic approach as a potential solution\nto these problems. We introduce Neural Probabilistic Soft Logic Dialogue\nStructure Induction (NEUPSL DSI), a principled approach that injects symbolic\nknowledge into the latent space of a generative neural model. We conduct a\nthorough empirical investigation on the effect of NEUPSL DSI learning on hidden\nrepresentation quality, few-shot learning, and out-of-domain generalization\nperformance. Over three dialog structure induction datasets and across\nunsupervised and semi-supervised settings for standard and cross-domain\ngeneralization, the injection of symbolic knowledge using NEUPSL DSI provides a\nconsistent boost in performance over the canonical baselines.\n","authors":["Connor Pryor","Quan Yuan","Jeremiah Liu","Mehran Kazemi","Deepak Ramachandran","Tania Bedrax-Weiss","Lise Getoor"],"pdf_url":"https://arxiv.org/pdf/2403.17853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17852v1","updated":"2024-03-26T16:40:08Z","published":"2024-03-26T16:40:08Z","title":"Counterfactual Fairness through Transforming Data Orthogonal to Bias","summary":"  Machine learning models have shown exceptional prowess in solving complex\nissues across various domains. Nonetheless, these models can sometimes exhibit\nbiased decision-making, leading to disparities in treatment across different\ngroups. Despite the extensive research on fairness, the nuanced effects of\nmultivariate and continuous sensitive variables on decision-making outcomes\nremain insufficiently studied. We introduce a novel data pre-processing\nalgorithm, Orthogonal to Bias (OB), designed to remove the influence of a group\nof continuous sensitive variables, thereby facilitating counterfactual fairness\nin machine learning applications. Our approach is grounded in the assumption of\na jointly normal distribution within a structural causal model (SCM), proving\nthat counterfactual fairness can be achieved by ensuring the data is\nuncorrelated with sensitive variables. The OB algorithm is model-agnostic,\ncatering to a wide array of machine learning models and tasks, and includes a\nsparse variant to enhance numerical stability through regularization. Through\nempirical evaluation on simulated and real-world datasets - including the adult\nincome and the COMPAS recidivism datasets - our methodology demonstrates its\ncapacity to enable fairer outcomes without compromising accuracy.\n","authors":["Shuyi Chen","Shixiang Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.17852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17847v1","updated":"2024-03-26T16:36:50Z","published":"2024-03-26T16:36:50Z","title":"Climate Downscaling: A Deep-Learning Based Super-resolution Model of\n  Precipitation Data with Attention Block and Skip Connections","summary":"  Human activities accelerate consumption of fossil fuels and produce\ngreenhouse gases, resulting in urgent issues today: global warming and the\nclimate change. These indirectly cause severe natural disasters, plenty of\nlives suffering and huge losses of agricultural properties. To mitigate impacts\non our lands, scientists are developing renewable, reusable, and clean energies\nand climatologists are trying to predict the extremes. Meanwhile, governments\nare publicizing resource-saving policies for a more eco-friendly society and\narousing environment awareness. One of the most influencing factors is the\nprecipitation, bringing condensed water vapor onto lands. Water resources are\nthe most significant but basic needs in society, not only supporting our\nlivings, but also economics. In Taiwan, although the average annual\nprecipitation is up to 2,500 millimeter (mm), the water allocation for each\nperson is lower than the global average due to drastically geographical\nelevation changes and uneven distribution through the year. Thus, it is crucial\nto track and predict the rainfall to make the most use of it and to prevent the\nfloods. However, climate models have limited resolution and require intensive\ncomputational power for local-scale use. Therefore, we proposed a deep\nconvolutional neural network with skip connections, attention blocks, and\nauxiliary data concatenation, in order to downscale the low-resolution\nprecipitation data into high-resolution one. Eventually, we compare with other\nclimate downscaling methods and show better performance in metrics of Mean\nAbsolute Error (MAE), Root Mean Square Error (RMSE), Pearson Correlation,\nstructural similarity index (SSIM), and forecast indicators.\n","authors":["Chia-Hao Chiang","Zheng-Han Huang","Liwen Liu","Hsin-Chien Liang","Yi-Chi Wang","Wan-Ling Tseng","Chao Wang","Che-Ta Chen","Ko-Chih Wang"],"pdf_url":"https://arxiv.org/pdf/2403.17847v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17846v1","updated":"2024-03-26T16:36:43Z","published":"2024-03-26T16:36:43Z","title":"Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot\n  Navigation","summary":"  Recent open-vocabulary robot mapping methods enrich dense geometric maps with\npre-trained visual-language features. While these maps allow for the prediction\nof point-wise saliency maps when queried for a certain language concept,\nlarge-scale environments and abstract queries beyond the object level still\npose a considerable hurdle, ultimately limiting language-grounded robotic\nnavigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D\nscene graph mapping approach for language-grounded robot navigation. Leveraging\nopen-vocabulary vision foundation models, we first obtain state-of-the-art\nopen-vocabulary segment-level maps in 3D and subsequently construct a 3D scene\ngraph hierarchy consisting of floor, room, and object concepts, each enriched\nwith open-vocabulary features. Our approach is able to represent multi-story\nbuildings and allows robotic traversal of those using a cross-floor Voronoi\ngraph. HOV-SG is evaluated on three distinct datasets and surpasses previous\nbaselines in open-vocabulary semantic accuracy on the object, room, and floor\nlevel while producing a 75% reduction in representation size compared to dense\nopen-vocabulary maps. In order to prove the efficacy and generalization\ncapabilities of HOV-SG, we showcase successful long-horizon\nlanguage-conditioned robot navigation within real-world multi-storage\nenvironments. We provide code and trial video data at http://hovsg.github.io/.\n","authors":["Abdelrhman Werby","Chenguang Huang","Martin Büchner","Abhinav Valada","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2403.17846v1.pdf","comment":"Code and video are available at http://hovsg.github.io/"},{"id":"http://arxiv.org/abs/2312.17336v2","updated":"2024-03-26T16:35:15Z","published":"2023-12-28T19:28:23Z","title":"PINN surrogate of Li-ion battery models for parameter inference. Part\n  II: Regularization and application of the pseudo-2D model","summary":"  Bayesian parameter inference is useful to improve Li-ion battery diagnostics\nand can help formulate battery aging models. However, it is computationally\nintensive and cannot be easily repeated for multiple cycles, multiple operating\nconditions, or multiple replicate cells. To reduce the computational cost of\nBayesian calibration, numerical solvers for physics-based models can be\nreplaced with faster surrogates. A physics-informed neural network (PINN) is\ndeveloped as a surrogate for the pseudo-2D (P2D) battery model calibration. For\nthe P2D surrogate, additional training regularization was needed as compared to\nthe PINN single-particle model (SPM) developed in Part I. Both the PINN SPM and\nP2D surrogate models are exercised for parameter inference and compared to data\nobtained from a direct numerical solution of the governing equations. A\nparameter inference study highlights the ability to use these PINNs to\ncalibrate scaling parameters for the cathode Li diffusion and the anode\nexchange current density. By realizing computational speed-ups of 2250x for the\nP2D model, as compared to using standard integrating methods, the PINN\nsurrogates enable rapid state-of-health diagnostics. In the low-data\navailability scenario, the testing error was estimated to 2mV for the SPM\nsurrogate and 10mV for the P2D surrogate which could be mitigated with\nadditional data.\n","authors":["Malik Hassanaly","Peter J. Weddle","Ryan N. King","Subhayan De","Alireza Doostan","Corey R. Randall","Eric J. Dufek","Andrew M. Colclasure","Kandler Smith"],"pdf_url":"https://arxiv.org/pdf/2312.17336v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17845v1","updated":"2024-03-26T16:34:05Z","published":"2024-03-26T16:34:05Z","title":"TractOracle: towards an anatomically-informed reward function for\n  RL-based tractography","summary":"  Reinforcement learning (RL)-based tractography is a competitive alternative\nto machine learning and classical tractography algorithms due to its high\nanatomical accuracy obtained without the need for any annotated data. However,\nthe reward functions so far used to train RL agents do not encapsulate\nanatomical knowledge which causes agents to generate spurious false positives\ntracts. In this paper, we propose a new RL tractography system, TractOracle,\nwhich relies on a reward network trained for streamline classification. This\nnetwork is used both as a reward function during training as well as a mean for\nstopping the tracking process early and thus reduce the number of false\npositive streamlines. This makes our system a unique method that evaluates and\nreconstructs WM streamlines at the same time. We report an improvement of true\npositive ratios by almost 20\\% and a reduction of 3x of false positive ratios\non one dataset and an increase between 2x and 7x in the number true positive\nstreamlines on another dataset.\n","authors":["Antoine Théberge","Maxime Descoteaux","Pierre-Marc Jodoin"],"pdf_url":"https://arxiv.org/pdf/2403.17845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17844v1","updated":"2024-03-26T16:33:12Z","published":"2024-03-26T16:33:12Z","title":"Mechanistic Design and Scaling of Hybrid Architectures","summary":"  The development of deep learning architectures is a resource-demanding\nprocess, due to a vast design space, long prototyping times, and high compute\ncosts associated with at-scale model training and evaluation. We set out to\nsimplify this process by grounding it in an end-to-end mechanistic architecture\ndesign (MAD) pipeline, encompassing small-scale capability unit tests\npredictive of scaling laws. Through a suite of synthetic token manipulation\ntasks such as compression and recall, designed to probe capabilities, we\nidentify and test new hybrid architectures constructed from a variety of\ncomputational primitives. We experimentally validate the resulting\narchitectures via an extensive compute-optimal and a new state-optimal scaling\nlaw analysis, training over 500 language models between 70M to 7B parameters.\nSurprisingly, we find MAD synthetics to correlate with compute-optimal\nperplexity, enabling accurate evaluation of new architectures via isolated\nproxy tasks. The new architectures found via MAD, based on simple ideas such as\nhybridization and sparsity, outperform state-of-the-art Transformer,\nconvolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in\nscaling, both at compute-optimal budgets and in overtrained regimes. Overall,\nthese results provide evidence that performance on curated synthetic tasks can\nbe predictive of scaling laws, and that an optimal architecture should leverage\nspecialized layers via a hybrid topology.\n","authors":["Michael Poli","Armin W Thomas","Eric Nguyen","Pragaash Ponnusamy","Björn Deiseroth","Kristian Kersting","Taiji Suzuki","Brian Hie","Stefano Ermon","Christopher Ré","Ce Zhang","Stefano Massaroli"],"pdf_url":"https://arxiv.org/pdf/2403.17844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17837v1","updated":"2024-03-26T16:24:42Z","published":"2024-03-26T16:24:42Z","title":"GTA-HDR: A Large-Scale Synthetic Dataset for HDR Image Reconstruction","summary":"  High Dynamic Range (HDR) content (i.e., images and videos) has a broad range\nof applications. However, capturing HDR content from real-world scenes is\nexpensive and time- consuming. Therefore, the challenging task of\nreconstructing visually accurate HDR images from their Low Dynamic Range (LDR)\ncounterparts is gaining attention in the vision research community. A major\nchallenge in this research problem is the lack of datasets, which capture\ndiverse scene conditions (e.g., lighting, shadows, weather, locations,\nlandscapes, objects, humans, buildings) and various image features (e.g.,\ncolor, contrast, saturation, hue, luminance, brightness, radiance). To address\nthis gap, in this paper, we introduce GTA-HDR, a large-scale synthetic dataset\nof photo-realistic HDR images sampled from the GTA-V video game. We perform\nthorough evaluation of the proposed dataset, which demonstrates significant\nqualitative and quantitative improvements of the state-of-the-art HDR image\nreconstruction methods. Furthermore, we demonstrate the effectiveness of the\nproposed dataset and its impact on additional computer vision tasks including\n3D human pose estimation, human body part segmentation, and holistic scene\nsegmentation. The dataset, data collection pipeline, and evaluation code are\navailable at: https://github.com/HrishavBakulBarua/GTA-HDR.\n","authors":["Hrishav Bakul Barua","Kalin Stefanov","KokSheik Wong","Abhinav Dhall","Ganesh Krishnasamy"],"pdf_url":"https://arxiv.org/pdf/2403.17837v1.pdf","comment":"Submitted to IEEE"},{"id":"http://arxiv.org/abs/2305.03123v2","updated":"2024-03-26T16:22:54Z","published":"2023-04-13T16:01:28Z","title":"ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and\n  Ethics) Evaluation: A Review","summary":"  ChatGPT is another large language model (LLM) vastly available for the\nconsumers on their devices but due to its performance and ability to converse\neffectively, it has gained a huge popularity amongst research as well as\nindustrial community. Recently, many studies have been published to show the\neffectiveness, efficiency, integration, and sentiments of chatGPT and other\nLLMs. In contrast, this study focuses on the important aspects that are mostly\noverlooked, i.e. sustainability, privacy, digital divide, and ethics and\nsuggests that not only chatGPT but every subsequent entry in the category of\nconversational bots should undergo Sustainability, PrivAcy, Digital divide, and\nEthics (SPADE) evaluation. This paper discusses in detail the issues and\nconcerns raised over chatGPT in line with aforementioned characteristics. We\nalso discuss the recent EU AI Act briefly in accordance with the SPADE\nevaluation. We support our hypothesis by some preliminary data collection and\nvisualizations along with hypothesized facts. We also suggest mitigations and\nrecommendations for each of the concerns. Furthermore, we also suggest some\npolicies and recommendations for AI policy act, if designed by the governments.\n","authors":["Sunder Ali Khowaja","Parus Khuwaja","Kapal Dev","Weizheng Wang","Lewis Nkenyereye"],"pdf_url":"https://arxiv.org/pdf/2305.03123v2.pdf","comment":"29 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2312.17329v2","updated":"2024-03-26T16:22:36Z","published":"2023-12-28T19:09:56Z","title":"PINN surrogate of Li-ion battery models for parameter inference. Part I:\n  Implementation and multi-fidelity hierarchies for the single-particle model","summary":"  To plan and optimize energy storage demands that account for Li-ion battery\naging dynamics, techniques need to be developed to diagnose battery internal\nstates accurately and rapidly. This study seeks to reduce the computational\nresources needed to determine a battery's internal states by replacing\nphysics-based Li-ion battery models -- such as the single-particle model (SPM)\nand the pseudo-2D (P2D) model -- with a physics-informed neural network (PINN)\nsurrogate. The surrogate model makes high-throughput techniques, such as\nBayesian calibration, tractable to determine battery internal parameters from\nvoltage responses. This manuscript is the first of a two-part series that\nintroduces PINN surrogates of Li-ion battery models for parameter inference\n(i.e., state-of-health diagnostics). In this first part, a method is presented\nfor constructing a PINN surrogate of the SPM. A multi-fidelity hierarchical\ntraining, where several neural nets are trained with multiple physics-loss\nfidelities is shown to significantly improve the surrogate accuracy when only\ntraining on the governing equation residuals. The implementation is made\navailable in a companion repository (https://github.com/NREL/pinnstripes). The\ntechniques used to develop a PINN surrogate of the SPM are extended in Part II\nfor the PINN surrogate for the P2D battery model, and explore the Bayesian\ncalibration capabilities of both surrogates.\n","authors":["Malik Hassanaly","Peter J. Weddle","Ryan N. King","Subhayan De","Alireza Doostan","Corey R. Randall","Eric J. Dufek","Andrew M. Colclasure","Kandler Smith"],"pdf_url":"https://arxiv.org/pdf/2312.17329v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14427v2","updated":"2024-03-26T16:15:40Z","published":"2023-11-24T12:00:50Z","title":"Disentangling the Spectral Properties of the Hodge Laplacian: Not All\n  Small Eigenvalues Are Equal","summary":"  The rich spectral information of the graph Laplacian has been instrumental in\ngraph theory, machine learning, and graph signal processing for applications\nsuch as graph classification, clustering, or eigenmode analysis. Recently, the\nHodge Laplacian has come into focus as a generalisation of the ordinary\nLaplacian for higher-order graph models such as simplicial and cellular\ncomplexes. Akin to the traditional analysis of graph Laplacians, many authors\nanalyse the smallest eigenvalues of the Hodge Laplacian, which are connected to\nimportant topological properties such as homology. However, small eigenvalues\nof the Hodge Laplacian can carry different information depending on whether\nthey are related to curl or gradient eigenmodes, and thus may not be\ncomparable. We therefore introduce the notion of persistent eigenvector\nsimilarity and provide a method to track individual harmonic, curl, and\ngradient eigenvectors/-values through the so-called persistence filtration,\nleveraging the full information contained in the Hodge-Laplacian spectrum\nacross all possible scales of a point cloud. Finally, we use our insights (a)\nto introduce a novel form of Hodge spectral clustering and (b) to classify\nedges and higher-order simplices based on their relationship to the smallest\nharmonic, curl, and gradient eigenvectors.\n","authors":["Vincent P. Grande","Michael T. Schaub"],"pdf_url":"https://arxiv.org/pdf/2311.14427v2.pdf","comment":"5 pages, 4 figures, comments welcome"},{"id":"http://arxiv.org/abs/2403.17833v1","updated":"2024-03-26T16:14:43Z","published":"2024-03-26T16:14:43Z","title":"GPFL: A Gradient Projection-Based Client Selection Framework for\n  Efficient Federated Learning","summary":"  Federated learning client selection is crucial for determining participant\nclients while balancing model accuracy and communication efficiency. Existing\nmethods have limitations in handling data heterogeneity, computational burdens,\nand independent client treatment. To address these challenges, we propose GPFL,\nwhich measures client value by comparing local and global descent directions.\nWe also employ an Exploit-Explore mechanism to enhance performance.\nExperimental results on FEMINST and CIFAR-10 datasets demonstrate that GPFL\noutperforms baselines in Non-IID scenarios, achieving over 9\\% improvement in\nFEMINST test accuracy. Moreover, GPFL exhibits shorter computation times\nthrough pre-selection and parameter reuse in federated learning.\n","authors":["Shijie Na","Yuzhi Liang","Siu-Ming Yiu"],"pdf_url":"https://arxiv.org/pdf/2403.17833v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.17831v1","updated":"2024-03-26T16:13:55Z","published":"2024-03-26T16:13:55Z","title":"Learning the Optimal Power Flow: Environment Design Matters","summary":"  To solve the optimal power flow (OPF) problem, reinforcement learning (RL)\nemerges as a promising new approach. However, the RL-OPF literature is strongly\ndivided regarding the exact formulation of the OPF problem as an RL\nenvironment. In this work, we collect and implement diverse environment design\ndecisions from the literature regarding training data, observation space,\nepisode definition, and reward function choice. In an experimental analysis, we\nshow the significant impact of these environment design options on RL-OPF\ntraining performance. Further, we derive some first recommendations regarding\nthe choice of these design decisions. The created environment framework is\nfully open-source and can serve as a benchmark for future research in the\nRL-OPF field.\n","authors":["Thomas Wolgast","Astrid Nieße"],"pdf_url":"https://arxiv.org/pdf/2403.17831v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.13969v3","updated":"2024-03-26T16:13:26Z","published":"2021-08-31T16:51:00Z","title":"Semi-Supervised Crowd Counting from Unlabeled Data","summary":"  Automatic Crowd behavior analysis can be applied to effectively help the\ndaily transportation statistics and planning, which helps the smart city\nconstruction. As one of the most important keys, crowd counting has drawn\nincreasing attention. Recent works achieved promising performance but relied on\nthe supervised paradigm with expensive crowd annotations. To alleviate the\nannotation cost in real-world transportation scenarios, in this work we\nproposed a semi-supervised learning framework $S^{4}\\textit{Crowd}$, which can\nleverage both unlabeled/labeled data for robust crowd counting. In the\nunsupervised pathway, two \\textit{self-supervised losses} were proposed to\nsimulate the crowd variations such as scale, illumination, based on which\nsupervised information pseudo labels were generated and gradually refined. We\nalso proposed a crowd-driven recurrent unit \\textit{Gated-Crowd-Recurrent-Unit\n(GCRU)}, which can preserve discriminant crowd information by extracting\nsecond-order statistics, yielding pseudo labels with improved quality. A joint\nloss including both unsupervised/supervised information was proposed, and a\ndynamic weighting strategy was employed to balance the importance of the\nunsupervised loss and supervised loss at different training stages. We\nconducted extensive experiments on four popular crowd counting datasets in\nsemi-supervised settings. Experimental results supported the effectiveness of\neach proposed component in our $S^{4}$Crowd framework. Our method achieved\ncompetitive performance in semi-supervised learning approaches on these crowd\ncounting datasets.\n","authors":["Haoran Duan","Fan Wan","Rui Sun","Zeyu Wang","Varun Ojha","Yu Guan","Hubert P. H. Shum","Bingzhang Hu","Yang Long"],"pdf_url":"https://arxiv.org/pdf/2108.13969v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17827v1","updated":"2024-03-26T16:06:42Z","published":"2024-03-26T16:06:42Z","title":"DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from\n  Textual Descriptions","summary":"  Generating natural hand-object interactions in 3D is challenging as the\nresulting hand and object motions are expected to be physically plausible and\nsemantically meaningful. Furthermore, generalization to unseen objects is\nhindered by the limited scale of available hand-object interaction datasets. We\npropose DiffH2O, a novel method to synthesize realistic, one or two-handed\nobject interactions from provided text prompts and geometry of the object. The\nmethod introduces three techniques that enable effective learning from limited\ndata. First, we decompose the task into a grasping stage and a text-based\ninteraction stage and use separate diffusion models for each. In the grasping\nstage, the model only generates hand motions, whereas in the interaction phase\nboth hand and object poses are synthesized. Second, we propose a compact\nrepresentation that tightly couples hand and object poses. Third, we propose\ntwo different guidance schemes to allow more control of the generated motions:\ngrasp guidance and detailed textual guidance. Grasp guidance takes a single\ntarget grasping pose and guides the diffusion model to reach this grasp at the\nend of the grasping stage, which provides control over the grasping pose. Given\na grasping motion from this stage, multiple different actions can be prompted\nin the interaction phase. For textual guidance, we contribute comprehensive\ntext descriptions to the GRAB dataset and show that they enable our method to\nhave more fine-grained control over hand-object interactions. Our quantitative\nand qualitative evaluation demonstrates that the proposed method outperforms\nbaseline methods and leads to natural hand-object motions. Moreover, we\ndemonstrate the practicality of our framework by utilizing a hand pose estimate\nfrom an off-the-shelf pose estimator for guidance, and then sampling multiple\ndifferent actions in the interaction stage.\n","authors":["Sammy Christen","Shreyas Hampali","Fadime Sener","Edoardo Remelli","Tomas Hodan","Eric Sauser","Shugao Ma","Bugra Tekin"],"pdf_url":"https://arxiv.org/pdf/2403.17827v1.pdf","comment":"Project Page: https://diffh2o.github.io/"},{"id":"http://arxiv.org/abs/2311.15964v2","updated":"2024-03-26T15:58:26Z","published":"2023-11-27T16:07:37Z","title":"Efficient Pre-training for Localized Instruction Generation of Videos","summary":"  Procedural videos show step-by-step demonstrations of tasks like recipe\npreparation. Understanding such videos is challenging, involving the precise\nlocalization of steps and the generation of textual instructions. Manually\nannotating steps and writing instructions is costly, which limits the size of\ncurrent datasets and hinders effective learning. Leveraging large but noisy\nvideo-transcript datasets for pre-training can boost performance, but demands\nsignificant computational resources. Furthermore, transcripts contain\nirrelevant content and exhibit style variation compared to instructions written\nby human annotators. To mitigate both issues, we propose a technique,\nSieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters\nirrelevant transcripts and (ii) Swap enhances the quality of the text\ninstruction by automatically replacing the transcripts with human-written\ninstructions from a text-only recipe dataset. The curated dataset, three orders\nof magnitude smaller than current web-scale datasets, enables efficient\ntraining of large-scale models with competitive performance. We complement our\nSieve-\\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step\nlocalization and instruction generation for procedural videos. When this model\nis pre-trained on our curated dataset, it achieves state-of-the-art performance\nin zero-shot and finetuning settings on YouCook2 and Tasty, while using a\nfraction of the computational resources.\n","authors":["Anil Batra","Davide Moltisanti","Laura Sevilla-Lara","Marcus Rohrbach","Frank Keller"],"pdf_url":"https://arxiv.org/pdf/2311.15964v2.pdf","comment":"This version has some missing experiments and elaborative technical\n  details"},{"id":"http://arxiv.org/abs/2403.17811v1","updated":"2024-03-26T15:50:37Z","published":"2024-03-26T15:50:37Z","title":"Are Compressed Language Models Less Subgroup Robust?","summary":"  To reduce the inference cost of large language models, model compression is\nincreasingly used to create smaller scalable models. However, little is known\nabout their robustness to minority subgroups defined by the labels and\nattributes of a dataset. In this paper, we investigate the effects of 18\ndifferent compression methods and settings on the subgroup robustness of BERT\nlanguage models. We show that worst-group performance does not depend on model\nsize alone, but also on the compression method used. Additionally, we find that\nmodel compression does not always worsen the performance on minority subgroups.\nAltogether, our analysis serves to further research into the subgroup\nrobustness of model compression.\n","authors":["Leonidas Gee","Andrea Zugarini","Novi Quadrianto"],"pdf_url":"https://arxiv.org/pdf/2403.17811v1.pdf","comment":"The 2023 Conference on Empirical Methods in Natural Language\n  Processing (EMNLP 2023)"},{"id":"http://arxiv.org/abs/2403.17808v1","updated":"2024-03-26T15:45:29Z","published":"2024-03-26T15:45:29Z","title":"Annotated Biomedical Video Generation using Denoising Diffusion\n  Probabilistic Models and Flow Fields","summary":"  The segmentation and tracking of living cells play a vital role within the\nbiomedical domain, particularly in cancer research, drug development, and\ndevelopmental biology. These are usually tedious and time-consuming tasks that\nare traditionally done by biomedical experts. Recently, to automatize these\nprocesses, deep learning based segmentation and tracking methods have been\nproposed. These methods require large-scale datasets and their full potential\nis constrained by the scarcity of annotated data in the biomedical imaging\ndomain. To address this limitation, we propose Biomedical Video Diffusion Model\n(BVDM), capable of generating realistic-looking synthetic microscopy videos.\nTrained only on a single real video, BVDM can generate videos of arbitrary\nlength with pixel-level annotations that can be used for training data-hungry\nmodels. It is composed of a denoising diffusion probabilistic model (DDPM)\ngenerating high-fidelity synthetic cell microscopy images and a flow prediction\nmodel (FPM) predicting the non-rigid transformation between consecutive video\nframes. During inference, initially, the DDPM imposes realistic cell textures\non synthetic cell masks which are generated based on real data statistics. The\nflow prediction model predicts the flow field between consecutive masks and\napplies that to the DDPM output from the previous time frame to create the next\none while keeping temporal consistency. BVDM outperforms state-of-the-art\nsynthetic live cell microscopy video generation models. Furthermore, we\ndemonstrate that a sufficiently large synthetic dataset enhances the\nperformance of cell segmentation and tracking models compared to using a\nlimited amount of available real data.\n","authors":["Rüveyda Yilmaz","Dennis Eschweiler","Johannes Stegmaier"],"pdf_url":"https://arxiv.org/pdf/2403.17808v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17806v1","updated":"2024-03-26T15:44:58Z","published":"2024-03-26T15:44:58Z","title":"Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding\n  Model Mechanisms","summary":"  Many recent language model (LM) interpretability studies have adopted the\ncircuits framework, which aims to find the minimal computational subgraph, or\ncircuit, that explains LM behavior on a given task. Most studies determine\nwhich edges belong in a LM's circuit by performing causal interventions on each\nedge independently, but this scales poorly with model size. Edge attribution\npatching (EAP), gradient-based approximation to interventions, has emerged as a\nscalable but imperfect solution to this problem. In this paper, we introduce a\nnew method - EAP with integrated gradients (EAP-IG) - that aims to better\nmaintain a core property of circuits: faithfulness. A circuit is faithful if\nall model edges outside the circuit can be ablated without changing the model's\nperformance on the task; faithfulness is what justifies studying circuits,\nrather than the full model. Our experiments demonstrate that circuits found\nusing EAP are less faithful than those found using EAP-IG, even though both\nhave high node overlap with circuits found previously using causal\ninterventions. We conclude more generally that when using circuits to compare\nthe mechanisms models use to solve tasks, faithfulness, not overlap, is what\nshould be measured.\n","authors":["Michael Hanna","Sandro Pezzelle","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2403.17806v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17805v1","updated":"2024-03-26T15:42:04Z","published":"2024-03-26T15:42:04Z","title":"Scenario-Based Curriculum Generation for Multi-Agent Autonomous Driving","summary":"  The automated generation of diverse and complex training scenarios has been\nan important ingredient in many complex learning tasks. Especially in\nreal-world application domains, such as autonomous driving, auto-curriculum\ngeneration is considered vital for obtaining robust and general policies.\nHowever, crafting traffic scenarios with multiple, heterogeneous agents is\ntypically considered as a tedious and time-consuming task, especially in more\ncomplex simulation environments. In our work, we introduce MATS-Gym, a\nMulti-Agent Traffic Scenario framework to train agents in CARLA, a\nhigh-fidelity driving simulator. MATS-Gym is a multi-agent training framework\nfor autonomous driving that uses partial scenario specifications to generate\ntraffic scenarios with variable numbers of agents. This paper unifies various\nexisting approaches to traffic scenario description into a single training\nframework and demonstrates how it can be integrated with techniques from\nunsupervised environment design to automate the generation of adaptive\nauto-curricula. The code is available at\nhttps://github.com/AutonomousDrivingExaminer/mats-gym.\n","authors":["Axel Brunnbauer","Luigi Berducci","Peter Priller","Dejan Nickovic","Radu Grosu"],"pdf_url":"https://arxiv.org/pdf/2403.17805v1.pdf","comment":"7 Pages, Under Review"},{"id":"http://arxiv.org/abs/2302.03788v4","updated":"2024-03-26T15:41:28Z","published":"2023-02-07T22:56:58Z","title":"Toward a Theory of Causation for Interpreting Neural Code Models","summary":"  Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly\nprogressing from research prototypes to commercial developer tools. As such,\nunderstanding the capabilities and limitations of such models is becoming\ncritical. However, the abilities of these models are typically measured using\nautomated metrics that often only reveal a portion of their real-world\nperformance. While, in general, the performance of NCMs appears promising,\ncurrently much is unknown about how such models arrive at decisions. To this\nend, this paper introduces $do_{code}$, a post hoc interpretability method\nspecific to NCMs that is capable of explaining model predictions. $do_{code}$\nis based upon causal inference to enable programming language-oriented\nexplanations. While the theoretical underpinnings of $do_{code}$ are extensible\nto exploring different model properties, we provide a concrete instantiation\nthat aims to mitigate the impact of spurious correlations by grounding\nexplanations of model behavior in properties of programming languages. To\ndemonstrate the practical benefit of $do_{code}$, we illustrate the insights\nthat our framework can provide by performing a case study on two popular deep\nlearning architectures and ten NCMs. The results of this case study illustrate\nthat our studied NCMs are sensitive to changes in code syntax. All our NCMs,\nexcept for the BERT-like model, statistically learn to predict tokens related\nto blocks of code (\\eg brackets, parenthesis, semicolon) with less confounding\nbias as compared to other programming language constructs. These insights\ndemonstrate the potential of $do_{code}$ as a useful method to detect and\nfacilitate the elimination of confounding bias in NCMs.\n","authors":["David N. Palacio","Alejandro Velasco","Nathan Cooper","Alvaro Rodriguez","Kevin Moran","Denys Poshyvanyk"],"pdf_url":"https://arxiv.org/pdf/2302.03788v4.pdf","comment":"Accepted to appear in IEEE Transactions on Software Engineering"},{"id":"http://arxiv.org/abs/2306.15909v4","updated":"2024-03-26T15:13:20Z","published":"2023-06-28T04:16:16Z","title":"RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$","summary":"  Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as\npromising approaches for learning data-efficient RL algorithms tailored to a\ngiven task distribution. However, they show poor asymptotic performance and\nstruggle with out-of-distribution tasks because they rely on sequence models,\nsuch as recurrent neural networks or transformers, to process experiences\nrather than summarize them using general-purpose RL components such as value\nfunctions. In contrast, traditional RL algorithms are data-inefficient as they\ndo not use domain knowledge, but they do converge to an optimal policy in the\nlimit. We propose RL$^3$, a principled hybrid approach that incorporates\naction-values, learned per task through traditional RL, in the inputs to\nmeta-RL. We show that RL$^3$ earns greater cumulative reward in the long term,\ncompared to RL$^2$, while maintaining data-efficiency in the short term, and\ngeneralizes better to out-of-distribution tasks. Experiments are conducted on\nboth custom and benchmark discrete domains from the meta-RL literature that\nexhibit a range of short-term, long-term, and complex dependencies.\n","authors":["Abhinav Bhatia","Samer B. Nashed","Shlomo Zilberstein"],"pdf_url":"https://arxiv.org/pdf/2306.15909v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12243v4","updated":"2024-03-26T15:12:19Z","published":"2023-08-23T16:42:27Z","title":"Multi-Objective Optimization for Sparse Deep Multi-Task Learning","summary":"  Different conflicting optimization criteria arise naturally in various Deep\nLearning scenarios. These can address different main tasks (i.e., in the\nsetting of Multi-Task Learning), but also main and secondary tasks such as loss\nminimization versus sparsity. The usual approach is a simple weighting of the\ncriteria, which formally only works in the convex setting. In this paper, we\npresent a Multi-Objective Optimization algorithm using a modified Weighted\nChebyshev scalarization for training Deep Neural Networks (DNNs) with respect\nto several tasks. By employing this scalarization technique, the algorithm can\nidentify all optimal solutions of the original problem while reducing its\ncomplexity to a sequence of single-objective problems. The simplified problems\nare then solved using an Augmented Lagrangian method, enabling the use of\npopular optimization techniques such as Adam and Stochastic Gradient Descent,\nwhile efficaciously handling constraints. Our work aims to address the\n(economical and also ecological) sustainability issue of DNN models, with a\nparticular focus on Deep Multi-Task models, which are typically designed with a\nvery large number of weights to perform equally well on multiple tasks. Through\nexperiments conducted on two Machine Learning datasets, we demonstrate the\npossibility of adaptively sparsifying the model during training without\nsignificantly impacting its performance, if we are willing to apply\ntask-specific adaptations to the network weights. Code is available at\nhttps://github.com/salomonhotegni/MDMTN\n","authors":["S. S. Hotegni","M. Berkemeier","S. Peitz"],"pdf_url":"https://arxiv.org/pdf/2308.12243v4.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.17775v1","updated":"2024-03-26T15:07:58Z","published":"2024-03-26T15:07:58Z","title":"Secure Aggregation is Not Private Against Membership Inference Attacks","summary":"  Secure aggregation (SecAgg) is a commonly-used privacy-enhancing mechanism in\nfederated learning, affording the server access only to the aggregate of model\nupdates while safeguarding the confidentiality of individual updates. Despite\nwidespread claims regarding SecAgg's privacy-preserving capabilities, a formal\nanalysis of its privacy is lacking, making such presumptions unjustified. In\nthis paper, we delve into the privacy implications of SecAgg by treating it as\na local differential privacy (LDP) mechanism for each local update. We design a\nsimple attack wherein an adversarial server seeks to discern which update\nvector a client submitted, out of two possible ones, in a single training round\nof federated learning under SecAgg. By conducting privacy auditing, we assess\nthe success probability of this attack and quantify the LDP guarantees provided\nby SecAgg. Our numerical results unveil that, contrary to prevailing claims,\nSecAgg offers weak privacy against membership inference attacks even in a\nsingle training round. Indeed, it is difficult to hide a local update by adding\nother independent local updates when the updates are of high dimension. Our\nfindings underscore the imperative for additional privacy-enhancing mechanisms,\nsuch as noise injection, in federated learning.\n","authors":["Khac-Hoang Ngo","Johan Östman","Giuseppe Durisi","Alexandre Graell i Amat"],"pdf_url":"https://arxiv.org/pdf/2403.17775v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17768v1","updated":"2024-03-26T14:54:48Z","published":"2024-03-26T14:54:48Z","title":"SciNews: From Scholarly Complexities to Public Narratives -- A Dataset\n  for Scientific News Report Generation","summary":"  Scientific news reports serve as a bridge, adeptly translating complex\nresearch articles into reports that resonate with the broader public. The\nautomated generation of such narratives enhances the accessibility of scholarly\ninsights. In this paper, we present a new corpus to facilitate this paradigm\ndevelopment. Our corpus comprises a parallel compilation of academic\npublications and their corresponding scientific news reports across nine\ndisciplines. To demonstrate the utility and reliability of our dataset, we\nconduct an extensive analysis, highlighting the divergences in readability and\nbrevity between scientific news narratives and academic manuscripts. We\nbenchmark our dataset employing state-of-the-art text generation models. The\nevaluation process involves both automatic and human evaluation, which lays the\ngroundwork for future explorations into the automated generation of scientific\nnews reports. The dataset and code related to this work are available at\nhttps://dongqi.me/projects/SciNews.\n","authors":["Dongqi Pu","Yifan Wang","Jia Loy","Vera Demberg"],"pdf_url":"https://arxiv.org/pdf/2403.17768v1.pdf","comment":"LREC-COLING 2024 Main Conference Paper"},{"id":"http://arxiv.org/abs/2403.17767v1","updated":"2024-03-26T14:54:35Z","published":"2024-03-26T14:54:35Z","title":"Asymptotic Bayes risk of semi-supervised learning with uncertain\n  labeling","summary":"  This article considers a semi-supervised classification setting on a Gaussian\nmixture model, where the data is not labeled strictly as usual, but instead\nwith uncertain labels. Our main aim is to compute the Bayes risk for this\nmodel. We compare the behavior of the Bayes risk and the best known algorithm\nfor this model. This comparison eventually gives new insights over the\nalgorithm.\n","authors":["Victor Leger","Romain Couillet"],"pdf_url":"https://arxiv.org/pdf/2403.17767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17757v1","updated":"2024-03-26T14:49:22Z","published":"2024-03-26T14:49:22Z","title":"Noise2Noise Denoising of CRISM Hyperspectral Data","summary":"  Hyperspectral data acquired by the Compact Reconnaissance Imaging\nSpectrometer for Mars (CRISM) have allowed for unparalleled mapping of the\nsurface mineralogy of Mars. Due to sensor degradation over time, a significant\nportion of the recently acquired data is considered unusable. Here a new\ndata-driven model architecture, Noise2Noise4Mars (N2N4M), is introduced to\nremove noise from CRISM images. Our model is self-supervised and does not\nrequire zero-noise target data, making it well suited for use in Planetary\nScience applications where high quality labelled data is scarce. We demonstrate\nits strong performance on synthetic-noise data and CRISM images, and its impact\non downstream classification performance, outperforming benchmark methods on\nmost metrics. This allows for detailed analysis for critical sites of interest\non the Martian surface, including proposed lander sites.\n","authors":["Robert Platt","Rossella Arcucci","Cédric John"],"pdf_url":"https://arxiv.org/pdf/2403.17757v1.pdf","comment":"5 pages, 3 figures. Accepted as a conference paper at the ICLR 2024\n  ML4RS Workshop"},{"id":"http://arxiv.org/abs/2403.11996v2","updated":"2024-03-26T14:46:04Z","published":"2024-03-18T17:30:27Z","title":"Accelerating Scientific Discovery with Generative Knowledge Extraction,\n  Graph-Based Representation, and Multimodal Intelligent Graph Reasoning","summary":"  Leveraging generative Artificial Intelligence (AI), we have transformed a\ndataset comprising 1,000 scientific papers into an ontological knowledge graph.\nThrough an in-depth structural analysis, we have calculated node degrees,\nidentified communities and connectivities, and evaluated clustering\ncoefficients and betweenness centrality of pivotal nodes, uncovering\nfascinating knowledge architectures. The graph has an inherently scale-free\nnature, is highly connected, and can be used for graph reasoning by taking\nadvantage of transitive and isomorphic properties that reveal unprecedented\ninterdisciplinary relationships that can be used to answer queries, identify\ngaps in knowledge, propose never-before-seen material designs, and predict\nmaterial behaviors. We compute deep node embeddings for combinatorial node\nsimilarity ranking for use in a path sampling strategy links dissimilar\nconcepts that have previously not been related. One comparison revealed\nstructural parallels between biological materials and Beethoven's 9th Symphony,\nhighlighting shared patterns of complexity through isomorphic mapping. In\nanother example, the algorithm proposed a hierarchical mycelium-based composite\nbased on integrating path sampling with principles extracted from Kandinsky's\n'Composition VII' painting. The resulting material integrates an innovative set\nof concepts that include a balance of chaos/order, adjustable porosity,\nmechanical strength, and complex patterned chemical functionalization. We\nuncover other isomorphisms across science, technology and art, revealing a\nnuanced ontology of immanence that reveal a context-dependent heterarchical\ninterplay of constituents. Graph-based generative AI achieves a far higher\ndegree of novelty, explorative capacity, and technical detail, than\nconventional approaches and establishes a widely useful framework for\ninnovation by revealing hidden connections.\n","authors":["Markus J. Buehler"],"pdf_url":"https://arxiv.org/pdf/2403.11996v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17753v1","updated":"2024-03-26T14:43:57Z","published":"2024-03-26T14:43:57Z","title":"CCDSReFormer: Traffic Flow Prediction with a Criss-Crossed Dual-Stream\n  Enhanced Rectified Transformer Model","summary":"  Accurate, and effective traffic forecasting is vital for smart traffic\nsystems, crucial in urban traffic planning and management. Current\nSpatio-Temporal Transformer models, despite their prediction capabilities,\nstruggle with balancing computational efficiency and accuracy, favoring global\nover local information, and handling spatial and temporal data separately,\nlimiting insight into complex interactions. We introduce the Criss-Crossed\nDual-Stream Enhanced Rectified Transformer model (CCDSReFormer), which includes\nthree innovative modules: Enhanced Rectified Spatial Self-attention (ReSSA),\nEnhanced Rectified Delay Aware Self-attention (ReDASA), and Enhanced Rectified\nTemporal Self-attention (ReTSA). These modules aim to lower computational needs\nvia sparse attention, focus on local information for better traffic dynamics\nunderstanding, and merge spatial and temporal insights through a unique\nlearning method. Extensive tests on six real-world datasets highlight\nCCDSReFormer's superior performance. An ablation study also confirms the\nsignificant impact of each component on the model's predictive accuracy,\nshowcasing our model's ability to forecast traffic flow effectively.\n","authors":["Zhiqi Shao","Michael G. H. Bell","Ze Wang","D. Glenn Geers","Xusheng Yao","Junbin Gao"],"pdf_url":"https://arxiv.org/pdf/2403.17753v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2310.02129v4","updated":"2024-03-26T14:38:23Z","published":"2023-10-03T15:10:46Z","title":"Unveiling the Pitfalls of Knowledge Editing for Large Language Models","summary":"  As the cost associated with fine-tuning Large Language Models (LLMs)\ncontinues to rise, recent research efforts have pivoted towards developing\nmethodologies to edit implicit knowledge embedded within LLMs. Yet, there's\nstill a dark cloud lingering overhead -- will knowledge editing trigger\nbutterfly effect? since it is still unclear whether knowledge editing might\nintroduce side effects that pose potential risks or not. This paper pioneers\nthe investigation into the potential pitfalls associated with knowledge editing\nfor LLMs. To achieve this, we introduce new benchmark datasets and propose\ninnovative evaluation metrics. Our results underline two pivotal concerns: (1)\nKnowledge Conflict: Editing groups of facts that logically clash can magnify\nthe inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)\nKnowledge Distortion: Altering parameters with the aim of editing factual\nknowledge can irrevocably warp the innate knowledge structure of LLMs.\nExperimental results vividly demonstrate that knowledge editing might\ninadvertently cast a shadow of unintended consequences on LLMs, which warrant\nattention and efforts for future works. Code and data are available at\nhttps://github.com/zjunlp/PitfallsKnowledgeEditing.\n","authors":["Zhoubo Li","Ningyu Zhang","Yunzhi Yao","Mengru Wang","Xi Chen","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2310.02129v4.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.17745v1","updated":"2024-03-26T14:36:22Z","published":"2024-03-26T14:36:22Z","title":"Leave No Patient Behind: Enhancing Medication Recommendation for Rare\n  Disease Patients","summary":"  Medication recommendation systems have gained significant attention in\nhealthcare as a means of providing tailored and effective drug combinations\nbased on patients' clinical information. However, existing approaches often\nsuffer from fairness issues, as recommendations tend to be more accurate for\npatients with common diseases compared to those with rare conditions. In this\npaper, we propose a novel model called Robust and Accurate REcommendations for\nMedication (RAREMed), which leverages the pretrain-finetune learning paradigm\nto enhance accuracy for rare diseases. RAREMed employs a transformer encoder\nwith a unified input sequence approach to capture complex relationships among\ndisease and procedure codes. Additionally, it introduces two self-supervised\npre-training tasks, namely Sequence Matching Prediction (SMP) and Self\nReconstruction (SR), to learn specialized medication needs and interrelations\namong clinical codes. Experimental results on two real-world datasets\ndemonstrate that RAREMed provides accurate drug sets for both rare and common\ndisease patients, thereby mitigating unfairness in medication recommendation\nsystems.\n","authors":["Zihao Zhao","Yi Jing","Fuli Feng","Jiancan Wu","Chongming Gao","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2403.17745v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17729v1","updated":"2024-03-26T14:18:43Z","published":"2024-03-26T14:18:43Z","title":"EulerFormer: Sequential User Behavior Modeling with Complex Vector\n  Attention","summary":"  To capture user preference, transformer models have been widely applied to\nmodel sequential user behavior data. The core of transformer architecture lies\nin the self-attention mechanism, which computes the pairwise attention scores\nin a sequence. Due to the permutation-equivariant nature, positional encoding\nis used to enhance the attention between token representations. In this\nsetting, the pairwise attention scores can be derived by both semantic\ndifference and positional difference. However, prior studies often model the\ntwo kinds of difference measurements in different ways, which potentially\nlimits the expressive capacity of sequence modeling. To address this issue,\nthis paper proposes a novel transformer variant with complex vector attention,\nnamed EulerFormer, which provides a unified theoretical framework to formulate\nboth semantic difference and positional difference. The EulerFormer involves\ntwo key technical improvements. First, it employs a new transformation function\nfor efficiently transforming the sequence tokens into polar-form complex\nvectors using Euler's formula, enabling the unified modeling of both semantic\nand positional information in a complex rotation form.Secondly, it develops a\ndifferential rotation mechanism, where the semantic rotation angles can be\ncontrolled by an adaptation function, enabling the adaptive integration of the\nsemantic and positional information according to the semantic\ncontexts.Furthermore, a phase contrastive learning task is proposed to improve\nthe anisotropy of contextual representations in EulerFormer. Our theoretical\nframework possesses a high degree of completeness and generality. It is more\nrobust to semantic variations and possesses moresuperior theoretical properties\nin principle. Extensive experiments conducted on four public datasets\ndemonstrate the effectiveness and efficiency of our approach.\n","authors":["Zhen Tian","Wayne Xin Zhao","Changwang Zhang","Xin Zhao","Zhongrui Ma","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2403.17729v1.pdf","comment":"Accepted for publication in SIGIR'24"},{"id":"http://arxiv.org/abs/2403.17728v1","updated":"2024-03-26T14:17:01Z","published":"2024-03-26T14:17:01Z","title":"Masked Autoencoders are PDE Learners","summary":"  Neural solvers for partial differential equations (PDEs) have great\npotential, yet their practicality is currently limited by their\ngeneralizability. PDEs evolve over broad scales and exhibit diverse behaviors;\npredicting these phenomena will require learning representations across a wide\nvariety of inputs, which may encompass different coefficients, geometries, or\nequations. As a step towards generalizable PDE modeling, we adapt masked\npretraining for PDEs. Through self-supervised learning across PDEs, masked\nautoencoders can learn useful latent representations for downstream tasks. In\nparticular, masked pretraining can improve coefficient regression and\ntimestepping performance of neural solvers on unseen equations. We hope that\nmasked pretraining can emerge as a unifying method across large, unlabeled, and\nheterogeneous datasets to learn latent physics at scale.\n","authors":["Anthony Zhou","Amir Barati Farimani"],"pdf_url":"https://arxiv.org/pdf/2403.17728v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2312.05337v2","updated":"2024-03-26T14:09:56Z","published":"2023-12-08T19:52:48Z","title":"Artificial Neural Nets and the Representation of Human Concepts","summary":"  What do artificial neural networks (ANNs) learn? The machine learning (ML)\ncommunity shares the narrative that ANNs must develop abstract human concepts\nto perform complex tasks. Some go even further and believe that these concepts\nare stored in individual units of the network. Based on current research, I\nsystematically investigate the assumptions underlying this narrative. I\nconclude that ANNs are indeed capable of performing complex prediction tasks,\nand that they may learn human and non-human concepts to do so. However,\nevidence indicates that ANNs do not represent these concepts in individual\nunits.\n","authors":["Timo Freiesleben"],"pdf_url":"https://arxiv.org/pdf/2312.05337v2.pdf","comment":"For: Philosophy of Science for Machine Learning: Core Issues and New\n  Perspectives, edited by Juan Duran and Giorgia Pozzi"},{"id":"http://arxiv.org/abs/2310.02969v2","updated":"2024-03-26T14:00:59Z","published":"2023-10-04T17:06:30Z","title":"Dual Conic Proxies for AC Optimal Power Flow","summary":"  In recent years, there has been significant interest in the development of\nmachine learning-based optimization proxies for AC Optimal Power Flow (AC-OPF).\nAlthough significant progress has been achieved in predicting high-quality\nprimal solutions, no existing learning-based approach can provide valid dual\nbounds for AC-OPF. This paper addresses this gap by training optimization\nproxies for a convex relaxation of AC-OPF. Namely, the paper considers a\nsecond-order cone (SOC) relaxation of AC-OPF, and proposes \\revision{a novel\narchitecture} that embeds a fast, differentiable (dual) feasibility recovery,\nthus providing valid dual bounds. The paper combines this new architecture with\na self-supervised learning scheme, which alleviates the need for costly\ntraining data generation. Extensive numerical experiments on medium- and\nlarge-scale power grids demonstrate the efficiency and scalability of the\nproposed methodology.\n","authors":["Guancheng Qiu","Mathieu Tanneau","Pascal Van Hentenryck"],"pdf_url":"https://arxiv.org/pdf/2310.02969v2.pdf","comment":"accepted to PSCC 2024"},{"id":"http://arxiv.org/abs/2310.02869v2","updated":"2024-03-26T13:43:55Z","published":"2023-10-04T15:03:56Z","title":"Harmonic Control Lyapunov Barrier Functions for Constrained Optimal\n  Control with Reach-Avoid Specifications","summary":"  This paper introduces harmonic control Lyapunov barrier functions (harmonic\nCLBF) that aid in constrained control problems such as reach-avoid problems.\nHarmonic CLBFs exploit the maximum principle that harmonic functions satisfy to\nencode the properties of control Lyapunov barrier functions (CLBFs). As a\nresult, they can be initiated at the start of an experiment rather than trained\nbased on sample trajectories. The control inputs are selected to maximize the\ninner product of the system dynamics with the steepest descent direction of the\nharmonic CLBF. Numerical results are presented with four different systems\nunder different reach-avoid environments. Harmonic CLBFs show a significantly\nlow risk of entering unsafe regions and a high probability of entering the goal\nregion.\n","authors":["Amartya Mukherjee","Ruikun Zhou","Haocheng Chang","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2310.02869v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17701v1","updated":"2024-03-26T13:40:18Z","published":"2024-03-26T13:40:18Z","title":"Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical\n  Image Segmentation","summary":"  Image segmentation holds a vital position in the realms of diagnosis and\ntreatment within the medical domain. Traditional convolutional neural networks\n(CNNs) and Transformer models have made significant advancements in this realm,\nbut they still encounter challenges because of limited receptive field or high\ncomputing complexity. Recently, State Space Models (SSMs), particularly Mamba\nand its variants, have demonstrated notable performance in the field of vision.\nHowever, their feature extraction methods may not be sufficiently effective and\nretain some redundant structures, leaving room for parameter reduction.\nMotivated by previous spatial and channel attention methods, we propose Triplet\nMamba-UNet. The method leverages residual VSS Blocks to extract intensive\ncontextual features, while Triplet SSM is employed to fuse features across\nspatial and channel dimensions. We conducted experiments on ISIC17, ISIC18,\nCVC-300, CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB, and Kvasir-Instrument datasets,\ndemonstrating the superior segmentation performance of our proposed TM-UNet.\nAdditionally, compared to the previous VM-UNet, our model achieves a one-third\nreduction in parameters.\n","authors":["Hao Tang","Lianglun Cheng","Guoheng Huang","Zhengguang Tan","Junhao Lu","Kaihong Wu"],"pdf_url":"https://arxiv.org/pdf/2403.17701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17698v1","updated":"2024-03-26T13:38:06Z","published":"2024-03-26T13:38:06Z","title":"MEP: Multiple Kernel Learning Enhancing Relative Positional Encoding\n  Length Extrapolation","summary":"  When the predicted sequence length exceeds the length seen during training,\nthe transformer's inference accuracy diminishes. Existing relative position\nencoding methods, such as those based on the ALiBi technique, address the\nlength extrapolation challenge exclusively through the implementation of a\nsingle kernel function, which introduces a constant bias to every post-softmax\nattention scores according to their distance. These approaches do not\ninvestigate or employ multiple kernel functions to address the extrapolation\nchallenge. Drawing on the ALiBi approach, this study proposes a novel relative\npositional encoding method, called MEP, which employs a weighted average to\ncombine distinct kernel functions(such as the exponential kernel and the\nGaussian kernel) to generate a bias that is applied to post-softmax attention\nscores. Initially, the framework utilizes various kernel functions to construct\nmultiple kernel functions. Each kernel function adheres to a consistent mean\nweight coefficient, harnessing the synergistic advantages of different kernels\nto formulate an innovative bias function. Subsequently, specific slopes are\ntailored for each kernel function, applying penalties at varying rates, to\nenhance the model's extrapolation capabilities. Finally, this bias is\nseamlessly incorporated as a penalty to the post-softmax scores. We present two\ndistinct versions of our method: a parameter-free variant that requires no new\nlearnable parameters, which enhances length extrapolation capabilities without\ncompromising training efficiency, and a parameterized variant capable of\nintegrating state-of-the-art techniques. Empirical evaluations across diverse\ndatasets have demonstrated that both variants of our method achieve\nstate-of-the-art performance, outperforming traditional parameter-free and\nparameterized approaches.\n","authors":["Weiguo Gao"],"pdf_url":"https://arxiv.org/pdf/2403.17698v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17695v1","updated":"2024-03-26T13:35:10Z","published":"2024-03-26T13:35:10Z","title":"PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition","summary":"  We present PlainMamba: a simple non-hierarchical state space model (SSM)\ndesigned for general visual recognition. The recent Mamba model has shown how\nSSMs can be highly competitive with other architectures on sequential data and\ninitial attempts have been made to apply it to images. In this paper, we\nfurther adapt the selective scanning process of Mamba to the visual domain,\nenhancing its ability to learn features from two-dimensional images by (i) a\ncontinuous 2D scanning process that improves spatial continuity by ensuring\nadjacency of tokens in the scanning sequence, and (ii) direction-aware updating\nwhich enables the model to discern the spatial relations of tokens by encoding\ndirectional information. Our architecture is designed to be easy to use and\neasy to scale, formed by stacking identical PlainMamba blocks, resulting in a\nmodel with constant width throughout all layers. The architecture is further\nsimplified by removing the need for special tokens. We evaluate PlainMamba on a\nvariety of visual recognition tasks including image classification, semantic\nsegmentation, object detection, and instance segmentation. Our method achieves\nperformance gains over previous non-hierarchical models and is competitive with\nhierarchical alternatives. For tasks requiring high-resolution inputs, in\nparticular, PlainMamba requires much less computing while maintaining high\nperformance. Code and models are available at\nhttps://github.com/ChenhongyiYang/PlainMamba\n","authors":["Chenhongyi Yang","Zehui Chen","Miguel Espinosa","Linus Ericsson","Zhenyu Wang","Jiaming Liu","Elliot J. Crowley"],"pdf_url":"https://arxiv.org/pdf/2403.17695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17692v1","updated":"2024-03-26T13:33:16Z","published":"2024-03-26T13:33:16Z","title":"Manifold-Guided Lyapunov Control with Diffusion Models","summary":"  This paper presents a novel approach to generating stabilizing controllers\nfor a large class of dynamical systems using diffusion models. The core\nobjective is to develop stabilizing control functions by identifying the\nclosest asymptotically stable vector field relative to a predetermined manifold\nand adjusting the control function based on this finding. To achieve this, we\nemploy a diffusion model trained on pairs consisting of asymptotically stable\nvector fields and their corresponding Lyapunov functions. Our numerical results\ndemonstrate that this pre-trained model can achieve stabilization over\npreviously unseen systems efficiently and rapidly, showcasing the potential of\nour approach in fast zero-shot control and generalizability.\n","authors":["Amartya Mukherjee","Thanin Quartz","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17692v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2301.12778v2","updated":"2024-03-26T13:29:16Z","published":"2023-01-30T10:48:10Z","title":"Investigating Feature and Model Importance in Android Malware Detection:\n  An Implemented Survey and Experimental Comparison of ML-Based Methods","summary":"  The popularity of Android means it is a common target for malware. Over the\nyears, various studies have found that machine learning models can effectively\ndiscriminate malware from benign applications. However, as the operating system\nevolves, so does malware, bringing into question the findings of these previous\nstudies, many of which report very high accuracies using small, outdated, and\noften imbalanced datasets. In this paper, we reimplement 18 representative past\nworks and reevaluate them using a balanced, relevant, and up-to-date dataset\ncomprising 124,000 applications. We also carry out new experiments designed to\nfill holes in existing knowledge, and use our findings to identify the most\neffective features and models to use for Android malware detection within a\ncontemporary environment. We show that high detection accuracies (up to 96.8%)\ncan be achieved using features extracted through static analysis alone,\nyielding a modest benefit (1%) from using far more expensive dynamic analysis.\nAPI calls and opcodes are the most productive static and TCP network traffic\nprovide the most predictive dynamic features. Random forests are generally the\nmost effective model, outperforming more complex deep learning approaches.\nWhilst directly combining static and dynamic features is generally ineffective,\nensembling models separately leads to performances comparable to the best\nmodels but using less brittle features.\n","authors":["Ali Muzaffar","Hani Ragab Hassen","Hind Zantout","Michael A Lones"],"pdf_url":"https://arxiv.org/pdf/2301.12778v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17094v2","updated":"2024-03-26T13:21:43Z","published":"2023-11-28T06:17:49Z","title":"In Search of a Data Transformation That Accelerates Neural Field\n  Training","summary":"  Neural field is an emerging paradigm in data representation that trains a\nneural network to approximate the given signal. A key obstacle that prevents\nits widespread adoption is the encoding speed-generating neural fields requires\nan overfitting of a neural network, which can take a significant number of SGD\nsteps to reach the desired fidelity level. In this paper, we delve into the\nimpacts of data transformations on the speed of neural field training,\nspecifically focusing on how permuting pixel locations affect the convergence\nspeed of SGD. Counterintuitively, we find that randomly permuting the pixel\nlocations can considerably accelerate the training. To explain this phenomenon,\nwe examine the neural field training through the lens of PSNR curves, loss\nlandscapes, and error patterns. Our analyses suggest that the random pixel\npermutations remove the easy-to-fit patterns, which facilitate easy\noptimization in the early stage but hinder capturing fine details of the\nsignal.\n","authors":["Junwon Seo","Sangyoon Lee","Kwang In Kim","Jaeho Lee"],"pdf_url":"https://arxiv.org/pdf/2311.17094v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.16915v2","updated":"2024-03-26T13:11:44Z","published":"2024-03-25T16:32:50Z","title":"Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language\n  Models","summary":"  Fine-tuning in information retrieval systems using pre-trained language\nmodels (PLM-based IR) requires learning query representations and\nquery-document relations, in addition to downstream task-specific learning.\nThis study introduces coarse-tuning as an intermediate learning stage that\nbridges pre-training and fine-tuning. By learning query representations and\nquery-document relations in coarse-tuning, we aim to reduce the load of\nfine-tuning and improve the learning effect of downstream IR tasks. We propose\nQuery-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the\nappropriateness of query-document pairs. Evaluation experiments show that the\nproposed method significantly improves MRR and/or nDCG@5 in four ad-hoc\ndocument retrieval datasets. Furthermore, the results of the query prediction\ntask suggested that coarse-tuning facilitated learning of query representation\nand query-document relations.\n","authors":["Atsushi Keyaki","Ribeka Keyaki"],"pdf_url":"https://arxiv.org/pdf/2403.16915v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17673v1","updated":"2024-03-26T13:02:43Z","published":"2024-03-26T13:02:43Z","title":"How Private is DP-SGD?","summary":"  We demonstrate a substantial gap between the privacy guarantees of the\nAdaptive Batch Linear Queries (ABLQ) mechanism under different types of batch\nsampling: (i) Shuffling, and (ii) Poisson subsampling; the typical analysis of\nDifferentially Private Stochastic Gradient Descent (DP-SGD) follows by\ninterpreting it as a post-processing of ABLQ. While shuffling based DP-SGD is\nmore commonly used in practical implementations, it is neither analytically nor\nnumerically amenable to easy privacy analysis. On the other hand, Poisson\nsubsampling based DP-SGD is challenging to scalably implement, but has a\nwell-understood privacy analysis, with multiple open-source numerically tight\nprivacy accountants available. This has led to a common practice of using\nshuffling based DP-SGD in practice, but using the privacy analysis for the\ncorresponding Poisson subsampling version. Our result shows that there can be a\nsubstantial gap between the privacy analysis when using the two types of batch\nsampling, and thus advises caution in reporting privacy parameters for DP-SGD.\n","authors":["Lynn Chua","Badih Ghazi","Pritish Kamath","Ravi Kumar","Pasin Manurangsi","Amer Sinha","Chiyuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.17673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01885v2","updated":"2024-03-26T12:59:44Z","published":"2023-11-03T12:54:05Z","title":"Domain Randomization via Entropy Maximization","summary":"  Varying dynamics parameters in simulation is a popular Domain Randomization\n(DR) approach for overcoming the reality gap in Reinforcement Learning (RL).\nNevertheless, DR heavily hinges on the choice of the sampling distribution of\nthe dynamics parameters, since high variability is crucial to regularize the\nagent's behavior but notoriously leads to overly conservative policies when\nrandomizing excessively. In this paper, we propose a novel approach to address\nsim-to-real transfer, which automatically shapes dynamics distributions during\ntraining in simulation without requiring real-world data. We introduce DOmain\nRAndomization via Entropy MaximizatiON (DORAEMON), a constrained optimization\nproblem that directly maximizes the entropy of the training distribution while\nretaining generalization capabilities. In achieving this, DORAEMON gradually\nincreases the diversity of sampled dynamics parameters as long as the\nprobability of success of the current policy is sufficiently high. We\nempirically validate the consistent benefits of DORAEMON in obtaining highly\nadaptive and generalizable policies, i.e. solving the task at hand across the\nwidest range of dynamics parameters, as opposed to representative baselines\nfrom the DR literature. Notably, we also demonstrate the Sim2Real applicability\nof DORAEMON through its successful zero-shot transfer in a robotic manipulation\nsetup under unknown real-world parameters.\n","authors":["Gabriele Tiboni","Pascal Klink","Jan Peters","Tatiana Tommasi","Carlo D'Eramo","Georgia Chalvatzaki"],"pdf_url":"https://arxiv.org/pdf/2311.01885v2.pdf","comment":"Published as a conference paper at ICLR 2024. Project website at\n  https://gabrieletiboni.github.io/doraemon/"},{"id":"http://arxiv.org/abs/2403.17660v1","updated":"2024-03-26T12:47:04Z","published":"2024-03-26T12:47:04Z","title":"CANOS: A Fast and Scalable Neural AC-OPF Solver Robust To N-1\n  Perturbations","summary":"  Optimal Power Flow (OPF) refers to a wide range of related optimization\nproblems with the goal of operating power systems efficiently and securely. In\nthe simplest setting, OPF determines how much power to generate in order to\nminimize costs while meeting demand for power and satisfying physical and\noperational constraints. In even the simplest case, power grid operators use\napproximations of the AC-OPF problem because solving the exact problem is\nprohibitively slow with state-of-the-art solvers. These approximations\nsacrifice accuracy and operational feasibility in favor of speed. This\ntrade-off leads to costly \"uplift payments\" and increased carbon emissions,\nespecially for large power grids. In the present work, we train a deep learning\nsystem (CANOS) to predict near-optimal solutions (within 1% of the true AC-OPF\ncost) without compromising speed (running in as little as 33--65 ms).\nImportantly, CANOS scales to realistic grid sizes with promising empirical\nresults on grids containing as many as 10,000 buses. Finally, because CANOS is\na Graph Neural Network, it is robust to changes in topology. We show that CANOS\nis accurate across N-1 topological perturbations of a base grid typically used\nin security-constrained analysis. This paves the way for more efficient\noptimization of more complex OPF problems which alter grid connectivity such as\nunit commitment, topology optimization and security-constrained OPF.\n","authors":["Luis Piloto","Sofia Liguori","Sephora Madjiheurem","Miha Zgubic","Sean Lovett","Hamish Tomlinson","Sophie Elster","Chris Apps","Sims Witherspoon"],"pdf_url":"https://arxiv.org/pdf/2403.17660v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17656v1","updated":"2024-03-26T12:39:02Z","published":"2024-03-26T12:39:02Z","title":"SGHormer: An Energy-Saving Graph Transformer Driven by Spikes","summary":"  Graph Transformers (GTs) with powerful representation learning ability make a\nhuge success in wide range of graph tasks. However, the costs behind\noutstanding performances of GTs are higher energy consumption and computational\noverhead. The complex structure and quadratic complexity during attention\ncalculation in vanilla transformer seriously hinder its scalability on the\nlarge-scale graph data. Though existing methods have made strides in\nsimplifying combinations among blocks or attention-learning paradigm to improve\nGTs' efficiency, a series of energy-saving solutions originated from\nbiologically plausible structures are rarely taken into consideration when\nconstructing GT framework. To this end, we propose a new spiking-based graph\ntransformer (SGHormer). It turns full-precision embeddings into sparse and\nbinarized spikes to reduce memory and computational costs. The spiking graph\nself-attention and spiking rectify blocks in SGHormer explicitly capture global\nstructure information and recover the expressive power of spiking embeddings,\nrespectively. In experiments, SGHormer achieves comparable performances to\nother full-precision GTs with extremely low computational energy consumption.\nThe results show that SGHomer makes a remarkable progress in the field of\nlow-energy GTs.\n","authors":["Huizhe Zhang","Jintang Li","Liang Chen","Zibin Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.17656v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.13374v2","updated":"2024-03-26T12:33:16Z","published":"2024-03-20T08:15:08Z","title":"Byzantine-resilient Federated Learning With Adaptivity to Data\n  Heterogeneity","summary":"  This paper deals with federated learning (FL) in the presence of malicious\nByzantine attacks and data heterogeneity. A novel Robust Average Gradient\nAlgorithm (RAGA) is proposed, which leverages the geometric median for\naggregation and can freely select the round number for local updating.\nDifferent from most existing resilient approaches, which perform convergence\nanalysis based on strongly-convex loss function or homogeneously distributed\ndataset, we conduct convergence analysis for not only strongly-convex but also\nnon-convex loss function over heterogeneous dataset. According to our\ntheoretical analysis, as long as the fraction of dataset from malicious users\nis less than half, RAGA can achieve convergence at rate\n$\\mathcal{O}({1}/{T^{2/3- \\delta}})$ where $T$ is the iteration number and\n$\\delta \\in (0, 2/3)$ for non-convex loss function, and at linear rate for\nstrongly-convex loss function. Moreover, stationary point or global optimal\nsolution is proved to obtainable as data heterogeneity vanishes. Experimental\nresults corroborate the robustness of RAGA to Byzantine attacks and verifies\nthe advantage of RAGA over baselines on convergence performance under various\nintensity of Byzantine attacks, for heterogeneous dataset.\n","authors":["Shiyuan Zuo","Xingrun Yan","Rongfei Fan","Han Hu","Hangguan Shan","Tony Q. S. Quek"],"pdf_url":"https://arxiv.org/pdf/2403.13374v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17646v1","updated":"2024-03-26T12:28:04Z","published":"2024-03-26T12:28:04Z","title":"Uncertainty-aware Distributional Offline Reinforcement Learning","summary":"  Offline reinforcement learning (RL) presents distinct challenges as it relies\nsolely on observational data. A central concern in this context is ensuring the\nsafety of the learned policy by quantifying uncertainties associated with\nvarious actions and environmental stochasticity. Traditional approaches\nprimarily emphasize mitigating epistemic uncertainty by learning risk-averse\npolicies, often overlooking environmental stochasticity. In this study, we\npropose an uncertainty-aware distributional offline RL method to simultaneously\naddress both epistemic uncertainty and environmental stochasticity. We propose\na model-free offline RL algorithm capable of learning risk-averse policies and\ncharacterizing the entire distribution of discounted cumulative rewards, as\nopposed to merely maximizing the expected value of accumulated discounted\nreturns. Our method is rigorously evaluated through comprehensive experiments\nin both risk-sensitive and risk-neutral benchmarks, demonstrating its superior\nperformance.\n","authors":["Xiaocong Chen","Siyu Wang","Tong Yu","Lina Yao"],"pdf_url":"https://arxiv.org/pdf/2403.17646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17637v1","updated":"2024-03-26T12:12:44Z","published":"2024-03-26T12:12:44Z","title":"PeersimGym: An Environment for Solving the Task Offloading Problem with\n  Reinforcement Learning","summary":"  Task offloading, crucial for balancing computational loads across devices in\nnetworks such as the Internet of Things, poses significant optimization\nchallenges, including minimizing latency and energy usage under strict\ncommunication and storage constraints. While traditional optimization falls\nshort in scalability; and heuristic approaches lack in achieving optimal\noutcomes, Reinforcement Learning (RL) offers a promising avenue by enabling the\nlearning of optimal offloading strategies through iterative interactions.\nHowever, the efficacy of RL hinges on access to rich datasets and\ncustom-tailored, realistic training environments. To address this, we introduce\nPeersimGym, an open-source, customizable simulation environment tailored for\ndeveloping and optimizing task offloading strategies within computational\nnetworks. PeersimGym supports a wide range of network topologies and\ncomputational constraints and integrates a \\textit{PettingZoo}-based interface\nfor RL agent deployment in both solo and multi-agent setups. Furthermore, we\ndemonstrate the utility of the environment through experiments with Deep\nReinforcement Learning agents, showcasing the potential of RL-based approaches\nto significantly enhance offloading strategies in distributed computing\nsettings. PeersimGym thus bridges the gap between theoretical RL models and\ntheir practical applications, paving the way for advancements in efficient task\noffloading methodologies.\n","authors":["Frederico Metelo","Stevo Racković","Pedro Ákos","Cláudia Soares"],"pdf_url":"https://arxiv.org/pdf/2403.17637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17634v1","updated":"2024-03-26T12:08:58Z","published":"2024-03-26T12:08:58Z","title":"Retentive Decision Transformer with Adaptive Masking for Reinforcement\n  Learning based Recommendation Systems","summary":"  Reinforcement Learning-based Recommender Systems (RLRS) have shown promise\nacross a spectrum of applications, from e-commerce platforms to streaming\nservices. Yet, they grapple with challenges, notably in crafting reward\nfunctions and harnessing large pre-existing datasets within the RL framework.\nRecent advancements in offline RLRS provide a solution for how to address these\ntwo challenges. However, existing methods mainly rely on the transformer\narchitecture, which, as sequence lengths increase, can introduce challenges\nassociated with computational resources and training costs. Additionally, the\nprevalent methods employ fixed-length input trajectories, restricting their\ncapacity to capture evolving user preferences. In this study, we introduce a\nnew offline RLRS method to deal with the above problems. We reinterpret the\nRLRS challenge by modeling sequential decision-making as an inference task,\nleveraging adaptive masking configurations. This adaptive approach selectively\nmasks input tokens, transforming the recommendation task into an inference\nchallenge based on varying token subsets, thereby enhancing the agent's ability\nto infer across diverse trajectory lengths. Furthermore, we incorporate a\nmulti-scale segmented retention mechanism that facilitates efficient modeling\nof long sequences, significantly enhancing computational efficiency. Our\nexperimental analysis, conducted on both online simulator and offline datasets,\nclearly demonstrates the advantages of our proposed method.\n","authors":["Siyu Wang","Xiaocong Chen","Lina Yao"],"pdf_url":"https://arxiv.org/pdf/2403.17634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17632v1","updated":"2024-03-26T12:08:05Z","published":"2024-03-26T12:08:05Z","title":"Data-driven Energy Consumption Modelling for Electric Micromobility\n  using an Open Dataset","summary":"  The escalating challenges of traffic congestion and environmental degradation\nunderscore the critical importance of embracing E-Mobility solutions in urban\nspaces. In particular, micro E-Mobility tools such as E-scooters and E-bikes,\nplay a pivotal role in this transition, offering sustainable alternatives for\nurban commuters. However, the energy consumption patterns for these tools are a\ncritical aspect that impacts their effectiveness in real-world scenarios and is\nessential for trip planning and boosting user confidence in using these. To\nthis effect, recent studies have utilised physical models customised for\nspecific mobility tools and conditions, but these models struggle with\ngeneralization and effectiveness in real-world scenarios due to a notable\nabsence of open datasets for thorough model evaluation and verification. To\nfill this gap, our work presents an open dataset, collected in Dublin, Ireland,\nspecifically designed for energy modelling research related to E-Scooters and\nE-Bikes. Furthermore, we provide a comprehensive analysis of energy consumption\nmodelling based on the dataset using a set of representative machine learning\nalgorithms and compare their performance against the contemporary mathematical\nmodels as a baseline. Our results demonstrate a notable advantage for\ndata-driven models in comparison to the corresponding mathematical models for\nestimating energy consumption. Specifically, data-driven models outperform\nphysical models in accuracy by up to 83.83% for E-Bikes and 82.16% for\nE-Scooters based on an in-depth analysis of the dataset under certain\nassumptions.\n","authors":["Yue Ding","Sen Yan","Maqsood Hussain Shah","Hongyuan Fang","Ji Li","Mingming Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17632v1.pdf","comment":"7 pages, 5 figures, 4 tables. This manuscript has been accepted by\n  the IEEE ITEC 2024"},{"id":"http://arxiv.org/abs/2311.07939v2","updated":"2024-03-26T11:54:27Z","published":"2023-11-14T06:33:41Z","title":"Discretized Distributed Optimization over Dynamic Digraphs","summary":"  We consider a discrete-time model of continuous-time distributed optimization\nover dynamic directed-graphs (digraphs) with applications to distributed\nlearning. Our optimization algorithm works over general strongly connected\ndynamic networks under switching topologies, e.g., in mobile multi-agent\nsystems and volatile networks due to link failures. Compared to many existing\nlines of work, there is no need for bi-stochastic weight designs on the links.\nThe existing literature mostly needs the link weights to be stochastic using\nspecific weight-design algorithms needed both at the initialization and at all\ntimes when the topology of the network changes. This paper eliminates the need\nfor such algorithms and paves the way for distributed optimization over\ntime-varying digraphs. We derive the bound on the gradient-tracking step-size\nand discrete time-step for convergence and prove dynamic stability using\narguments from consensus algorithms, matrix perturbation theory, and Lyapunov\ntheory. This work, particularly, is an improvement over existing\nstochastic-weight undirected networks in case of link removal or packet drops.\nThis is because the existing literature may need to rerun time-consuming and\ncomputationally complex algorithms for stochastic design, while the proposed\nstrategy works as long as the underlying network is weight-symmetric and\nbalanced. The proposed optimization framework finds applications to distributed\nclassification and learning.\n","authors":["Mohammadreza Doostmohammadian","Wei Jiang","Muwahida Liaquat","Alireza Aghasi","Houman Zarrabi"],"pdf_url":"https://arxiv.org/pdf/2311.07939v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.15694v5","updated":"2024-03-26T11:52:59Z","published":"2023-10-24T10:05:32Z","title":"COPR: Continual Learning Human Preference through Optimal Policy\n  Regularization","summary":"  The technique of Reinforcement Learning from Human Feedback (RLHF) is a\ncommonly employed method to improve pre-trained Language Models (LM), enhancing\ntheir ability to conform to human preferences. Nevertheless, the current\nRLHF-based LMs necessitate full retraining each time novel queries or feedback\nare introduced, which becomes a challenging task because human preferences can\nvary between different domains or tasks. Retraining LMs poses practical\ndifficulties in many real-world situations due to the significant time and\ncomputational resources required, along with concerns related to data privacy.\nTo address this limitation, we propose a new method called Continual Optimal\nPolicy Regularization (COPR), in which we compute the distribution of optimal\npolicy bypassing the partition function and then regularize the current policy\nbased on the historically optimal distribution to mitigate Catastrophic\nForgetting (CF). COPR involves a single learning phase and doesn't necessitate\ncomplex reinforcement learning. Importantly, it shares the capability with RLHF\nto learn from unlabeled data by maintaining a scoring module, similar to reward\nmodel, making it flexible for continually learning without human feedback. Our\nexperimental results show that COPR outperforms strong Continuous Learning (CL)\nbaselines when it comes to consistently aligning with human preferences on\nincremental tasks and domains.\n","authors":["Han Zhang","Lin Gui","Yuanzhao Zhai","Hui Wang","Yu Lei","Ruifeng Xu"],"pdf_url":"https://arxiv.org/pdf/2310.15694v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17569v2","updated":"2024-03-26T11:52:23Z","published":"2023-10-26T16:58:01Z","title":"SD4Match: Learning to Prompt Stable Diffusion Model for Semantic\n  Matching","summary":"  In this paper, we address the challenge of matching semantically similar\nkeypoints across image pairs. Existing research indicates that the intermediate\noutput of the UNet within the Stable Diffusion (SD) can serve as robust image\nfeature maps for such a matching task. We demonstrate that by employing a basic\nprompt tuning technique, the inherent potential of Stable Diffusion can be\nharnessed, resulting in a significant enhancement in accuracy over previous\napproaches. We further introduce a novel conditional prompting module that\nconditions the prompt on the local details of the input image pairs, leading to\na further improvement in performance. We designate our approach as SD4Match,\nshort for Stable Diffusion for Semantic Matching. Comprehensive evaluations of\nSD4Match on the PF-Pascal, PF-Willow, and SPair-71k datasets show that it sets\nnew benchmarks in accuracy across all these datasets. Particularly, SD4Match\noutperforms the previous state-of-the-art by a margin of 12 percentage points\non the challenging SPair-71k dataset.\n","authors":["Xinghui Li","Jingyi Lu","Kai Han","Victor Prisacariu"],"pdf_url":"https://arxiv.org/pdf/2310.17569v2.pdf","comment":"Accepted to CVPR 2024. Project website:\n  https://sd4match.active.vision/"},{"id":"http://arxiv.org/abs/2403.17608v1","updated":"2024-03-26T11:39:00Z","published":"2024-03-26T11:39:00Z","title":"Fake or JPEG? Revealing Common Biases in Generated Image Detection\n  Datasets","summary":"  The widespread adoption of generative image models has highlighted the urgent\nneed to detect artificial content, which is a crucial step in combating\nwidespread manipulation and misinformation. Consequently, numerous detectors\nand associated datasets have emerged. However, many of these datasets\ninadvertently introduce undesirable biases, thereby impacting the effectiveness\nand evaluation of detectors. In this paper, we emphasize that many datasets for\nAI-generated image detection contain biases related to JPEG compression and\nimage size. Using the GenImage dataset, we demonstrate that detectors indeed\nlearn from these undesired factors. Furthermore, we show that removing the\nnamed biases substantially increases robustness to JPEG compression and\nsignificantly alters the cross-generator performance of evaluated detectors.\nSpecifically, it leads to more than 11 percentage points increase in\ncross-generator performance for ResNet50 and Swin-T detectors on the GenImage\ndataset, achieving state-of-the-art results.\n  We provide the dataset and source codes of this paper on the anonymous\nwebsite: https://www.unbiased-genimage.org\n","authors":["Patrick Grommelt","Louis Weiss","Franz-Josef Pfreundt","Janis Keuper"],"pdf_url":"https://arxiv.org/pdf/2403.17608v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.03197v4","updated":"2024-03-26T11:37:38Z","published":"2023-11-06T15:39:05Z","title":"Stable Linear Subspace Identification: A Machine Learning Approach","summary":"  Machine Learning (ML) and linear System Identification (SI) have been\nhistorically developed independently. In this paper, we leverage\nwell-established ML tools - especially the automatic differentiation framework\n- to introduce SIMBa, a family of discrete linear multi-step-ahead state-space\nSI methods using backpropagation. SIMBa relies on a novel\nLinear-Matrix-Inequality-based free parametrization of Schur matrices to ensure\nthe stability of the identified model.\n  We show how SIMBa generally outperforms traditional linear state-space SI\nmethods, and sometimes significantly, although at the price of a higher\ncomputational burden. This performance gap is particularly remarkable compared\nto other SI methods with stability guarantees, where the gain is frequently\nabove 25% in our investigations, hinting at SIMBa's ability to simultaneously\nachieve state-of-the-art fitting performance and enforce stability.\nInterestingly, these observations hold for a wide variety of input-output\nsystems and on both simulated and real-world data, showcasing the flexibility\nof the proposed approach. We postulate that this new SI paradigm presents a\ngreat extension potential to identify structured nonlinear models from data,\nand we hence open-source SIMBa on https://github.com/Cemempamoi/simba.\n","authors":["Loris Di Natale","Muhammad Zakwan","Bratislav Svetozarevic","Philipp Heer","Giancarlo Ferrari-Trecate","Colin N. Jones"],"pdf_url":"https://arxiv.org/pdf/2311.03197v4.pdf","comment":"Accepted at ECC 2024"},{"id":"http://arxiv.org/abs/2403.16451v2","updated":"2024-03-26T11:35:08Z","published":"2024-03-25T06:30:54Z","title":"DeepMachining: Online Prediction of Machining Errors of Lathe Machines","summary":"  We describe DeepMachining, a deep learning-based AI system for online\nprediction of machining errors of lathe machine operations. We have built and\nevaluated DeepMachining based on manufacturing data from factories.\nSpecifically, we first pretrain a deep learning model for a given lathe\nmachine's operations to learn the salient features of machining states. Then,\nwe fine-tune the pretrained model to adapt to specific machining tasks. We\ndemonstrate that DeepMachining achieves high prediction accuracy for multiple\ntasks that involve different workpieces and cutting tools. To the best of our\nknowledge, this work is one of the first factory experiments using pre-trained\ndeep-learning models to predict machining errors of lathe machines.\n","authors":["Xiang-Li Lu","Hwai-Jung Hsu","Che-Wei Chou","H. T. Kung","Chen-Hsin Lee"],"pdf_url":"https://arxiv.org/pdf/2403.16451v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.16212v2","updated":"2024-03-26T11:32:36Z","published":"2023-08-30T15:09:22Z","title":"RetroBridge: Modeling Retrosynthesis with Markov Bridges","summary":"  Retrosynthesis planning is a fundamental challenge in chemistry which aims at\ndesigning reaction pathways from commercially available starting materials to a\ntarget molecule. Each step in multi-step retrosynthesis planning requires\naccurate prediction of possible precursor molecules given the target molecule\nand confidence estimates to guide heuristic search algorithms. We model\nsingle-step retrosynthesis planning as a distribution learning problem in a\ndiscrete state space. First, we introduce the Markov Bridge Model, a generative\nframework aimed to approximate the dependency between two intractable discrete\ndistributions accessible via a finite sample of coupled data points. Our\nframework is based on the concept of a Markov bridge, a Markov process pinned\nat its endpoints. Unlike diffusion-based methods, our Markov Bridge Model does\nnot need a tractable noise distribution as a sampling proxy and directly\noperates on the input product molecules as samples from the intractable prior\ndistribution. We then address the retrosynthesis planning problem with our\nnovel framework and introduce RetroBridge, a template-free retrosynthesis\nmodeling approach that achieves state-of-the-art results on standard evaluation\nbenchmarks.\n","authors":["Ilia Igashov","Arne Schneuing","Marwin Segler","Michael Bronstein","Bruno Correia"],"pdf_url":"https://arxiv.org/pdf/2308.16212v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00093v2","updated":"2024-03-26T11:20:02Z","published":"2024-01-31T12:41:27Z","title":"ChIRAAG: ChatGPT Informed Rapid and Automated Assertion Generation","summary":"  System Verilog Assertion (SVA) formulation- a critical yet complex task is a\nprerequisite in the Formal Property Verification (FPV) process. Traditionally,\nSVA formulation involves expert-driven interpretation of specifications, which\nis timeconsuming and prone to human error. However, LLM-informed automatic\nassertion generation is gaining interest. We designeda novel framework called\nChIRAAG, based on OpenAI GPT4, to generate SVA assertions from natural language\nspecifications. ChIRAAG constitutes the systematic breakdown of design\nspecifications into a standardized format, further generating assertions from\nformatted specifications using LLM. Furthermore, we developed testbenches to\nverify/validate the LLM-generated assertions. Automatic feedback of log files\nfrom the simulation tool to the LLM ensures that the framework can generate\ncorrec SVAs automatically. Only 33% of LLM-generated raw assertions had errors.\nOur results on OpenTitan designs shows that LLMs can streamline and assist\nengineers in the assertion generation process, reshaping verification\nworkflows.\n","authors":["Bhabesh Mali","Karthik Maddala","Sweeya Reddy","Vatsal Gupta","Chandan Karfa","Ramesh Karri"],"pdf_url":"https://arxiv.org/pdf/2402.00093v2.pdf","comment":"6 pages, 5 figures and 2 table"},{"id":"http://arxiv.org/abs/2403.17601v1","updated":"2024-03-26T11:13:35Z","published":"2024-03-26T11:13:35Z","title":"LASIL: Learner-Aware Supervised Imitation Learning For Long-term\n  Microscopic Traffic Simulation","summary":"  Microscopic traffic simulation plays a crucial role in transportation\nengineering by providing insights into individual vehicle behavior and overall\ntraffic flow. However, creating a realistic simulator that accurately\nreplicates human driving behaviors in various traffic conditions presents\nsignificant challenges. Traditional simulators relying on heuristic models\noften fail to deliver accurate simulations due to the complexity of real-world\ntraffic environments. Due to the covariate shift issue, existing imitation\nlearning-based simulators often fail to generate stable long-term simulations.\nIn this paper, we propose a novel approach called learner-aware supervised\nimitation learning to address the covariate shift problem in multi-agent\nimitation learning. By leveraging a variational autoencoder simultaneously\nmodeling the expert and learner state distribution, our approach augments\nexpert states such that the augmented state is aware of learner state\ndistribution. Our method, applied to urban traffic simulation, demonstrates\nsignificant improvements over existing state-of-the-art baselines in both\nshort-term microscopic and long-term macroscopic realism when evaluated on the\nreal-world dataset pNEUMA.\n","authors":["Ke Guo","Zhenwei Miao","Wei Jing","Weiwei Liu","Weizi Li","Dayang Hao","Jia Pan"],"pdf_url":"https://arxiv.org/pdf/2403.17601v1.pdf","comment":"accepted by cvpr 2024. arXiv admin note: text overlap with\n  arXiv:2306.06401"},{"id":"http://arxiv.org/abs/2403.15905v2","updated":"2024-03-26T11:11:49Z","published":"2024-03-23T18:19:02Z","title":"Towards Low-Energy Adaptive Personalization for Resource-Constrained\n  Devices","summary":"  The personalization of machine learning (ML) models to address data drift is\na significant challenge in the context of Internet of Things (IoT)\napplications. Presently, most approaches focus on fine-tuning either the full\nbase model or its last few layers to adapt to new data, while often neglecting\nenergy costs. However, various types of data drift exist, and fine-tuning the\nfull base model or the last few layers may not result in optimal performance in\ncertain scenarios. We propose Target Block Fine-Tuning (TBFT), a low-energy\nadaptive personalization framework designed for resource-constrained devices.\nWe categorize data drift and personalization into three types: input-level,\nfeature-level, and output-level. For each type, we fine-tune different blocks\nof the model to achieve optimal performance with reduced energy costs.\nSpecifically, input-, feature-, and output-level correspond to fine-tuning the\nfront, middle, and rear blocks of the model. We evaluate TBFT on a ResNet\nmodel, three datasets, three different training sizes, and a Raspberry Pi.\nCompared with the $Block Avg$, where each block is fine-tuned individually and\ntheir performance improvements are averaged, TBFT exhibits an improvement in\nmodel accuracy by an average of 15.30% whilst saving 41.57% energy consumption\non average compared with full fine-tuning.\n","authors":["Yushan Huang","Josh Millar","Yuxuan Long","Yuchen Zhao","Hamed Hadaddi"],"pdf_url":"https://arxiv.org/pdf/2403.15905v2.pdf","comment":"Accepetd to The 4th Workshop on Machine Learning and Systems\n  (EuroMLSys '24)"},{"id":"http://arxiv.org/abs/2306.00038v3","updated":"2024-03-26T11:07:30Z","published":"2023-05-31T09:51:45Z","title":"FedCSD: A Federated Learning Based Approach for Code-Smell Detection","summary":"  This paper proposes a Federated Learning Code Smell Detection (FedCSD)\napproach that allows organizations to collaboratively train federated ML models\nwhile preserving their data privacy. These assertions have been supported by\nthree experiments that have significantly leveraged three manually validated\ndatasets aimed at detecting and examining different code smell scenarios. In\nexperiment 1, which was concerned with a centralized training experiment,\ndataset two achieved the lowest accuracy (92.30%) with fewer smells, while\ndatasets one and three achieved the highest accuracy with a slight difference\n(98.90% and 99.5%, respectively). This was followed by experiment 2, which was\nconcerned with cross-evaluation, where each ML model was trained using one\ndataset, which was then evaluated over the other two datasets. Results from\nthis experiment show a significant drop in the model's accuracy (lowest\naccuracy: 63.80\\%) where fewer smells exist in the training dataset, which has\na noticeable reflection (technical debt) on the model's performance. Finally,\nthe last and third experiments evaluate our approach by splitting the dataset\ninto 10 companies. The ML model was trained on the company's site, then all\nmodel-updated weights were transferred to the server. Ultimately, an accuracy\nof 98.34% was achieved by the global model that has been trained using 10\ncompanies for 100 training rounds. The results reveal a slight difference in\nthe global model's accuracy compared to the highest accuracy of the centralized\nmodel, which can be ignored in favour of the global model's comprehensive\nknowledge, lower training cost, preservation of data privacy, and avoidance of\nthe technical debt problem.\n","authors":["Sadi Alawadi","Khalid Alkharabsheh","Fahed Alkhabbas","Victor Kebande","Feras M. Awaysheh","Fabio Palomba","Mohammed Awad"],"pdf_url":"https://arxiv.org/pdf/2306.00038v3.pdf","comment":"17 pages, 7 figures, Journal paper"},{"id":"http://arxiv.org/abs/2403.14438v2","updated":"2024-03-26T11:02:32Z","published":"2024-03-21T14:44:03Z","title":"A Multimodal Approach to Device-Directed Speech Detection with Large\n  Language Models","summary":"  Interactions with virtual assistants typically start with a predefined\ntrigger phrase followed by the user command. To make interactions with the\nassistant more intuitive, we explore whether it is feasible to drop the\nrequirement that users must begin each command with a trigger phrase. We\nexplore this task in three ways: First, we train classifiers using only\nacoustic information obtained from the audio waveform. Second, we take the\ndecoder outputs of an automatic speech recognition (ASR) system, such as 1-best\nhypotheses, as input features to a large language model (LLM). Finally, we\nexplore a multimodal system that combines acoustic and lexical features, as\nwell as ASR decoder signals in an LLM. Using multimodal information yields\nrelative equal-error-rate improvements over text-only and audio-only models of\nup to 39% and 61%. Increasing the size of the LLM and training with low-rank\nadaption leads to further relative EER reductions of up to 18% on our dataset.\n","authors":["Dominik Wagner","Alexander Churchill","Siddharth Sigtia","Panayiotis Georgiou","Matt Mirsamadi","Aarshee Mishra","Erik Marchi"],"pdf_url":"https://arxiv.org/pdf/2403.14438v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2312.03632"},{"id":"http://arxiv.org/abs/2403.17592v1","updated":"2024-03-26T11:01:53Z","published":"2024-03-26T11:01:53Z","title":"On the Benefits of Over-parameterization for Out-of-Distribution\n  Generalization","summary":"  In recent years, machine learning models have achieved success based on the\nindependently and identically distributed assumption. However, this assumption\ncan be easily violated in real-world applications, leading to the\nOut-of-Distribution (OOD) problem. Understanding how modern over-parameterized\nDNNs behave under non-trivial natural distributional shifts is essential, as\ncurrent theoretical understanding is insufficient. Existing theoretical works\noften provide meaningless results for over-parameterized models in OOD\nscenarios or even contradict empirical findings. To this end, we are\ninvestigating the performance of the over-parameterized model in terms of OOD\ngeneralization under the general benign overfitting conditions. Our analysis\nfocuses on a random feature model and examines non-trivial natural\ndistributional shifts, where the benign overfitting estimators demonstrate a\nconstant excess OOD loss, despite achieving zero excess in-distribution (ID)\nloss. We demonstrate that in this scenario, further increasing the model's\nparameterization can significantly reduce the OOD loss. Intuitively, the\nvariance term of ID loss remains low due to orthogonality of long-tail\nfeatures, meaning overfitting noise during training generally doesn't raise\ntesting loss. However, in OOD cases, distributional shift increases the\nvariance term. Thankfully, the inherent shift is unrelated to individual x,\nmaintaining the orthogonality of long-tail features. Expanding the hidden\ndimension can additionally improve this orthogonality by mapping the features\ninto higher-dimensional spaces, thereby reducing the variance term. We further\nshow that model ensembles also improve OOD loss, akin to increasing model\ncapacity. These insights explain the empirical phenomenon of enhanced OOD\ngeneralization through model ensembles, supported by consistent simulations\nwith theoretical results.\n","authors":["Yifan Hao","Yong Lin","Difan Zou","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.17592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17588v1","updated":"2024-03-26T10:54:07Z","published":"2024-03-26T10:54:07Z","title":"Forest-ORE: Mining Optimal Rule Ensemble to interpret Random Forest\n  models","summary":"  Random Forest (RF) is well-known as an efficient ensemble learning method in\nterms of predictive performance. It is also considered a Black Box because of\nits hundreds of deep decision trees. This lack of interpretability can be a\nreal drawback for acceptance of RF models in several real-world applications,\nespecially those affecting one's lives, such as in healthcare, security, and\nlaw. In this work, we present Forest-ORE, a method that makes RF interpretable\nvia an optimized rule ensemble (ORE) for local and global interpretation.\nUnlike other rule-based approaches aiming at interpreting the RF model, this\nmethod simultaneously considers several parameters that influence the choice of\nan interpretable rule ensemble. Existing methods often prioritize predictive\nperformance over interpretability coverage and do not provide information about\nexisting overlaps or interactions between rules. Forest-ORE uses a\nmixed-integer optimization program to build an ORE that considers the trade-off\nbetween predictive performance, interpretability coverage, and model size (size\nof the rule ensemble, rule lengths, and rule overlaps). In addition to\nproviding an ORE competitive in predictive performance with RF, this method\nenriches the ORE through other rules that afford complementary information. It\nalso enables monitoring of the rule selection process and delivers various\nmetrics that can be used to generate a graphical representation of the final\nmodel. This framework is illustrated through an example, and its robustness is\nassessed through 36 benchmark datasets. A comparative analysis of well-known\nmethods shows that Forest-ORE provides an excellent trade-off between\npredictive performance, interpretability coverage, and model size.\n","authors":["Haddouchi Maissae","Berrado Abdelaziz"],"pdf_url":"https://arxiv.org/pdf/2403.17588v1.pdf","comment":"48 pages, 11 figures"},{"id":"http://arxiv.org/abs/2403.17589v1","updated":"2024-03-26T10:54:07Z","published":"2024-03-26T10:54:07Z","title":"Dual Memory Networks: A Versatile Adaptation Approach for\n  Vision-Language Models","summary":"  With the emergence of pre-trained vision-language models like CLIP, how to\nadapt them to various downstream classification tasks has garnered significant\nattention in recent research. The adaptation strategies can be typically\ncategorized into three paradigms: zero-shot adaptation, few-shot adaptation,\nand the recently-proposed training-free few-shot adaptation. Most existing\napproaches are tailored for a specific setting and can only cater to one or two\nof these paradigms. In this paper, we introduce a versatile adaptation approach\nthat can effectively work under all three settings. Specifically, we propose\nthe dual memory networks that comprise dynamic and static memory components.\nThe static memory caches training data knowledge, enabling training-free\nfew-shot adaptation, while the dynamic memory preserves historical test\nfeatures online during the testing process, allowing for the exploration of\nadditional data insights beyond the training set. This novel capability\nenhances model performance in the few-shot setting and enables model usability\nin the absence of training data. The two memory networks employ the same\nflexible memory interactive strategy, which can operate in a training-free mode\nand can be further enhanced by incorporating learnable projection layers. Our\napproach is tested across 11 datasets under the three task settings.\nRemarkably, in the zero-shot scenario, it outperforms existing methods by over\n3\\% and even shows superior results against methods utilizing external training\ndata. Additionally, our method exhibits robust performance against natural\ndistribution shifts. Codes are available at \\url{https://github.com/YBZh/DMN}.\n","authors":["Yabin Zhang","Wenjie Zhu","Hui Tang","Zhiyuan Ma","Kaiyang Zhou","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.17589v1.pdf","comment":"CVPR2024; Codes are available at \\url{https://github.com/YBZh/DMN}"},{"id":"http://arxiv.org/abs/2403.17582v1","updated":"2024-03-26T10:45:11Z","published":"2024-03-26T10:45:11Z","title":"Towards a Zero-Data, Controllable, Adaptive Dialog System","summary":"  Conversational Tree Search (V\\\"ath et al., 2023) is a recent approach to\ncontrollable dialog systems, where domain experts shape the behavior of a\nReinforcement Learning agent through a dialog tree. The agent learns to\nefficiently navigate this tree, while adapting to information needs, e.g.,\ndomain familiarity, of different users. However, the need for additional\ntraining data hinders deployment in new domains. To address this, we explore\napproaches to generate this data directly from dialog trees. We improve the\noriginal approach, and show that agents trained on synthetic data can achieve\ncomparable dialog success to models trained on human data, both when using a\ncommercial Large Language Model for generation, or when using a smaller\nopen-source model, running on a single GPU. We further demonstrate the\nscalability of our approach by collecting and testing on two new datasets:\nONBOARD, a new domain helping foreign residents moving to a new city, and the\nmedical domain DIAGNOSE, a subset of Wikipedia articles related to scalp and\nhead symptoms. Finally, we perform human testing, where no statistically\nsignificant differences were found in either objective or subjective measures\nbetween models trained on human and generated data.\n","authors":["Dirk Väth","Lindsey Vanderlyn","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2403.17582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.07773v2","updated":"2024-03-26T10:34:14Z","published":"2022-04-16T10:27:23Z","title":"FedCau: A Proactive Stop Policy for Communication and Computation\n  Efficient Federated Learning","summary":"  This paper investigates efficient distributed training of a Federated\nLearning~(FL) model over a wireless network of wireless devices. The\ncommunication iterations of the distributed training algorithm may be\nsubstantially deteriorated or even blocked by the effects of the devices'\nbackground traffic, packet losses, congestion, or latency. We abstract the\ncommunication-computation impacts as an `iteration cost' and propose a\ncost-aware causal FL algorithm~(FedCau) to tackle this problem. We propose an\niteration-termination method that trade-offs the training performance and\nnetworking costs. We apply our approach when clients use the slotted-ALOHA, the\ncarrier-sense multiple access with collision avoidance~(CSMA/CA), and the\northogonal frequency-division multiple access~(OFDMA) protocols. We show that,\ngiven a total cost budget, the training performance degrades as either the\nbackground communication traffic or the dimension of the training problem\nincreases. Our results demonstrate the importance of proactively designing\noptimal cost-efficient stopping criteria to avoid unnecessary\ncommunication-computation costs to achieve only a marginal FL training\nimprovement. We validate our method by training and testing FL over the MNIST\ndataset. Finally, we apply our approach to existing communication efficient FL\nmethods from the literature, achieving further efficiency. We conclude that\ncost-efficient stopping criteria are essential for the success of practical FL\nover wireless networks.\n","authors":["Afsaneh Mahmoudi","Hossein S. Ghadikolaei","José Mairton Barros Da Silva Júnior","Carlo Fischione"],"pdf_url":"https://arxiv.org/pdf/2204.07773v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17572v1","updated":"2024-03-26T10:25:21Z","published":"2024-03-26T10:25:21Z","title":"Enhancing Privacy in Federated Learning through Local Training","summary":"  In this paper we propose the federated private local training algorithm\n(Fed-PLT) for federated learning, to overcome the challenges of (i) expensive\ncommunications and (ii) privacy preservation. We address (i) by allowing for\nboth partial participation and local training, which significantly reduce the\nnumber of communication rounds between the central coordinator and computing\nagents. The algorithm matches the state of the art in the sense that the use of\nlocal training demonstrably does not impact accuracy. Additionally, agents have\nthe flexibility to choose from various local training solvers, such as\n(stochastic) gradient descent and accelerated gradient descent. Further, we\ninvestigate how employing local training can enhance privacy, addressing point\n(ii). In particular, we derive differential privacy bounds and highlight their\ndependence on the number of local training epochs. We assess the effectiveness\nof the proposed algorithm by comparing it to alternative techniques,\nconsidering both theoretical analysis and numerical results from a\nclassification task.\n","authors":["Nicola Bastianello","Changxin Liu","Karl H. Johansson"],"pdf_url":"https://arxiv.org/pdf/2403.17572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03611v2","updated":"2024-03-26T10:13:11Z","published":"2023-12-06T16:55:53Z","title":"DreamComposer: Controllable 3D Object Generation via Multi-View\n  Conditions","summary":"  Utilizing pre-trained 2D large-scale generative models, recent works are\ncapable of generating high-quality novel views from a single in-the-wild image.\nHowever, due to the lack of information from multiple views, these works\nencounter difficulties in generating controllable novel views. In this paper,\nwe present DreamComposer, a flexible and scalable framework that can enhance\nexisting view-aware diffusion models by injecting multi-view conditions.\nSpecifically, DreamComposer first uses a view-aware 3D lifting module to obtain\n3D representations of an object from multiple views. Then, it renders the\nlatent features of the target view from 3D representations with the multi-view\nfeature fusion module. Finally the target view features extracted from\nmulti-view inputs are injected into a pre-trained diffusion model. Experiments\nshow that DreamComposer is compatible with state-of-the-art diffusion models\nfor zero-shot novel view synthesis, further enhancing them to generate\nhigh-fidelity novel view images with multi-view conditions, ready for\ncontrollable 3D object reconstruction and various other applications.\n","authors":["Yunhan Yang","Yukun Huang","Xiaoyang Wu","Yuan-Chen Guo","Song-Hai Zhang","Hengshuang Zhao","Tong He","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2312.03611v2.pdf","comment":"Project Page: https://yhyang-myron.github.io/DreamComposer/"},{"id":"http://arxiv.org/abs/2403.17561v1","updated":"2024-03-26T10:10:53Z","published":"2024-03-26T10:10:53Z","title":"A Survey on Deep Learning and State-of-the-arts Applications","summary":"  Deep learning, a branch of artificial intelligence, is a computational model\nthat uses multiple layers of interconnected units (neurons) to learn intricate\npatterns and representations directly from raw input data. Empowered by this\nlearning capability, it has become a powerful tool for solving complex problems\nand is the core driver of many groundbreaking technologies and innovations.\nBuilding a deep learning model is a challenging task due to the algorithm`s\ncomplexity and the dynamic nature of real-world problems. Several studies have\nreviewed deep learning concepts and applications. However, the studies mostly\nfocused on the types of deep learning models and convolutional neural network\narchitectures, offering limited coverage of the state-of-the-art of deep\nlearning models and their applications in solving complex problems across\ndifferent domains. Therefore, motivated by the limitations, this study aims to\ncomprehensively review the state-of-the-art deep learning models in computer\nvision, natural language processing, time series analysis and pervasive\ncomputing. We highlight the key features of the models and their effectiveness\nin solving the problems within each domain. Furthermore, this study presents\nthe fundamentals of deep learning, various deep learning model types and\nprominent convolutional neural network architectures. Finally, challenges and\nfuture directions in deep learning research are discussed to offer a broader\nperspective for future researchers.\n","authors":["Mohd Halim Mohd Noor","Ayokunle Olalekan Ige"],"pdf_url":"https://arxiv.org/pdf/2403.17561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17550v1","updated":"2024-03-26T09:58:06Z","published":"2024-03-26T09:58:06Z","title":"DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping","summary":"  Recently, significant progress has been achieved in sensing real large-scale\noutdoor 3D environments, particularly by using modern acquisition equipment\nsuch as LiDAR sensors. Unfortunately, they are fundamentally limited in their\nability to produce dense, complete 3D scenes. To address this issue, recent\nlearning-based methods integrate neural implicit representations and\noptimizable feature grids to approximate surfaces of 3D scenes. However,\nnaively fitting samples along raw LiDAR rays leads to noisy 3D mapping results\ndue to the nature of sparse, conflicting LiDAR measurements. Instead, in this\nwork we depart from fitting LiDAR data exactly, instead letting the network\noptimize a non-metric monotonic implicit field defined in 3D space. To fit our\nfield, we design a learning system integrating a monotonicity loss that enables\noptimizing neural monotonic fields and leverages recent progress in large-scale\n3D mapping. Our algorithm achieves high-quality dense 3D mapping performance as\ncaptured by multiple quantitative and perceptual measures and visual results\nobtained for Mai City, Newer College, and KITTI benchmarks. The code of our\napproach will be made publicly available.\n","authors":["Kutay Yılmaz","Matthias Nießner","Anastasiia Kornilova","Alexey Artemov"],"pdf_url":"https://arxiv.org/pdf/2403.17550v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2307.01050v6","updated":"2024-03-26T09:48:49Z","published":"2023-07-03T14:28:36Z","title":"Transport meets Variational Inference: Controlled Monte Carlo Diffusions","summary":"  Connecting optimal transport and variational inference, we present a\nprincipled and systematic framework for sampling and generative modelling\ncentred around divergences on path space. Our work culminates in the\ndevelopment of the \\emph{Controlled Monte Carlo Diffusion} sampler (CMCD) for\nBayesian computation, a score-based annealing technique that crucially adapts\nboth forward and backward dynamics in a diffusion model. On the way, we clarify\nthe relationship between the EM-algorithm and iterative proportional fitting\n(IPF) for Schr{\\\"o}dinger bridges, deriving as well a regularised objective\nthat bypasses the iterative bottleneck of standard IPF-updates. Finally, we\nshow that CMCD has a strong foundation in the Jarzinsky and Crooks identities\nfrom statistical physics, and that it convincingly outperforms competing\napproaches across a wide array of experiments.\n","authors":["Francisco Vargas","Shreyas Padhy","Denis Blessing","Nikolas Nüsken"],"pdf_url":"https://arxiv.org/pdf/2307.01050v6.pdf","comment":"Workshop on New Frontiers in Learning, Control, and Dynamical Systems\n  at the International Conference on Machine Learning (ICML), Honolulu, Hawaii,\n  USA, 2023"},{"id":"http://arxiv.org/abs/2403.17542v1","updated":"2024-03-26T09:44:57Z","published":"2024-03-26T09:44:57Z","title":"VDSC: Enhancing Exploration Timing with Value Discrepancy and State\n  Counts","summary":"  Despite the considerable attention given to the questions of \\textit{how\nmuch} and \\textit{how to} explore in deep reinforcement learning, the\ninvestigation into \\textit{when} to explore remains relatively less researched.\nWhile more sophisticated exploration strategies can excel in specific, often\nsparse reward environments, existing simpler approaches, such as\n$\\epsilon$-greedy, persist in outperforming them across a broader spectrum of\ndomains. The appeal of these simpler strategies lies in their ease of\nimplementation and generality across a wide range of domains. The downside is\nthat these methods are essentially a blind switching mechanism, which\ncompletely disregards the agent's internal state. In this paper, we propose to\nleverage the agent's internal state to decide \\textit{when} to explore,\naddressing the shortcomings of blind switching mechanisms. We present Value\nDiscrepancy and State Counts through homeostasis (VDSC), a novel approach for\nefficient exploration timing. Experimental results on the Atari suite\ndemonstrate the superiority of our strategy over traditional methods such as\n$\\epsilon$-greedy and Boltzmann, as well as more sophisticated techniques like\nNoisy Nets.\n","authors":["Marius Captari","Remo Sasso","Matthia Sabatelli"],"pdf_url":"https://arxiv.org/pdf/2403.17542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17533v1","updated":"2024-03-26T09:39:21Z","published":"2024-03-26T09:39:21Z","title":"BVR Gym: A Reinforcement Learning Environment for Beyond-Visual-Range\n  Air Combat","summary":"  Creating new air combat tactics and discovering novel maneuvers can require\nnumerous hours of expert pilots' time. Additionally, for each different combat\nscenario, the same strategies may not work since small changes in equipment\nperformance may drastically change the air combat outcome. For this reason, we\ncreated a reinforcement learning environment to help investigate potential air\ncombat tactics in the field of beyond-visual-range (BVR) air combat: the BVR\nGym. This type of air combat is important since long-range missiles are often\nthe first weapon to be used in aerial combat. Some existing environments\nprovide high-fidelity simulations but are either not open source or are not\nadapted to the BVR air combat domain. Other environments are open source but\nuse less accurate simulation models. Our work provides a high-fidelity\nenvironment based on the open-source flight dynamics simulator JSBSim and is\nadapted to the BVR air combat domain. This article describes the building\nblocks of the environment and some use cases.\n","authors":["Edvards Scukins","Markus Klein","Lars Kroon","Petter Ögren"],"pdf_url":"https://arxiv.org/pdf/2403.17533v1.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2207.12730v2","updated":"2024-03-26T09:35:03Z","published":"2022-07-26T08:34:17Z","title":"P2ANet: A Dataset and Benchmark for Dense Action Detection from Table\n  Tennis Match Broadcasting Videos","summary":"  While deep learning has been widely used for video analytics, such as video\nclassification and action detection, dense action detection with fast-moving\nsubjects from sports videos is still challenging. In this work, we release yet\nanother sports video benchmark \\TheName{} for \\emph{\\underline{P}}ing\n\\emph{\\underline{P}}ong-\\emph{\\underline{A}}ction detection, which consists of\n2,721 video clips collected from the broadcasting videos of professional table\ntennis matches in World Table Tennis Championships and Olympiads. We work with\na crew of table tennis professionals and referees on a specially designed\nannotation toolbox to obtain fine-grained action labels (in 14 classes) for\nevery ping-pong action that appeared in the dataset, and formulate two sets of\naction detection problems -- \\emph{action localization} and \\emph{action\nrecognition}. We evaluate a number of commonly-seen action recognition (e.g.,\nTSM, TSN, Video SwinTransformer, and Slowfast) and action localization models\n(e.g., BSN, BSN++, BMN, TCANet), using \\TheName{} for both problems, under\nvarious settings. These models can only achieve 48\\% area under the AR-AN curve\nfor localization and 82\\% top-one accuracy for recognition since the ping-pong\nactions are dense with fast-moving subjects but broadcasting videos are with\nonly 25 FPS. The results confirm that \\TheName{} is still a challenging task\nand can be used as a special benchmark for dense action detection from videos.\n","authors":["Jiang Bian","Xuhong Li","Tao Wang","Qingzhong Wang","Jun Huang","Chen Liu","Jun Zhao","Feixiang Lu","Dejing Dou","Haoyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2207.12730v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17520v1","updated":"2024-03-26T09:22:37Z","published":"2024-03-26T09:22:37Z","title":"Boosting Adversarial Training via Fisher-Rao Norm-based Regularization","summary":"  Adversarial training is extensively utilized to improve the adversarial\nrobustness of deep neural networks. Yet, mitigating the degradation of standard\ngeneralization performance in adversarial-trained models remains an open\nproblem. This paper attempts to resolve this issue through the lens of model\ncomplexity. First, We leverage the Fisher-Rao norm, a geometrically invariant\nmetric for model complexity, to establish the non-trivial bounds of the\nCross-Entropy Loss-based Rademacher complexity for a ReLU-activated Multi-Layer\nPerceptron. Then we generalize a complexity-related variable, which is\nsensitive to the changes in model width and the trade-off factors in\nadversarial training. Moreover, intensive empirical evidence validates that\nthis variable highly correlates with the generalization gap of Cross-Entropy\nloss between adversarial-trained and standard-trained models, especially during\nthe initial and final phases of the training process. Building upon this\nobservation, we propose a novel regularization framework, called Logit-Oriented\nAdversarial Training (LOAT), which can mitigate the trade-off between\nrobustness and accuracy while imposing only a negligible increase in\ncomputational overhead. Our extensive experiments demonstrate that the proposed\nregularization strategy can boost the performance of the prevalent adversarial\ntraining algorithms, including PGD-AT, TRADES, TRADES (LSE), MART, and DM-AT,\nacross various network architectures. Our code will be available at\nhttps://github.com/TrustAI/LOAT.\n","authors":["Xiangyu Yin","Wenjie Ruan"],"pdf_url":"https://arxiv.org/pdf/2403.17520v1.pdf","comment":"This paper has been accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2403.17515v1","updated":"2024-03-26T09:18:50Z","published":"2024-03-26T09:18:50Z","title":"Prediction-sharing During Training and Inference","summary":"  Two firms are engaged in a competitive prediction task. Each firm has two\nsources of data -- labeled historical data and unlabeled inference-time data --\nand uses the former to derive a prediction model, and the latter to make\npredictions on new instances. We study data-sharing contracts between the\nfirms. The novelty of our study is to introduce and highlight the differences\nbetween contracts that share prediction models only, contracts to share\ninference-time predictions only, and contracts to share both. Our analysis\nproceeds on three levels. First, we develop a general Bayesian framework that\nfacilitates our study. Second, we narrow our focus to two natural settings\nwithin this framework: (i) a setting in which the accuracy of each firm's\nprediction model is common knowledge, but the correlation between the\nrespective models is unknown; and (ii) a setting in which two hypotheses exist\nregarding the optimal predictor, and one of the firms has a structural\nadvantage in deducing it. Within these two settings we study optimal contract\nchoice. More specifically, we find the individually rational and Pareto-optimal\ncontracts for some notable cases, and describe specific settings where each of\nthe different sharing contracts emerge as optimal. Finally, in the third level\nof our analysis we demonstrate the applicability of our concepts in a synthetic\nsimulation using real loan data.\n","authors":["Yotam Gafni","Ronen Gradwohl","Moshe Tennenholtz"],"pdf_url":"https://arxiv.org/pdf/2403.17515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17507v1","updated":"2024-03-26T09:09:40Z","published":"2024-03-26T09:09:40Z","title":"EL-MLFFs: Ensemble Learning of Machine Leaning Force Fields","summary":"  Machine learning force fields (MLFFs) have emerged as a promising approach to\nbridge the accuracy of quantum mechanical methods and the efficiency of\nclassical force fields. However, the abundance of MLFF models and the challenge\nof accurately predicting atomic forces pose significant obstacles in their\npractical application. In this paper, we propose a novel ensemble learning\nframework, EL-MLFFs, which leverages the stacking method to integrate\npredictions from diverse MLFFs and enhance force prediction accuracy. By\nconstructing a graph representation of molecular structures and employing a\ngraph neural network (GNN) as the meta-model, EL-MLFFs effectively captures\natomic interactions and refines force predictions. We evaluate our approach on\ntwo distinct datasets: methane molecules and methanol adsorbed on a Cu(100)\nsurface. The results demonstrate that EL-MLFFs significantly improves force\nprediction accuracy compared to individual MLFFs, with the ensemble of all\neight models yielding the best performance. Moreover, our ablation study\nhighlights the crucial roles of the residual network and graph attention layers\nin the model's architecture. The EL-MLFFs framework offers a promising solution\nto the challenges of model selection and force prediction accuracy in MLFFs,\npaving the way for more reliable and efficient molecular simulations.\n","authors":["Bangchen Yin","Yue Yin","Yuda W. Tang","Hai Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.17507v1.pdf","comment":"12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.17503v1","updated":"2024-03-26T09:04:18Z","published":"2024-03-26T09:04:18Z","title":"DS-AL: A Dual-Stream Analytic Learning for Exemplar-Free\n  Class-Incremental Learning","summary":"  Class-incremental learning (CIL) under an exemplar-free constraint has\npresented a significant challenge. Existing methods adhering to this constraint\nare prone to catastrophic forgetting, far more so than replay-based techniques\nthat retain access to past samples. In this paper, to solve the exemplar-free\nCIL problem, we propose a Dual-Stream Analytic Learning (DS-AL) approach. The\nDS-AL contains a main stream offering an analytical (i.e., closed-form) linear\nsolution, and a compensation stream improving the inherent under-fitting\nlimitation due to adopting linear mapping. The main stream redefines the CIL\nproblem into a Concatenated Recursive Least Squares (C-RLS) task, allowing an\nequivalence between the CIL and its joint-learning counterpart. The\ncompensation stream is governed by a Dual-Activation Compensation (DAC) module.\nThis module re-activates the embedding with a different activation function\nfrom the main stream one, and seeks fitting compensation by projecting the\nembedding to the null space of the main stream's linear mapping. Empirical\nresults demonstrate that the DS-AL, despite being an exemplar-free technique,\ndelivers performance comparable with or better than that of replay-based\nmethods across various datasets, including CIFAR-100, ImageNet-100 and\nImageNet-Full. Additionally, the C-RLS' equivalent property allows the DS-AL to\nexecute CIL in a phase-invariant manner. This is evidenced by a\nnever-before-seen 500-phase CIL ImageNet task, which performs on a level\nidentical to a 5-phase one. Our codes are available at\nhttps://github.com/ZHUANGHP/Analytic-continual-learning.\n","authors":["Huiping Zhuang","Run He","Kai Tong","Ziqian Zeng","Cen Chen","Zhiping Lin"],"pdf_url":"https://arxiv.org/pdf/2403.17503v1.pdf","comment":"Accepted in AAAI 2024"},{"id":"http://arxiv.org/abs/2403.17500v1","updated":"2024-03-26T08:59:37Z","published":"2024-03-26T08:59:37Z","title":"Variational Graph Auto-Encoder Based Inductive Learning Method for\n  Semi-Supervised Classification","summary":"  Graph representation learning is a fundamental research issue in various\ndomains of applications, of which the inductive learning problem is\nparticularly challenging as it requires models to generalize to unseen graph\nstructures during inference. In recent years, graph neural networks (GNNs) have\nemerged as powerful graph models for inductive learning tasks such as node\nclassification, whereas they typically heavily rely on the annotated nodes\nunder a fully supervised training setting. Compared with the GNN-based methods,\nvariational graph auto-encoders (VGAEs) are known to be more generalizable to\ncapture the internal structural information of graphs independent of node\nlabels and have achieved prominent performance on multiple unsupervised\nlearning tasks. However, so far there is still a lack of work focusing on\nleveraging the VGAE framework for inductive learning, due to the difficulties\nin training the model in a supervised manner and avoiding over-fitting the\nproximity information of graphs. To solve these problems and improve the model\nperformance of VGAEs for inductive graph representation learning, in this work,\nwe propose the Self-Label Augmented VGAE model. To leverage the label\ninformation for training, our model takes node labels as one-hot encoded inputs\nand then performs label reconstruction in model training. To overcome the\nscarcity problem of node labels for semi-supervised settings, we further\npropose the Self-Label Augmentation Method (SLAM), which uses pseudo labels\ngenerated by our model with a node-wise masking approach to enhance the label\ninformation. Experiments on benchmark inductive learning graph datasets verify\nthat our proposed model archives promising results on node classification with\nparticular superiority under semi-supervised learning settings.\n","authors":["Hanxuan Yang","Zhaoxin Yu","Qingchao Kong","Wei Liu","Wenji Mao"],"pdf_url":"https://arxiv.org/pdf/2403.17500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.10891v3","updated":"2024-03-26T08:50:19Z","published":"2023-02-06T10:08:42Z","title":"An Implicit GNN Solver for Poisson-like problems","summary":"  This paper presents $\\Psi$-GNN, a novel Graph Neural Network (GNN) approach\nfor solving the ubiquitous Poisson PDE problems with mixed boundary conditions.\nBy leveraging the Implicit Layer Theory, $\\Psi$-GNN models an \"infinitely\" deep\nnetwork, thus avoiding the empirical tuning of the number of required Message\nPassing layers to attain the solution. Its original architecture explicitly\ntakes into account the boundary conditions, a critical prerequisite for\nphysical applications, and is able to adapt to any initially provided solution.\n$\\Psi$-GNN is trained using a \"physics-informed\" loss, and the training process\nis stable by design, and insensitive to its initialization. Furthermore, the\nconsistency of the approach is theoretically proven, and its flexibility and\ngeneralization efficiency are experimentally demonstrated: the same learned\nmodel can accurately handle unstructured meshes of various sizes, as well as\ndifferent boundary conditions. To the best of our knowledge, $\\Psi$-GNN is the\nfirst physics-informed GNN-based method that can handle various unstructured\ndomains, boundary conditions and initial solutions while also providing\nconvergence guarantees.\n","authors":["Matthieu Nastorg","Michele Alessandro Bucci","Thibault Faney","Jean-Marc Gratien","Guillaume Charpiat","Marc Schoenauer"],"pdf_url":"https://arxiv.org/pdf/2302.10891v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.13143v2","updated":"2024-03-26T08:36:47Z","published":"2023-02-25T19:11:44Z","title":"Ensemble learning for Physics Informed Neural Networks: a Gradient\n  Boosting approach","summary":"  While the popularity of physics-informed neural networks (PINNs) is steadily\nrising, to this date, PINNs have not been successful in simulating multi-scale\nand singular perturbation problems. In this work, we present a new training\nparadigm referred to as \"gradient boosting\" (GB), which significantly enhances\nthe performance of physics informed neural networks (PINNs). Rather than\nlearning the solution of a given PDE using a single neural network directly,\nour algorithm employs a sequence of neural networks to achieve a superior\noutcome. This approach allows us to solve problems presenting great challenges\nfor traditional PINNs. Our numerical experiments demonstrate the effectiveness\nof our algorithm through various benchmarks, including comparisons with finite\nelement methods and PINNs. Furthermore, this work also unlocks the door to\nemploying ensemble learning techniques in PINNs, providing opportunities for\nfurther improvement in solving PDEs.\n","authors":["Zhiwei Fang","Sifan Wang","Paris Perdikaris"],"pdf_url":"https://arxiv.org/pdf/2302.13143v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17480v1","updated":"2024-03-26T08:22:09Z","published":"2024-03-26T08:22:09Z","title":"Capacity Provisioning Motivated Online Non-Convex Optimization Problem\n  with Memory and Switching Cost","summary":"  An online non-convex optimization problem is considered where the goal is to\nminimize the flow time (total delay) of a set of jobs by modulating the number\nof active servers, but with a switching cost associated with changing the\nnumber of active servers over time. Each job can be processed by at most one\nfixed speed server at any time. Compared to the usual online convex\noptimization (OCO) problem with switching cost, the objective function\nconsidered is non-convex and more importantly, at each time, it depends on all\npast decisions and not just the present one. Both worst-case and stochastic\ninputs are considered; for both cases, competitive algorithms are derived.\n","authors":["Rahul Vaze","Jayakrishnan Nair"],"pdf_url":"https://arxiv.org/pdf/2403.17480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2102.01432v2","updated":"2024-03-26T08:21:24Z","published":"2021-02-02T11:05:34Z","title":"Bayesian data-driven discovery of partial differential equations with\n  variable coefficients","summary":"  The discovery of Partial Differential Equations (PDEs) is an essential task\nfor applied science and engineering. However, data-driven discovery of PDEs is\ngenerally challenging, primarily stemming from the sensitivity of the\ndiscovered equation to noise and the complexities of model selection. In this\nwork, we propose an advanced Bayesian sparse learning algorithm for PDE\ndiscovery with variable coefficients, predominantly when the coefficients are\nspatially or temporally dependent. Specifically, we apply threshold Bayesian\ngroup Lasso regression with a spike-and-slab prior (tBGL-SS) and leverage a\nGibbs sampler for Bayesian posterior estimation of PDE coefficients. This\napproach not only enhances the robustness of point estimation with valid\nuncertainty quantification but also relaxes the computational burden from\nBayesian inference through the integration of coefficient thresholds as an\napproximate MCMC method. Moreover, from the quantified uncertainties, we\npropose a Bayesian total error bar criteria for model selection, which\noutperforms classic metrics including the root mean square and the Akaike\ninformation criterion. The capability of this method is illustrated by the\ndiscovery of several classical benchmark PDEs with spatially or temporally\nvarying coefficients from solution data obtained from the reference\nsimulations. In the experiments, we show that the tBGL-SS method is more robust\nthan the baseline methods under noisy environments and provides better model\nselection criteria along the regularization path.\n","authors":["Aoxue Chen","Yifan Du","Liyao Mars Gao","Guang Lin"],"pdf_url":"https://arxiv.org/pdf/2102.01432v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17479v1","updated":"2024-03-26T08:19:29Z","published":"2024-03-26T08:19:29Z","title":"Natural Language Requirements Testability Measurement Based on\n  Requirement Smells","summary":"  Requirements form the basis for defining software systems' obligations and\ntasks. Testable requirements help prevent failures, reduce maintenance costs,\nand make it easier to perform acceptance tests. However, despite the importance\nof measuring and quantifying requirements testability, no automatic approach\nfor measuring requirements testability has been proposed based on the\nrequirements smells, which are at odds with the requirements testability. This\npaper presents a mathematical model to evaluate and rank the natural language\nrequirements testability based on an extensive set of nine requirements smells,\ndetected automatically, and acceptance test efforts determined by requirement\nlength and its application domain. Most of the smells stem from uncountable\nadjectives, context-sensitive, and ambiguous words. A comprehensive dictionary\nis required to detect such words. We offer a neural word-embedding technique to\ngenerate such a dictionary automatically. Using the dictionary, we could\nautomatically detect Polysemy smell (domain-specific ambiguity) for the first\ntime in 10 application domains. Our empirical study on nearly 1000 software\nrequirements from six well-known industrial and academic projects demonstrates\nthat the proposed smell detection approach outperforms Smella, a\nstate-of-the-art tool, in detecting requirements smells. The precision and\nrecall of smell detection are improved with an average of 0.03 and 0.33,\nrespectively, compared to the state-of-the-art. The proposed requirement\ntestability model measures the testability of 985 requirements with a mean\nabsolute error of 0.12 and a mean squared error of 0.03, demonstrating the\nmodel's potential for practical use.\n","authors":["Morteza Zakeri-Nasrabadi","Saeed Parsa"],"pdf_url":"https://arxiv.org/pdf/2403.17479v1.pdf","comment":"45 pages, 16 figures, and 13 tables; submitted as a journal paper"},{"id":"http://arxiv.org/abs/2311.08744v2","updated":"2024-03-26T08:14:22Z","published":"2023-11-15T07:25:14Z","title":"Graph Signal Diffusion Model for Collaborative Filtering","summary":"  Collaborative filtering is a critical technique in recommender systems. Among\nvarious methods, an increasingly popular paradigm is to reconstruct user-item\ninteractions based on the historical observations. This can be viewed as a\nconditional generative task, where recently developed diffusion model\ndemonstrates great potential. However, existing studies on diffusion models\nlack effective solutions for modeling implicit feedback data. Particularly, the\nisotropic nature of the standard diffusion process fails to account for the\nheterogeneous dependencies among items, leading to a misalignment with the\ngraphical structure of the interaction space. Meanwhile, random noise\ndestroying personalized information in interaction vectors, causing difficulty\nin reverse reconstruction. In this paper, we make novel adaptions of diffusion\nmodel and propose Graph Signal Diffusion Model for Collaborative Filtering\n(named GiffCF). To better represent the high-dimensional and sparse\ndistribution of implicit feedback, we define a generalized form of denoising\ndiffusion using heat equation on the item-item similarity graph. Our forward\nprocess smooths interaction signals with an advanced family of graph filters.\nHence, instead of losing information, it involves item-item similarities as\nbeneficial prior knowledge for recommendation. To reconstruct high-quality\ninteractions, our reverse process iteratively refines and sharpens preference\nsignals in a deterministic manner, where the update direction is conditioned on\nthe user history and computed from a carefully designed two-stage denoiser.\nFinally, through extensive experiments, we show that GiffCF effectively\nleverages the advantages of both diffusion model and graph signal processing,\nand achieves state-of-the-art performance on three benchmark datasets.\n","authors":["Yunqin Zhu","Chao Wang","Qi Zhang","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2311.08744v2.pdf","comment":"11 pages, 8 figures, Accepted by SIGIR 2024"},{"id":"http://arxiv.org/abs/2311.02766v3","updated":"2024-03-26T08:05:11Z","published":"2023-11-05T20:51:03Z","title":"Riemannian Laplace Approximation with the Fisher Metric","summary":"  Laplace's method approximates a target density with a Gaussian distribution\nat its mode. It is computationally efficient and asymptotically exact for\nBayesian inference due to the Bernstein-von Mises theorem, but for complex\ntargets and finite-data posteriors it is often too crude an approximation. A\nrecent generalization of the Laplace Approximation transforms the Gaussian\napproximation according to a chosen Riemannian geometry providing a richer\napproximation family, while still retaining computational efficiency. However,\nas shown here, its properties depend heavily on the chosen metric, indeed the\nmetric adopted in previous work results in approximations that are overly\nnarrow as well as being biased even at the limit of infinite data. We correct\nthis shortcoming by developing the approximation family further, deriving two\nalternative variants that are exact at the limit of infinite data, extending\nthe theoretical analysis of the method, and demonstrating practical\nimprovements in a range of experiments.\n","authors":["Hanlin Yu","Marcelo Hartmann","Bernardo Williams","Mark Girolami","Arto Klami"],"pdf_url":"https://arxiv.org/pdf/2311.02766v3.pdf","comment":"AISTATS 2024, with additional fixes"},{"id":"http://arxiv.org/abs/2403.16861v2","updated":"2024-03-26T07:56:21Z","published":"2024-03-25T15:26:10Z","title":"DISL: Fueling Research with A Large Dataset of Solidity Smart Contracts","summary":"  The DISL dataset features a collection of $514,506$ unique Solidity files\nthat have been deployed to Ethereum mainnet. It caters to the need for a large\nand diverse dataset of real-world smart contracts. DISL serves as a resource\nfor developing machine learning systems and for benchmarking software\nengineering tools designed for smart contracts. By aggregating every verified\nsmart contract from Etherscan up to January 15, 2024, DISL surpasses existing\ndatasets in size and recency.\n","authors":["Gabriele Morello","Mojtaba Eshghie","Sofia Bobadilla","Martin Monperrus"],"pdf_url":"https://arxiv.org/pdf/2403.16861v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17467v1","updated":"2024-03-26T07:55:45Z","published":"2024-03-26T07:55:45Z","title":"A Unified Kernel for Neural Network Learning","summary":"  Past decades have witnessed a great interest in the distinction and\nconnection between neural network learning and kernel learning. Recent\nadvancements have made theoretical progress in connecting infinite-wide neural\nnetworks and Gaussian processes. Two predominant approaches have emerged: the\nNeural Network Gaussian Process (NNGP) and the Neural Tangent Kernel (NTK). The\nformer, rooted in Bayesian inference, represents a zero-order kernel, while the\nlatter, grounded in the tangent space of gradient descents, is a first-order\nkernel. In this paper, we present the Unified Neural Kernel (UNK), which\ncharacterizes the learning dynamics of neural networks with gradient descents\nand parameter initialization. The proposed UNK kernel maintains the limiting\nproperties of both NNGP and NTK, exhibiting behaviors akin to NTK with a finite\nlearning step and converging to NNGP as the learning step approaches infinity.\nBesides, we also theoretically characterize the uniform tightness and learning\nconvergence of the UNK kernel, providing comprehensive insights into this\nunified kernel. Experimental results underscore the effectiveness of our\nproposed method.\n","authors":["Shao-Qun Zhang","Zong-Yi Chen","Yong-Ming Tian","Xun Lu"],"pdf_url":"https://arxiv.org/pdf/2403.17467v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12086v2","updated":"2024-03-26T07:54:02Z","published":"2023-11-20T13:45:21Z","title":"Masked Autoencoders Are Robust Neural Architecture Search Learners","summary":"  Neural Architecture Search (NAS) currently relies heavily on labeled data,\nwhich is both expensive and time-consuming to acquire. In this paper, we\npropose a novel NAS framework based on Masked Autoencoders (MAE) that\neliminates the need for labeled data during the search process. By replacing\nthe supervised learning objective with an image reconstruction task, our\napproach enables the robust discovery of network architectures without\ncompromising performance and generalization ability. Additionally, we address\nthe problem of performance collapse encountered in the widely-used\nDifferentiable Architecture Search (DARTS) method in the unsupervised paradigm\nby introducing a multi-scale decoder. Through extensive experiments conducted\non various search spaces and datasets, we demonstrate the effectiveness and\nrobustness of the proposed method, providing empirical evidence of its\nsuperiority over baseline approaches.\n","authors":["Yiming Hu","Xiangxiang Chu","Bo Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.12086v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17458v1","updated":"2024-03-26T07:46:27Z","published":"2024-03-26T07:46:27Z","title":"Expectations Versus Reality: Evaluating Intrusion Detection Systems in\n  Practice","summary":"  Our paper provides empirical comparisons between recent IDSs to provide an\nobjective comparison between them to help users choose the most appropriate\nsolution based on their requirements. Our results show that no one solution is\nthe best, but is dependent on external variables such as the types of attacks,\ncomplexity, and network environment in the dataset. For example, BoT_IoT and\nStratosphere IoT datasets both capture IoT-related attacks, but the deep neural\nnetwork performed the best when tested using the BoT_IoT dataset while HELAD\nperformed the best when tested using the Stratosphere IoT dataset. So although\nwe found that a deep neural network solution had the highest average F1 scores\non tested datasets, it is not always the best-performing one. We further\ndiscuss difficulties in using IDS from literature and project repositories,\nwhich complicated drawing definitive conclusions regarding IDS selection.\n","authors":["Jake Hesford","Daniel Cheng","Alan Wan","Larry Huynh","Seungho Kim","Hyoungshick Kim","Jin B. Hong"],"pdf_url":"https://arxiv.org/pdf/2403.17458v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2308.07728v5","updated":"2024-03-26T07:43:08Z","published":"2023-08-15T12:08:43Z","title":"Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability","summary":"  Fine-tuning pre-trained neural network models has become a widely adopted\napproach across various domains. However, it can lead to the distortion of\npre-trained feature extractors that already possess strong generalization\ncapabilities. Mitigating feature distortion during adaptation to new target\ndomains is crucial. Recent studies have shown promising results in handling\nfeature distortion by aligning the head layer on in-distribution datasets\nbefore performing fine-tuning. Nonetheless, a significant limitation arises\nfrom the treatment of batch normalization layers during fine-tuning, leading to\nsuboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning\n(DAFT), a novel approach that incorporates batch normalization conversion and\nthe integration of linear probing and fine-tuning. Our batch normalization\nconversion method effectively mitigates feature distortion by reducing\nmodifications to the neural network during fine-tuning. Additionally, we\nintroduce the integration of linear probing and fine-tuning to optimize the\nhead layer with gradual adaptation of the feature extractor. By leveraging\nbatch normalization layers and integrating linear probing and fine-tuning, our\nDAFT significantly mitigates feature distortion and achieves improved model\nperformance on both in-distribution and out-of-distribution datasets. Extensive\nexperiments demonstrate that our method outperforms other baseline methods,\ndemonstrating its effectiveness in not only improving performance but also\nmitigating feature distortion.\n","authors":["Seokhyeon Ha","Sunbeom Jung","Jungwoo Lee"],"pdf_url":"https://arxiv.org/pdf/2308.07728v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17456v1","updated":"2024-03-26T07:41:54Z","published":"2024-03-26T07:41:54Z","title":"Imitating Cost-Constrained Behaviors in Reinforcement Learning","summary":"  Complex planning and scheduling problems have long been solved using various\noptimization or heuristic approaches. In recent years, imitation learning that\naims to learn from expert demonstrations has been proposed as a viable\nalternative to solving these problems. Generally speaking, imitation learning\nis designed to learn either the reward (or preference) model or directly the\nbehavioral policy by observing the behavior of an expert. Existing work in\nimitation learning and inverse reinforcement learning has focused on imitation\nprimarily in unconstrained settings (e.g., no limit on fuel consumed by the\nvehicle). However, in many real-world domains, the behavior of an expert is\ngoverned not only by reward (or preference) but also by constraints. For\ninstance, decisions on self-driving delivery vehicles are dependent not only on\nthe route preferences/rewards (depending on past demand data) but also on the\nfuel in the vehicle and the time available. In such problems, imitation\nlearning is challenging as decisions are not only dictated by the reward model\nbut are also dependent on a cost-constrained model. In this paper, we provide\nmultiple methods that match expert distributions in the presence of trajectory\ncost constraints through (a) Lagrangian-based method; (b) Meta-gradients to\nfind a good trade-off between expected return and minimizing constraint\nviolation; and (c) Cost-violation-based alternating gradient. We empirically\nshow that leading imitation learning approaches imitate cost-constrained\nbehaviors poorly and our meta-gradient-based approach achieves the best\nperformance.\n","authors":["Qian Shao","Pradeep Varakantham","Shih-Fen Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.17456v1.pdf","comment":"Accepted to the 34th International Conference on Automated Planning\n  and Scheduling (ICAPS-24)"},{"id":"http://arxiv.org/abs/2403.17447v1","updated":"2024-03-26T07:26:00Z","published":"2024-03-26T07:26:00Z","title":"Chain of Compression: A Systematic Approach to Combinationally Compress\n  Convolutional Neural Networks","summary":"  Convolutional neural networks (CNNs) have achieved significant popularity,\nbut their computational and memory intensity poses challenges for\nresource-constrained computing systems, particularly with the prerequisite of\nreal-time performance. To release this burden, model compression has become an\nimportant research focus. Many approaches like quantization, pruning, early\nexit, and knowledge distillation have demonstrated the effect of reducing\nredundancy in neural networks. Upon closer examination, it becomes apparent\nthat each approach capitalizes on its unique features to compress the neural\nnetwork, and they can also exhibit complementary behavior when combined. To\nexplore the interactions and reap the benefits from the complementary features,\nwe propose the Chain of Compression, which works on the combinational sequence\nto apply these common techniques to compress the neural network. Validated on\nthe image-based regression and classification networks across different data\nsets, our proposed Chain of Compression can significantly compress the\ncomputation cost by 100-1000 times with ignorable accuracy loss compared with\nthe baseline model.\n","authors":["Yingtao Shen","Minqing Sun","Jie Zhao","An Zou"],"pdf_url":"https://arxiv.org/pdf/2403.17447v1.pdf","comment":"10 pages, 15 figures"},{"id":"http://arxiv.org/abs/2403.17445v1","updated":"2024-03-26T07:23:46Z","published":"2024-03-26T07:23:46Z","title":"Incorporating Exponential Smoothing into MLP: A Simple but Effective\n  Sequence Model","summary":"  Modeling long-range dependencies in sequential data is a crucial step in\nsequence learning. A recently developed model, the Structured State Space (S4),\ndemonstrated significant effectiveness in modeling long-range sequences.\nHowever, It is unclear whether the success of S4 can be attributed to its\nintricate parameterization and HiPPO initialization or simply due to State\nSpace Models (SSMs). To further investigate the potential of the deep SSMs, we\nstart with exponential smoothing (ETS), a simple SSM, and propose a stacked\narchitecture by directly incorporating it into an element-wise MLP. We augment\nsimple ETS with additional parameters and complex field to reduce the inductive\nbias. Despite increasing less than 1\\% of parameters of element-wise MLP, our\nmodels achieve comparable results to S4 on the LRA benchmark.\n","authors":["Jiqun Chu","Zuoquan Lin"],"pdf_url":"https://arxiv.org/pdf/2403.17445v1.pdf","comment":"12 pages, 5 tables, 3 figures"},{"id":"http://arxiv.org/abs/2403.17436v1","updated":"2024-03-26T07:05:06Z","published":"2024-03-26T07:05:06Z","title":"Particle identification with machine learning from incomplete data in\n  the ALICE experiment","summary":"  The ALICE experiment at the LHC measures properties of the strongly\ninteracting matter formed in ultrarelativistic heavy-ion collisions. Such\nstudies require accurate particle identification (PID). ALICE provides PID\ninformation via several detectors for particles with momentum from about 100\nMeV/c up to 20 GeV/c. Traditionally, particles are selected with rectangular\ncuts. Acmuch better performance can be achieved with machine learning (ML)\nmethods. Our solution uses multiple neural networks (NN) serving as binary\nclassifiers. Moreover, we extended our particle classifier with Feature Set\nEmbedding and attention in order to train on data with incomplete samples. We\nalso present the integration of the ML project with the ALICE analysis\nsoftware, and we discuss domain adaptation, the ML technique needed to transfer\nthe knowledge between simulated and real experimental data.\n","authors":["Maja Karwowska","Łukasz Graczykowski","Kamil Deja","Miłosz Kasak","Małgorzata Janik"],"pdf_url":"https://arxiv.org/pdf/2403.17436v1.pdf","comment":"Proceedings of 3rd Artificial Intelligence for the Electron Ion\n  Collider workshop -- AI4EIC2023, 28.11-1.12.2023. Prepared for submission to\n  JINST"},{"id":"http://arxiv.org/abs/2403.17431v1","updated":"2024-03-26T06:57:23Z","published":"2024-03-26T06:57:23Z","title":"Robust and Scalable Model Editing for Large Language Models","summary":"  Large language models (LLMs) can make predictions using parametric\nknowledge--knowledge encoded in the model weights--or contextual\nknowledge--knowledge presented in the context. In many scenarios, a desirable\nbehavior is that LLMs give precedence to contextual knowledge when it conflicts\nwith the parametric knowledge, and fall back to using their parametric\nknowledge when the context is irrelevant. This enables updating and correcting\nthe model's knowledge by in-context editing instead of retraining. Previous\nworks have shown that LLMs are inclined to ignore contextual knowledge and fail\nto reliably fall back to parametric knowledge when presented with irrelevant\ncontext. In this work, we discover that, with proper prompting methods,\ninstruction-finetuned LLMs can be highly controllable by contextual knowledge\nand robust to irrelevant context. Utilizing this feature, we propose EREN (Edit\nmodels by REading Notes) to improve the scalability and robustness of LLM\nediting. To better evaluate the robustness of model editors, we collect a new\ndataset, that contains irrelevant questions that are more challenging than the\nones in existing datasets. Empirical results show that our method outperforms\ncurrent state-of-the-art methods by a large margin. Unlike existing techniques,\nit can integrate knowledge from multiple edits, and correctly respond to\nsyntactically similar but semantically unrelated inputs (and vice versa). The\nsource code can be found at https://github.com/thunlp/EREN.\n","authors":["Yingfa Chen","Zhengyan Zhang","Xu Han","Chaojun Xiao","Zhiyuan Liu","Chen Chen","Kuai Li","Tao Yang","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2403.17431v1.pdf","comment":"LREC-COLING 2024 paper, 16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2308.01557v2","updated":"2024-03-26T06:50:43Z","published":"2023-08-03T06:36:21Z","title":"Motion Planning Diffusion: Learning and Planning of Robot Motions with\n  Diffusion Models","summary":"  Learning priors on trajectory distributions can help accelerate robot motion\nplanning optimization. Given previously successful plans, learning trajectory\ngenerative models as priors for a new planning problem is highly desirable.\nPrior works propose several ways on utilizing this prior to bootstrapping the\nmotion planning problem. Either sampling the prior for initializations or using\nthe prior distribution in a maximum-a-posterior formulation for trajectory\noptimization. In this work, we propose learning diffusion models as priors. We\nthen can sample directly from the posterior trajectory distribution conditioned\non task goals, by leveraging the inverse denoising process of diffusion models.\nFurthermore, diffusion has been recently shown to effectively encode data\nmultimodality in high-dimensional settings, which is particularly well-suited\nfor large trajectory dataset. To demonstrate our method efficacy, we compare\nour proposed method - Motion Planning Diffusion - against several baselines in\nsimulated planar robot and 7-dof robot arm manipulator environments. To assess\nthe generalization capabilities of our method, we test it in environments with\npreviously unseen obstacles. Our experiments show that diffusion models are\nstrong priors to encode high-dimensional trajectory distributions of robot\nmotions.\n","authors":["Joao Carvalho","An T. Le","Mark Baierl","Dorothea Koert","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2308.01557v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.04088v4","updated":"2024-03-26T06:47:13Z","published":"2021-06-08T04:05:30Z","title":"A Lightweight and Gradient-Stable Neural Layer","summary":"  To enhance resource efficiency and model deployability of neural networks, we\npropose a neural-layer architecture based on Householder weighting and\nabsolute-value activating, called Householder-absolute neural layer or simply\nHan-layer. Compared to a fully connected layer with $d$-neurons and $d$\noutputs, a Han-layer reduces the number of parameters and the corresponding\ncomputational complexity from $O(d^2)$ to $O(d)$. {The Han-layer structure\nguarantees that the Jacobian of the layer function is always orthogonal, thus\nensuring gradient stability (i.e., free of gradient vanishing or exploding\nissues) for any Han-layer sub-networks.} Extensive numerical experiments show\nthat one can strategically use Han-layers to replace fully connected (FC)\nlayers, reducing the number of model parameters while maintaining or even\nimproving the generalization performance. We will also showcase the\ncapabilities of the Han-layer architecture on a few small stylized models, and\ndiscuss its current limitations.\n","authors":["Yueyao Yu","Yin Zhang"],"pdf_url":"https://arxiv.org/pdf/2106.04088v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17425v1","updated":"2024-03-26T06:42:23Z","published":"2024-03-26T06:42:23Z","title":"Masked Multi-Domain Network: Multi-Type and Multi-Scenario Conversion\n  Rate Prediction with a Single Model","summary":"  In real-world advertising systems, conversions have different types in nature\nand ads can be shown in different display scenarios, both of which highly\nimpact the actual conversion rate (CVR). This results in the multi-type and\nmulti-scenario CVR prediction problem. A desired model for this problem should\nsatisfy the following requirements: 1) Accuracy: the model should achieve\nfine-grained accuracy with respect to any conversion type in any display\nscenario. 2) Scalability: the model parameter size should be affordable. 3)\nConvenience: the model should not require a large amount of effort in data\npartitioning, subset processing and separate storage. Existing approaches\ncannot simultaneously satisfy these requirements. For example, building a\nseparate model for each (conversion type, display scenario) pair is neither\nscalable nor convenient. Building a unified model trained on all the data with\nconversion type and display scenario included as two features is not accurate\nenough. In this paper, we propose the Masked Multi-domain Network (MMN) to\nsolve this problem. To achieve the accuracy requirement, we model\ndomain-specific parameters and propose a dynamically weighted loss to account\nfor the loss scale imbalance issue within each mini-batch. To achieve the\nscalability requirement, we propose a parameter sharing and composition\nstrategy to reduce model parameters from a product space to a sum space. To\nachieve the convenience requirement, we propose an auto-masking strategy which\ncan take mixed data from all the domains as input. It avoids the overhead\ncaused by data partitioning, individual processing and separate storage. Both\noffline and online experimental results validate the superiority of MMN for\nmulti-type and multi-scenario CVR prediction. MMN is now the serving model for\nreal-time CVR prediction in UC Toutiao.\n","authors":["Wentao Ouyang","Xiuwu Zhang","Chaofeng Guo","Shukui Ren","Yupei Sui","Kun Zhang","Jinmei Luo","Yunfeng Chen","Dongbo Xu","Xiangzheng Liu","Yanlong Du"],"pdf_url":"https://arxiv.org/pdf/2403.17425v1.pdf","comment":"CIKM 2023 (larger figures)"},{"id":"http://arxiv.org/abs/2403.17410v1","updated":"2024-03-26T06:06:01Z","published":"2024-03-26T06:06:01Z","title":"On permutation-invariant neural networks","summary":"  Conventional machine learning algorithms have traditionally been designed\nunder the assumption that input data follows a vector-based format, with an\nemphasis on vector-centric paradigms. However, as the demand for tasks\ninvolving set-based inputs has grown, there has been a paradigm shift in the\nresearch community towards addressing these challenges. In recent years, the\nemergence of neural network architectures such as Deep Sets and Transformers\nhas presented a significant advancement in the treatment of set-based data.\nThese architectures are specifically engineered to naturally accommodate sets\nas input, enabling more effective representation and processing of set\nstructures. Consequently, there has been a surge of research endeavors\ndedicated to exploring and harnessing the capabilities of these architectures\nfor various tasks involving the approximation of set functions. This\ncomprehensive survey aims to provide an overview of the diverse problem\nsettings and ongoing research efforts pertaining to neural networks that\napproximate set functions. By delving into the intricacies of these approaches\nand elucidating the associated challenges, the survey aims to equip readers\nwith a comprehensive understanding of the field. Through this comprehensive\nperspective, we hope that researchers can gain valuable insights into the\npotential applications, inherent limitations, and future directions of\nset-based neural networks. Indeed, from this survey we gain two insights: i)\nDeep Sets and its variants can be generalized by differences in the aggregation\nfunction, and ii) the behavior of Deep Sets is sensitive to the choice of the\naggregation function. From these observations, we show that Deep Sets, one of\nthe well-known permutation-invariant neural networks, can be generalized in the\nsense of a quasi-arithmetic mean.\n","authors":["Masanari Kimura","Ryotaro Shimizu","Yuki Hirakawa","Ryosuke Goto","Yuki Saito"],"pdf_url":"https://arxiv.org/pdf/2403.17410v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.03520v2","updated":"2024-03-26T06:05:13Z","published":"2023-11-06T20:58:07Z","title":"Brain Networks and Intelligence: A Graph Neural Network Based Approach\n  to Resting State fMRI Data","summary":"  Resting-state functional magnetic resonance imaging (rsfMRI) is a powerful\ntool for investigating the relationship between brain function and cognitive\nprocesses as it allows for the functional organization of the brain to be\ncaptured without relying on a specific task or stimuli. In this paper, we\npresent a novel modeling architecture called BrainRGIN for predicting\nintelligence (fluid, crystallized, and total intelligence) using graph neural\nnetworks on rsfMRI derived static functional network connectivity matrices.\nExtending from the existing graph convolution networks, our approach\nincorporates a clustering-based embedding and graph isomorphism network in the\ngraph convolutional layer to reflect the nature of the brain sub-network\norganization and efficient network expression, in combination with TopK pooling\nand attention-based readout functions. We evaluated our proposed architecture\non a large dataset, specifically the Adolescent Brain Cognitive Development\nDataset, and demonstrated its effectiveness in predicting individual\ndifferences in intelligence. Our model achieved lower mean squared errors and\nhigher correlation scores than existing relevant graph architectures and other\ntraditional machine learning models for all of the intelligence prediction\ntasks. The middle frontal gyrus exhibited a significant contribution to both\nfluid and crystallized intelligence, suggesting their pivotal role in these\ncognitive processes. Total composite scores identified a diverse set of brain\nregions to be relevant which underscores the complex nature of total\nintelligence.\n","authors":["Bishal Thapaliya","Esra Akbas","Jiayu Chen","Raam Sapkota","Bhaskar Ray","Pranav Suresh","Vince Calhoun","Jingyu Liu"],"pdf_url":"https://arxiv.org/pdf/2311.03520v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17407v1","updated":"2024-03-26T05:55:21Z","published":"2024-03-26T05:55:21Z","title":"Transcribing Bengali Text with Regional Dialects to IPA using District\n  Guided Tokens","summary":"  Accurate transcription of Bengali text to the International Phonetic Alphabet\n(IPA) is a challenging task due to the complex phonology of the language and\ncontext-dependent sound changes. This challenge is even more for regional\nBengali dialects due to unavailability of standardized spelling conventions for\nthese dialects, presence of local and foreign words popular in those regions\nand phonological diversity across different regions. This paper presents an\napproach to this sequence-to-sequence problem by introducing the District\nGuided Tokens (DGT) technique on a new dataset spanning six districts of\nBangladesh. The key idea is to provide the model with explicit information\nabout the regional dialect or \"district\" of the input text before generating\nthe IPA transcription. This is achieved by prepending a district token to the\ninput sequence, effectively guiding the model to understand the unique phonetic\npatterns associated with each district. The DGT technique is applied to\nfine-tune several transformer-based models, on this new dataset. Experimental\nresults demonstrate the effectiveness of DGT, with the ByT5 model achieving\nsuperior performance over word-based models like mT5, BanglaT5, and umT5. This\nis attributed to ByT5's ability to handle a high percentage of\nout-of-vocabulary words in the test set. The proposed approach highlights the\nimportance of incorporating regional dialect information into ubiquitous\nnatural language processing systems for languages with diverse phonological\nvariations. The following work was a result of the \"Bhashamul\" challenge, which\nis dedicated to solving the problem of Bengali text with regional dialects to\nIPA transcription https://www.kaggle.com/competitions/regipa/. The training and\ninference notebooks are available through the competition link.\n","authors":["S M Jishanul Islam","Sadia Ahmmed","Sahid Hossain Mustakim"],"pdf_url":"https://arxiv.org/pdf/2403.17407v1.pdf","comment":"This work became the champion of the Bhashamul challenge"},{"id":"http://arxiv.org/abs/2403.17404v1","updated":"2024-03-26T05:48:02Z","published":"2024-03-26T05:48:02Z","title":"Generalization Error Analysis for Sparse Mixture-of-Experts: A\n  Preliminary Study","summary":"  Mixture-of-Experts (MoE) represents an ensemble methodology that amalgamates\npredictions from several specialized sub-models (referred to as experts). This\nfusion is accomplished through a router mechanism, dynamically assigning\nweights to each expert's contribution based on the input data. Conventional MoE\nmechanisms select all available experts, incurring substantial computational\ncosts. In contrast, Sparse Mixture-of-Experts (Sparse MoE) selectively engages\nonly a limited number, or even just one expert, significantly reducing\ncomputation overhead while empirically preserving, and sometimes even\nenhancing, performance. Despite its wide-ranging applications and these\nadvantageous characteristics, MoE's theoretical underpinnings have remained\nelusive. In this paper, we embark on an exploration of Sparse MoE's\ngeneralization error concerning various critical factors. Specifically, we\ninvestigate the impact of the number of data samples, the total number of\nexperts, the sparsity in expert selection, the complexity of the routing\nmechanism, and the complexity of individual experts. Our analysis sheds light\non \\textit{how \\textbf{sparsity} contributes to the MoE's generalization},\noffering insights from the perspective of classical learning theory.\n","authors":["Jinze Zhao","Peihao Wang","Zhangyang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.17404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05264v2","updated":"2024-03-26T05:35:38Z","published":"2023-12-05T19:15:51Z","title":"All Rivers Run to the Sea: Private Learning with Asymmetric Flows","summary":"  Data privacy is of great concern in cloud machine-learning service platforms,\nwhen sensitive data are exposed to service providers. While private computing\nenvironments (e.g., secure enclaves), and cryptographic approaches (e.g.,\nhomomorphic encryption) provide strong privacy protection, their computing\nperformance still falls short compared to cloud GPUs. To achieve privacy\nprotection with high computing performance, we propose Delta, a new private\ntraining and inference framework, with comparable model performance as\nnon-private centralized training. Delta features two asymmetric data flows: the\nmain information-sensitive flow and the residual flow. The main part flows into\na small model while the residuals are offloaded to a large model. Specifically,\nDelta embeds the information-sensitive representations into a low-dimensional\nspace while pushing the information-insensitive part into high-dimension\nresiduals. To ensure privacy protection, the low-dimensional\ninformation-sensitive part is secured and fed to a small model in a private\nenvironment. On the other hand, the residual part is sent to fast cloud GPUs,\nand processed by a large model. To further enhance privacy and reduce the\ncommunication cost, Delta applies a random binary quantization technique along\nwith a DP-based technique to the residuals before sharing them with the public\nplatform. We theoretically show that Delta guarantees differential privacy in\nthe public environment and greatly reduces the complexity in the private\nenvironment. We conduct empirical analyses on CIFAR-10, CIFAR-100 and ImageNet\ndatasets and ResNet-18 and ResNet-34, showing that Delta achieves strong\nprivacy protection, fast training, and inference without significantly\ncompromising the model utility.\n","authors":["Yue Niu","Ramy E. Ali","Saurav Prakash","Salman Avestimehr"],"pdf_url":"https://arxiv.org/pdf/2312.05264v2.pdf","comment":"Camera-ready for CVPR 2024"},{"id":"http://arxiv.org/abs/2403.14736v2","updated":"2024-03-26T05:25:04Z","published":"2024-03-21T13:27:57Z","title":"NaNa and MiGu: Semantic Data Augmentation Techniques to Enhance Protein\n  Classification in Graph Neural Networks","summary":"  Protein classification tasks are essential in drug discovery. Real-world\nprotein structures are dynamic, which will determine the properties of\nproteins. However, the existing machine learning methods, like ProNet (Wang et\nal., 2022a), only access limited conformational characteristics and protein\nside-chain features, leading to impractical protein structure and inaccuracy of\nprotein classes in their predictions. In this paper, we propose novel semantic\ndata augmentation methods, Novel Augmentation of New Node Attributes (NaNa),\nand Molecular Interactions and Geometric Upgrading (MiGu) to incorporate\nbackbone chemical and side-chain biophysical information into protein\nclassification tasks and a co-embedding residual learning framework.\nSpecifically, we leverage molecular biophysical, secondary structure, chemical\nbonds, and ionic features of proteins to facilitate protein classification\ntasks. Furthermore, our semantic augmentation methods and the co-embedding\nresidual learning framework can improve the performance of GIN (Xu et al.,\n2019) on EC and Fold datasets (Bairoch, 2000; Andreeva et al., 2007) by 16.41%\nand 11.33% respectively. Our code is available at\nhttps://github.com/r08b46009/Code_for_MIGU_NANA/tree/main.\n","authors":["Yi-Shan Lan","Pin-Yu Chen","Tsung-Yi Ho"],"pdf_url":"https://arxiv.org/pdf/2403.14736v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.19125v4","updated":"2024-03-26T05:18:13Z","published":"2023-05-30T15:36:37Z","title":"Graph Generation with $K^2$-trees","summary":"  Generating graphs from a target distribution is a significant challenge\nacross many domains, including drug discovery and social network analysis. In\nthis work, we introduce a novel graph generation method leveraging $K^2$-tree\nrepresentation, originally designed for lossless graph compression. The\n$K^2$-tree representation {encompasses inherent hierarchy while enabling\ncompact graph generation}. In addition, we make contributions by (1) presenting\na sequential $K^2$-treerepresentation that incorporates pruning, flattening,\nand tokenization processes and (2) introducing a Transformer-based architecture\ndesigned to generate the sequence by incorporating a specialized tree\npositional encoding scheme. Finally, we extensively evaluate our algorithm on\nfour general and two molecular graph datasets to confirm its superiority for\ngraph generation.\n","authors":["Yunhui Jang","Dongwoo Kim","Sungsoo Ahn"],"pdf_url":"https://arxiv.org/pdf/2305.19125v4.pdf","comment":"International Conference on Learning Representations (ICLR) 2024"},{"id":"http://arxiv.org/abs/2312.02230v2","updated":"2024-03-26T05:10:53Z","published":"2023-12-04T03:43:26Z","title":"A Simple and Scalable Representation for Graph Generation","summary":"  Recently, there has been a surge of interest in employing neural networks for\ngraph generation, a fundamental statistical learning problem with critical\napplications like molecule design and community analysis. However, most\napproaches encounter significant limitations when generating large-scale\ngraphs. This is due to their requirement to output the full adjacency matrices\nwhose size grows quadratically with the number of nodes. In response to this\nchallenge, we introduce a new, simple, and scalable graph representation named\ngap encoded edge list (GEEL) that has a small representation size that aligns\nwith the number of edges. In addition, GEEL significantly reduces the\nvocabulary size by incorporating the gap encoding and bandwidth restriction\nschemes. GEEL can be autoregressively generated with the incorporation of node\npositional encoding, and we further extend GEEL to deal with attributed graphs\nby designing a new grammar. Our findings reveal that the adoption of this\ncompact representation not only enhances scalability but also bolsters\nperformance by simplifying the graph generation process. We conduct a\ncomprehensive evaluation across ten non-attributed and two molecular graph\ngeneration tasks, demonstrating the effectiveness of GEEL.\n","authors":["Yunhui Jang","Seul Lee","Sungsoo Ahn"],"pdf_url":"https://arxiv.org/pdf/2312.02230v2.pdf","comment":"International Conference on Learning Representations (ICLR) 2024"},{"id":"http://arxiv.org/abs/2403.17381v1","updated":"2024-03-26T04:59:27Z","published":"2024-03-26T04:59:27Z","title":"Application-Driven Innovation in Machine Learning","summary":"  As applications of machine learning proliferate, innovative algorithms\ninspired by specific real-world challenges have become increasingly important.\nSuch work offers the potential for significant impact not merely in domains of\napplication but also in machine learning itself. In this paper, we describe the\nparadigm of application-driven research in machine learning, contrasting it\nwith the more standard paradigm of methods-driven research. We illustrate the\nbenefits of application-driven machine learning and how this approach can\nproductively synergize with methods-driven work. Despite these benefits, we\nfind that reviewing, hiring, and teaching practices in machine learning often\nhold back application-driven innovation. We outline how these processes may be\nimproved.\n","authors":["David Rolnick","Alan Aspuru-Guzik","Sara Beery","Bistra Dilkina","Priya L. Donti","Marzyeh Ghassemi","Hannah Kerner","Claire Monteleoni","Esther Rolf","Milind Tambe","Adam White"],"pdf_url":"https://arxiv.org/pdf/2403.17381v1.pdf","comment":"12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.00033v4","updated":"2024-03-26T04:58:01Z","published":"2024-02-29T04:01:38Z","title":"Identification of Craving Maps among Marijuana Users via the Analysis of\n  Functional Brain Networks with High-Order Attention Graph Neural Networks","summary":"  The excessive consumption of marijuana can induce substantial psychological\nand social consequences. In this investigation, we propose an elucidative\nframework termed high-order graph attention neural networks (HOGANN) for the\nclassification of Marijuana addiction, coupled with an analysis of localized\nbrain network communities exhibiting abnormal activities among chronic\nmarijuana users. HOGANN integrates dynamic intrinsic functional brain networks,\nestimated from resting-state functional magnetic resonance imaging (rs-fMRI),\nusing long short-term memory (LSTM) to capture temporal network dynamics. We\nemploy a high-order attention module for information fusion and message passing\namong neighboring nodes, enhancing the network community analysis. Our model is\nvalidated across two distinct data cohorts, yielding substantially higher\nclassification accuracy than benchmark algorithms. Furthermore, we discern the\nmost pertinent subnetworks and cognitive regions affected by persistent\nmarijuana consumption, indicating adverse effects on functional brain networks,\nparticularly within the dorsal attention and frontoparietal networks.\nIntriguingly, our model demonstrates superior performance in cohorts exhibiting\nprolonged dependence, implying that prolonged marijuana usage induces more\npronounced alterations in brain networks. The model proficiently identifies\ncraving brain maps, thereby delineating critical brain regions for analysis.\n","authors":["Jun-En Ding","Shihao Yang","Anna Zilverstand","Feng Liu"],"pdf_url":"https://arxiv.org/pdf/2403.00033v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17377v1","updated":"2024-03-26T04:49:11Z","published":"2024-03-26T04:49:11Z","title":"Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance","summary":"  Recent studies have demonstrated that diffusion models are capable of\ngenerating high-quality samples, but their quality heavily depends on sampling\nguidance techniques, such as classifier guidance (CG) and classifier-free\nguidance (CFG). These techniques are often not applicable in unconditional\ngeneration or in various downstream tasks such as image restoration. In this\npaper, we propose a novel sampling guidance, called Perturbed-Attention\nGuidance (PAG), which improves diffusion sample quality across both\nunconditional and conditional settings, achieving this without requiring\nadditional training or the integration of external modules. PAG is designed to\nprogressively enhance the structure of samples throughout the denoising\nprocess. It involves generating intermediate samples with degraded structure by\nsubstituting selected self-attention maps in diffusion U-Net with an identity\nmatrix, by considering the self-attention mechanisms' ability to capture\nstructural information, and guiding the denoising process away from these\ndegraded samples. In both ADM and Stable Diffusion, PAG surprisingly improves\nsample quality in conditional and even unconditional scenarios. Moreover, PAG\nsignificantly improves the baseline performance in various downstream tasks\nwhere existing guidances such as CG or CFG cannot be fully utilized, including\nControlNet with empty prompts and image restoration such as inpainting and\ndeblurring.\n","authors":["Donghoon Ahn","Hyoungwon Cho","Jaewon Min","Wooseok Jang","Jungwoo Kim","SeonHwa Kim","Hyun Hee Park","Kyong Hwan Jin","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2403.17377v1.pdf","comment":"Project page is available at\n  https://ku-cvlab.github.io/Perturbed-Attention-Guidance"},{"id":"http://arxiv.org/abs/2403.17373v1","updated":"2024-03-26T04:27:56Z","published":"2024-03-26T04:27:56Z","title":"AIDE: An Automatic Data Engine for Object Detection in Autonomous\n  Driving","summary":"  Autonomous vehicle (AV) systems rely on robust perception models as a\ncornerstone of safety assurance. However, objects encountered on the road\nexhibit a long-tailed distribution, with rare or unseen categories posing\nchallenges to a deployed perception model. This necessitates an expensive\nprocess of continuously curating and annotating data with significant human\neffort. We propose to leverage recent advances in vision-language and large\nlanguage models to design an Automatic Data Engine (AIDE) that automatically\nidentifies issues, efficiently curates data, improves the model through\nauto-labeling, and verifies the model through generation of diverse scenarios.\nThis process operates iteratively, allowing for continuous self-improvement of\nthe model. We further establish a benchmark for open-world detection on AV\ndatasets to comprehensively evaluate various learning paradigms, demonstrating\nour method's superior performance at a reduced cost.\n","authors":["Mingfu Liang","Jong-Chyi Su","Samuel Schulter","Sparsh Garg","Shiyu Zhao","Ying Wu","Manmohan Chandraker"],"pdf_url":"https://arxiv.org/pdf/2403.17373v1.pdf","comment":"Accepted by CVPR-2024"},{"id":"http://arxiv.org/abs/2403.17364v1","updated":"2024-03-26T04:02:09Z","published":"2024-03-26T04:02:09Z","title":"A Moreau Envelope Approach for LQR Meta-Policy Estimation","summary":"  We study the problem of policy estimation for the Linear Quadratic Regulator\n(LQR) in discrete-time linear time-invariant uncertain dynamical systems. We\npropose a Moreau Envelope-based surrogate LQR cost, built from a finite set of\nrealizations of the uncertain system, to define a meta-policy efficiently\nadjustable to new realizations. Moreover, we design an algorithm to find an\napproximate first-order stationary point of the meta-LQR cost function.\nNumerical results show that the proposed approach outperforms naive averaging\nof controllers on new realizations of the linear system. We also provide\nempirical evidence that our method has better sample complexity than\nModel-Agnostic Meta-Learning (MAML) approaches.\n","authors":["Ashwin Aravind","Mohammad Taha Toghani","César A. Uribe"],"pdf_url":"https://arxiv.org/pdf/2403.17364v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.17353v1","updated":"2024-03-26T03:32:45Z","published":"2024-03-26T03:32:45Z","title":"Multi-Objective Trajectory Planning with Dual-Encoder","summary":"  Time-jerk optimal trajectory planning is crucial in advancing robotic arms'\nperformance in dynamic tasks. Traditional methods rely on solving complex\nnonlinear programming problems, bringing significant delays in generating\noptimized trajectories. In this paper, we propose a two-stage approach to\naccelerate time-jerk optimal trajectory planning. Firstly, we introduce a\ndual-encoder based transformer model to establish a good preliminary\ntrajectory. This trajectory is subsequently refined through sequential\nquadratic programming to improve its optimality and robustness. Our approach\noutperforms the state-of-the-art by up to 79.72\\% in reducing trajectory\nplanning time. Compared with existing methods, our method shrinks the\noptimality gap with the objective function value decreasing by up to 29.9\\%.\n","authors":["Beibei Zhang","Tian Xiang","Chentao Mao","Yuhua Zheng","Shuai Li","Haoyi Niu","Xiangming Xi","Wenyuan Bai","Feng Gao"],"pdf_url":"https://arxiv.org/pdf/2403.17353v1.pdf","comment":"6 pages, 7 figures, conference"},{"id":"http://arxiv.org/abs/2403.17351v1","updated":"2024-03-26T03:29:42Z","published":"2024-03-26T03:29:42Z","title":"Learn from Heterophily: Heterophilous Information-enhanced Graph Neural\n  Network","summary":"  Under circumstances of heterophily, where nodes with different labels tend to\nbe connected based on semantic meanings, Graph Neural Networks (GNNs) often\nexhibit suboptimal performance. Current studies on graph heterophily mainly\nfocus on aggregation calibration or neighbor extension and address the\nheterophily issue by utilizing node features or structural information to\nimprove GNN representations. In this paper, we propose and demonstrate that the\nvaluable semantic information inherent in heterophily can be utilized\neffectively in graph learning by investigating the distribution of neighbors\nfor each individual node within the graph. The theoretical analysis is carried\nout to demonstrate the efficacy of the idea in enhancing graph learning. Based\non this analysis, we propose HiGNN, an innovative approach that constructs an\nadditional new graph structure, that integrates heterophilous information by\nleveraging node distribution to enhance connectivity between nodes that share\nsimilar semantic characteristics. We conduct empirical assessments on node\nclassification tasks using both homophilous and heterophilous benchmark\ndatasets and compare HiGNN to popular GNN baselines and SoTA methods,\nconfirming the effectiveness in improving graph representations. In addition,\nby incorporating heterophilous information, we demonstrate a notable\nenhancement in existing GNN-based approaches, and the homophily degree across\nreal-world datasets, thus affirming the efficacy of our approach.\n","authors":["Yilun Zheng","Jiahao Xu","Lihui Chen"],"pdf_url":"https://arxiv.org/pdf/2403.17351v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15230v2","updated":"2024-03-26T03:07:56Z","published":"2023-03-27T14:10:26Z","title":"Troika: Multi-Path Cross-Modal Traction for Compositional Zero-Shot\n  Learning","summary":"  Recent compositional zero-shot learning (CZSL) methods adapt pre-trained\nvision-language models (VLMs) by constructing trainable prompts only for\ncomposed state-object pairs. Relying on learning the joint representation of\nseen compositions, these methods ignore the explicit modeling of the state and\nobject, thus limiting the exploitation of pre-trained knowledge and\ngeneralization to unseen compositions. With a particular focus on the\nuniversality of the solution, in this work, we propose a novel paradigm for\nCZSL models that establishes three identification branches (i.e., Multi-Path)\nto jointly model the state, object, and composition. The presented Troika is\nour implementation that aligns the branch-specific prompt representations with\ndecomposed visual features. To calibrate the bias between semantically similar\nmulti-modal representations, we further devise a Cross-Modal Traction module\ninto Troika that shifts the prompt representation towards the current visual\ncontent. We conduct extensive experiments on three popular benchmarks, where\nour method significantly outperforms existing methods in both closed-world and\nopen-world settings. The code will be available at\nhttps://github.com/bighuang624/Troika.\n","authors":["Siteng Huang","Biao Gong","Yutong Feng","Min Zhang","Yiliang Lv","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2303.15230v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17343v1","updated":"2024-03-26T03:05:20Z","published":"2024-03-26T03:05:20Z","title":"Language Models are Free Boosters for Biomedical Imaging Tasks","summary":"  In this study, we uncover the unexpected efficacy of residual-based large\nlanguage models (LLMs) as part of encoders for biomedical imaging tasks, a\ndomain traditionally devoid of language or textual data. The approach diverges\nfrom established methodologies by utilizing a frozen transformer block,\nextracted from pre-trained LLMs, as an innovative encoder layer for the direct\nprocessing of visual tokens. This strategy represents a significant departure\nfrom the standard multi-modal vision-language frameworks, which typically hinge\non language-driven prompts and inputs. We found that these LLMs could boost\nperformance across a spectrum of biomedical imaging applications, including\nboth 2D and 3D visual classification tasks, serving as plug-and-play boosters.\nMore interestingly, as a byproduct, we found that the proposed framework\nachieved superior performance, setting new state-of-the-art results on\nextensive, standardized datasets in MedMNIST-2D and 3D. Through this work, we\naim to open new avenues for employing LLMs in biomedical imaging and enriching\nthe understanding of their potential in this specialized domain.\n","authors":["Zhixin Lai","Jing Wu","Suiyao Chen","Yucheng Zhou","Anna Hovakimyan","Naira Hovakimyan"],"pdf_url":"https://arxiv.org/pdf/2403.17343v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17333v1","updated":"2024-03-26T02:33:36Z","published":"2024-03-26T02:33:36Z","title":"The Pursuit of Fairness in Artificial Intelligence Models: A Survey","summary":"  Artificial Intelligence (AI) models are now being utilized in all facets of\nour lives such as healthcare, education and employment. Since they are used in\nnumerous sensitive environments and make decisions that can be life altering,\npotential biased outcomes are a pressing matter. Developers should ensure that\nsuch models don't manifest any unexpected discriminatory practices like\npartiality for certain genders, ethnicities or disabled people. With the\nubiquitous dissemination of AI systems, researchers and practitioners are\nbecoming more aware of unfair models and are bound to mitigate bias in them.\nSignificant research has been conducted in addressing such issues to ensure\nmodels don't intentionally or unintentionally perpetuate bias. This survey\noffers a synopsis of the different ways researchers have promoted fairness in\nAI systems. We explore the different definitions of fairness existing in the\ncurrent literature. We create a comprehensive taxonomy by categorizing\ndifferent types of bias and investigate cases of biased AI in different\napplication domains. A thorough study is conducted of the approaches and\ntechniques employed by researchers to mitigate bias in AI models. Moreover, we\nalso delve into the impact of biased models on user experience and the ethical\nconsiderations to contemplate when developing and deploying such models. We\nhope this survey helps researchers and practitioners understand the intricate\ndetails of fairness and bias in AI systems. By sharing this thorough survey, we\naim to promote additional discourse in the domain of equitable and responsible\nAI.\n","authors":["Tahsin Alamgir Kheya","Mohamed Reda Bouadjenek","Sunil Aryal"],"pdf_url":"https://arxiv.org/pdf/2403.17333v1.pdf","comment":"37 pages, 6 figures"},{"id":"http://arxiv.org/abs/2309.00736v3","updated":"2024-03-26T02:33:12Z","published":"2023-09-01T21:16:02Z","title":"Prediction Error Estimation in Random Forests","summary":"  In this paper, error estimates of classification Random Forests are\nquantitatively assessed. Based on the initial theoretical framework built by\nBates et al. (2023), the true error rate and expected error rate are\ntheoretically and empirically investigated in the context of a variety of error\nestimation methods common to Random Forests. We show that in the classification\ncase, Random Forests' estimates of prediction error is closer on average to the\ntrue error rate instead of the average prediction error. This is opposite the\nfindings of Bates et al. (2023) which are given for logistic regression. We\nfurther show that our result holds across different error estimation strategies\nsuch as cross-validation, bagging, and data splitting.\n","authors":["Ian Krupkin","Johanna Hardin"],"pdf_url":"https://arxiv.org/pdf/2309.00736v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2104.00673 by other authors"},{"id":"http://arxiv.org/abs/2403.16950v2","updated":"2024-03-26T02:28:42Z","published":"2024-03-25T17:11:28Z","title":"Aligning with Human Judgement: The Role of Pairwise Preference in Large\n  Language Model Evaluators","summary":"  Large Language Models (LLMs) have demonstrated promising capabilities as\nautomatic evaluators in assessing the quality of generated natural language.\nHowever, LLMs still exhibit biases in evaluation and often struggle to generate\ncoherent evaluations that align with human assessments. In this work, we first\nconduct a systematic study of the misalignment between LLM evaluators and human\njudgement, revealing that existing calibration methods aimed at mitigating\nbiases are insufficient for effectively aligning LLM evaluators. Inspired by\nthe use of preference data in RLHF, we formulate the evaluation as a ranking\nproblem and introduce Pairwise-preference Search (PairS), an uncertainty-guided\nsearch method that employs LLMs to conduct pairwise comparisons and efficiently\nranks candidate texts. PairS achieves state-of-the-art performance on\nrepresentative evaluation tasks and demonstrates significant improvements over\ndirect scoring. Furthermore, we provide insights into the role of pairwise\npreference in quantifying the transitivity of LLMs and demonstrate how PairS\nbenefits from calibration.\n","authors":["Yinhong Liu","Han Zhou","Zhijiang Guo","Ehsan Shareghi","Ivan Vulić","Anna Korhonen","Nigel Collier"],"pdf_url":"https://arxiv.org/pdf/2403.16950v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17329v1","updated":"2024-03-26T02:24:32Z","published":"2024-03-26T02:24:32Z","title":"Deep Support Vectors","summary":"  While the success of deep learning is commonly attributed to its theoretical\nequivalence with Support Vector Machines (SVM), the practical implications of\nthis relationship have not been thoroughly explored. This paper pioneers an\nexploration in this domain, specifically focusing on the identification of Deep\nSupport Vectors (DSVs) within deep learning models. We introduce the concept of\nDeepKKT conditions, an adaptation of the traditional Karush-Kuhn-Tucker (KKT)\nconditions tailored for deep learning. Through empirical investigations, we\nillustrate that DSVs exhibit similarities to support vectors in SVM, offering a\ntangible method to interpret the decision-making criteria of models.\nAdditionally, our findings demonstrate that models can be effectively\nreconstructed using DSVs, resembling the process in SVM. The code will be\navailable.\n","authors":["Junhoo Lee","Hyunho Lee","Kyomin Hwang","Nojun Kwak"],"pdf_url":"https://arxiv.org/pdf/2403.17329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.13339v4","updated":"2024-03-26T01:53:30Z","published":"2023-09-23T11:21:12Z","title":"Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models\n  through Logic","summary":"  Recent advancements in large language models have showcased their remarkable\ngeneralizability across various domains. However, their reasoning abilities\nstill have significant room for improvement, especially when confronted with\nscenarios requiring multi-step reasoning. Although large language models\npossess extensive knowledge, their reasoning often fails to effectively utilize\nthis knowledge to establish a coherent thinking paradigm. These models\nsometimes show hallucinations as their reasoning procedures are unconstrained\nby logical principles. Aiming at improving the zero-shot chain-of-thought\nreasoning ability of large language models, we propose LoT (Logical Thoughts),\na self-improvement prompting framework that leverages principles rooted in\nsymbolic logic, particularly Reductio ad Absurdum, to systematically verify and\nrectify the reasoning processes step by step. Experimental evaluations\nconducted on language tasks in diverse domains, including arithmetic,\ncommonsense, symbolic, causal inference, and social problems, demonstrate the\nefficacy of enhanced reasoning by logic. The implementation code for LoT can be\naccessed at: https://github.com/xf-zhao/LoT.\n","authors":["Xufeng Zhao","Mengdi Li","Wenhao Lu","Cornelius Weber","Jae Hee Lee","Kun Chu","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2309.13339v4.pdf","comment":"Accepted in COLING 2024. Code see https://github.com/xf-zhao/LoT"},{"id":"http://arxiv.org/abs/2312.12467v3","updated":"2024-03-26T01:50:54Z","published":"2023-12-19T05:30:08Z","title":"Learning Flexible Body Collision Dynamics with Hierarchical Contact Mesh\n  Transformer","summary":"  Recently, many mesh-based graph neural network (GNN) models have been\nproposed for modeling complex high-dimensional physical systems. Remarkable\nachievements have been made in significantly reducing the solving time compared\nto traditional numerical solvers. These methods are typically designed to i)\nreduce the computational cost in solving physical dynamics and/or ii) propose\ntechniques to enhance the solution accuracy in fluid and rigid body dynamics.\nHowever, it remains under-explored whether they are effective in addressing the\nchallenges of flexible body dynamics, where instantaneous collisions occur\nwithin a very short timeframe. In this paper, we present Hierarchical Contact\nMesh Transformer (HCMT), which uses hierarchical mesh structures and can learn\nlong-range dependencies (occurred by collisions) among spatially distant\npositions of a body -- two close positions in a higher-level mesh correspond to\ntwo distant positions in a lower-level mesh. HCMT enables long-range\ninteractions, and the hierarchical mesh structure quickly propagates collision\neffects to faraway positions. To this end, it consists of a contact mesh\nTransformer and a hierarchical mesh Transformer (CMT and HMT, respectively).\nLastly, we propose a flexible body dynamics dataset, consisting of trajectories\nthat reflect experimental settings frequently used in the display industry for\nproduct designs. We also compare the performance of several baselines using\nwell-known benchmark datasets. Our results show that HCMT provides significant\nperformance improvements over existing methods. Our code is available at\nhttps://github.com/yuyudeep/hcmt.\n","authors":["Youn-Yeol Yu","Jeongwhan Choi","Woojin Cho","Kookjin Lee","Nayong Kim","Kiseok Chang","Chang-Seung Woo","Ilho Kim","Seok-Woo Lee","Joon-Young Yang","Sooyoung Yoon","Noseong Park"],"pdf_url":"https://arxiv.org/pdf/2312.12467v3.pdf","comment":"Accepted at ICLR 2024"}],"Multimedia":[{"id":"http://arxiv.org/abs/2403.17870v1","updated":"2024-03-26T16:57:55Z","published":"2024-03-26T16:57:55Z","title":"Boosting Diffusion Models with Moving Average Sampling in Frequency\n  Domain","summary":"  Diffusion models have recently brought a powerful revolution in image\ngeneration. Despite showing impressive generative capabilities, most of these\nmodels rely on the current sample to denoise the next one, possibly resulting\nin denoising instability. In this paper, we reinterpret the iterative denoising\nprocess as model optimization and leverage a moving average mechanism to\nensemble all the prior samples. Instead of simply applying moving average to\nthe denoised samples at different timesteps, we first map the denoised samples\nto data space and then perform moving average to avoid distribution shift\nacross timesteps. In view that diffusion models evolve the recovery from\nlow-frequency components to high-frequency details, we further decompose the\nsamples into different frequency components and execute moving average\nseparately on each component. We name the complete approach \"Moving Average\nSampling in Frequency domain (MASF)\". MASF could be seamlessly integrated into\nmainstream pre-trained diffusion models and sampling schedules. Extensive\nexperiments on both unconditional and conditional diffusion models demonstrate\nthat our MASF leads to superior performances compared to the baselines, with\nalmost negligible additional complexity cost.\n","authors":["Yurui Qian","Qi Cai","Yingwei Pan","Yehao Li","Ting Yao","Qibin Sun","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2403.17870v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17837v1","updated":"2024-03-26T16:24:42Z","published":"2024-03-26T16:24:42Z","title":"GTA-HDR: A Large-Scale Synthetic Dataset for HDR Image Reconstruction","summary":"  High Dynamic Range (HDR) content (i.e., images and videos) has a broad range\nof applications. However, capturing HDR content from real-world scenes is\nexpensive and time- consuming. Therefore, the challenging task of\nreconstructing visually accurate HDR images from their Low Dynamic Range (LDR)\ncounterparts is gaining attention in the vision research community. A major\nchallenge in this research problem is the lack of datasets, which capture\ndiverse scene conditions (e.g., lighting, shadows, weather, locations,\nlandscapes, objects, humans, buildings) and various image features (e.g.,\ncolor, contrast, saturation, hue, luminance, brightness, radiance). To address\nthis gap, in this paper, we introduce GTA-HDR, a large-scale synthetic dataset\nof photo-realistic HDR images sampled from the GTA-V video game. We perform\nthorough evaluation of the proposed dataset, which demonstrates significant\nqualitative and quantitative improvements of the state-of-the-art HDR image\nreconstruction methods. Furthermore, we demonstrate the effectiveness of the\nproposed dataset and its impact on additional computer vision tasks including\n3D human pose estimation, human body part segmentation, and holistic scene\nsegmentation. The dataset, data collection pipeline, and evaluation code are\navailable at: https://github.com/HrishavBakulBarua/GTA-HDR.\n","authors":["Hrishav Bakul Barua","Kalin Stefanov","KokSheik Wong","Abhinav Dhall","Ganesh Krishnasamy"],"pdf_url":"https://arxiv.org/pdf/2403.17837v1.pdf","comment":"Submitted to IEEE"},{"id":"http://arxiv.org/abs/2403.17727v1","updated":"2024-03-26T14:16:56Z","published":"2024-03-26T14:16:56Z","title":"FastPerson: Enhancing Video Learning through Effective Video\n  Summarization that Preserves Linguistic and Visual Contexts","summary":"  Quickly understanding lengthy lecture videos is essential for learners with\nlimited time and interest in various topics to improve their learning\nefficiency. To this end, video summarization has been actively researched to\nenable users to view only important scenes from a video. However, these studies\nfocus on either the visual or audio information of a video and extract\nimportant segments in the video. Therefore, there is a risk of missing\nimportant information when both the teacher's speech and visual information on\nthe blackboard or slides are important, such as in a lecture video. To tackle\nthis issue, we propose FastPerson, a video summarization approach that\nconsiders both the visual and auditory information in lecture videos.\nFastPerson creates summary videos by utilizing audio transcriptions along with\non-screen images and text, minimizing the risk of overlooking crucial\ninformation for learners. Further, it provides a feature that allows learners\nto switch between the summary and original videos for each chapter of the\nvideo, enabling them to adjust the pace of learning based on their interests\nand level of understanding. We conducted an evaluation with 40 participants to\nassess the effectiveness of our method and confirmed that it reduced viewing\ntime by 53\\% at the same level of comprehension as that when using traditional\nvideo playback methods.\n","authors":["Kazuki Kawamura","Jun Rekimoto"],"pdf_url":"https://arxiv.org/pdf/2403.17727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17708v1","updated":"2024-03-26T13:54:52Z","published":"2024-03-26T13:54:52Z","title":"Panonut360: A Head and Eye Tracking Dataset for Panoramic Video","summary":"  With the rapid development and widespread application of VR/AR technology,\nmaximizing the quality of immersive panoramic video services that match users'\npersonal preferences and habits has become a long-standing challenge.\nUnderstanding the saliency region where users focus, based on data collected\nwith HMDs, can promote multimedia encoding, transmission, and quality\nassessment. At the same time, large-scale datasets are essential for\nresearchers and developers to explore short/long-term user behavior patterns\nand train AI models related to panoramic videos. However, existing panoramic\nvideo datasets often include low-frequency user head or eye movement data\nthrough short-term videos only, lacking sufficient data for analyzing users'\nField of View (FoV) and generating video saliency regions.\n  Driven by these practical factors, in this paper, we present a head and eye\ntracking dataset involving 50 users (25 males and 25 females) watching 15\npanoramic videos. The dataset provides details on the viewport and gaze\nattention locations of users. Besides, we present some statistics samples\nextracted from the dataset. For example, the deviation between head and eye\nmovements challenges the widely held assumption that gaze attention decreases\nfrom the center of the FoV following a Gaussian distribution. Our analysis\nreveals a consistent downward offset in gaze fixations relative to the FoV in\nexperimental settings involving multiple users and videos. That's why we name\nthe dataset Panonut, a saliency weighting shaped like a donut. Finally, we also\nprovide a script that generates saliency distributions based on given head or\neye coordinates and pre-generated saliency distribution map sets of each video\nfrom the collected eye tracking data.\n  The dataset is available on website: https://dianvrlab.github.io/Panonut360/.\n","authors":["Yutong Xu","Junhao Du","Jiahe Wang","Yuwei Ning","Sihan Zhou Yang Cao"],"pdf_url":"https://arxiv.org/pdf/2403.17708v1.pdf","comment":"7 pages,ACM MMSys'24 accepted"},{"id":"http://arxiv.org/abs/2312.02512v2","updated":"2024-03-26T13:21:28Z","published":"2023-12-05T05:36:44Z","title":"AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation\n  with Unified Audio-Visual Speech Representation","summary":"  This paper proposes a novel direct Audio-Visual Speech to Audio-Visual Speech\nTranslation (AV2AV) framework, where the input and output of the system are\nmultimodal (i.e., audio and visual speech). With the proposed AV2AV, two key\nadvantages can be brought: 1) We can perform real-like conversations with\nindividuals worldwide in a virtual meeting by utilizing our own primary\nlanguages. In contrast to Speech-to-Speech Translation (A2A), which solely\ntranslates between audio modalities, the proposed AV2AV directly translates\nbetween audio-visual speech. This capability enhances the dialogue experience\nby presenting synchronized lip movements along with the translated speech. 2)\nWe can improve the robustness of the spoken language translation system. By\nemploying the complementary information of audio-visual speech, the system can\neffectively translate spoken language even in the presence of acoustic noise,\nshowcasing robust performance. To mitigate the problem of the absence of a\nparallel AV2AV translation dataset, we propose to train our spoken language\ntranslation system with the audio-only dataset of A2A. This is done by learning\nunified audio-visual speech representations through self-supervised learning in\nadvance to train the translation system. Moreover, we propose an AV-Renderer\nthat can generate raw audio and video in parallel. It is designed with\nzero-shot speaker modeling, thus the speaker in source audio-visual speech can\nbe maintained at the target translated audio-visual speech. The effectiveness\nof AV2AV is evaluated with extensive experiments in a many-to-many language\ntranslation setting. Demo page is available on\nhttps://choijeongsoo.github.io/av2av.\n","authors":["Jeongsoo Choi","Se Jin Park","Minsu Kim","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2312.02512v2.pdf","comment":"CVPR 2024. Code & Demo: https://choijeongsoo.github.io/av2av"},{"id":"http://arxiv.org/abs/2403.17589v1","updated":"2024-03-26T10:54:07Z","published":"2024-03-26T10:54:07Z","title":"Dual Memory Networks: A Versatile Adaptation Approach for\n  Vision-Language Models","summary":"  With the emergence of pre-trained vision-language models like CLIP, how to\nadapt them to various downstream classification tasks has garnered significant\nattention in recent research. The adaptation strategies can be typically\ncategorized into three paradigms: zero-shot adaptation, few-shot adaptation,\nand the recently-proposed training-free few-shot adaptation. Most existing\napproaches are tailored for a specific setting and can only cater to one or two\nof these paradigms. In this paper, we introduce a versatile adaptation approach\nthat can effectively work under all three settings. Specifically, we propose\nthe dual memory networks that comprise dynamic and static memory components.\nThe static memory caches training data knowledge, enabling training-free\nfew-shot adaptation, while the dynamic memory preserves historical test\nfeatures online during the testing process, allowing for the exploration of\nadditional data insights beyond the training set. This novel capability\nenhances model performance in the few-shot setting and enables model usability\nin the absence of training data. The two memory networks employ the same\nflexible memory interactive strategy, which can operate in a training-free mode\nand can be further enhanced by incorporating learnable projection layers. Our\napproach is tested across 11 datasets under the three task settings.\nRemarkably, in the zero-shot scenario, it outperforms existing methods by over\n3\\% and even shows superior results against methods utilizing external training\ndata. Additionally, our method exhibits robust performance against natural\ndistribution shifts. Codes are available at \\url{https://github.com/YBZh/DMN}.\n","authors":["Yabin Zhang","Wenjie Zhu","Hui Tang","Zhiyuan Ma","Kaiyang Zhou","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.17589v1.pdf","comment":"CVPR2024; Codes are available at \\url{https://github.com/YBZh/DMN}"},{"id":"http://arxiv.org/abs/2403.17420v1","updated":"2024-03-26T06:27:50Z","published":"2024-03-26T06:27:50Z","title":"Learning to Visually Localize Sound Sources from Mixtures without Prior\n  Source Knowledge","summary":"  The goal of the multi-sound source localization task is to localize sound\nsources from the mixture individually. While recent multi-sound source\nlocalization methods have shown improved performance, they face challenges due\nto their reliance on prior information about the number of objects to be\nseparated. In this paper, to overcome this limitation, we present a novel\nmulti-sound source localization method that can perform localization without\nprior knowledge of the number of sound sources. To achieve this goal, we\npropose an iterative object identification (IOI) module, which can recognize\nsound-making objects in an iterative manner. After finding the regions of\nsound-making objects, we devise object similarity-aware clustering (OSC) loss\nto guide the IOI module to effectively combine regions of the same object but\nalso distinguish between different objects and backgrounds. It enables our\nmethod to perform accurate localization of sound-making objects without any\nprior knowledge. Extensive experimental results on the MUSIC and VGGSound\nbenchmarks show the significant performance improvements of the proposed method\nover the existing methods for both single and multi-source. Our code is\navailable at: https://github.com/VisualAIKHU/NoPrior_MultiSSL\n","authors":["Dongjin Kim","Sung Jin Um","Sangmin Lee","Jung Uk Kim"],"pdf_url":"https://arxiv.org/pdf/2403.17420v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2402.19330v2","updated":"2024-03-26T04:15:53Z","published":"2024-02-29T16:33:12Z","title":"A Novel Approach to Industrial Defect Generation through Blended Latent\n  Diffusion Model with Online Adaptation","summary":"  Effectively addressing the challenge of industrial Anomaly Detection (AD)\nnecessitates an ample supply of defective samples, a constraint often hindered\nby their scarcity in industrial contexts. This paper introduces a novel\nalgorithm designed to augment defective samples, thereby enhancing AD\nperformance. The proposed method tailors the blended latent diffusion model for\ndefect sample generation, employing a diffusion model to generate defective\nsamples in the latent space. A feature editing process, controlled by a\n``trimap\" mask and text prompts, refines the generated samples. The image\ngeneration inference process is structured into three stages: a free diffusion\nstage, an editing diffusion stage, and an online decoder adaptation stage. This\nsophisticated inference strategy yields high-quality synthetic defective\nsamples with diverse pattern variations, leading to significantly improved AD\naccuracies based on the augmented training set. Specifically, on the widely\nrecognized MVTec AD dataset, the proposed method elevates the state-of-the-art\n(SOTA) performance of AD with augmented data by 1.5%, 1.9%, and 3.1% for AD\nmetrics AP, IAP, and IAP90, respectively. The implementation code of this work\ncan be found at the GitHub repository\nhttps://github.com/GrandpaXun242/AdaBLDM.git\n","authors":["Hanxi Li","Zhengxun Zhang","Hao Chen","Lin Wu","Bo Li","Deyin Liu","Mingwen Wang"],"pdf_url":"https://arxiv.org/pdf/2402.19330v2.pdf","comment":"13 pages,7 figures"},{"id":"http://arxiv.org/abs/2403.18063v1","updated":"2024-03-26T19:29:21Z","published":"2024-03-26T19:29:21Z","title":"Spectral Convolutional Transformer: Harmonizing Real vs. Complex\n  Multi-View Spectral Operators for Vision Transformer","summary":"  Transformers used in vision have been investigated through diverse\narchitectures - ViT, PVT, and Swin. These have worked to improve the attention\nmechanism and make it more efficient. Differently, the need for including local\ninformation was felt, leading to incorporating convolutions in transformers\nsuch as CPVT and CvT. Global information is captured using a complex Fourier\nbasis to achieve global token mixing through various methods, such as AFNO,\nGFNet, and Spectformer. We advocate combining three diverse views of data -\nlocal, global, and long-range dependence. We also investigate the simplest\nglobal representation using only the real domain spectral representation -\nobtained through the Hartley transform. We use a convolutional operator in the\ninitial layers to capture local information. Through these two contributions,\nwe are able to optimize and obtain a spectral convolution transformer (SCT)\nthat provides improved performance over the state-of-the-art methods while\nreducing the number of parameters. Through extensive experiments, we show that\nSCT-C-small gives state-of-the-art performance on the ImageNet dataset and\nreaches 84.5\\% top-1 accuracy, while SCT-C-Large reaches 85.9\\% and SCT-C-Huge\nreaches 86.4\\%. We evaluate SCT on transfer learning on datasets such as\nCIFAR-10, CIFAR-100, Oxford Flower, and Stanford Car. We also evaluate SCT on\ndownstream tasks i.e. instance segmentation on the MSCOCO dataset. The project\npage is available on this webpage.\\url{https://github.com/badripatro/sct}\n","authors":["Badri N. Patro","Vinay P. Namboodiri","Vijay S. Agneeswaran"],"pdf_url":"https://arxiv.org/pdf/2403.18063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17837v1","updated":"2024-03-26T16:24:42Z","published":"2024-03-26T16:24:42Z","title":"GTA-HDR: A Large-Scale Synthetic Dataset for HDR Image Reconstruction","summary":"  High Dynamic Range (HDR) content (i.e., images and videos) has a broad range\nof applications. However, capturing HDR content from real-world scenes is\nexpensive and time-consuming. Therefore, the challenging task of reconstructing\nvisually accurate HDR images from their Low Dynamic Range (LDR) counterparts is\ngaining attention in the vision research community. A major challenge in this\nresearch problem is the lack of datasets, which capture diverse scene\nconditions (e.g., lighting, shadows, weather, locations, landscapes, objects,\nhumans, buildings) and various image features (e.g., color, contrast,\nsaturation, hue, luminance, brightness, radiance). To address this gap, in this\npaper, we introduce GTA-HDR, a large-scale synthetic dataset of photo-realistic\nHDR images sampled from the GTA-V video game. We perform thorough evaluation of\nthe proposed dataset, which demonstrates significant qualitative and\nquantitative improvements of the state-of-the-art HDR image reconstruction\nmethods. Furthermore, we demonstrate the effectiveness of the proposed dataset\nand its impact on additional computer vision tasks including 3D human pose\nestimation, human body part segmentation, and holistic scene segmentation. The\ndataset, data collection pipeline, and evaluation code are available at:\nhttps://github.com/HrishavBakulBarua/GTA-HDR.\n","authors":["Hrishav Bakul Barua","Kalin Stefanov","KokSheik Wong","Abhinav Dhall","Ganesh Krishnasamy"],"pdf_url":"https://arxiv.org/pdf/2403.17837v1.pdf","comment":"Submitted to IEEE"}]},"2024-03-27T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2403.18814v1","updated":"2024-03-27T17:59:04Z","published":"2024-03-27T17:59:04Z","title":"Mini-Gemini: Mining the Potential of Multi-modality Vision Language\n  Models","summary":"  In this work, we introduce Mini-Gemini, a simple and effective framework\nenhancing multi-modality Vision Language Models (VLMs). Despite the\nadvancements in VLMs facilitating basic visual dialog and reasoning, a\nperformance gap persists compared to advanced models like GPT-4 and Gemini. We\ntry to narrow the gap by mining the potential of VLMs for better performance\nand any-to-any workflow from three aspects, i.e., high-resolution visual\ntokens, high-quality data, and VLM-guided generation. To enhance visual tokens,\nwe propose to utilize an additional visual encoder for high-resolution\nrefinement without increasing the visual token count. We further construct a\nhigh-quality dataset that promotes precise image comprehension and\nreasoning-based generation, expanding the operational scope of current VLMs. In\ngeneral, Mini-Gemini further mines the potential of VLMs and empowers current\nframeworks with image understanding, reasoning, and generation simultaneously.\nMini-Gemini supports a series of dense and MoE Large Language Models (LLMs)\nfrom 2B to 34B. It is demonstrated to achieve leading performance in several\nzero-shot benchmarks and even surpasses the developed private models. Code and\nmodels are available at https://github.com/dvlab-research/MiniGemini.\n","authors":["Yanwei Li","Yuechen Zhang","Chengyao Wang","Zhisheng Zhong","Yixin Chen","Ruihang Chu","Shaoteng Liu","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2403.18814v1.pdf","comment":"Code and models are available at\n  https://github.com/dvlab-research/MiniGemini"},{"id":"http://arxiv.org/abs/2403.18804v1","updated":"2024-03-27T17:50:00Z","published":"2024-03-27T17:50:00Z","title":"Is Modularity Transferable? A Case Study through the Lens of Knowledge\n  Distillation","summary":"  The rise of Modular Deep Learning showcases its potential in various Natural\nLanguage Processing applications. Parameter-efficient fine-tuning (PEFT)\nmodularity has been shown to work for various use cases, from domain adaptation\nto multilingual setups. However, all this work covers the case where the\nmodular components are trained and deployed within one single Pre-trained\nLanguage Model (PLM). This model-specific setup is a substantial limitation on\nthe very modularity that modular architectures are trying to achieve. We ask\nwhether current modular approaches are transferable between models and whether\nwe can transfer the modules from more robust and larger PLMs to smaller ones.\nIn this work, we aim to fill this gap via a lens of Knowledge Distillation,\ncommonly used for model compression, and present an extremely straightforward\napproach to transferring pre-trained, task-specific PEFT modules between\nsame-family PLMs. Moreover, we propose a method that allows the transfer of\nmodules between incompatible PLMs without any change in the inference\ncomplexity. The experiments on Named Entity Recognition, Natural Language\nInference, and Paraphrase Identification tasks over multiple languages and PEFT\nmethods showcase the initial potential of transferable modularity.\n","authors":["Mateusz Klimaszewski","Piotr Andruszkiewicz","Alexandra Birch"],"pdf_url":"https://arxiv.org/pdf/2403.18804v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18803v1","updated":"2024-03-27T17:49:31Z","published":"2024-03-27T17:49:31Z","title":"Projective Methods for Mitigating Gender Bias in Pre-trained Language\n  Models","summary":"  Mitigation of gender bias in NLP has a long history tied to debiasing static\nword embeddings. More recently, attention has shifted to debiasing pre-trained\nlanguage models. We study to what extent the simplest projective debiasing\nmethods, developed for word embeddings, can help when applied to BERT's\ninternal representations. Projective methods are fast to implement, use a small\nnumber of saved parameters, and make no updates to the existing model\nparameters. We evaluate the efficacy of the methods in reducing both intrinsic\nbias, as measured by BERT's next sentence prediction task, and in mitigating\nobserved bias in a downstream setting when fine-tuned. To this end, we also\nprovide a critical analysis of a popular gender-bias assessment test for\nquantifying intrinsic bias, resulting in an enhanced test set and new bias\nmeasures. We find that projective methods can be effective at both intrinsic\nbias and downstream bias mitigation, but that the two outcomes are not\nnecessarily correlated. This finding serves as a warning that intrinsic bias\ntest sets, based either on language modeling tasks or next sentence prediction,\nshould not be the only benchmark in developing a debiased language model.\n","authors":["Hillary Dawkins","Isar Nejadgholi","Daniel Gillis","Judi McCuaig"],"pdf_url":"https://arxiv.org/pdf/2403.18803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18802v1","updated":"2024-03-27T17:48:55Z","published":"2024-03-27T17:48:55Z","title":"Long-form factuality in large language models","summary":"  Large language models (LLMs) often generate content that contains factual\nerrors when responding to fact-seeking prompts on open-ended topics. To\nbenchmark a model's long-form factuality in open domains, we first use GPT-4 to\ngenerate LongFact, a prompt set comprising thousands of questions spanning 38\ntopics. We then propose that LLM agents can be used as automated evaluators for\nlong-form factuality through a method which we call Search-Augmented Factuality\nEvaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into\na set of individual facts and to evaluate the accuracy of each fact using a\nmulti-step reasoning process comprising sending search queries to Google Search\nand determining whether a fact is supported by the search results. Furthermore,\nwe propose extending F1 score as an aggregated metric for long-form factuality.\nTo do so, we balance the percentage of supported facts in a response\n(precision) with the percentage of provided facts relative to a hyperparameter\nrepresenting a user's preferred response length (recall).\n  Empirically, we demonstrate that LLM agents can achieve superhuman rating\nperformance - on a set of ~16k individual facts, SAFE agrees with crowdsourced\nhuman annotators 72% of the time, and on a random subset of 100 disagreement\ncases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times\ncheaper than human annotators. We also benchmark thirteen language models on\nLongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding\nthat larger language models generally achieve better long-form factuality.\nLongFact, SAFE, and all experimental code are available at\nhttps://github.com/google-deepmind/long-form-factuality.\n","authors":["Jerry Wei","Chengrun Yang","Xinying Song","Yifeng Lu","Nathan Hu","Dustin Tran","Daiyi Peng","Ruibo Liu","Da Huang","Cosmo Du","Quoc V. Le"],"pdf_url":"https://arxiv.org/pdf/2403.18802v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17574v2","updated":"2024-03-27T17:34:57Z","published":"2024-02-27T15:09:20Z","title":"Agent-Pro: Learning to Evolve via Policy-Level Reflection and\n  Optimization","summary":"  Large Language Models exhibit robust problem-solving capabilities for diverse\ntasks. However, most LLM-based agents are designed as specific task solvers\nwith sophisticated prompt engineering, rather than agents capable of learning\nand evolving through interactions. These task solvers necessitate manually\ncrafted prompts to inform task rules and regulate LLM behaviors, inherently\nincapacitating to address complex dynamic scenarios e.g., large interactive\ngames. In light of this, we propose Agent-Pro: an LLM-based Agent with\nPolicy-level Reflection and Optimization that can learn a wealth of expertise\nfrom interactive experiences and progressively elevate its behavioral policy.\nSpecifically, it involves a dynamic belief generation and reflection process\nfor policy evolution. Rather than action-level reflection, Agent-Pro\niteratively reflects on past trajectories and beliefs, fine-tuning its\nirrational beliefs for a better policy. Moreover, a depth-first search is\nemployed for policy optimization, ensuring continual enhancement in policy\npayoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em,\noutperforming vanilla LLM and specialized models. Our results show Agent-Pro\ncan learn and evolve in complex and dynamic scenes, which also benefits\nnumerous LLM-based applications.\n","authors":["Wenqi Zhang","Ke Tang","Hai Wu","Mengna Wang","Yongliang Shen","Guiyang Hou","Zeqi Tan","Peng Li","Yueting Zhuang","Weiming Lu"],"pdf_url":"https://arxiv.org/pdf/2402.17574v2.pdf","comment":"LLM-based Agent"},{"id":"http://arxiv.org/abs/2403.18783v1","updated":"2024-03-27T17:31:39Z","published":"2024-03-27T17:31:39Z","title":"Towards a World-English Language Model for On-Device Virtual Assistants","summary":"  Neural Network Language Models (NNLMs) for Virtual Assistants (VAs) are\ngenerally language-, region-, and in some cases, device-dependent, which\nincreases the effort to scale and maintain them. Combining NNLMs for one or\nmore of the categories is one way to improve scalability. In this work, we\ncombine regional variants of English to build a ``World English'' NNLM for\non-device VAs. In particular, we investigate the application of adapter\nbottlenecks to model dialect-specific characteristics in our existing\nproduction NNLMs {and enhance the multi-dialect baselines}. We find that\nadapter modules are more effective in modeling dialects than specializing\nentire sub-networks. Based on this insight and leveraging the design of our\nproduction models, we introduce a new architecture for World English NNLM that\nmeets the accuracy, latency, and memory constraints of our single-dialect\nmodels.\n","authors":["Rricha Jalota","Lyan Verwimp","Markus Nussbaum-Thom","Amr Mousa","Arturo Argueta","Youssef Oualil"],"pdf_url":"https://arxiv.org/pdf/2403.18783v1.pdf","comment":"Accepted in ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.02009v2","updated":"2024-03-27T17:24:47Z","published":"2024-01-04T00:32:33Z","title":"Self-Contrast: Better Reflection Through Inconsistent Solving\n  Perspectives","summary":"  The reflection capacity of Large Language Model (LLM) has garnered extensive\nattention. A post-hoc prompting strategy, e.g., reflexion and self-refine,\nrefines LLM's response based on self-evaluated or external feedback. However,\nrecent research indicates without external feedback, LLM's intrinsic reflection\nis unstable. Our investigation unveils that the key bottleneck is the quality\nof the self-evaluated feedback. We find LLMs often exhibit overconfidence or\nhigh randomness when self-evaluate, offering stubborn or inconsistent feedback,\nwhich causes poor reflection. To remedy this, we advocate Self-Contrast: It\nadaptively explores diverse solving perspectives tailored to the request,\ncontrasts the differences, and summarizes these discrepancies into a checklist\nwhich could be used to re-examine and eliminate discrepancies. Our method\nendows LLM with diverse perspectives to alleviate stubborn biases. Moreover,\ntheir discrepancies indicate potential errors or inherent uncertainties that\nLLM often overlooks. Reflecting upon these can catalyze more accurate and\nstable reflection. Experiments conducted on a series of reasoning and\ntranslation tasks with different LLMs serve to underscore the effectiveness and\ngenerality of our strategy.\n","authors":["Wenqi Zhang","Yongliang Shen","Linjuan Wu","Qiuying Peng","Jun Wang","Yueting Zhuang","Weiming Lu"],"pdf_url":"https://arxiv.org/pdf/2401.02009v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18771v1","updated":"2024-03-27T17:20:39Z","published":"2024-03-27T17:20:39Z","title":"CheckEval: Robust Evaluation Framework using Large Language Model via\n  Checklist","summary":"  We introduce CheckEval, a novel evaluation framework using Large Language\nModels, addressing the challenges of ambiguity and inconsistency in current\nevaluation methods. CheckEval addresses these challenges by dividing evaluation\ncriteria into detailed sub-aspects and constructing a checklist of Boolean\nquestions for each, simplifying the evaluation. This approach not only renders\nthe process more interpretable but also significantly enhances the robustness\nand reliability of results by focusing on specific evaluation dimensions.\nValidated through a focused case study using the SummEval benchmark, CheckEval\nindicates a strong correlation with human judgments. Furthermore, it\ndemonstrates a highly consistent Inter-Annotator Agreement. These findings\nhighlight the effectiveness of CheckEval for objective, flexible, and precise\nevaluations. By offering a customizable and interactive framework, CheckEval\nsets a new standard for the use of LLMs in evaluation, responding to the\nevolving needs of the field and establishing a clear method for future\nLLM-based evaluation.\n","authors":["Yukyung Lee","Joonghoon Kim","Jaehee Kim","Hyowon Cho","Pilsung Kang"],"pdf_url":"https://arxiv.org/pdf/2403.18771v1.pdf","comment":"HEAL at CHI 2024"},{"id":"http://arxiv.org/abs/2403.18769v1","updated":"2024-03-27T17:13:38Z","published":"2024-03-27T17:13:38Z","title":"Improved Neural Protoform Reconstruction via Reflex Prediction","summary":"  Protolanguage reconstruction is central to historical linguistics. The\ncomparative method, one of the most influential theoretical and methodological\nframeworks in the history of the language sciences, allows linguists to infer\nprotoforms (reconstructed ancestral words) from their reflexes (related modern\nwords) based on the assumption of regular sound change. Not surprisingly,\nnumerous computational linguists have attempted to operationalize comparative\nreconstruction through various computational models, the most successful of\nwhich have been supervised encoder-decoder models, which treat the problem of\npredicting protoforms given sets of reflexes as a sequence-to-sequence problem.\nWe argue that this framework ignores one of the most important aspects of the\ncomparative method: not only should protoforms be inferable from cognate sets\n(sets of related reflexes) but the reflexes should also be inferable from the\nprotoforms. Leveraging another line of research -- reflex prediction -- we\npropose a system in which candidate protoforms from a reconstruction model are\nreranked by a reflex prediction model. We show that this more complete\nimplementation of the comparative method allows us to surpass state-of-the-art\nprotoform reconstruction methods on three of four Chinese and Romance datasets.\n","authors":["Liang Lu","Jingzhi Wang","David R. Mortensen"],"pdf_url":"https://arxiv.org/pdf/2403.18769v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18746v1","updated":"2024-03-27T16:45:02Z","published":"2024-03-27T16:45:02Z","title":"CYCLE: Learning to Self-Refine the Code Generation","summary":"  Pre-trained code language models have achieved promising performance in code\ngeneration and improved the programming efficiency of human developers.\nHowever, their self-refinement capability is typically overlooked by the\nexisting evaluations of code LMs, which focus only on the accuracy of the\none-time prediction. For the cases when code LMs fail to implement the correct\nprogram, developers actually find it hard to debug and fix the faulty\nprediction since it is not written by the developers themselves. Unfortunately,\nour study reveals that code LMs cannot efficiently self-refine their faulty\ngenerations as well.\n  In this paper, we propose CYCLE framework, learning to self-refine the faulty\ngeneration according to the available feedback, such as the execution results\nreported by the test suites. We evaluate CYCLE on three popular code generation\nbenchmarks, HumanEval, MBPP, and APPS. The results reveal that CYCLE\nsuccessfully maintains, sometimes improves, the quality of one-time code\ngeneration, while significantly improving the self-refinement capability of\ncode LMs. We implement four variants of CYCLE with varied numbers of parameters\nacross 350M, 1B, 2B, and 3B, and the experiments show that CYCLE consistently\nboosts the code generation performance, by up to 63.5%, across benchmarks and\nvaried model sizes. We also notice that CYCLE outperforms code LMs that have\n3$\\times$ more parameters in self-refinement.\n","authors":["Yangruibo Ding","Marcus J. Min","Gail Kaiser","Baishakhi Ray"],"pdf_url":"https://arxiv.org/pdf/2403.18746v1.pdf","comment":"Camera-ready for OOPSLA'24"},{"id":"http://arxiv.org/abs/2403.03100v2","updated":"2024-03-27T16:14:34Z","published":"2024-03-05T16:35:25Z","title":"NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and\n  Diffusion Models","summary":"  While recent large-scale text-to-speech (TTS) models have achieved\nsignificant progress, they still fall short in speech quality, similarity, and\nprosody. Considering speech intricately encompasses various attributes (e.g.,\ncontent, prosody, timbre, and acoustic details) that pose significant\nchallenges for generation, a natural idea is to factorize speech into\nindividual subspaces representing different attributes and generate them\nindividually. Motivated by it, we propose NaturalSpeech 3, a TTS system with\nnovel factorized diffusion models to generate natural speech in a zero-shot\nway. Specifically, 1) we design a neural codec with factorized vector\nquantization (FVQ) to disentangle speech waveform into subspaces of content,\nprosody, timbre, and acoustic details; 2) we propose a factorized diffusion\nmodel to generate attributes in each subspace following its corresponding\nprompt. With this factorization design, NaturalSpeech 3 can effectively and\nefficiently model intricate speech with disentangled subspaces in a\ndivide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the\nstate-of-the-art TTS systems on quality, similarity, prosody, and\nintelligibility, and achieves on-par quality with human recordings.\nFurthermore, we achieve better performance by scaling to 1B parameters and 200K\nhours of training data.\n","authors":["Zeqian Ju","Yuancheng Wang","Kai Shen","Xu Tan","Detai Xin","Dongchao Yang","Yanqing Liu","Yichong Leng","Kaitao Song","Siliang Tang","Zhizheng Wu","Tao Qin","Xiang-Yang Li","Wei Ye","Shikun Zhang","Jiang Bian","Lei He","Jinyu Li","Sheng Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.03100v2.pdf","comment":"Achieving human-level quality and naturalness on multi-speaker\n  datasets (e.g., LibriSpeech) in a zero-shot way"},{"id":"http://arxiv.org/abs/2403.18715v1","updated":"2024-03-27T16:04:47Z","published":"2024-03-27T16:04:47Z","title":"Mitigating Hallucinations in Large Vision-Language Models with\n  Instruction Contrastive Decoding","summary":"  Large Vision-Language Models (LVLMs) are increasingly adept at generating\ncontextually detailed and coherent responses from visual inputs. However, their\napplication in multimodal decision-making and open-ended generation is hindered\nby a notable rate of hallucinations, where generated text inaccurately\nrepresents the visual contents. To address this issue, this paper introduces\nthe Instruction Contrastive Decoding (ICD) method, a novel approach designed to\nreduce hallucinations during LVLM inference. Our method is inspired by our\nobservation that what we call disturbance instructions significantly exacerbate\nhallucinations in multimodal fusion modules. ICD contrasts distributions from\nstandard and instruction disturbance, thereby increasing alignment uncertainty\nand effectively subtracting hallucinated concepts from the original\ndistribution. Through comprehensive experiments on discriminative benchmarks\n(POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that\nICD significantly mitigates both object-level and attribute-level\nhallucinations. Moreover, our method not only addresses hallucinations but also\nsignificantly enhances the general perception and recognition capabilities of\nLVLMs.\n","authors":["Xintong Wang","Jingheng Pan","Liang Ding","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2403.18715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.03123v3","updated":"2024-03-27T16:03:32Z","published":"2023-04-13T16:01:28Z","title":"ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and\n  Ethics) Evaluation: A Review","summary":"  ChatGPT is another large language model (LLM) vastly available for the\nconsumers on their devices but due to its performance and ability to converse\neffectively, it has gained a huge popularity amongst research as well as\nindustrial community. Recently, many studies have been published to show the\neffectiveness, efficiency, integration, and sentiments of chatGPT and other\nLLMs. In contrast, this study focuses on the important aspects that are mostly\noverlooked, i.e. sustainability, privacy, digital divide, and ethics and\nsuggests that not only chatGPT but every subsequent entry in the category of\nconversational bots should undergo Sustainability, PrivAcy, Digital divide, and\nEthics (SPADE) evaluation. This paper discusses in detail the issues and\nconcerns raised over chatGPT in line with aforementioned characteristics. We\nalso discuss the recent EU AI Act briefly in accordance with the SPADE\nevaluation. We support our hypothesis by some preliminary data collection and\nvisualizations along with hypothesized facts. We also suggest mitigations and\nrecommendations for each of the concerns. Furthermore, we also suggest some\npolicies and recommendations for EU AI policy act concerning ethics, digital\ndivide, and sustainability.\n","authors":["Sunder Ali Khowaja","Parus Khuwaja","Kapal Dev","Weizheng Wang","Lewis Nkenyereye"],"pdf_url":"https://arxiv.org/pdf/2305.03123v3.pdf","comment":"29 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.18697v1","updated":"2024-03-27T15:46:25Z","published":"2024-03-27T15:46:25Z","title":"The Invalsi Benchmark: measuring Language Models Mathematical and\n  Language understanding in Italian","summary":"  While Italian is by all metrics a high resource language, currently, there\nare isn't a Language Model pre-trained exclusively in this language. This\nresults in a lower number of available benchmarks to evaluate the performance\nof language models in Italian.\n  This work presents two new benchmarks to evaluate the models performance on\nmathematical understanding and language understanding in Italian. These\nbenchmarks are based on real tests that are undertaken by students of age\nbetween 11 and 18 within the Italian school system and have therefore been\nvalidated by several experts in didactics and pedagogy.\n  To validate this dataset we evaluate the performance of 9 language models\nthat are the best performing when writing in Italian, including our own\nfine-tuned models. We show that this is a challenging benchmark where current\nlanguage models are bound by 60\\% accuracy.\n  We believe that the release of this dataset paves the way for improving\nfuture models mathematical and language understanding in Italian.\n","authors":["Andrea Esuli","Giovanni Puccetti"],"pdf_url":"https://arxiv.org/pdf/2403.18697v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18684v1","updated":"2024-03-27T15:27:36Z","published":"2024-03-27T15:27:36Z","title":"Scaling Laws For Dense Retrieval","summary":"  Scaling up neural models has yielded significant advancements in a wide array\nof tasks, particularly in language generation. Previous studies have found that\nthe performance of neural models frequently adheres to predictable scaling\nlaws, correlated with factors such as training set size and model size. This\ninsight is invaluable, especially as large-scale experiments grow increasingly\nresource-intensive. Yet, such scaling law has not been fully explored in dense\nretrieval due to the discrete nature of retrieval metrics and complex\nrelationships between training data and model sizes in retrieval tasks. In this\nstudy, we investigate whether the performance of dense retrieval models follows\nthe scaling law as other neural models. We propose to use contrastive\nlog-likelihood as the evaluation metric and conduct extensive experiments with\ndense retrieval models implemented with different numbers of parameters and\ntrained with different amounts of annotated data. Results indicate that, under\nour settings, the performance of dense retrieval models follows a precise\npower-law scaling related to the model size and the number of annotations.\nAdditionally, we examine scaling with prevalent data augmentation methods to\nassess the impact of annotation quality, and apply the scaling law to find the\nbest resource allocation strategy under a budget constraint. We believe that\nthese insights will significantly contribute to understanding the scaling\neffect of dense retrieval models and offer meaningful guidance for future\nresearch endeavors.\n","authors":["Yan Fang","Jingtao Zhan","Qingyao Ai","Jiaxin Mao","Weihang Su","Jia Chen","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.18684v1.pdf","comment":"Accepted at SIGIR 2024"},{"id":"http://arxiv.org/abs/2403.11128v2","updated":"2024-03-27T15:22:53Z","published":"2024-03-17T07:34:12Z","title":"Beyond Static Evaluation: A Dynamic Approach to Assessing AI Assistants'\n  API Invocation Capabilities","summary":"  With the rise of Large Language Models (LLMs), AI assistants' ability to\nutilize tools, especially through API calls, has advanced notably. This\nprogress has necessitated more accurate evaluation methods. Many existing\nstudies adopt static evaluation, where they assess AI assistants' API call\nbased on pre-defined dialogue histories. However, such evaluation method can be\nmisleading, as an AI assistant might fail in generating API calls from\npreceding human interaction in real cases. Instead of the resource-intensive\nmethod of direct human-machine interactions, we propose Automated Dynamic\nEvaluation (AutoDE) to assess an assistant's API call capability without human\ninvolvement. In our framework, we endeavor to closely mirror genuine human\nconversation patterns in human-machine interactions, using a LLM-based user\nagent, equipped with a user script to ensure human alignment. Experimental\nresults highlight that AutoDE uncovers errors overlooked by static evaluations,\naligning more closely with human assessment. Testing four AI assistants using\nour crafted benchmark, our method further mirrored human evaluation compared to\nconventional static evaluations.\n","authors":["Honglin Mu","Yang Xu","Yunlong Feng","Xiaofeng Han","Yitong Li","Yutai Hou","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2403.11128v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18680v1","updated":"2024-03-27T15:22:16Z","published":"2024-03-27T15:22:16Z","title":"NL-ITI: Optimizing Probing and Intervention for Improvement of ITI\n  Method","summary":"  Large Language Models (LLM) are prone to returning false information. It\nconstitutes one of major challenges in the AI field. In our work, we explore\nparadigm introduced by Inference-Time-Intervention (ITI). In first stage, it\nidentifies attention heads, which contain the highest amount of desired type of\nknowledge (e.g., truthful). Afterwards, during inference, LLM activations are\nshifted for chosen subset of attention heads. We further improved the ITI\nframework by introducing a nonlinear probing and multi-token intervention -\nNon-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice\nbenchmarks, including TruthfulQA, on which we report around 14% MC1 metric\nimprovement with respect to the baseline ITI results. NL-ITI achieves also\nencouraging results on other testsets - on Business Ethics subdomain of MMLU,\naround 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI\nperforms better while being less invasive in the behavior of LLM at the same\ntime (as measured by Kullback-Leibler divergence).\n","authors":["Jakub Hoscilowicz","Adam Wiacek","Jan Chojnacki","Adam Cieslak","Leszek Michon","Vitalii Urbanevych","Artur Janicki"],"pdf_url":"https://arxiv.org/pdf/2403.18680v1.pdf","comment":"Code is available at https://github.com/Samsung/NL-ITI"},{"id":"http://arxiv.org/abs/2403.17143v2","updated":"2024-03-27T15:15:16Z","published":"2024-03-25T19:40:26Z","title":"Guided Distant Supervision for Multilingual Relation Extraction Data:\n  Adapting to a New Language","summary":"  Relation extraction is essential for extracting and understanding\nbiographical information in the context of digital humanities and related\nsubjects. There is a growing interest in the community to build datasets\ncapable of training machine learning models to extract relationships. However,\nannotating such datasets can be expensive and time-consuming, in addition to\nbeing limited to English. This paper applies guided distant supervision to\ncreate a large biographical relationship extraction dataset for German. Our\ndataset, composed of more than 80,000 instances for nine relationship types, is\nthe largest biographical German relationship extraction dataset. We also create\na manually annotated dataset with 2000 instances to evaluate the models and\nrelease it together with the dataset compiled using guided distant supervision.\nWe train several state-of-the-art machine learning models on the automatically\ncreated dataset and release them as well. Furthermore, we experiment with\nmultilingual and cross-lingual experiments that could benefit many low-resource\nlanguages.\n","authors":["Alistair Plum","Tharindu Ranasinghe","Christoph Purschke"],"pdf_url":"https://arxiv.org/pdf/2403.17143v2.pdf","comment":"Accepted to LREC-COLING 2024 (The 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation)"},{"id":"http://arxiv.org/abs/2403.18671v1","updated":"2024-03-27T15:15:14Z","published":"2024-03-27T15:15:14Z","title":"Fact Checking Beyond Training Set","summary":"  Evaluating the veracity of everyday claims is time consuming and in some\ncases requires domain expertise. We empirically demonstrate that the commonly\nused fact checking pipeline, known as the retriever-reader, suffers from\nperformance deterioration when it is trained on the labeled data from one\ndomain and used in another domain. Afterwards, we delve into each component of\nthe pipeline and propose novel algorithms to address this problem. We propose\nan adversarial algorithm to make the retriever component robust against\ndistribution shift. Our core idea is to initially train a bi-encoder on the\nlabeled source data, and then, to adversarially train two separate document and\nclaim encoders using unlabeled target data. We then focus on the reader\ncomponent and propose to train it such that it is insensitive towards the order\nof claims and evidence documents. Our empirical evaluations support the\nhypothesis that such a reader shows a higher robustness against distribution\nshift. To our knowledge, there is no publicly available multi-topic fact\nchecking dataset. Thus, we propose a simple automatic method to re-purpose two\nwell-known fact checking datasets. We then construct eight fact checking\nscenarios from these datasets, and compare our model to a set of strong\nbaseline models, including recent domain adaptation models that use GPT4 for\ngenerating synthetic data.\n","authors":["Payam Karisani","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2403.18671v1.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.18667v1","updated":"2024-03-27T15:11:00Z","published":"2024-03-27T15:11:00Z","title":"Improving Content Recommendation: Knowledge Graph-Based Semantic\n  Contrastive Learning for Diversity and Cold-Start Users","summary":"  Addressing the challenges related to data sparsity, cold-start problems, and\ndiversity in recommendation systems is both crucial and demanding. Many current\nsolutions leverage knowledge graphs to tackle these issues by combining both\nitem-based and user-item collaborative signals. A common trend in these\napproaches focuses on improving ranking performance at the cost of escalating\nmodel complexity, reducing diversity, and complicating the task. It is\nessential to provide recommendations that are both personalized and diverse,\nrather than solely relying on achieving high rank-based performance, such as\nClick-through Rate, Recall, etc. In this paper, we propose a hybrid multi-task\nlearning approach, training on user-item and item-item interactions. We apply\nitem-based contrastive learning on descriptive text, sampling positive and\nnegative pairs based on item metadata. Our approach allows the model to better\nunderstand the relationships between entities within the knowledge graph by\nutilizing semantic information from text. It leads to more accurate, relevant,\nand diverse user recommendations and a benefit that extends even to cold-start\nusers who have few interactions with items. We perform extensive experiments on\ntwo widely used datasets to validate the effectiveness of our approach. Our\nfindings demonstrate that jointly training user-item interactions and\nitem-based signals using synopsis text is highly effective. Furthermore, our\nresults provide evidence that item-based contrastive learning enhances the\nquality of entity embeddings, as indicated by metrics such as uniformity and\nalignment.\n","authors":["Yejin Kim","Scott Rome","Kevin Foley","Mayur Nankani","Rimon Melamed","Javier Morales","Abhay Yadav","Maria Peifer","Sardar Hamidian","H. Howie Huang"],"pdf_url":"https://arxiv.org/pdf/2403.18667v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2309.13320v2","updated":"2024-03-27T14:57:29Z","published":"2023-09-23T09:35:55Z","title":"GlotScript: A Resource and Tool for Low Resource Writing System\n  Identification","summary":"  We present GlotScript, an open resource and tool for low resource writing\nsystem identification. GlotScript-R is a resource that provides the attested\nwriting systems for more than 7,000 languages. It is compiled by aggregating\ninformation from existing writing system resources. GlotScript-T is a writing\nsystem identification tool that covers all 161 Unicode 15.0 scripts. For an\ninput text, it returns its script distribution where scripts are identified by\nISO 15924 codes. We also present two use cases for GlotScript. First, we\ndemonstrate that GlotScript can help cleaning multilingual corpora such as mC4\nand OSCAR. Second, we analyze the tokenization of a number of language models\nsuch as GPT-4 using GlotScript and provide insights on the coverage of low\nresource scripts and languages by each language model. We hope that GlotScript\nwill become a useful resource for work on low resource languages in the NLP\ncommunity. GlotScript-R and GlotScript-T are available at\nhttps://github.com/cisnlp/GlotScript.\n","authors":["Amir Hossein Kargaran","François Yvon","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2309.13320v2.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18647v1","updated":"2024-03-27T14:54:27Z","published":"2024-03-27T14:54:27Z","title":"SDSAT: Accelerating LLM Inference through Speculative Decoding with\n  Semantic Adaptive Tokens","summary":"  We propose an acceleration scheme for large language models (LLMs) through\nSpeculative Decoding with Semantic Adaptive Tokens (SDSAT). The primary\nobjective of this design is to enhance the LLM model's ability to generate\ndraft tokens more accurately without compromising the model's accuracy. The\ncore strategies involve: 1) Fine-tune the model by incorporating semantic\nadaptive tokens that possess flexible decoding capabilities without changing\nits structure, allowing them to generate high-quality draft tokens. 2) By\nemploying a training method that does not affect the standard tokens, the model\ncan acquire parallel decoding abilities atop its original framework with\nminimal training overhead. 3) We have designed the \"two-step-draft-then-verify\"\ngeneration strategies using both greedy search and nucleus sampling.\nExperiments conducted on the CodeLlama-13B and 7B models have yielded speed\nincreases of over 3.5X and 3.0X, respectively. Please refer to\nhttps://github.com/hasuoshenyun/SDSAT.\n","authors":["Chengbo Liu","Yong Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.18647v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.04507v2","updated":"2024-03-27T14:50:56Z","published":"2024-03-07T14:07:00Z","title":"NLPre: a revised approach towards language-centric benchmarking of\n  Natural Language Preprocessing systems","summary":"  With the advancements of transformer-based architectures, we observe the rise\nof natural language preprocessing (NLPre) tools capable of solving preliminary\nNLP tasks (e.g. tokenisation, part-of-speech tagging, dependency parsing, or\nmorphological analysis) without any external linguistic guidance. It is arduous\nto compare novel solutions to well-entrenched preprocessing toolkits, relying\non rule-based morphological analysers or dictionaries. Aware of the\nshortcomings of existing NLPre evaluation approaches, we investigate a novel\nmethod of reliable and fair evaluation and performance reporting. Inspired by\nthe GLUE benchmark, the proposed language-centric benchmarking system enables\ncomprehensive ongoing evaluation of multiple NLPre tools, while credibly\ntracking their performance. The prototype application is configured for Polish\nand integrated with the thoroughly assembled NLPre-PL benchmark. Based on this\nbenchmark, we conduct an extensive evaluation of a variety of Polish NLPre\nsystems. To facilitate the construction of benchmarking environments for other\nlanguages, e.g. NLPre-GA for Irish or NLPre-ZH for Chinese, we ensure full\ncustomization of the publicly released source code of the benchmarking system.\nThe links to all the resources (deployed platforms, source code, trained\nmodels, datasets etc.) can be found on the project website:\nhttps://sites.google.com/view/nlpre-benchmark.\n","authors":["Martyna Wiącek","Piotr Rybak","Łukasz Pszenny","Alina Wróblewska"],"pdf_url":"https://arxiv.org/pdf/2403.04507v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18624v1","updated":"2024-03-27T14:34:29Z","published":"2024-03-27T14:34:29Z","title":"Vulnerability Detection with Code Language Models: How Far Are We?","summary":"  In the context of the rising interest in code language models (code LMs) and\nvulnerability detection, we study the effectiveness of code LMs for detecting\nvulnerabilities. Our analysis reveals significant shortcomings in existing\nvulnerability datasets, including poor data quality, low label accuracy, and\nhigh duplication rates, leading to unreliable model performance in realistic\nvulnerability detection scenarios. Additionally, the evaluation methods used\nwith these datasets are not representative of real-world vulnerability\ndetection.\n  To address these challenges, we introduce PrimeVul, a new dataset for\ntraining and evaluating code LMs for vulnerability detection. PrimeVul\nincorporates a novel set of data labeling techniques that achieve comparable\nlabel accuracy to human-verified benchmarks while significantly expanding the\ndataset. It also implements a rigorous data de-duplication and chronological\ndata splitting strategy to mitigate data leakage issues, alongside introducing\nmore realistic evaluation metrics and settings. This comprehensive approach\naims to provide a more accurate assessment of code LMs' performance in\nreal-world conditions.\n  Evaluating code LMs on PrimeVul reveals that existing benchmarks\nsignificantly overestimate the performance of these models. For instance, a\nstate-of-the-art 7B model scored 68.26% F1 on BigVul but only 3.09% F1 on\nPrimeVul. Attempts to improve performance through advanced training techniques\nand larger models like GPT-3.5 and GPT-4 were unsuccessful, with results akin\nto random guessing in the most stringent settings. These findings underscore\nthe considerable gap between current capabilities and the practical\nrequirements for deploying code LMs in security roles, highlighting the need\nfor more innovative research in this domain.\n","authors":["Yangruibo Ding","Yanjun Fu","Omniyyah Ibrahim","Chawin Sitawarin","Xinyun Chen","Basel Alomair","David Wagner","Baishakhi Ray","Yizheng Chen"],"pdf_url":"https://arxiv.org/pdf/2403.18624v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13284v2","updated":"2024-03-27T14:30:44Z","published":"2024-02-19T09:07:59Z","title":"Structure Guided Large Language Model for SQL Generation","summary":"  Generating accurate Structured Querying Language (SQL) is a long-standing\nproblem, especially in matching users' semantic queries with structured\ndatabases and then generating structured SQL. Existing models typically input\nqueries and database schemas into the LLM and rely on the LLM to perform\nsemantic-structure matching and generate structured SQL. However, such\nsolutions overlook the structural information within user queries and\ndatabases, which can be utilized to enhance the generation of structured SQL.\nThis oversight can lead to inaccurate or unexecutable SQL generation. To fully\nexploit the structure, we propose a structure-to-SQL framework, which leverages\nthe inherent structure information to improve the SQL generation of LLMs.\nSpecifically, we introduce our Structure Guided SQL~(SGU-SQL) generation model.\nSGU-SQL first links user queries and databases in a structure-enhanced manner.\nIt then decomposes complicated linked structures with grammar trees to guide\nthe LLM to generate the SQL step by step. Extensive experiments on two\nbenchmark datasets illustrate that SGU-SQL can outperform sixteen SQL\ngeneration baselines.\n","authors":["Qinggang Zhang","Junnan Dong","Hao Chen","Wentao Li","Feiran Huang","Xiao Huang"],"pdf_url":"https://arxiv.org/pdf/2402.13284v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18609v1","updated":"2024-03-27T14:26:41Z","published":"2024-03-27T14:26:41Z","title":"A survey on learning models of spiking neural membrane systems and\n  spiking neural networks","summary":"  Spiking neural networks (SNN) are a biologically inspired model of neural\nnetworks with certain brain-like properties. In the past few decades, this\nmodel has received increasing attention in computer science community, owing\nalso to the successful phenomenon of deep learning. In SNN, communication\nbetween neurons takes place through the spikes and spike trains. This\ndifferentiates these models from the ``standard'' artificial neural networks\n(ANN) where the frequency of spikes is replaced by real-valued signals. Spiking\nneural P systems (SNPS) can be considered a branch of SNN based more on the\nprinciples of formal automata, with many variants developed within the\nframework of the membrane computing theory. In this paper, we first briefly\ncompare structure and function, advantages and drawbacks of SNN and SNPS. A key\npart of the article is a survey of recent results and applications of machine\nlearning and deep learning models of both SNN and SNPS formalisms.\n","authors":["Prithwineel Paul","Petr Sosik","Lucie Ciencialova"],"pdf_url":"https://arxiv.org/pdf/2403.18609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12997v3","updated":"2024-03-27T13:59:57Z","published":"2024-02-20T13:25:16Z","title":"Towards Trustworthy Reranking: A Simple yet Effective Abstention\n  Mechanism","summary":"  Neural Information Retrieval (NIR) has significantly improved upon\nheuristic-based IR systems. Yet, failures remain frequent, the models used\noften being unable to retrieve documents relevant to the user's query. We\naddress this challenge by proposing a lightweight abstention mechanism tailored\nfor real-world constraints, with particular emphasis placed on the reranking\nphase. We introduce a protocol for evaluating abstention strategies in a\nblack-box scenario, demonstrating their efficacy, and propose a simple yet\neffective data-driven mechanism. We provide open-source code for experiment\nreplication and abstention implementation, fostering wider adoption and\napplication in diverse contexts.\n","authors":["Hippolyte Gisserot-Boukhlef","Manuel Faysse","Emmanuel Malherbe","Céline Hudelot","Pierre Colombo"],"pdf_url":"https://arxiv.org/pdf/2402.12997v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09283v3","updated":"2024-03-27T13:55:14Z","published":"2024-02-14T16:14:03Z","title":"Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey","summary":"  Large Language Models (LLMs) are now commonplace in conversation\napplications. However, their risks of misuse for generating harmful responses\nhave raised serious societal concerns and spurred recent research on LLM\nconversation safety. Therefore, in this survey, we provide a comprehensive\noverview of recent studies, covering three critical aspects of LLM conversation\nsafety: attacks, defenses, and evaluations. Our goal is to provide a structured\nsummary that enhances understanding of LLM conversation safety and encourages\nfurther investigation into this important subject. For easy reference, we have\ncategorized all the studies mentioned in this survey according to our taxonomy,\navailable at: https://github.com/niconi19/LLM-conversation-safety.\n","authors":["Zhichen Dong","Zhanhui Zhou","Chao Yang","Jing Shao","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2402.09283v3.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2308.12531v2","updated":"2024-03-27T13:46:37Z","published":"2023-08-24T03:40:54Z","title":"CARE: Co-Attention Network for Joint Entity and Relation Extraction","summary":"  Joint entity and relation extraction is the fundamental task of information\nextraction, consisting of two subtasks: named entity recognition and relation\nextraction. However, most existing joint extraction methods suffer from issues\nof feature confusion or inadequate interaction between the two subtasks.\nAddressing these challenges, in this work, we propose a Co-Attention network\nfor joint entity and Relation Extraction (CARE). Our approach includes adopting\na parallel encoding strategy to learn separate representations for each\nsubtask, aiming to avoid feature overlap or confusion. At the core of our\napproach is the co-attention module that captures two-way interaction between\nthe two subtasks, allowing the model to leverage entity information for\nrelation prediction and vice versa, thus promoting mutual enhancement. Through\nextensive experiments on three benchmark datasets for joint entity and relation\nextraction (NYT, WebNLG, and SciERC), we demonstrate that our proposed model\noutperforms existing baseline models. Our code will be available at\nhttps://github.com/kwj0x7f/CARE.\n","authors":["Wenjun Kong","Yamei Xia"],"pdf_url":"https://arxiv.org/pdf/2308.12531v2.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2401.06712v2","updated":"2024-03-27T13:38:35Z","published":"2024-01-12T17:26:51Z","title":"Few-Shot Detection of Machine-Generated Text using Style Representations","summary":"  The advent of instruction-tuned language models that convincingly mimic human\nwriting poses a significant risk of abuse. However, such abuse may be\ncounteracted with the ability to detect whether a piece of text was composed by\na language model rather than a human author. Some previous approaches to this\nproblem have relied on supervised methods by training on corpora of confirmed\nhuman- and machine- written documents. Unfortunately, model under-specification\nposes an unavoidable challenge for neural network-based detectors, making them\nbrittle in the face of data shifts, such as the release of newer language\nmodels producing still more fluent text than the models used to train the\ndetectors. Other approaches require access to the models that may have\ngenerated a document in question, which is often impractical. In light of these\nchallenges, we pursue a fundamentally different approach not relying on samples\nfrom language models of concern at training time. Instead, we propose to\nleverage representations of writing style estimated from human-authored text.\nIndeed, we find that features effective at distinguishing among human authors\nare also effective at distinguishing human from machine authors, including\nstate-of-the-art large language models like Llama-2, ChatGPT, and GPT-4.\nFurthermore, given a handful of examples composed by each of several specific\nlanguage models of interest, our approach affords the ability to predict which\nmodel generated a given document. The code and data to reproduce our\nexperiments are available at\nhttps://github.com/LLNL/LUAR/tree/main/fewshot_iclr2024.\n","authors":["Rafael Rivera Soto","Kailin Koch","Aleem Khan","Barry Chen","Marcus Bishop","Nicholas Andrews"],"pdf_url":"https://arxiv.org/pdf/2401.06712v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18555v1","updated":"2024-03-27T13:34:59Z","published":"2024-03-27T13:34:59Z","title":"Debiasing Sentence Embedders through Contrastive Word Pairs","summary":"  Over the last years, various sentence embedders have been an integral part in\nthe success of current machine learning approaches to Natural Language\nProcessing (NLP). Unfortunately, multiple sources have shown that the bias,\ninherent in the datasets upon which these embedding methods are trained, is\nlearned by them. A variety of different approaches to remove biases in\nembeddings exists in the literature. Most of these approaches are applicable to\nword embeddings and in fewer cases to sentence embeddings. It is problematic\nthat most debiasing approaches are directly transferred from word embeddings,\ntherefore these approaches fail to take into account the nonlinear nature of\nsentence embedders and the embeddings they produce. It has been shown in\nliterature that bias information is still present if sentence embeddings are\ndebiased using such methods. In this contribution, we explore an approach to\nremove linear and nonlinear bias information for NLP solutions, without\nimpacting downstream performance. We compare our approach to common debiasing\nmethods on classical bias metrics and on bias metrics which take nonlinear\ninformation into account.\n","authors":["Philip Kenneweg","Sarah Schröder","Alexander Schulz","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2403.18555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08268v3","updated":"2024-03-27T13:29:31Z","published":"2023-11-14T16:02:16Z","title":"A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can\n  Fool Large Language Models Easily","summary":"  Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to\nprovide useful and safe responses. However, adversarial prompts known as\n'jailbreaks' can circumvent safeguards, leading LLMs to generate potentially\nharmful content. Exploring jailbreak prompts can help to better reveal the\nweaknesses of LLMs and further steer us to secure them. Unfortunately, existing\njailbreak methods either suffer from intricate manual design or require\noptimization on other white-box models, which compromises either generalization\nor efficiency. In this paper, we generalize jailbreak prompt attacks into two\naspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we\npropose ReNeLLM, an automatic framework that leverages LLMs themselves to\ngenerate effective jailbreak prompts. Extensive experiments demonstrate that\nReNeLLM significantly improves the attack success rate while greatly reducing\nthe time cost compared to existing baselines. Our study also reveals the\ninadequacy of current defense methods in safeguarding LLMs. Finally, we analyze\nthe failure of LLMs defense from the perspective of prompt execution priority,\nand propose corresponding defense strategies. We hope that our research can\ncatalyze both the academic community and LLMs developers towards the provision\nof safer and more regulated LLMs. The code is available at\nhttps://github.com/NJUNLP/ReNeLLM.\n","authors":["Peng Ding","Jun Kuang","Dan Ma","Xuezhi Cao","Yunsen Xian","Jiajun Chen","Shujian Huang"],"pdf_url":"https://arxiv.org/pdf/2311.08268v3.pdf","comment":"Acccepted by NAACL 2024, 18 pages, 7 figures, 13 tables"},{"id":"http://arxiv.org/abs/2403.18542v1","updated":"2024-03-27T13:22:38Z","published":"2024-03-27T13:22:38Z","title":"Attention-aware semantic relevance predicting Chinese sentence reading","summary":"  In recent years, several influential computational models and metrics have\nbeen proposed to predict how humans comprehend and process sentence. One\nparticularly promising approach is contextual semantic similarity. Inspired by\nthe attention algorithm in Transformer and human memory mechanisms, this study\nproposes an ``attention-aware'' approach for computing contextual semantic\nrelevance. This new approach takes into account the different contributions of\ncontextual parts and the expectation effect, allowing it to incorporate\ncontextual information fully. The attention-aware approach also facilitates the\nsimulation of existing reading models and evaluate them. The resulting\n``attention-aware'' metrics of semantic relevance can more accurately predict\nfixation durations in Chinese reading tasks recorded in an eye-tracking corpus\nthan those calculated by existing approaches. The study's findings further\nprovide strong support for the presence of semantic preview benefits in Chinese\nnaturalistic reading. Furthermore, the attention-aware metrics of semantic\nrelevance, being memory-based, possess high interpretability from both\nlinguistic and cognitive standpoints, making them a valuable computational tool\nfor modeling eye-movements in reading and further gaining insight into the\nprocess of language comprehension. Our approach underscores the potential of\nthese metrics to advance our comprehension of how humans understand and process\nlanguage, ultimately leading to a better understanding of language\ncomprehension and processing.\n","authors":["Kun Sun"],"pdf_url":"https://arxiv.org/pdf/2403.18542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18537v1","updated":"2024-03-27T13:12:57Z","published":"2024-03-27T13:12:57Z","title":"A Path Towards Legal Autonomy: An interoperable and explainable approach\n  to extracting, transforming, loading and computing legal information using\n  large language models, expert systems and Bayesian networks","summary":"  Legal autonomy - the lawful activity of artificial intelligence agents - can\nbe achieved in one of two ways. It can be achieved either by imposing\nconstraints on AI actors such as developers, deployers and users, and on AI\nresources such as data, or by imposing constraints on the range and scope of\nthe impact that AI agents can have on the environment. The latter approach\ninvolves encoding extant rules concerning AI driven devices into the software\nof AI agents controlling those devices (e.g., encoding rules about limitations\non zones of operations into the agent software of an autonomous drone device).\nThis is a challenge since the effectivity of such an approach requires a method\nof extracting, loading, transforming and computing legal information that would\nbe both explainable and legally interoperable, and that would enable AI agents\nto reason about the law. In this paper, we sketch a proof of principle for such\na method using large language models (LLMs), expert legal systems known as\nlegal decision paths, and Bayesian networks. We then show how the proposed\nmethod could be applied to extant regulation in matters of autonomous cars,\nsuch as the California Vehicle Code.\n","authors":["Axel Constant","Hannes Westermann","Bryan Wilson","Alex Kiefer","Ines Hipolito","Sylvain Pronovost","Steven Swanson","Mahault Albarracin","Maxwell J. D. Ramstead"],"pdf_url":"https://arxiv.org/pdf/2403.18537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18525v1","updated":"2024-03-27T12:59:44Z","published":"2024-03-27T12:59:44Z","title":"Language Plays a Pivotal Role in the Object-Attribute Compositional\n  Generalization of CLIP","summary":"  Vision-language models, such as CLIP, have shown promising\nOut-of-Distribution (OoD) generalization under various types of distribution\nshifts. Recent studies attempted to investigate the leading cause of this\ncapability. In this work, we follow the same path, but focus on a specific type\nof OoD data - images with novel compositions of attribute-object pairs - and\nstudy whether such models can successfully classify those images into\ncomposition classes. We carefully designed an authentic image test dataset\ncalled ImageNet-AO, consisting of attributes for objects that are unlikely\nencountered in the CLIP training sets. We found that CLIPs trained with large\ndatasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude\nimprovement in effective compositional OoD generalization compared to both\nsupervised models and CLIPs trained with smaller datasets, such as CC-12M and\nYFCC-15M. Our results provide evidence that the scale and diversity of training\ndata and language supervision play a key role in unlocking the compositional\ngeneralization abilities of vision-language models.\n","authors":["Reza Abbasi","Mohammad Samiei","Mohammad Hossein Rohban","Mahdieh Soleymani Baghshah"],"pdf_url":"https://arxiv.org/pdf/2403.18525v1.pdf","comment":"Oral accepted at OODCV 2023(http://www.ood-cv.org)"},{"id":"http://arxiv.org/abs/2403.18504v1","updated":"2024-03-27T12:33:42Z","published":"2024-03-27T12:33:42Z","title":"AcTED: Automatic Acquisition of Typical Event Duration for\n  Semi-supervised Temporal Commonsense QA","summary":"  We propose a voting-driven semi-supervised approach to automatically acquire\nthe typical duration of an event and use it as pseudo-labeled data. The human\nevaluation demonstrates that our pseudo labels exhibit surprisingly high\naccuracy and balanced coverage. In the temporal commonsense QA task,\nexperimental results show that using only pseudo examples of 400 events, we\nachieve performance comparable to the existing BERT-based weakly supervised\napproaches that require a significant amount of training examples. When\ncompared to the RoBERTa baselines, our best approach establishes\nstate-of-the-art performance with a 7% improvement in Exact Match.\n","authors":["Felix Virgo","Fei Cheng","Lis Kanashiro Pereira","Masayuki Asahara","Ichiro Kobayashi","Sadao Kurohashi"],"pdf_url":"https://arxiv.org/pdf/2403.18504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16516v2","updated":"2024-03-27T12:32:31Z","published":"2024-03-25T08:00:43Z","title":"Visually Guided Generative Text-Layout Pre-training for Document\n  Intelligence","summary":"  Prior study shows that pre-training techniques can boost the performance of\nvisual document understanding (VDU), which typically requires models to gain\nabilities to perceive and reason both document texts and layouts (e.g.,\nlocations of texts and table-cells). To this end, we propose visually guided\ngenerative text-layout pre-training, named ViTLP. Given a document image, the\nmodel optimizes hierarchical language and layout modeling objectives to\ngenerate the interleaved text and layout sequence. In addition, to address the\nlimitation of processing long documents by Transformers, we introduce a\nstraightforward yet effective multi-segment generative pre-training scheme,\nfacilitating ViTLP to process word-intensive documents of any length. ViTLP can\nfunction as a native OCR model to localize and recognize texts of document\nimages. Besides, ViTLP can be effectively applied to various downstream VDU\ntasks. Extensive experiments show that ViTLP achieves competitive performance\nover existing baselines on benchmark VDU tasks, including information\nextraction, document classification, and document question answering.\n","authors":["Zhiming Mao","Haoli Bai","Lu Hou","Jiansheng Wei","Xin Jiang","Qun Liu","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2403.16516v2.pdf","comment":"Accepted to NAACL 2024 main conference. The first version of this\n  paper was submitted to OpenReview\n  (https://openreview.net/forum?id=ARtBIBAmNR) in June 2023"},{"id":"http://arxiv.org/abs/2403.16432v2","updated":"2024-03-27T11:37:58Z","published":"2024-03-25T05:27:35Z","title":"$\\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on\n  Prompt-based Language Models","summary":"  Prompt-based learning is a new language model training paradigm that adapts\nthe Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes\nthe performance benchmarks across various natural language processing (NLP)\ntasks. Instead of using a fixed prompt template to fine-tune the model, some\nresearch demonstrates the effectiveness of searching for the prompt via\noptimization. Such prompt optimization process of prompt-based learning on PLMs\nalso gives insight into generating adversarial prompts to mislead the model,\nraising concerns about the adversarial vulnerability of this paradigm. Recent\nstudies have shown that universal adversarial triggers (UATs) can be generated\nto alter not only the predictions of the target PLMs but also the prediction of\ncorresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based\nlearning paradigm. However, UATs found in previous works are often unreadable\ntokens or characters and can be easily distinguished from natural texts with\nadaptive defenses. In this work, we consider the naturalness of the UATs and\ndevelop $\\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs\nby a gradient-based beam search algorithm that not only effectively attacks the\ntarget PLMs and PFMs but also maintains the naturalness among the trigger\ntokens. Extensive results demonstrate the effectiveness of\n$\\textit{LinkPrompt}$, as well as the transferability of UATs generated by\n$\\textit{LinkPrompt}$ to open-sourced Large Language Model (LLM) Llama2 and\nAPI-accessed LLM GPT-3.5-turbo.\n","authors":["Yue Xu","Wenjie Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16432v2.pdf","comment":"Accepted to the main conference of NAACL2024"},{"id":"http://arxiv.org/abs/2311.07838v3","updated":"2024-03-27T11:36:46Z","published":"2023-11-14T01:38:02Z","title":"LLatrieval: LLM-Verified Retrieval for Verifiable Generation","summary":"  Verifiable generation aims to let the large language model (LLM) generate\ntext with supporting documents, which enables the user to flexibly verify the\nanswer and makes the LLM's output more reliable. Retrieval plays a crucial role\nin verifiable generation. Specifically, the retrieved documents not only\nsupplement knowledge to help the LLM generate correct answers, but also serve\nas supporting evidence for the user to verify the LLM's output. However, the\nwidely used retrievers become the bottleneck of the entire pipeline and limit\nthe overall performance. Their capabilities are usually inferior to LLMs since\nthey often have much fewer parameters than the large language model and have\nnot been demonstrated to scale well to the size of LLMs. If the retriever does\nnot correctly find the supporting documents, the LLM can not generate the\ncorrect and verifiable answer, which overshadows the LLM's remarkable\nabilities. To address these limitations, we propose \\LLatrieval (Large Language\nModel Verified Retrieval), where the LLM updates the retrieval result until it\nverifies that the retrieved documents can sufficiently support answering the\nquestion. Thus, the LLM can iteratively provide feedback to retrieval and\nfacilitate the retrieval result to fully support verifiable generation.\nExperiments show that LLatrieval significantly outperforms extensive baselines\nand achieves state-of-the-art results.\n","authors":["Xiaonan Li","Changtai Zhu","Linyang Li","Zhangyue Yin","Tianxiang Sun","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2311.07838v3.pdf","comment":"Accepted by NAACL 2024 (Main Conference)"},{"id":"http://arxiv.org/abs/2403.18447v1","updated":"2024-03-27T11:06:44Z","published":"2024-03-27T11:06:44Z","title":"Can Language Beat Numerical Regression? Language-Based Multimodal\n  Trajectory Prediction","summary":"  Language models have demonstrated impressive ability in context understanding\nand generative performance. Inspired by the recent success of language\nfoundation models, in this paper, we propose LMTraj (Language-based Multimodal\nTrajectory predictor), which recasts the trajectory prediction task into a sort\nof question-answering problem. Departing from traditional numerical regression\nmodels, which treat the trajectory coordinate sequence as continuous signals,\nwe consider them as discrete signals like text prompts. Specially, we first\ntransform an input space for the trajectory coordinate into the natural\nlanguage space. Here, the entire time-series trajectories of pedestrians are\nconverted into a text prompt, and scene images are described as text\ninformation through image captioning. The transformed numerical and image data\nare then wrapped into the question-answering template for use in a language\nmodel. Next, to guide the language model in understanding and reasoning\nhigh-level knowledge, such as scene context and social relationships between\npedestrians, we introduce an auxiliary multi-task question and answering. We\nthen train a numerical tokenizer with the prompt data. We encourage the\ntokenizer to separate the integer and decimal parts well, and leverage it to\ncapture correlations between the consecutive numbers in the language model.\nLastly, we train the language model using the numerical tokenizer and all of\nthe question-answer prompts. Here, we propose a beam-search-based most-likely\nprediction and a temperature-based multimodal prediction to implement both\ndeterministic and stochastic inferences. Applying our LMTraj, we show that the\nlanguage-based model can be a powerful pedestrian trajectory predictor, and\noutperforms existing numerical-based predictor methods. Code is publicly\navailable at https://github.com/inhwanbae/LMTrajectory .\n","authors":["Inhwan Bae","Junoh Lee","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2403.18447v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2304.03544v2","updated":"2024-03-27T10:53:42Z","published":"2023-04-07T08:49:43Z","title":"InfoCTM: A Mutual Information Maximization Perspective of Cross-Lingual\n  Topic Modeling","summary":"  Cross-lingual topic models have been prevalent for cross-lingual text\nanalysis by revealing aligned latent topics. However, most existing methods\nsuffer from producing repetitive topics that hinder further analysis and\nperformance decline caused by low-coverage dictionaries. In this paper, we\npropose the Cross-lingual Topic Modeling with Mutual Information (InfoCTM).\nInstead of the direct alignment in previous work, we propose a topic alignment\nwith mutual information method. This works as a regularization to properly\nalign topics and prevent degenerate topic representations of words, which\nmitigates the repetitive topic issue. To address the low-coverage dictionary\nissue, we further propose a cross-lingual vocabulary linking method that finds\nmore linked cross-lingual words for topic alignment beyond the translations of\na given dictionary. Extensive experiments on English, Chinese, and Japanese\ndatasets demonstrate that our method outperforms state-of-the-art baselines,\nproducing more coherent, diverse, and well-aligned topics and showing better\ntransferability for cross-lingual classification tasks.\n","authors":["Xiaobao Wu","Xinshuai Dong","Thong Nguyen","Chaoqun Liu","Liangming Pan","Anh Tuan Luu"],"pdf_url":"https://arxiv.org/pdf/2304.03544v2.pdf","comment":"Accepted to AAAI2023 conference. Code is available at\n  https://github.com/BobXWu/InfoCTM"},{"id":"http://arxiv.org/abs/2309.13322v2","updated":"2024-03-27T10:50:24Z","published":"2023-09-23T09:51:37Z","title":"From Text to Source: Results in Detecting Large Language Model-Generated\n  Content","summary":"  The widespread use of Large Language Models (LLMs), celebrated for their\nability to generate human-like text, has raised concerns about misinformation\nand ethical implications. Addressing these concerns necessitates the\ndevelopment of robust methods to detect and attribute text generated by LLMs.\nThis paper investigates \"Cross-Model Detection,\" by evaluating whether a\nclassifier trained to distinguish between source LLM-generated and\nhuman-written text can also detect text from a target LLM without further\ntraining. The study comprehensively explores various LLM sizes and families,\nand assesses the impact of conversational fine-tuning techniques, quantization,\nand watermarking on classifier generalization. The research also explores Model\nAttribution, encompassing source model identification, model family, and model\nsize classification, in addition to quantization and watermarking detection.\nOur results reveal several key findings: a clear inverse relationship between\nclassifier effectiveness and model size, with larger LLMs being more\nchallenging to detect, especially when the classifier is trained on data from\nsmaller models. Training on data from similarly sized LLMs can improve\ndetection performance from larger models but may lead to decreased performance\nwhen dealing with smaller models. Additionally, model attribution experiments\nshow promising results in identifying source models and model families,\nhighlighting detectable signatures in LLM-generated text, with particularly\nremarkable outcomes in watermarking detection, while no detectable signatures\nof quantization were observed. Overall, our study contributes valuable insights\ninto the interplay of model size, family, and training data in LLM detection\nand attribution.\n","authors":["Wissam Antoun","Benoît Sagot","Djamé Seddah"],"pdf_url":"https://arxiv.org/pdf/2309.13322v2.pdf","comment":"Accepted to COLING-LREC 2024"},{"id":"http://arxiv.org/abs/2403.18435v1","updated":"2024-03-27T10:40:14Z","published":"2024-03-27T10:40:14Z","title":"DELTA: Pre-train a Discriminative Encoder for Legal Case Retrieval via\n  Structural Word Alignment","summary":"  Recent research demonstrates the effectiveness of using pre-trained language\nmodels for legal case retrieval. Most of the existing works focus on improving\nthe representation ability for the contextualized embedding of the [CLS] token\nand calculate relevance using textual semantic similarity. However, in the\nlegal domain, textual semantic similarity does not always imply that the cases\nare relevant enough. Instead, relevance in legal cases primarily depends on the\nsimilarity of key facts that impact the final judgment. Without proper\ntreatments, the discriminative ability of learned representations could be\nlimited since legal cases are lengthy and contain numerous non-key facts. To\nthis end, we introduce DELTA, a discriminative model designed for legal case\nretrieval. The basic idea involves pinpointing key facts in legal cases and\npulling the contextualized embedding of the [CLS] token closer to the key facts\nwhile pushing away from the non-key facts, which can warm up the case embedding\nspace in an unsupervised manner. To be specific, this study brings the word\nalignment mechanism to the contextual masked auto-encoder. First, we leverage\nshallow decoders to create information bottlenecks, aiming to enhance the\nrepresentation ability. Second, we employ the deep decoder to enable\ntranslation between different structures, with the goal of pinpointing key\nfacts to enhance discriminative ability. Comprehensive experiments conducted on\npublicly available legal benchmarks show that our approach can outperform\nexisting state-of-the-art methods in legal case retrieval. It provides a new\nperspective on the in-depth understanding and processing of legal case\ndocuments.\n","authors":["Haitao Li","Qingyao Ai","Xinyan Han","Jia Chen","Qian Dong","Yiqun Liu","Chong Chen","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2403.18435v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2403.18430v1","updated":"2024-03-27T10:36:17Z","published":"2024-03-27T10:36:17Z","title":"Exploring language relations through syntactic distances and geographic\n  proximity","summary":"  Languages are grouped into families that share common linguistic traits.\nWhile this approach has been successful in understanding genetic relations\nbetween diverse languages, more analyses are needed to accurately quantify\ntheir relatedness, especially in less studied linguistic levels such as syntax.\nHere, we explore linguistic distances using series of parts of speech (POS)\nextracted from the Universal Dependencies dataset. Within an\ninformation-theoretic framework, we show that employing POS trigrams maximizes\nthe possibility of capturing syntactic variations while being at the same time\ncompatible with the amount of available data. Linguistic connections are then\nestablished by assessing pairwise distances based on the POS distributions.\nIntriguingly, our analysis reveals definite clusters that correspond to well\nknown language families and groups, with exceptions explained by distinct\nmorphological typologies. Furthermore, we obtain a significant correlation\nbetween language similarity and geographic distance, which underscores the\ninfluence of spatial proximity on language kinships.\n","authors":["Juan De Gregorio","Raúl Toral","David Sánchez"],"pdf_url":"https://arxiv.org/pdf/2403.18430v1.pdf","comment":"36 pages"},{"id":"http://arxiv.org/abs/2403.18426v1","updated":"2024-03-27T10:27:28Z","published":"2024-03-27T10:27:28Z","title":"TriviaHG: A Dataset for Automatic Hint Generation from Factoid Questions","summary":"  Nowadays, individuals tend to engage in dialogues with Large Language Models,\nseeking answers to their questions. In times when such answers are readily\naccessible to anyone, the stimulation and preservation of human's cognitive\nabilities, as well as the assurance of maintaining good reasoning skills by\nhumans becomes crucial. This study addresses such needs by proposing hints\n(instead of final answers or before giving answers) as a viable solution. We\nintroduce a framework for the automatic hint generation for factoid questions,\nemploying it to construct TriviaHG, a novel large-scale dataset featuring\n160,230 hints corresponding to 16,645 questions from the TriviaQA dataset.\nAdditionally, we present an automatic evaluation method that measures the\nConvergence and Familiarity quality attributes of hints. To evaluate the\nTriviaHG dataset and the proposed evaluation method, we enlisted 10 individuals\nto annotate 2,791 hints and tasked 6 humans with answering questions using the\nprovided hints. The effectiveness of hints varied, with success rates of 96%,\n78%, and 36% for questions with easy, medium, and hard answers, respectively.\nMoreover, the proposed automatic evaluation methods showed a robust correlation\nwith annotators' results. Conclusively, the findings highlight three key\ninsights: the facilitative role of hints in resolving unknown questions, the\ndependence of hint quality on answer difficulty, and the feasibility of\nemploying automatic evaluation methods for hint assessment.\n","authors":["Jamshid Mozafari","Anubhav Jangra","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2403.18426v1.pdf","comment":"Accepted at SIGIR 2024"},{"id":"http://arxiv.org/abs/2403.18423v1","updated":"2024-03-27T10:24:25Z","published":"2024-03-27T10:24:25Z","title":"SemRoDe: Macro Adversarial Training to Learn Representations That are\n  Robust to Word-Level Attacks","summary":"  Language models (LMs) are indispensable tools for natural language processing\ntasks, but their vulnerability to adversarial attacks remains a concern. While\ncurrent research has explored adversarial training techniques, their\nimprovements to defend against word-level attacks have been limited. In this\nwork, we propose a novel approach called Semantic Robust Defence (SemRoDe), a\nMacro Adversarial Training strategy to enhance the robustness of LMs. Drawing\ninspiration from recent studies in the image domain, we investigate and later\nconfirm that in a discrete data setting such as language, adversarial samples\ngenerated via word substitutions do indeed belong to an adversarial domain\nexhibiting a high Wasserstein distance from the base domain. Our method learns\na robust representation that bridges these two domains. We hypothesize that if\nsamples were not projected into an adversarial domain, but instead to a domain\nwith minimal shift, it would improve attack robustness. We align the domains by\nincorporating a new distance-based objective. With this, our model is able to\nlearn more generalized representations by aligning the model's high-level\noutput features and therefore better handling unseen adversarial samples. This\nmethod can be generalized across word embeddings, even when they share minimal\noverlap at both vocabulary and word-substitution levels. To evaluate the\neffectiveness of our approach, we conduct experiments on BERT and RoBERTa\nmodels on three datasets. The results demonstrate promising state-of-the-art\nrobustness.\n","authors":["Brian Formento","Wenjie Feng","Chuan Sheng Foo","Luu Anh Tuan","See-Kiong Ng"],"pdf_url":"https://arxiv.org/pdf/2403.18423v1.pdf","comment":"Published in NAACL 2024 (Main Track)"},{"id":"http://arxiv.org/abs/2402.01739v2","updated":"2024-03-27T10:21:24Z","published":"2024-01-29T12:05:02Z","title":"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models","summary":"  To help the open-source community have a better understanding of\nMixture-of-Experts (MoE) based large language models (LLMs), we train and\nrelease OpenMoE, a series of fully open-sourced and reproducible decoder-only\nMoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T\ntokens. Our investigation confirms that MoE-based LLMs can offer a more\nfavorable cost-effectiveness trade-off than dense LLMs, highlighting the\npotential effectiveness for future LLM development.\n  One more important contribution of this study is an in-depth analysis of the\nrouting mechanisms within our OpenMoE models, leading to three significant\nfindings: Context-Independent Specialization, Early Routing Learning, and\nDrop-towards-the-End. We discovered that routing decisions in MoE models are\npredominantly based on token IDs, with minimal context relevance. The\ntoken-to-expert assignments are determined early in the pre-training phase and\nremain largely unchanged. This imperfect routing can result in performance\ndegradation, particularly in sequential tasks like multi-turn conversations,\nwhere tokens appearing later in a sequence are more likely to be dropped.\nFinally, we rethink our design based on the above-mentioned observations and\nanalysis. To facilitate future MoE LLM development, we propose potential\nstrategies for mitigating the issues we found and further improving\noff-the-shelf MoE LLM designs.\n","authors":["Fuzhao Xue","Zian Zheng","Yao Fu","Jinjie Ni","Zangwei Zheng","Wangchunshu Zhou","Yang You"],"pdf_url":"https://arxiv.org/pdf/2402.01739v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18421v1","updated":"2024-03-27T10:18:21Z","published":"2024-03-27T10:18:21Z","title":"BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text","summary":"  Models such as GPT-4 and Med-PaLM 2 have demonstrated impressive performance\non a wide variety of biomedical NLP tasks. However, these models have hundreds\nof billions of parameters, are computationally expensive to run, require users\nto send their input data over the internet, and are trained on unknown data\nsources. Can smaller, more targeted models compete? To address this question,\nwe build and release BioMedLM, a 2.7 billion parameter GPT-style autoregressive\nmodel trained exclusively on PubMed abstracts and full articles. When\nfine-tuned, BioMedLM can produce strong multiple-choice biomedical\nquestion-answering results competitive with much larger models, such as\nachieving a score of 57.3% on MedMCQA (dev) and 69.0% on the MMLU Medical\nGenetics exam. BioMedLM can also be fine-tuned to produce useful answers to\npatient questions on medical topics. This demonstrates that smaller models can\npotentially serve as transparent, privacy-preserving, economical and\nenvironmentally friendly foundations for particular NLP applications, such as\nin biomedicine. The model is available on the Hugging Face Hub:\nhttps://huggingface.co/stanford-crfm/BioMedLM.\n","authors":["Elliot Bolton","Abhinav Venigalla","Michihiro Yasunaga","David Hall","Betty Xiong","Tony Lee","Roxana Daneshjou","Jonathan Frankle","Percy Liang","Michael Carbin","Christopher D. Manning"],"pdf_url":"https://arxiv.org/pdf/2403.18421v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2403.17647v2","updated":"2024-03-27T10:07:59Z","published":"2024-03-26T12:29:18Z","title":"Intrinsic Subgraph Generation for Interpretable Graph based Visual\n  Question Answering","summary":"  The large success of deep learning based methods in Visual Question Answering\n(VQA) has concurrently increased the demand for explainable methods. Most\nmethods in Explainable Artificial Intelligence (XAI) focus on generating\npost-hoc explanations rather than taking an intrinsic approach, the latter\ncharacterizing an interpretable model. In this work, we introduce an\ninterpretable approach for graph-based VQA and demonstrate competitive\nperformance on the GQA dataset. This approach bridges the gap between\ninterpretability and performance. Our model is designed to intrinsically\nproduce a subgraph during the question-answering process as its explanation,\nproviding insight into the decision making. To evaluate the quality of these\ngenerated subgraphs, we compare them against established post-hoc\nexplainability methods for graph neural networks, and perform a human\nevaluation. Moreover, we present quantitative metrics that correlate with the\nevaluations of human assessors, acting as automatic metrics for the generated\nexplanatory subgraphs. Our implementation is available at\nhttps://github.com/DigitalPhonetics/Intrinsic-Subgraph-Generation-for-VQA.\n","authors":["Pascal Tilli","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2403.17647v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18406v1","updated":"2024-03-27T09:48:23Z","published":"2024-03-27T09:48:23Z","title":"An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering\n  Using a VLM","summary":"  Stimulated by the sophisticated reasoning capabilities of recent Large\nLanguage Models (LLMs), a variety of strategies for bridging video modality\nhave been devised. A prominent strategy involves Video Language Models\n(VideoLMs), which train a learnable interface with video data to connect\nadvanced vision encoders with LLMs. Recently, an alternative strategy has\nsurfaced, employing readily available foundation models, such as VideoLMs and\nLLMs, across multiple stages for modality bridging. In this study, we introduce\na simple yet novel strategy where only a single Vision Language Model (VLM) is\nutilized. Our starting point is the plain insight that a video comprises a\nseries of images, or frames, interwoven with temporal information. The essence\nof video comprehension lies in adeptly managing the temporal aspects along with\nthe spatial details of each frame. Initially, we transform a video into a\nsingle composite image by arranging multiple frames in a grid layout. The\nresulting single image is termed as an image grid. This format, while\nmaintaining the appearance of a solitary image, effectively retains temporal\ninformation within the grid structure. Therefore, the image grid approach\nenables direct application of a single high-performance VLM without\nnecessitating any video-data training. Our extensive experimental analysis\nacross ten zero-shot video question answering benchmarks, including five\nopen-ended and five multiple-choice benchmarks, reveals that the proposed Image\nGrid Vision Language Model (IG-VLM) surpasses the existing methods in nine out\nof ten benchmarks.\n","authors":["Wonkyun Kim","Changin Choi","Wonseok Lee","Wonjong Rhee"],"pdf_url":"https://arxiv.org/pdf/2403.18406v1.pdf","comment":"Our code is available at https://github.com/imagegridworth/IG-VLM"},{"id":"http://arxiv.org/abs/2403.18381v1","updated":"2024-03-27T09:19:13Z","published":"2024-03-27T09:19:13Z","title":"Improving Attributed Text Generation of Large Language Models via\n  Preference Learning","summary":"  Large language models have been widely adopted in natural language\nprocessing, yet they face the challenge of generating unreliable content.\nRecent works aim to reduce misinformation and hallucinations by resorting to\nattribution as a means to provide evidence (i.e., citations). However, current\nattribution methods usually focus on the retrieval stage and automatic\nevaluation that neglect mirroring the citation mechanisms in human scholarly\nwriting to bolster credibility. In this paper, we address these challenges by\nmodelling the attribution task as preference learning and introducing an\nAutomatic Preference Optimization (APO) framework. First, we create a curated\ncollection for post-training with 6,330 examples by collecting and filtering\nfrom existing datasets. Second, considering the high cost of labelling\npreference data, we further propose an automatic method to synthesize\nattribution preference data resulting in 95,263 pairs. Moreover, inspired by\nthe human citation process, we further propose a progressive preference\noptimization method by leveraging fine-grained information. Extensive\nexperiments on three datasets (i.e., ASQA, StrategyQA, and ELI5) demonstrate\nthat APO achieves state-of-the-art citation F1 with higher answer quality.\n","authors":["Dongfang Li","Zetian Sun","Baotian Hu","Zhenyu Liu","Xinshuo Hu","Xuebo Liu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.18381v1.pdf","comment":"23 pages, 15 tables, 2 figures"},{"id":"http://arxiv.org/abs/2312.10997v5","updated":"2024-03-27T09:16:57Z","published":"2023-12-18T07:47:33Z","title":"Retrieval-Augmented Generation for Large Language Models: A Survey","summary":"  Large Language Models (LLMs) showcase impressive capabilities but encounter\nchallenges like hallucination, outdated knowledge, and non-transparent,\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\nemerged as a promising solution by incorporating knowledge from external\ndatabases. This enhances the accuracy and credibility of the generation,\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\nupdates and integration of domain-specific information. RAG synergistically\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\nexternal databases. This comprehensive review paper offers a detailed\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\ntripartite foundation of RAG frameworks, which includes the retrieval, the\ngeneration and the augmentation techniques. The paper highlights the\nstate-of-the-art technologies embedded in each of these critical components,\nproviding a profound understanding of the advancements in RAG systems.\nFurthermore, this paper introduces up-to-date evaluation framework and\nbenchmark. At the end, this article delineates the challenges currently faced\nand points out prospective avenues for research and development.\n","authors":["Yunfan Gao","Yun Xiong","Xinyu Gao","Kangxiang Jia","Jinliu Pan","Yuxi Bi","Yi Dai","Jiawei Sun","Meng Wang","Haofen Wang"],"pdf_url":"https://arxiv.org/pdf/2312.10997v5.pdf","comment":"Ongoing Work"},{"id":"http://arxiv.org/abs/2403.18365v1","updated":"2024-03-27T08:57:21Z","published":"2024-03-27T08:57:21Z","title":"BLADE: Enhancing Black-box Large Language Models with Small\n  Domain-Specific Models","summary":"  Large Language Models (LLMs) like ChatGPT and GPT-4 are versatile and capable\nof addressing a diverse range of tasks. However, general LLMs, which are\ndeveloped on open-domain data, may lack the domain-specific knowledge essential\nfor tasks in vertical domains, such as legal, medical, etc. To address this\nissue, previous approaches either conduct continuous pre-training with\ndomain-specific data or employ retrieval augmentation to support general LLMs.\nUnfortunately, these strategies are either cost-intensive or unreliable in\npractical applications. To this end, we present a novel framework named BLADE,\nwhich enhances Black-box LArge language models with small Domain-spEcific\nmodels. BLADE consists of a black-box LLM and a small domain-specific LM. The\nsmall LM preserves domain-specific knowledge and offers specialized insights,\nwhile the general LLM contributes robust language comprehension and reasoning\ncapabilities. Specifically, our method involves three steps: 1) pre-training\nthe small LM with domain-specific data, 2) fine-tuning this model using\nknowledge instruction data, and 3) joint Bayesian optimization of the general\nLLM and the small LM. Extensive experiments conducted on public legal and\nmedical benchmarks reveal that BLADE significantly outperforms existing\napproaches. This shows the potential of BLADE as an effective and\ncost-efficient solution in adapting general LLMs for vertical domains.\n","authors":["Haitao Li","Qingyao Ai","Jia Chen","Qian Dong","Zhijing Wu","Yiqun Liu","Chong Chen","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2403.18365v1.pdf","comment":"11pages"},{"id":"http://arxiv.org/abs/2307.16071v2","updated":"2024-03-27T08:56:01Z","published":"2023-07-29T20:42:50Z","title":"ÌròyìnSpeech: A multi-purpose Yorùbá Speech Corpus","summary":"  We introduce \\`{I}r\\`{o}y\\`{i}nSpeech, a new corpus influenced by the desire\nto increase the amount of high quality, contemporary Yor\\`{u}b\\'{a} speech\ndata, which can be used for both Text-to-Speech (TTS) and Automatic Speech\nRecognition (ASR) tasks. We curated about 23000 text sentences from news and\ncreative writing domains with the open license CC-BY-4.0. To encourage a\nparticipatory approach to data creation, we provide 5000 curated sentences to\nthe Mozilla Common Voice platform to crowd-source the recording and validation\nof Yor\\`{u}b\\'{a} speech data. In total, we created about 42 hours of speech\ndata recorded by 80 volunteers in-house, and 6 hours of validated recordings on\nMozilla Common Voice platform. Our TTS evaluation suggests that a\nhigh-fidelity, general domain, single-speaker Yor\\`{u}b\\'{a} voice is possible\nwith as little as 5 hours of speech. Similarly, for ASR we obtained a baseline\nword error rate (WER) of 23.8.\n","authors":["Tolulope Ogunremi","Kola Tubosun","Anuoluwapo Aremu","Iroro Orife","David Ifeoluwa Adelani"],"pdf_url":"https://arxiv.org/pdf/2307.16071v2.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.15837v2","updated":"2024-03-27T08:54:06Z","published":"2024-03-23T13:24:31Z","title":"Centered Masking for Language-Image Pre-Training","summary":"  We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel,\nstraightforward, and effective technique for masking image patches during\npre-training of a vision-language model. GLIP builds on Fast Language-Image\nPre-Training (FLIP), which randomly masks image patches while training a CLIP\nmodel. GLIP replaces random masking with centered masking, that uses a Gaussian\ndistribution and is inspired by the importance of image patches at the center\nof the image. GLIP retains the same computational savings as FLIP, while\nimproving performance across a range of downstream datasets and tasks, as\ndemonstrated by our experimental results. We show the benefits of GLIP to be\neasy to obtain, requiring no delicate tuning of the Gaussian, and also\napplicable to data sets containing images without an obvious center focus.\n","authors":["Mingliang Liang","Martha Larson"],"pdf_url":"https://arxiv.org/pdf/2403.15837v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.02151v2","updated":"2024-03-27T08:43:28Z","published":"2023-05-03T14:33:23Z","title":"Identifying the Correlation Between Language Distance and Cross-Lingual\n  Transfer in a Multilingual Representation Space","summary":"  Prior research has investigated the impact of various linguistic features on\ncross-lingual transfer performance. In this study, we investigate the manner in\nwhich this effect can be mapped onto the representation space. While past\nstudies have focused on the impact on cross-lingual alignment in multilingual\nlanguage models during fine-tuning, this study examines the absolute evolution\nof the respective language representation spaces produced by MLLMs. We place a\nspecific emphasis on the role of linguistic characteristics and investigate\ntheir inter-correlation with the impact on representation spaces and\ncross-lingual transfer performance. Additionally, this paper provides\npreliminary evidence of how these findings can be leveraged to enhance transfer\nto linguistically distant languages.\n","authors":["Fred Philippy","Siwen Guo","Shohreh Haddadan"],"pdf_url":"https://arxiv.org/pdf/2305.02151v2.pdf","comment":"SIGTYP Workshop 2023 (co-located with EACL 2023)"},{"id":"http://arxiv.org/abs/2403.18350v1","updated":"2024-03-27T08:42:31Z","published":"2024-03-27T08:42:31Z","title":"Evaluation of Semantic Search and its Role in\n  Retrieved-Augmented-Generation (RAG) for Arabic Language","summary":"  The latest advancements in machine learning and deep learning have brought\nforth the concept of semantic similarity, which has proven immensely beneficial\nin multiple applications and has largely replaced keyword search. However,\nevaluating semantic similarity and conducting searches for a specific query\nacross various documents continue to be a complicated task. This complexity is\ndue to the multifaceted nature of the task, the lack of standard benchmarks,\nwhereas these challenges are further amplified for Arabic language. This paper\nendeavors to establish a straightforward yet potent benchmark for semantic\nsearch in Arabic. Moreover, to precisely evaluate the effectiveness of these\nmetrics and the dataset, we conduct our assessment of semantic search within\nthe framework of retrieval augmented generation (RAG).\n","authors":["Ali Mahboub","Muhy Eddin Za'ter","Bashar Alfrou","Yazan Estaitia","Adnan Jaljuli","Asma Hakouz"],"pdf_url":"https://arxiv.org/pdf/2403.18350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18349v1","updated":"2024-03-27T08:39:56Z","published":"2024-03-27T08:39:56Z","title":"Rejection Improves Reliability: Training LLMs to Refuse Unknown\n  Questions Using RL from Knowledge Feedback","summary":"  Large Language Models (LLMs) often generate erroneous outputs, known as\nhallucinations, due to their limitations in discerning questions beyond their\nknowledge scope. While addressing hallucination has been a focal point in\nresearch, previous efforts primarily concentrate on enhancing correctness\nwithout giving due consideration to the significance of rejection mechanisms.\nIn this paper, we conduct a comprehensive examination of the role of rejection,\nintroducing the notion of model reliability along with corresponding metrics.\nThese metrics measure the model's ability to provide accurate responses while\nadeptly rejecting questions exceeding its knowledge boundaries, thereby\nminimizing hallucinations. To improve the inherent reliability of LLMs, we\npresent a novel alignment framework called Reinforcement Learning from\nKnowledge Feedback (RLKF). RLKF leverages knowledge feedback to dynamically\ndetermine the model's knowledge boundary and trains a reliable reward model to\nencourage the refusal of out-of-knowledge questions. Experimental results on\nmathematical questions affirm the substantial efficacy of RLKF in significantly\nenhancing LLM reliability.\n","authors":["Hongshen Xu","Zichen Zhu","Da Ma","Situo Zhang","Shuai Fan","Lu Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2403.18349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18346v1","updated":"2024-03-27T08:38:49Z","published":"2024-03-27T08:38:49Z","title":"Quantifying and Mitigating Unimodal Biases in Multimodal Large Language\n  Models: A Causal Perspective","summary":"  Recent advancements in Large Language Models (LLMs) have facilitated the\ndevelopment of Multimodal LLMs (MLLMs). Despite their impressive capabilities,\nMLLMs often suffer from an over-reliance on unimodal biases (e.g., language\nbias and vision bias), leading to incorrect answers in complex multimodal\ntasks. To investigate this issue, we propose a causal framework to interpret\nthe biases in Visual Question Answering (VQA) problems. Within our framework,\nwe devise a causal graph to elucidate the predictions of MLLMs on VQA problems,\nand assess the causal effect of biases through an in-depth causal analysis.\nMotivated by the causal graph, we introduce a novel MORE dataset, consisting of\n12,000 VQA instances. This dataset is designed to challenge MLLMs' abilities,\nnecessitating multi-hop reasoning and the surmounting of unimodal biases.\nFurthermore, we propose two strategies to mitigate unimodal biases and enhance\nMLLMs' reasoning capabilities, including a Decompose-Verify-Answer (DeVA)\nframework for limited-access MLLMs and the refinement of open-source MLLMs\nthrough fine-tuning. Extensive quantitative and qualitative experiments offer\nvaluable insights for future research.\n","authors":["Meiqi Chen","Yixin Cao","Yan Zhang","Chaochao Lu"],"pdf_url":"https://arxiv.org/pdf/2403.18346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18341v1","updated":"2024-03-27T08:32:19Z","published":"2024-03-27T08:32:19Z","title":"IterAlign: Iterative Constitutional Alignment of Large Language Models","summary":"  With the rapid development of large language models (LLMs), aligning LLMs\nwith human values and societal norms to ensure their reliability and safety has\nbecome crucial. Reinforcement learning with human feedback (RLHF) and\nConstitutional AI (CAI) have been proposed for LLM alignment. However, these\nmethods require either heavy human annotations or explicitly pre-defined\nconstitutions, which are labor-intensive and resource-consuming. To overcome\nthese drawbacks, we study constitution-based LLM alignment and propose a\ndata-driven constitution discovery and self-alignment framework called\nIterAlign. IterAlign leverages red teaming to unveil the weaknesses of an LLM\nand automatically discovers new constitutions using a stronger LLM. These\nconstitutions are then used to guide self-correction of the base LLM. Such a\nconstitution discovery pipeline can be run iteratively and automatically to\ndiscover new constitutions that specifically target the alignment gaps in the\ncurrent LLM. Empirical results on several safety benchmark datasets and\nmultiple base LLMs show that IterAlign successfully improves truthfulness,\nhelpfulness, harmlessness and honesty, improving the LLM alignment by up to\n$13.5\\%$ in harmlessness.\n","authors":["Xiusi Chen","Hongzhi Wen","Sreyashi Nag","Chen Luo","Qingyu Yin","Ruirui Li","Zheng Li","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18341v1.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.18336v1","updated":"2024-03-27T08:21:01Z","published":"2024-03-27T08:21:01Z","title":"A Dataset for Pharmacovigilance in German, French, and Japanese:\n  Annotating Adverse Drug Reactions across Languages","summary":"  User-generated data sources have gained significance in uncovering Adverse\nDrug Reactions (ADRs), with an increasing number of discussions occurring in\nthe digital world. However, the existing clinical corpora predominantly revolve\naround scientific articles in English. This work presents a multilingual corpus\nof texts concerning ADRs gathered from diverse sources, including patient fora,\nsocial media, and clinical reports in German, French, and Japanese. Our corpus\ncontains annotations covering 12 entity types, four attribute types, and 13\nrelation types. It contributes to the development of real-world multilingual\nlanguage models for healthcare. We provide statistics to highlight certain\nchallenges associated with the corpus and conduct preliminary experiments\nresulting in strong baselines for extracting entities and relations between\nthese entities, both within and across languages.\n","authors":["Lisa Raithel","Hui-Syuan Yeh","Shuntaro Yada","Cyril Grouin","Thomas Lavergne","Aurélie Névéol","Patrick Paroubek","Philippe Thomas","Tomohiro Nishiyama","Sebastian Möller","Eiji Aramaki","Yuji Matsumoto","Roland Roller","Pierre Zweigenbaum"],"pdf_url":"https://arxiv.org/pdf/2403.18336v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18327v1","updated":"2024-03-27T08:08:00Z","published":"2024-03-27T08:08:00Z","title":"Can LLMs Converse Formally? Automatically Assessing LLMs in Translating\n  and Interpreting Formal Specifications","summary":"  Stakeholders often describe system requirements using natural language which\nare then converted to formal syntax by a domain-expert leading to increased\ndesign costs. This paper assesses the capabilities of Large Language Models\n(LLMs) in converting between natural language descriptions and formal\nspecifications. Existing work has evaluated the capabilities of LLMs in\ngenerating formal syntax such as source code but such experiments are typically\nhand-crafted and use problems that are likely to be in the training set of\nLLMs, and often require human-annotated datasets. We propose an approach that\ncan use two copies of an LLM in conjunction with an off-the-shelf verifier to\nautomatically evaluate its translation abilities without any additional human\ninput. Our approach generates formal syntax using language grammars to\nautomatically generate a dataset. We conduct an empirical evaluation to measure\nthe accuracy of this translation task and show that SOTA LLMs cannot adequately\nsolve this task, limiting their current utility in the design of complex\nsystems.\n","authors":["Rushang Karia","Daksh Dobhal","Daniel Bramblett","Pulkit Verma","Siddharth Srivastava"],"pdf_url":"https://arxiv.org/pdf/2403.18327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18314v1","updated":"2024-03-27T07:34:44Z","published":"2024-03-27T07:34:44Z","title":"Chinese Offensive Language Detection:Current Status and Future\n  Directions","summary":"  Despite the considerable efforts being made to monitor and regulate\nuser-generated content on social media platforms, the pervasiveness of\noffensive language, such as hate speech or cyberbullying, in the digital space\nremains a significant challenge. Given the importance of maintaining a\ncivilized and respectful online environment, there is an urgent and growing\nneed for automatic systems capable of detecting offensive speech in real time.\nHowever, developing effective systems for processing languages such as Chinese\npresents a significant challenge, owing to the language's complex and nuanced\nnature, which makes it difficult to process automatically. This paper provides\na comprehensive overview of offensive language detection in Chinese, examining\ncurrent benchmarks and approaches and highlighting specific models and tools\nfor addressing the unique challenges of detecting offensive language in this\ncomplex language. The primary objective of this survey is to explore the\nexisting techniques and identify potential avenues for further research that\ncan address the cultural and linguistic complexities of Chinese.\n","authors":["Yunze Xiao","Houda Bouamor","Wajdi Zaghouani"],"pdf_url":"https://arxiv.org/pdf/2403.18314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11399v2","updated":"2024-03-27T07:05:22Z","published":"2024-03-18T01:14:47Z","title":"X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment","summary":"  The impressive development of large language models (LLMs) is expanding into\nthe realm of large multimodal models (LMMs), which incorporate multiple types\nof data beyond text. However, the nature of multimodal models leads to\nsignificant expenses in the creation of training data. Furthermore,\nconstructing multilingual data for LMMs presents its own set of challenges due\nto language diversity and complexity. Therefore, in this study, we propose two\ncost-effective methods to solve this problem: (1) vocabulary expansion and\npretraining of multilingual LLM for specific languages, and (2) automatic and\nelaborate construction of multimodal datasets using GPT4-V. Based on015 these\nmethods, we constructed a 91K English-Korean-Chinese multilingual, multimodal\ntraining dataset. Additionally, we developed a bilingual multimodal model that\nexhibits excellent performance in both Korean and English, surpassing existing\napproaches.\n","authors":["Dongjae Shin","Hyunseok Lim","Inho Won","Changsu Choi","Minjun Kim","Seungwoo Song","Hangyeol Yoo","Sangmin Kim","Kyungtae Lim"],"pdf_url":"https://arxiv.org/pdf/2403.11399v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.12468v3","updated":"2024-03-27T06:46:56Z","published":"2023-02-24T05:48:53Z","title":"Adapting Knowledge for Few-shot Table-to-Text Generation","summary":"  Pretrained language models (PLMs) have made remarkable progress in\ntable-to-text generation tasks. However, the lack of domain-specific knowledge\nmakes it challenging to bridge the topological gap between tabular data and\ntext, especially in real-world applications with limited resources. To mitigate\nthe limitation of insufficient labeled data, we propose a novel framework:\nAdapt-Knowledge-to-Generate (AKG). The core insight of AKG is to adapt\nunlabeled domain-specific knowledge into the model, which brings at least three\nbenefits: (1) it injects representation of normal table-related descriptions to\nbridge the topological gap between tabular data and texts; (2) it enables us to\nuse large amounts of unlabeled domain-specific knowledge fully, which can\nalleviate the PLMs' inherent shortcomings of lacking domain knowledge; (3) it\nallows us to design various tasks to employ the domain-specific knowledge.\nExtensive experiments and analyses are conducted on three open-domain, few-shot\nnatural language generation (NLG) data sets: Humans, Songs, and Books. Compared\nto previous state-of-the-art approaches, our model achieves superior\nperformance in terms of both fluency and accuracy as judged by human and\nautomatic evaluations.\n","authors":["Zhixin Guo","Minyxuan Yan","Jiexing Qi","Jianping Zhou","Ziwei He","Guanjie Zheng","Xinbing Wang"],"pdf_url":"https://arxiv.org/pdf/2302.12468v3.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2302.04415"},{"id":"http://arxiv.org/abs/2403.18295v1","updated":"2024-03-27T06:43:58Z","published":"2024-03-27T06:43:58Z","title":"Dual Instruction Tuning with Large Language Models for Mathematical\n  Reasoning","summary":"  Recent advancements highlight the success of instruction tuning with large\nlanguage models (LLMs) utilizing Chain-of-Thought (CoT) data for mathematical\nreasoning tasks. Despite the fine-tuned LLMs, challenges persist, such as\nincorrect, missing, and redundant steps in CoT generation leading to\ninaccuracies in answer predictions. To alleviate this problem, we propose a\ndual instruction tuning strategy to meticulously model mathematical reasoning\nfrom both forward and reverse directions. This involves introducing the\nIntermediate Reasoning State Prediction task (forward reasoning) and the\nInstruction Reconstruction task (reverse reasoning) to enhance the LLMs'\nunderstanding and execution of instructions. Training instances for these tasks\nare constructed based on existing mathematical instruction tuning datasets.\nSubsequently, LLMs undergo multi-task fine-tuning using both existing\nmathematical instructions and the newly created data. Comprehensive experiments\nvalidate the effectiveness and domain generalization of the dual instruction\ntuning strategy across various mathematical reasoning tasks.\n","authors":["Yongwei Zhou","Tiejun Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.18295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06201v3","updated":"2024-03-27T06:31:42Z","published":"2024-01-11T15:45:11Z","title":"EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction","summary":"  To address intricate real-world tasks, there has been a rising interest in\ntool utilization in applications of large language models (LLMs). To develop\nLLM-based agents, it usually requires LLMs to understand many tool functions\nfrom different tool documentation. But these documentations could be diverse,\nredundant or incomplete, which immensely affects the capability of LLMs in\nusing tools. To solve this, we introduce EASYTOOL, a framework transforming\ndiverse and lengthy tool documentation into a unified and concise tool\ninstruction for easier tool usage. EasyTool purifies essential information from\nextensive tool documentation of different sources, and elaborates a unified\ninterface (i.e., tool instruction) to offer standardized tool descriptions and\nfunctionalities for LLM-based agents. Extensive experiments on multiple\ndifferent tasks demonstrate that EasyTool can significantly reduce token\nconsumption and improve the performance of tool utilization in real-world\nscenarios. Our code will be available at\n\\url{https://github.com/microsoft/JARVIS/} in the future.\n","authors":["Siyu Yuan","Kaitao Song","Jiangjie Chen","Xu Tan","Yongliang Shen","Ren Kan","Dongsheng Li","Deqing Yang"],"pdf_url":"https://arxiv.org/pdf/2401.06201v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18286v1","updated":"2024-03-27T06:25:40Z","published":"2024-03-27T06:25:40Z","title":"Few-Shot Recalibration of Language Models","summary":"  Recent work has uncovered promising ways to extract well-calibrated\nconfidence estimates from language models (LMs), where the model's confidence\nscore reflects how likely it is to be correct. However, while LMs may appear\nwell-calibrated over broad distributions, this often hides significant\nmiscalibration within narrower slices (e.g., systemic over-confidence in math\ncan balance out systemic under-confidence in history, yielding perfect\ncalibration in aggregate). To attain well-calibrated confidence estimates for\nany slice of a distribution, we propose a new framework for few-shot\nslice-specific recalibration. Specifically, we train a recalibration model that\ntakes in a few unlabeled examples from any given slice and predicts a curve\nthat remaps confidence scores to be more accurate for that slice. Our trained\nmodel can recalibrate for arbitrary new slices, without using any labeled data\nfrom that slice. This enables us to identify domain-specific confidence\nthresholds above which the LM's predictions can be trusted, and below which it\nshould abstain. Experiments show that our few-shot recalibrator consistently\noutperforms existing calibration methods, for instance improving calibration\nerror for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.\n","authors":["Xiang Lisa Li","Urvashi Khandelwal","Kelvin Guu"],"pdf_url":"https://arxiv.org/pdf/2403.18286v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2403.16512v2","updated":"2024-03-27T06:25:10Z","published":"2024-03-25T07:55:29Z","title":"LLMs Are Few-Shot In-Context Low-Resource Language Learners","summary":"  In-context learning (ICL) empowers large language models (LLMs) to perform\ndiverse tasks in underrepresented languages using only short in-context\ninformation, offering a crucial avenue for narrowing the gap between\nhigh-resource and low-resource languages. Nonetheless, there is only a handful\nof works explored ICL for low-resource languages with most of them focusing on\nrelatively high-resource languages, such as French and Spanish. In this work,\nwe extensively study ICL and its cross-lingual variation (X-ICL) on 25\nlow-resource and 7 relatively higher-resource languages. Our study not only\nassesses the effectiveness of ICL with LLMs in low-resource languages but also\nidentifies the shortcomings of in-context label alignment, and introduces a\nmore effective alternative: query alignment. Moreover, we provide valuable\ninsights into various facets of ICL for low-resource languages. Our study\nconcludes the significance of few-shot in-context information on enhancing the\nlow-resource understanding quality of LLMs through semantically relevant\ninformation by closing the language gap in the target language and aligning the\nsemantics between the targeted low-resource and the high-resource language that\nthe model is proficient in. Our work highlights the importance of advancing ICL\nresearch, particularly for low-resource languages.\n","authors":["Samuel Cahyawijaya","Holy Lovenia","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2403.16512v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18277v1","updated":"2024-03-27T06:13:04Z","published":"2024-03-27T06:13:04Z","title":"BlendX: Complex Multi-Intent Detection with Blended Patterns","summary":"  Task-oriented dialogue (TOD) systems are commonly designed with the\npresumption that each utterance represents a single intent. However, this\nassumption may not accurately reflect real-world situations, where users\nfrequently express multiple intents within a single utterance. While there is\nan emerging interest in multi-intent detection (MID), existing in-domain\ndatasets such as MixATIS and MixSNIPS have limitations in their formulation. To\naddress these issues, we present BlendX, a suite of refined datasets featuring\nmore diverse patterns than their predecessors, elevating both its complexity\nand diversity. For dataset construction, we utilize both rule-based heuristics\nas well as a generative tool -- OpenAI's ChatGPT -- which is augmented with a\nsimilarity-driven strategy for utterance selection. To ensure the quality of\nthe proposed datasets, we also introduce three novel metrics that assess the\nstatistical properties of an utterance related to word count, conjunction use,\nand pronoun usage. Extensive experiments on BlendX reveal that state-of-the-art\nMID models struggle with the challenges posed by the new datasets, highlighting\nthe need to reexamine the current state of the MID field. The dataset is\navailable at https://github.com/HYU-NLP/BlendX.\n","authors":["Yejin Yoon","Jungyeon Lee","Kangsan Kim","Chanhee Park","Taeuk Kim"],"pdf_url":"https://arxiv.org/pdf/2403.18277v1.pdf","comment":"Accepted to LREC-COLING2024"},{"id":"http://arxiv.org/abs/2403.18276v1","updated":"2024-03-27T06:07:05Z","published":"2024-03-27T06:07:05Z","title":"RankMamba, Benchmarking Mamba's Document Ranking Performance in the Era\n  of Transformers","summary":"  Transformer structure has achieved great success in multiple applied machine\nlearning communities, such as natural language processing (NLP), computer\nvision (CV) and information retrieval (IR). Transformer architecture's core\nmechanism -- attention requires $O(n^2)$ time complexity in training and $O(n)$\ntime complexity in inference. Many works have been proposed to improve the\nattention mechanism's scalability, such as Flash Attention and Multi-query\nAttention. A different line of work aims to design new mechanisms to replace\nattention. Recently, a notable model structure -- Mamba, which is based on\nstate space models, has achieved transformer-equivalent performance in multiple\nsequence modeling tasks.\n  In this work, we examine \\mamba's efficacy through the lens of a classical IR\ntask -- document ranking. A reranker model takes a query and a document as\ninput, and predicts a scalar relevance score. This task demands the language\nmodel's ability to comprehend lengthy contextual inputs and to capture the\ninteraction between query and document tokens. We find that (1) Mamba models\nachieve competitive performance compared to transformer-based models with the\nsame training recipe; (2) but also have a lower training throughput in\ncomparison to efficient transformer implementations such as flash attention. We\nhope this study can serve as a starting point to explore Mamba models in other\nclassical IR tasks. Our code implementation and trained checkpoints are made\npublic to facilitate\nreproducibility.\\footnote{https://github.com/zhichaoxu-shufe/RankMamba}.\n","authors":["Zhichao Xu"],"pdf_url":"https://arxiv.org/pdf/2403.18276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17636v2","updated":"2024-03-27T05:55:35Z","published":"2024-03-26T12:11:29Z","title":"Mix-Initiative Response Generation with Dynamic Prefix Tuning","summary":"  Mixed initiative serves as one of the key factors in controlling conversation\ndirections. For a speaker, responding passively or leading proactively would\nresult in rather different responses. However, most dialogue systems focus on\ntraining a holistic response generation model without any distinction among\ndifferent initiatives. It leads to the cross-contamination problem, where the\nmodel confuses different initiatives and generates inappropriate responses.\nMoreover, obtaining plenty of human annotations for initiative labels can be\nexpensive. To address this issue, we propose a general mix-Initiative Dynamic\nPrefix Tuning framework (IDPT) to decouple different initiatives from the\ngeneration model, which learns initiative-aware prefixes in both supervised and\nunsupervised settings. Specifically, IDPT decouples initiative factors into\ndifferent prefix parameters and uses the attention mechanism to adjust the\nselection of initiatives in guiding generation dynamically. The prefix\nparameters can be tuned towards accurate initiative prediction as well as\nmix-initiative response generation. Extensive experiments on two public\ndialogue datasets show that the proposed IDPT outperforms previous baselines on\nboth automatic metrics and human evaluations. It also manages to generate\nappropriate responses with manipulated initiatives.\n","authors":["Yuxiang Nie","Heyan Huang","Xian-Ling Mao","Lizi Liao"],"pdf_url":"https://arxiv.org/pdf/2403.17636v2.pdf","comment":"Accepted to the main conference of NAACL 2024"},{"id":"http://arxiv.org/abs/2311.08590v2","updated":"2024-03-27T05:53:58Z","published":"2023-11-14T23:20:51Z","title":"PEMA: An Offsite-Tunable Plug-in External Memory Adaptation for Language\n  Models","summary":"  Pre-trained language models (PLMs) show impressive performance in various\ndownstream NLP tasks. However, pre-training large language models demands\nsubstantial memory and training compute. Furthermore, due to the substantial\nresources required, many PLM weights are confidential. Consequently, users are\ncompelled to share their data with model owners for fine-tuning specific tasks.\nTo overcome the limitations, we introduce Plug-in External Memory Adaptation\n(PEMA), a Parameter-Efficient Fine-Tuning (PEFT) method, enabling PLM\nfine-tuning without requiring access to all the weights. PEMA integrates with\ncontext representations from test data during inference to perform downstream\ntasks. It uses external memory to store PLM-generated context representations\nmapped with target tokens. Our method utilizes weight matrices of LoRA-like\nbottlenecked adapter in the PLM's final layer to enhance efficiency. Our\napproach also includes Gradual Unrolling, a novel interpolation strategy to\nimprove generation quality. We validate PEMA's effectiveness through\nexperiments on syntactic and real datasets for machine translation and style\ntransfer. Our findings show that PEMA outperforms other PEFT approaches in\nmemory and latency efficiency for training, and also excels in maintaining\nsentence meaning and generating appropriate language and styles.\n","authors":["HyunJin Kim","Young Jin Kim","JinYeong Bak"],"pdf_url":"https://arxiv.org/pdf/2311.08590v2.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2403.18260v1","updated":"2024-03-27T05:22:06Z","published":"2024-03-27T05:22:06Z","title":"Toward Interactive Regional Understanding in Vision-Large Language\n  Models","summary":"  Recent Vision-Language Pre-training (VLP) models have demonstrated\nsignificant advancements. Nevertheless, these models heavily rely on image-text\npairs that capture only coarse and global information of an image, leading to a\nlimitation in their regional understanding ability. In this work, we introduce\n\\textbf{RegionVLM}, equipped with explicit regional modeling capabilities,\nallowing them to understand user-indicated image regions. To achieve this, we\ndesign a simple yet innovative architecture, requiring no modifications to the\nmodel architecture or objective function. Additionally, we leverage a dataset\nthat contains a novel source of information, namely Localized Narratives, which\nhas been overlooked in previous VLP research. Our experiments demonstrate that\nour single generalist model not only achieves an interactive dialogue system\nbut also exhibits superior performance on various zero-shot region\nunderstanding tasks, without compromising its ability for global image\nunderstanding.\n","authors":["Jungbeom Lee","Sanghyuk Chun","Sangdoo Yun"],"pdf_url":"https://arxiv.org/pdf/2403.18260v1.pdf","comment":"NAACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2403.09131v2","updated":"2024-03-27T05:02:55Z","published":"2024-03-14T06:49:16Z","title":"ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate\n  Professional and Non-Professional Styled Text","summary":"  Large Language Models (LLMs) have demonstrated efficacy in various linguistic\napplications, including text summarization and controlled text generation.\nHowever, studies into their capacity of switching between styles via\nfine-tuning remain underexplored. This study concentrates on textual\nprofessionalism and introduces a novel methodology, named ProSwitch, which\nequips a language model with the ability to produce both professional and\nnon-professional responses through knowledge-guided instruction tuning.\nProSwitch unfolds across three phases: data preparation for gathering domain\nknowledge and training corpus; instruction tuning for optimizing language\nmodels with multiple levels of instruction formats; and comprehensive\nevaluation for assessing the professionalism discrimination and reference-based\nquality of generated text. Comparative analysis of ProSwitch against both\ngeneral and specialized language models reveals that our approach outperforms\nbaselines in switching between professional and non-professional text\ngeneration.\n","authors":["Chang Zong","Yuyan Chen","Weiming Lu","Jian Shao","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2403.09131v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2312.07950v3","updated":"2024-03-27T04:51:51Z","published":"2023-12-13T07:56:27Z","title":"CBQ: Cross-Block Quantization for Large Language Models","summary":"  Post-training quantization (PTQ) has played a key role in compressing large\nlanguage models (LLMs) with ultra-low costs. However, existing PTQ methods only\nfocus on handling the outliers within one layer or one block, which ignores the\ndependency of blocks and leads to severe performance degradation in low-bit\nsettings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ\nmethod for LLMs. CBQ employs a cross-block dependency using a homologous\nreconstruction scheme, establishing long-range dependencies across multiple\nblocks to minimize error accumulation. Furthermore, CBQ incorporates a\ncoarse-to-fine preprocessing (CFP) strategy for suppressing weight and\nactivation outliers, coupled with an adaptive LoRA-Rounding technique for\nprecise weight quantization. These innovations enable CBQ to not only handle\nextreme outliers effectively but also improve overall quantization accuracy.\nExtensive experiments show that CBQ achieves superior low-bit quantization\n(W4A4, W4A8, W2A16) and outperforms existing state-of-the-art methods across\nvarious LLMs and datasets. Notably, CBQ quantizes the 4-bit LLAMA1-65B model\nwithin only 4.3 hours on a single GPU, achieving a commendable tradeoff between\nperformance and quantization efficiency.\n","authors":["Xin Ding","Xiaoyu Liu","Zhijun Tu","Yun Zhang","Wei Li","Jie Hu","Hanting Chen","Yehui Tang","Zhiwei Xiong","Baoqun Yin","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2312.07950v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18253v1","updated":"2024-03-27T04:51:42Z","published":"2024-03-27T04:51:42Z","title":"MD-PK: Metaphor Detection via Prompt Learning and Knowledge Distillation","summary":"  Metaphors are ubiquitous in daily life, yet detecting them poses a\nsignificant challenge. Previous approaches often struggled with improper\napplication of language rules and overlooked the issue of data sparsity. To\naddress these challenges, we introduce knowledge distillation and prompt\nlearning into metaphor detection. Specifically, we devise a prompt learning\ntemplate tailored for the metaphor detection task. By masking target words and\nproviding relevant prompt information, we guide the model to accurately infer\nthe contextual meaning of these words. This approach not only mitigates the\ninterference from the literal meaning of target words but also ensures the\nproper utilization of MIP language rules for metaphor detection. Moreover, we\nemploy a teacher model equipped with prior knowledge to generate meaningful\nsoft labels, guiding the optimization process of the student model. The\ninclusion of soft labels, akin to label smoothing, helps alleviate the model's\ntendency towards over-confidence and effectively addresses the challenge of\ndata sparsity. Experimental results demonstrate that our proposed model\nachieves state-of-the-art performance across multiple datasets.\n","authors":["Kaidi Jia","Rongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2403.18253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18252v1","updated":"2024-03-27T04:49:23Z","published":"2024-03-27T04:49:23Z","title":"Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models","summary":"  Visual representation learning has been a cornerstone in computer vision,\nevolving from supervised learning with human-annotated labels to aligning\nimage-text pairs from the Internet. Despite recent advancements in multi-modal\nlarge language models (MLLMs), the visual representations they rely on, such as\nCLIP embeddings, often lack access to external world knowledge critical for\nreal-world visual reasoning. In this work, we propose Visual Table, a novel\nvisual representation tailored for MLLMs. It provides hierarchical text\ndescriptions of holistic visual scenes, consisting of a scene description and\nmultiple object-centric descriptions that encompass categories, attributes, and\nknowledge at instance level. We further develop a scalable generator for visual\ntable generation and train it on small-scale annotations from GPT4V. Extensive\nevaluations demonstrate that, with generated visual tables as additional visual\nrepresentations, our model can consistently outperform the state-of-the-art\n(SOTA) MLLMs across diverse benchmarks. When visual tables serve as standalone\nvisual representations, our model can closely match or even beat the SOTA MLLMs\nthat are built on CLIP visual embeddings. Our code is available at\nhttps://github.com/LaVi-Lab/Visual-Table.\n","authors":["Yiwu Zhong","Zi-Yuan Hu","Michael R. Lyu","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18252v1.pdf","comment":"Project page: https://github.com/LaVi-Lab/Visual-Table"},{"id":"http://arxiv.org/abs/2403.18251v1","updated":"2024-03-27T04:47:10Z","published":"2024-03-27T04:47:10Z","title":"Since the Scientific Literature Is Multilingual, Our Models Should Be\n  Too","summary":"  English has long been assumed the $\\textit{lingua franca}$ of scientific\nresearch, and this notion is reflected in the natural language processing (NLP)\nresearch involving scientific document representation. In this position piece,\nwe quantitatively show that the literature is largely multilingual and argue\nthat current models and benchmarks should reflect this linguistic diversity. We\nprovide evidence that text-based models fail to create meaningful\nrepresentations for non-English papers and highlight the negative user-facing\nimpacts of using English-only models non-discriminately across a multilingual\ndomain. We end with suggestions for the NLP community on how to improve\nperformance on non-English documents.\n","authors":["Abteen Ebrahimi","Kenneth Church"],"pdf_url":"https://arxiv.org/pdf/2403.18251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18249v1","updated":"2024-03-27T04:39:18Z","published":"2024-03-27T04:39:18Z","title":"Exploring the Deceptive Power of LLM-Generated Fake News: A Study of\n  Real-World Detection Challenges","summary":"  Recent advancements in Large Language Models (LLMs) have enabled the creation\nof fake news, particularly in complex fields like healthcare. Studies highlight\nthe gap in the deceptive power of LLM-generated fake news with and without\nhuman assistance, yet the potential of prompting techniques has not been fully\nexplored. Thus, this work aims to determine whether prompting strategies can\neffectively narrow this gap. Current LLM-based fake news attacks require human\nintervention for information gathering and often miss details and fail to\nmaintain context consistency. Therefore, to better understand threat tactics,\nwe propose a strong fake news attack method called conditional\nVariational-autoencoder-Like Prompt (VLPrompt). Unlike current methods,\nVLPrompt eliminates the need for additional data collection while maintaining\ncontextual coherence and preserving the intricacies of the original text. To\npropel future research on detecting VLPrompt attacks, we created a new dataset\nnamed VLPrompt fake news (VLPFN) containing real and fake texts. Our\nexperiments, including various detection methods and novel human study metrics,\nwere conducted to assess their performance on our dataset, yielding numerous\nfindings.\n","authors":["Yanshen Sun","Jianfeng He","Limeng Cui","Shuo Lei","Chang-Tien Lu"],"pdf_url":"https://arxiv.org/pdf/2403.18249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14965v4","updated":"2024-03-27T04:38:44Z","published":"2023-05-24T09:57:37Z","title":"Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting\n  Jailbreaks","summary":"  Recent explorations with commercial Large Language Models (LLMs) have shown\nthat non-expert users can jailbreak LLMs by simply manipulating their prompts;\nresulting in degenerate output behavior, privacy and security breaches,\noffensive outputs, and violations of content regulator policies. Limited\nstudies have been conducted to formalize and analyze these attacks and their\nmitigations. We bridge this gap by proposing a formalism and a taxonomy of\nknown (and possible) jailbreaks. We survey existing jailbreak methods and their\neffectiveness on open-source and commercial LLMs (such as GPT-based models,\nOPT, BLOOM, and FLAN-T5-XXL). We further discuss the challenges of jailbreak\ndetection in terms of their effectiveness against known attacks. For further\nanalysis, we release a dataset of model outputs across 3700 jailbreak prompts\nover 4 tasks.\n","authors":["Abhinav Rao","Sachin Vashistha","Atharva Naik","Somak Aditya","Monojit Choudhury"],"pdf_url":"https://arxiv.org/pdf/2305.14965v4.pdf","comment":"Accepted at LREC-COLING 2024 - The 2024 Joint International\n  Conference on Computational Linguistics, Language Resources and Evaluation"},{"id":"http://arxiv.org/abs/2206.08657v6","updated":"2024-03-27T03:53:23Z","published":"2022-06-17T09:42:35Z","title":"BridgeTower: Building Bridges Between Encoders in Vision-Language\n  Representation Learning","summary":"  Vision-Language (VL) models with the Two-Tower architecture have dominated\nvisual-language representation learning in recent years. Current VL models\neither use lightweight uni-modal encoders and learn to extract, align and fuse\nboth modalities simultaneously in a deep cross-modal encoder, or feed the\nlast-layer uni-modal representations from the deep pre-trained uni-modal\nencoders into the top cross-modal encoder. Both approaches potentially restrict\nvision-language representation learning and limit model performance. In this\npaper, we propose BridgeTower, which introduces multiple bridge layers that\nbuild a connection between the top layers of uni-modal encoders and each layer\nof the cross-modal encoder. This enables effective bottom-up cross-modal\nalignment and fusion between visual and textual representations of different\nsemantic levels of pre-trained uni-modal encoders in the cross-modal encoder.\nPre-trained with only 4M images, BridgeTower achieves state-of-the-art\nperformance on various downstream vision-language tasks. In particular, on the\nVQAv2 test-std set, BridgeTower achieves an accuracy of 78.73%, outperforming\nthe previous state-of-the-art model METER by 1.09% with the same pre-training\ndata and almost negligible additional parameters and computational costs.\nNotably, when further scaling the model, BridgeTower achieves an accuracy of\n81.15%, surpassing models that are pre-trained on orders-of-magnitude larger\ndatasets. Code and checkpoints are available at\nhttps://github.com/microsoft/BridgeTower.\n","authors":["Xiao Xu","Chenfei Wu","Shachar Rosenman","Vasudev Lal","Wanxiang Che","Nan Duan"],"pdf_url":"https://arxiv.org/pdf/2206.08657v6.pdf","comment":"Accepted by AAAI 2023, Oral"},{"id":"http://arxiv.org/abs/2306.04357v5","updated":"2024-03-27T03:06:13Z","published":"2023-06-07T11:40:07Z","title":"Dial-MAE: ConTextual Masked Auto-Encoder for Retrieval-based Dialogue\n  Systems","summary":"  Dialogue response selection aims to select an appropriate response from\nseveral candidates based on a given user and system utterance history. Most\nexisting works primarily focus on post-training and fine-tuning tailored for\ncross-encoders. However, there are no post-training methods tailored for dense\nencoders in dialogue response selection. We argue that when the current\nlanguage model, based on dense dialogue systems (such as BERT), is employed as\na dense encoder, it separately encodes dialogue context and response, leading\nto a struggle to achieve the alignment of both representations. Thus, we\npropose Dial-MAE (Dialogue Contextual Masking Auto-Encoder), a straightforward\nyet effective post-training technique tailored for dense encoders in dialogue\nresponse selection. Dial-MAE uses an asymmetric encoder-decoder architecture to\ncompress the dialogue semantics into dense vectors, which achieves better\nalignment between the features of the dialogue context and response. Our\nexperiments have demonstrated that Dial-MAE is highly effective, achieving\nstate-of-the-art performance on two commonly evaluated benchmarks.\n","authors":["Zhenpeng Su","Xing Wu","Wei Zhou","Guangyuan Ma","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2306.04357v5.pdf","comment":"This paper has been accepted by NAACL 2024"},{"id":"http://arxiv.org/abs/2403.00868v2","updated":"2024-03-27T03:03:00Z","published":"2024-03-01T04:39:16Z","title":"SoftTiger: A Clinical Foundation Model for Healthcare Workflows","summary":"  We introduce SoftTiger, a clinical large language model (CLaM) designed as a\nfoundation model for healthcare workflows. The narrative and unstructured\nnature of clinical notes is a major obstacle for healthcare intelligentization.\nWe address a critical problem of structuring clinical notes into clinical data,\naccording to international interoperability standards. We collect and annotate\ndata for three subtasks, namely, international patient summary, clinical\nimpression and medical encounter. We then supervised fine-tuned a\nstate-of-the-art LLM using public and credentialed clinical data. The training\nis orchestrated in a way that the target model can first support basic clinical\ntasks such as abbreviation expansion and temporal information extraction, and\nthen learn to perform more complex downstream clinical tasks. Moreover, we\naddress several modeling challenges in the healthcare context, e.g., extra long\ncontext window. Our blind pairwise evaluation shows that SoftTiger outperforms\nother popular open-source models and GPT-3.5, comparable to Gemini-pro, with a\nmild gap from GPT-4. We believe that LLMs may become a step-stone towards\nhealthcare digitalization and democratization. Therefore, we publicly release\nSoftTiger models at scales of 13 billion and 70 billion parameters, as well as\ndatasets and code for our innovative scalable evaluation, hopefully, making a\nsignificant contribution to the healthcare industry.\n","authors":["Ye Chen","Igor Couto","Wei Cai","Cong Fu","Bruno Dorneles"],"pdf_url":"https://arxiv.org/pdf/2403.00868v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17304v2","updated":"2024-03-27T02:59:57Z","published":"2024-02-27T08:27:15Z","title":"Probing Multimodal Large Language Models for Global and Local Semantic\n  Representations","summary":"  The advancement of Multimodal Large Language Models (MLLMs) has greatly\naccelerated the development of applications in understanding integrated texts\nand images. Recent works leverage image-caption datasets to train MLLMs,\nachieving state-of-the-art performance on image-to-text tasks. However, there\nare few studies exploring which layers of MLLMs make the most effort to the\nglobal image information, which plays vital roles in multimodal comprehension\nand generation. In this study, we find that the intermediate layers of models\ncan encode more global semantic information, whose representation vectors\nperform better on visual-language entailment tasks, rather than the topmost\nlayers. We further probe models regarding local semantic representations\nthrough object recognition tasks. We find that the topmost layers may\nexcessively focus on local information, leading to a diminished ability to\nencode global information. Our code and data are released via\nhttps://github.com/kobayashikanna01/probing_MLLM_rep.\n","authors":["Mingxu Tao","Quzhe Huang","Kun Xu","Liwei Chen","Yansong Feng","Dongyan Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.17304v2.pdf","comment":"Accepted by LREC-COLING 2024 as a short paper (Camera Ready)"},{"id":"http://arxiv.org/abs/2403.17343v2","updated":"2024-03-27T02:49:16Z","published":"2024-03-26T03:05:20Z","title":"Language Models are Free Boosters for Biomedical Imaging Tasks","summary":"  In this study, we uncover the unexpected efficacy of residual-based large\nlanguage models (LLMs) as part of encoders for biomedical imaging tasks, a\ndomain traditionally devoid of language or textual data. The approach diverges\nfrom established methodologies by utilizing a frozen transformer block,\nextracted from pre-trained LLMs, as an innovative encoder layer for the direct\nprocessing of visual tokens. This strategy represents a significant departure\nfrom the standard multi-modal vision-language frameworks, which typically hinge\non language-driven prompts and inputs. We found that these LLMs could boost\nperformance across a spectrum of biomedical imaging applications, including\nboth 2D and 3D visual classification tasks, serving as plug-and-play boosters.\nMore interestingly, as a byproduct, we found that the proposed framework\nachieved superior performance, setting new state-of-the-art results on\nextensive, standardized datasets in MedMNIST-2D and 3D. Through this work, we\naim to open new avenues for employing LLMs in biomedical imaging and enriching\nthe understanding of their potential in this specialized domain.\n","authors":["Zhixin Lai","Jing Wu","Suiyao Chen","Yucheng Zhou","Naira Hovakimyan"],"pdf_url":"https://arxiv.org/pdf/2403.17343v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16915v3","updated":"2024-03-27T01:53:36Z","published":"2024-03-25T16:32:50Z","title":"Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language\n  Models","summary":"  Fine-tuning in information retrieval systems using pre-trained language\nmodels (PLM-based IR) requires learning query representations and\nquery-document relations, in addition to downstream task-specific learning.\nThis study introduces coarse-tuning as an intermediate learning stage that\nbridges pre-training and fine-tuning. By learning query representations and\nquery-document relations in coarse-tuning, we aim to reduce the load of\nfine-tuning and improve the learning effect of downstream IR tasks. We propose\nQuery-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the\nappropriateness of query-document pairs. Evaluation experiments show that the\nproposed method significantly improves MRR and/or nDCG@5 in four ad-hoc\ndocument retrieval datasets. Furthermore, the results of the query prediction\ntask suggested that coarse-tuning facilitated learning of query representation\nand query-document relations.\n","authors":["Atsushi Keyaki","Ribeka Keyaki"],"pdf_url":"https://arxiv.org/pdf/2403.16915v3.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2402.15764v2","updated":"2024-03-27T01:23:58Z","published":"2024-02-24T08:40:30Z","title":"Look Before You Leap: Problem Elaboration Prompting Improves\n  Mathematical Reasoning in Large Language Models","summary":"  Large language models (LLMs) still grapple with complex tasks like\nmathematical reasoning. Despite significant efforts invested in improving\nprefix prompts or reasoning process, the crucial role of problem context might\nhave been neglected. Accurate recognition of inputs is fundamental for solving\nmathematical tasks, as ill-formed problems could potentially mislead LLM's\nreasoning. In this study, we propose a new approach named Problem Elaboration\nPrompting (PEP) to enhance the mathematical capacities of LLMs. Specifically,\nPEP decomposes and elucidates the problem context before reasoning, therefore\nenhancing the context modeling and parsing efficiency. Experiments across\ndatasets and models demonstrate promising performances: (1) PEP demonstrates an\noverall enhancement in various mathematical tasks. For instance, with the\nGPT-3.5 model, PEP exhibits improvements of 9.93% and 8.80% on GSM8k through\ngreedy decoding and self-consistency, respectively. (2) PEP can be easily\nimplemented and integrated with other prompting methods. (3) PEP shows\nparticular strength in handling distraction problems.\n","authors":["Haoran Liao","Jidong Tian","Shaohua Hu","Hao He","Yaohui Jin"],"pdf_url":"https://arxiv.org/pdf/2402.15764v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18182v1","updated":"2024-03-27T01:19:23Z","published":"2024-03-27T01:19:23Z","title":"ZAEBUC-Spoken: A Multilingual Multidialectal Arabic-English Speech\n  Corpus","summary":"  We present ZAEBUC-Spoken, a multilingual multidialectal Arabic-English speech\ncorpus. The corpus comprises twelve hours of Zoom meetings involving multiple\nspeakers role-playing a work situation where Students brainstorm ideas for a\ncertain topic and then discuss it with an Interlocutor. The meetings cover\ndifferent topics and are divided into phases with different language setups.\nThe corpus presents a challenging set for automatic speech recognition (ASR),\nincluding two languages (Arabic and English) with Arabic spoken in multiple\nvariants (Modern Standard Arabic, Gulf Arabic, and Egyptian Arabic) and English\nused with various accents. Adding to the complexity of the corpus, there is\nalso code-switching between these languages and dialects. As part of our work,\nwe take inspiration from established sets of transcription guidelines to\npresent a set of guidelines handling issues of conversational speech,\ncode-switching and orthography of both languages. We further enrich the corpus\nwith two layers of annotations; (1) dialectness level annotation for the\nportion of the corpus where mixing occurs between different variants of Arabic,\nand (2) automatic morphological annotations, including tokenization,\nlemmatization, and part-of-speech tagging.\n","authors":["Injy Hamed","Fadhl Eryani","David Palfreyman","Nizar Habash"],"pdf_url":"https://arxiv.org/pdf/2403.18182v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2301.10856v3","updated":"2024-03-27T00:47:21Z","published":"2023-01-25T22:27:40Z","title":"Partial Mobilization: Tracking Multilingual Information Flows Amongst\n  Russian Media Outlets and Telegram","summary":"  In response to disinformation and propaganda from Russian online media\nfollowing the invasion of Ukraine, Russian media outlets such as Russia Today\nand Sputnik News were banned throughout Europe. To maintain viewership, many of\nthese Russian outlets began to heavily promote their content on messaging\nservices like Telegram. In this work, we study how 16 Russian media outlets\ninteracted with and utilized 732 Telegram channels throughout 2022. Leveraging\nthe foundational model MPNet, DP-means clustering, and Hawkes processes, we\ntrace how narratives spread between news sites and Telegram channels. We show\nthat news outlets not only propagate existing narratives through Telegram but\nthat they source material from the messaging platform. For example, across the\nwebsites in our study, between 2.3% (ura.news) and 26.7% (ukraina.ru) of\narticles discussed content that originated/resulted from activity on Telegram.\nFinally, tracking the spread of individual topics, we measure the rate at which\nnews outlets and Telegram channels disseminate content within the Russian media\necosystem, finding that websites like ura.news and Telegram channels such as\n@genshab are the most effective at disseminating their content.\n","authors":["Hans W. A. Hanley","Zakir Durumeric"],"pdf_url":"https://arxiv.org/pdf/2301.10856v3.pdf","comment":"Accepted to ICWSM 2024"},{"id":"http://arxiv.org/abs/2308.11138v3","updated":"2024-03-27T00:29:33Z","published":"2023-08-22T02:39:42Z","title":"NLP-based detection of systematic anomalies among the narratives of\n  consumer complaints","summary":"  We develop an NLP-based procedure for detecting systematic nonmeritorious\nconsumer complaints, simply called systematic anomalies, among complaint\nnarratives. While classification algorithms are used to detect pronounced\nanomalies, in the case of smaller and frequent systematic anomalies, the\nalgorithms may falter due to a variety of reasons, including technical ones as\nwell as natural limitations of human analysts. Therefore, as the next step\nafter classification, we convert the complaint narratives into quantitative\ndata, which are then analyzed using an algorithm for detecting systematic\nanomalies. We illustrate the entire procedure using complaint narratives from\nthe Consumer Complaint Database of the Consumer Financial Protection Bureau.\n","authors":["Peiheng Gao","Ning Sun","Xuefeng Wang","Chen Yang","Ričardas Zitikis"],"pdf_url":"https://arxiv.org/pdf/2308.11138v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18167v1","updated":"2024-03-27T00:23:03Z","published":"2024-03-27T00:23:03Z","title":"Mechanisms of non-factual hallucinations in language models","summary":"  State-of-the-art language models (LMs) sometimes generate non-factual\nhallucinations that misalign with world knowledge. Despite extensive efforts to\ndetect and mitigate hallucinations, understanding their internal mechanisms\nremains elusive. Our study investigates the mechanistic causes of\nhallucination, specifically non-factual ones where the LM incorrectly predicts\nobject attributes in response to subject-relation queries. With causal\nmediation analysis and embedding space projection, we identify two general\nmechanistic causes of hallucinations shared across LMs of various scales and\ndesigns: 1) insufficient subject attribute knowledge in lower layer MLPs, and\n2) failing to select the correct object attribute in upper layer attention\nheads and MLPs. These two mechanisms exhibit varying degrees of subject-object\nassociation, predictive uncertainty and perturbation robustness. Additionally,\nwe scrutinize LM pre-training checkpoints, revealing distinct learning dynamics\nfor the two mechanistic causes of hallucinations. We also highlight how\nattribution features from our causal analysis can effectively construct\nhallucination detectors. Our work proposes a mechanistic understanding of LM\nfactual errors.\n","authors":["Lei Yu","Meng Cao","Jackie Chi Kit Cheung","Yue Dong"],"pdf_url":"https://arxiv.org/pdf/2403.18167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19056v1","updated":"2024-03-27T23:45:31Z","published":"2024-03-27T23:45:31Z","title":"CAUSE: Counterfactual Assessment of User Satisfaction Estimation in\n  Task-Oriented Dialogue Systems","summary":"  An important unexplored aspect in previous work on user satisfaction\nestimation for Task-Oriented Dialogue (TOD) systems is their evaluation in\nterms of robustness for the identification of user dissatisfaction: current\nbenchmarks for user satisfaction estimation in TOD systems are highly skewed\ntowards dialogues for which the user is satisfied. The effect of having a more\nbalanced set of satisfaction labels on performance is unknown. However,\nbalancing the data with more dissatisfactory dialogue samples requires further\ndata collection and human annotation, which is costly and time-consuming. In\nthis work, we leverage large language models (LLMs) and unlock their ability to\ngenerate satisfaction-aware counterfactual dialogues to augment the set of\noriginal dialogues of a test collection. We gather human annotations to ensure\nthe reliability of the generated samples. We evaluate two open-source LLMs as\nuser satisfaction estimators on our augmented collection against\nstate-of-the-art fine-tuned models. Our experiments show that when used as\nfew-shot user satisfaction estimators, open-source LLMs show higher robustness\nto the increase in the number of dissatisfaction labels in the test collection\nthan the fine-tuned state-of-the-art models. Our results shed light on the need\nfor data augmentation approaches for user satisfaction estimation in TOD\nsystems. We release our aligned counterfactual dialogues, which are curated by\nhuman annotation, to facilitate further research on this topic.\n","authors":["Amin Abolghasemi","Zhaochun Ren","Arian Askari","Mohammad Aliannejadi","Maarten de Rijke","Suzan Verberne"],"pdf_url":"https://arxiv.org/pdf/2403.19056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09682v3","updated":"2024-03-27T23:43:54Z","published":"2023-11-16T08:52:27Z","title":"MacGyver: Are Large Language Models Creative Problem Solvers?","summary":"  We explore the creative problem-solving capabilities of modern LLMs in a\nnovel constrained setting. To this end, we create MACGYVER, an automatically\ngenerated dataset consisting of over 1,600 real-world problems deliberately\ndesigned to trigger innovative usage of objects and necessitate out-of-the-box\nthinking. We then present our collection to both LLMs and humans to compare and\ncontrast their problem-solving abilities. MACGYVER is challenging for both\ngroups, but in unique and complementary ways. For instance, humans excel in\ntasks they are familiar with but struggle with domain-specific knowledge,\nleading to a higher variance. In contrast, LLMs, exposed to a variety of\nspecialized knowledge, attempt broader problems but fail by proposing\nphysically-infeasible actions. Finally, we provide a detailed error analysis of\nLLMs, and demonstrate the potential of enhancing their problem-solving ability\nwith novel prompting techniques such as iterative step-wise reflection and\ndivergent-convergent thinking.\n  This work (1) introduces a fresh arena for intelligent agents focusing on\nintricate aspects of physical reasoning, planning, and unconventional thinking,\nwhich supplements the existing spectrum of machine intelligence; and (2)\nprovides insight into the constrained problem-solving capabilities of both\nhumans and AI.\n","authors":["Yufei Tian","Abhilasha Ravichander","Lianhui Qin","Ronan Le Bras","Raja Marjieh","Nanyun Peng","Yejin Choi","Thomas L. Griffiths","Faeze Brahman"],"pdf_url":"https://arxiv.org/pdf/2311.09682v3.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.19031v1","updated":"2024-03-27T22:05:10Z","published":"2024-03-27T22:05:10Z","title":"Evaluating Large Language Models for Health-Related Text Classification\n  Tasks with Public Social Media Data","summary":"  Large language models (LLMs) have demonstrated remarkable success in NLP\ntasks. However, there is a paucity of studies that attempt to evaluate their\nperformances on social media-based health-related natural language processing\ntasks, which have traditionally been difficult to achieve high scores in. We\nbenchmarked one supervised classic machine learning model based on Support\nVector Machines (SVMs), three supervised pretrained language models (PLMs)\nbased on RoBERTa, BERTweet, and SocBERT, and two LLM based classifiers (GPT3.5\nand GPT4), across 6 text classification tasks. We developed three approaches\nfor leveraging LLMs for text classification: employing LLMs as zero-shot\nclassifiers, us-ing LLMs as annotators to annotate training data for supervised\nclassifiers, and utilizing LLMs with few-shot examples for augmentation of\nmanually annotated data. Our comprehensive experiments demonstrate that\nemploy-ing data augmentation using LLMs (GPT-4) with relatively small\nhuman-annotated data to train lightweight supervised classification models\nachieves superior results compared to training with human-annotated data alone.\nSupervised learners also outperform GPT-4 and GPT-3.5 in zero-shot settings. By\nleveraging this data augmentation strategy, we can harness the power of LLMs to\ndevelop smaller, more effective domain-specific NLP models. LLM-annotated data\nwithout human guidance for training light-weight supervised classification\nmodels is an ineffective strategy. However, LLM, as a zero-shot classifier,\nshows promise in excluding false negatives and potentially reducing the human\neffort required for data annotation. Future investigations are imperative to\nexplore optimal training data sizes and the optimal amounts of augmented data.\n","authors":["Yuting Guo","Anthony Ovadje","Mohammed Ali Al-Garadi","Abeed Sarker"],"pdf_url":"https://arxiv.org/pdf/2403.19031v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09519v2","updated":"2024-03-27T21:52:11Z","published":"2023-11-16T02:50:06Z","title":"Leveraging Code to Improve In-context Learning for Semantic Parsing","summary":"  In-context learning (ICL) is an appealing approach for semantic parsing due\nto its few-shot nature and improved generalization. However, learning to parse\nto rare domain-specific languages (DSLs) from just a few demonstrations is\nchallenging, limiting the performance of even the most capable LLMs. In this\nwork, we improve the effectiveness of ICL for semantic parsing by (1) using\ngeneral-purpose programming languages such as Python instead of DSLs, and (2)\naugmenting prompts with a structured domain description that includes, e.g.,\nthe available classes and functions. We show that both these changes\nsignificantly improve accuracy across three popular datasets. Combined, they\nlead to dramatic improvements (e.g. 7.9% to 66.5% on SMCalFlow compositional\nsplit), nearly closing the performance gap between easier i.i.d.\\ and harder\ncompositional splits when used with a strong model, and reducing the need for a\nlarge number of demonstrations. We find that the resemblance of the target\nparse language to general-purpose code is a more important factor than the\nlanguage's popularity in pre-training corpora. Our findings provide an improved\nmethodology for building semantic parsers in the modern context of ICL with\nLLMs.\n","authors":["Ben Bogin","Shivanshu Gupta","Peter Clark","Ashish Sabharwal"],"pdf_url":"https://arxiv.org/pdf/2311.09519v2.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2403.12984v2","updated":"2024-03-27T21:51:03Z","published":"2024-03-03T11:09:32Z","title":"When SMILES have Language: Drug Classification using Text Classification\n  Methods on Drug SMILES Strings","summary":"  Complex chemical structures, like drugs, are usually defined by SMILES\nstrings as a sequence of molecules and bonds. These SMILES strings are used in\ndifferent complex machine learning-based drug-related research and\nrepresentation works. Escaping from complex representation, in this work, we\npose a single question: What if we treat drug SMILES as conventional sentences\nand engage in text classification for drug classification? Our experiments\naffirm the possibility with very competitive scores. The study explores the\nnotion of viewing each atom and bond as sentence components, employing basic\nNLP methods to categorize drug types, proving that complex problems can also be\nsolved with simpler perspectives. The data and code are available here:\nhttps://github.com/azminewasi/Drug-Classification-NLP.\n","authors":["Azmine Toushik Wasi","Šerbetar Karlo","Raima Islam","Taki Hasan Rafi","Dong-Kyu Chae"],"pdf_url":"https://arxiv.org/pdf/2403.12984v2.pdf","comment":"7 pages, 2 figures, 5 tables, Accepted (invited to present) to the\n  The Second Tiny Papers Track at ICLR 2024\n  (https://openreview.net/forum?id=VUYCyH8fCw)"},{"id":"http://arxiv.org/abs/2403.19021v1","updated":"2024-03-27T21:22:37Z","published":"2024-03-27T21:22:37Z","title":"Towards LLM-RecSys Alignment with Textual ID Learning","summary":"  Generative recommendation based on Large Language Models (LLMs) have\ntransformed the traditional ranking-based recommendation style into a\ntext-to-text generation paradigm. However, in contrast to standard NLP tasks\nthat inherently operate on human vocabulary, current research in generative\nrecommendations struggles to effectively encode recommendation items within the\ntext-to-text framework using concise yet meaningful ID representations. To\nbetter align LLMs with recommendation needs, we propose IDGen, representing\neach item as a unique, concise, semantically rich, platform-agnostic textual ID\nusing human language tokens. This is achieved by training a textual ID\ngenerator alongside the LLM-based recommender, enabling seamless integration of\npersonalized recommendations into natural language generation. Notably, as user\nhistory is expressed in natural language and decoupled from the original\ndataset, our approach suggests the potential for a foundational generative\nrecommendation model. Experiments show that our framework consistently\nsurpasses existing models in sequential recommendation under standard\nexperimental setting. Then, we explore the possibility of training a foundation\nrecommendation model with the proposed method on data collected from 19\ndifferent datasets and tested its recommendation performance on 6 unseen\ndatasets across different platforms under a completely zero-shot setting. The\nresults show that the zero-shot performance of the pre-trained foundation model\nis comparable to or even better than some traditional recommendation models\nbased on supervised training, showing the potential of the IDGen paradigm\nserving as the foundation model for generative recommendation. Code and data\nare open-sourced at https://github.com/agiresearch/IDGenRec.\n","authors":["Juntao Tan","Shuyuan Xu","Wenyue Hua","Yingqiang Ge","Zelong Li","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.19021v1.pdf","comment":"Accepted in SIGIR 2024"},{"id":"http://arxiv.org/abs/2403.10667v2","updated":"2024-03-27T21:11:19Z","published":"2024-03-15T20:21:31Z","title":"Towards Unified Multi-Modal Personalization: Large Vision-Language\n  Models for Generative Recommendation and Beyond","summary":"  Developing a universal model that can effectively harness heterogeneous\nresources and respond to a wide range of personalized needs has been a\nlongstanding community aspiration. Our daily choices, especially in domains\nlike fashion and retail, are substantially shaped by multi-modal data, such as\npictures and textual descriptions. These modalities not only offer intuitive\nguidance but also cater to personalized user preferences. However, the\npredominant personalization approaches mainly focus on the ID or text-based\nrecommendation problem, failing to comprehend the information spanning various\ntasks or modalities. In this paper, our goal is to establish a Unified paradigm\nfor Multi-modal Personalization systems (UniMP), which effectively leverages\nmulti-modal data while eliminating the complexities associated with task- and\nmodality-specific customization. We argue that the advancements in foundational\ngenerative modeling have provided the flexibility and effectiveness necessary\nto achieve the objective. In light of this, we develop a generic and extensible\npersonalization generative framework, that can handle a wide range of\npersonalized needs including item recommendation, product search, preference\nprediction, explanation generation, and further user-guided image generation.\nOur methodology enhances the capabilities of foundational language models for\npersonalized tasks by seamlessly ingesting interleaved cross-modal user history\ninformation, ensuring a more precise and customized experience for users. To\ntrain and evaluate the proposed multi-modal personalized tasks, we also\nintroduce a novel and comprehensive benchmark covering a variety of user\nrequirements. Our experiments on the real-world benchmark showcase the model's\npotential, outperforming competitive methods specialized for each task.\n","authors":["Tianxin Wei","Bowen Jin","Ruirui Li","Hansi Zeng","Zhengyang Wang","Jianhui Sun","Qingyu Yin","Hanqing Lu","Suhang Wang","Jingrui He","Xianfeng Tang"],"pdf_url":"https://arxiv.org/pdf/2403.10667v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.19012v1","updated":"2024-03-27T21:10:07Z","published":"2024-03-27T21:10:07Z","title":"ReflectSumm: A Benchmark for Course Reflection Summarization","summary":"  This paper introduces ReflectSumm, a novel summarization dataset specifically\ndesigned for summarizing students' reflective writing. The goal of ReflectSumm\nis to facilitate developing and evaluating novel summarization techniques\ntailored to real-world scenarios with little training data, %practical tasks\nwith potential implications in the opinion summarization domain in general and\nthe educational domain in particular. The dataset encompasses a diverse range\nof summarization tasks and includes comprehensive metadata, enabling the\nexploration of various research questions and supporting different\napplications. To showcase its utility, we conducted extensive evaluations using\nmultiple state-of-the-art baselines. The results provide benchmarks for\nfacilitating further research in this area.\n","authors":["Yang Zhong","Mohamed Elaraby","Diane Litman","Ahmed Ashraf Butt","Muhsin Menekse"],"pdf_url":"https://arxiv.org/pdf/2403.19012v1.pdf","comment":"LREC-COLING 2024 camera ready; code and dataset are available at\n  https://github.com/EngSalem/ReflectSUMM"},{"id":"http://arxiv.org/abs/2402.11815v2","updated":"2024-03-27T20:30:08Z","published":"2024-02-19T04:11:34Z","title":"HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to\n  Detect Machine-Generated Text?","summary":"  This paper describes our system developed for SemEval-2024 Task 8,\n``Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated\nText Detection'' Machine-generated texts have been one of the main concerns due\nto the use of large language models (LLM) in fake text generation, phishing,\ncheating in exams, or even plagiarizing copyright materials. A lot of systems\nhave been developed to detect machine-generated text. Nonetheless, the majority\nof these systems rely on the text-generating model. This limitation is\nimpractical in real-world scenarios, as it's often impossible to know which\nspecific model the user has used for text generation. In this work, we propose\na $\\textbf{single}$ model based on contrastive learning, which uses\n$\\textbf{$\\approx$40% of the baseline's parameters}$ (149M vs. 355M) but shows\na comparable performance on the test dataset $(\\textbf{21st out of 137\nparticipants})$. Our key finding is that even without an ensemble of multiple\nmodels, a single base model can have comparable performance with the help of\ndata augmentation and contrastive learning. Our code is publicly available at\nhttps://github.com/dipta007/SemEval24-Task8.\n","authors":["Shubhashis Roy Dipta","Sadat Shahriar"],"pdf_url":"https://arxiv.org/pdf/2402.11815v2.pdf","comment":"Camera Ready Version - Accepted in SemEval 2024 (Colocated with NAACL\n  2024)"},{"id":"http://arxiv.org/abs/2311.13628v2","updated":"2024-03-27T20:20:22Z","published":"2023-11-22T18:50:47Z","title":"Prompt Risk Control: A Rigorous Framework for Responsible Deployment of\n  Large Language Models","summary":"  The recent explosion in the capabilities of large language models has led to\na wave of interest in how best to prompt a model to perform a given task. While\nit may be tempting to simply choose a prompt based on average performance on a\nvalidation set, this can lead to a deployment where unexpectedly poor responses\nare generated, especially for the worst-off users. To mitigate this prospect,\nwe propose Prompt Risk Control, a lightweight framework for selecting a prompt\nbased on rigorous upper bounds on families of informative risk measures. We\noffer methods for producing bounds on a diverse set of metrics, including\nquantities that measure worst-case responses and disparities in generation\nquality across the population of users. In addition, we extend the underlying\nstatistical bounding techniques to accommodate the possibility of distribution\nshifts in deployment. Experiments on applications such as open-ended chat,\nmedical question summarization, and code generation highlight how such a\nframework can foster responsible deployment by reducing the risk of the worst\noutcomes.\n","authors":["Thomas P. Zollo","Todd Morrill","Zhun Deng","Jake C. Snell","Toniann Pitassi","Richard Zemel"],"pdf_url":"https://arxiv.org/pdf/2311.13628v2.pdf","comment":"34 pages, 10 figures, published as conference paper at ICLR 2024, and\n  accepted to the Socially Responsible Language Modelling Research (SoLaR)\n  workshop at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2403.18976v1","updated":"2024-03-27T19:45:09Z","published":"2024-03-27T19:45:09Z","title":"\"Sorry, Come Again?\" Prompting -- Enhancing Comprehension and\n  Diminishing Hallucination with [PAUSE]-injected Optimal Paraphrasing","summary":"  Hallucination has emerged as the most vulnerable aspect of contemporary Large\nLanguage Models (LLMs). In this paper, we introduce the Sorry, Come Again (SCA)\nprompting, aimed to avoid LLM hallucinations by enhancing comprehension\nthrough: (i) optimal paraphrasing and (ii) injecting [PAUSE] tokens to delay\nLLM generation. First, we provide an in-depth analysis of linguistic nuances:\nformality, readability, and concreteness of prompts for 21 LLMs, and elucidate\nhow these nuances contribute to hallucinated generation. Prompts with lower\nreadability, formality, or concreteness pose comprehension challenges for LLMs,\nsimilar to those faced by humans. In such scenarios, an LLM tends to speculate\nand generate content based on its imagination (associative memory) to fill\nthese information gaps. Although these speculations may occasionally align with\nfactual information, their accuracy is not assured, often resulting in\nhallucination. Recent studies reveal that an LLM often neglects the middle\nsections of extended prompts, a phenomenon termed as lost in the middle. While\na specific paraphrase may suit one LLM, the same paraphrased version may elicit\na different response from another LLM. Therefore, we propose an optimal\nparaphrasing technique to identify the most comprehensible paraphrase of a\ngiven prompt, evaluated using Integrated Gradient (and its variations) to\nguarantee that the LLM accurately processes all words. While reading lengthy\nsentences, humans often pause at various points to better comprehend the\nmeaning read thus far. We have fine-tuned an LLM with injected [PAUSE] tokens,\nallowing the LLM to pause while reading lengthier prompts. This has brought\nseveral key contributions: (i) determining the optimal position to inject\n[PAUSE], (ii) determining the number of [PAUSE] tokens to be inserted, and\n(iii) introducing reverse proxy tuning to fine-tune the LLM for [PAUSE]\ninsertion.\n","authors":["Vipula Rawte","S. M Towhidul Islam Tonmoy","S M Mehedi Zaman","Prachi Priya","Aman Chadha","Amit P. Sheth","Amitava Das"],"pdf_url":"https://arxiv.org/pdf/2403.18976v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18975v1","updated":"2024-03-27T19:43:45Z","published":"2024-03-27T19:43:45Z","title":"A Novel Corpus of Annotated Medical Imaging Reports and Information\n  Extraction Results Using BERT-based Language Models","summary":"  Medical imaging is critical to the diagnosis, surveillance, and treatment of\nmany health conditions, including oncological, neurological, cardiovascular,\nand musculoskeletal disorders, among others. Radiologists interpret these\ncomplex, unstructured images and articulate their assessments through narrative\nreports that remain largely unstructured. This unstructured narrative must be\nconverted into a structured semantic representation to facilitate secondary\napplications such as retrospective analyses or clinical decision support. Here,\nwe introduce the Corpus of Annotated Medical Imaging Reports (CAMIR), which\nincludes 609 annotated radiology reports from three imaging modality types:\nComputed Tomography, Magnetic Resonance Imaging, and Positron Emission\nTomography-Computed Tomography. Reports were annotated using an event-based\nschema that captures clinical indications, lesions, and medical problems. Each\nevent consists of a trigger and multiple arguments, and a majority of the\nargument types, including anatomy, normalize the spans to pre-defined concepts\nto facilitate secondary use. CAMIR uniquely combines a granular event structure\nand concept normalization. To extract CAMIR events, we explored two BERT\n(Bi-directional Encoder Representation from Transformers)-based architectures,\nincluding an existing architecture (mSpERT) that jointly extracts all event\ninformation and a multi-step approach (PL-Marker++) that we augmented for the\nCAMIR schema.\n","authors":["Namu Park","Kevin Lybarger","Giridhar Kaushik Ramachandran","Spencer Lewis","Aashka Damani","Ozlem Uzuner","Martin Gunn","Meliha Yetisgen"],"pdf_url":"https://arxiv.org/pdf/2403.18975v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18973v1","updated":"2024-03-27T19:42:01Z","published":"2024-03-27T19:42:01Z","title":"Conformal Intent Classification and Clarification for Fast and Accurate\n  Intent Recognition","summary":"  We present Conformal Intent Classification and Clarification (CICC), a\nframework for fast and accurate intent classification for task-oriented\ndialogue systems. The framework turns heuristic uncertainty scores of any\nintent classifier into a clarification question that is guaranteed to contain\nthe true intent at a pre-defined confidence level. By disambiguating between a\nsmall number of likely intents, the user query can be resolved quickly and\naccurately. Additionally, we propose to augment the framework for out-of-scope\ndetection. In a comparative evaluation using seven intent recognition datasets\nwe find that CICC generates small clarification questions and is capable of\nout-of-scope detection. CICC can help practitioners and researchers\nsubstantially in improving the user experience of dialogue agents with specific\nclarification questions.\n","authors":["Floris den Hengst","Ralf Wolter","Patrick Altmeyer","Arda Kaygan"],"pdf_url":"https://arxiv.org/pdf/2403.18973v1.pdf","comment":"9 pages,2 figures,3 tables,6 appendices,to be published in ACL's\n  NAACL Findings 2024"},{"id":"http://arxiv.org/abs/2403.18969v1","updated":"2024-03-27T19:35:41Z","published":"2024-03-27T19:35:41Z","title":"A Survey on Large Language Models from Concept to Implementation","summary":"  Recent advancements in Large Language Models (LLMs), particularly those built\non Transformer architectures, have significantly broadened the scope of natural\nlanguage processing (NLP) applications, transcending their initial use in\nchatbot technology. This paper investigates the multifaceted applications of\nthese models, with an emphasis on the GPT series. This exploration focuses on\nthe transformative impact of artificial intelligence (AI) driven tools in\nrevolutionizing traditional tasks like coding and problem-solving, while also\npaving new paths in research and development across diverse industries. From\ncode interpretation and image captioning to facilitating the construction of\ninteractive systems and advancing computational domains, Transformer models\nexemplify a synergy of deep learning, data analysis, and neural network design.\nThis survey provides an in-depth look at the latest research in Transformer\nmodels, highlighting their versatility and the potential they hold for\ntransforming diverse application sectors, thereby offering readers a\ncomprehensive understanding of the current and future landscape of\nTransformer-based LLMs in practical applications.\n","authors":["Chen Wang","Jin Zhao","Jiaqi Gong"],"pdf_url":"https://arxiv.org/pdf/2403.18969v1.pdf","comment":"Preprint in ArXiv template, total 24 pages, 5 figures, 5 tables"},{"id":"http://arxiv.org/abs/2403.18957v1","updated":"2024-03-27T19:02:13Z","published":"2024-03-27T19:02:13Z","title":"Moderating Illicit Online Image Promotion for Unsafe User-Generated\n  Content Games Using Large Vision-Language Models","summary":"  Online user-generated content games (UGCGs) are increasingly popular among\nchildren and adolescents for social interaction and more creative online\nentertainment. However, they pose a heightened risk of exposure to explicit\ncontent, raising growing concerns for the online safety of children and\nadolescents. Despite these concerns, few studies have addressed the issue of\nillicit image-based promotions of unsafe UGCGs on social media, which can\ninadvertently attract young users. This challenge arises from the difficulty of\nobtaining comprehensive training data for UGCG images and the unique nature of\nthese images, which differ from traditional unsafe content. In this work, we\ntake the first step towards studying the threat of illicit promotions of unsafe\nUGCGs. We collect a real-world dataset comprising 2,924 images that display\ndiverse sexually explicit and violent content used to promote UGCGs by their\ngame creators. Our in-depth studies reveal a new understanding of this problem\nand the urgent need for automatically flagging illicit UGCG promotions. We\nadditionally create a cutting-edge system, UGCG-Guard, designed to aid social\nmedia platforms in effectively identifying images used for illicit UGCG\npromotions. This system leverages recently introduced large vision-language\nmodels (VLMs) and employs a novel conditional prompting strategy for zero-shot\ndomain adaptation, along with chain-of-thought (CoT) reasoning for contextual\nidentification. UGCG-Guard achieves outstanding results, with an accuracy rate\nof 94% in detecting these images used for the illicit promotion of such games\nin real-world scenarios.\n","authors":["Keyan Guo","Ayush Utkarsh","Wenbo Ding","Isabelle Ondracek","Ziming Zhao","Guo Freeman","Nishant Vishwamitra","Hongxin Hu"],"pdf_url":"https://arxiv.org/pdf/2403.18957v1.pdf","comment":"To Appear in the 33rd USENIX Security Symposium, August 14-16, 2024"},{"id":"http://arxiv.org/abs/2402.13492v3","updated":"2024-03-27T18:48:34Z","published":"2024-02-21T03:05:50Z","title":"Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval\n  Augmentation to Language Models","summary":"  While large language models (LMs) demonstrate remarkable performance, they\nencounter challenges in providing accurate responses when queried for\ninformation beyond their pre-trained memorization. Although augmenting them\nwith relevant external information can mitigate these issues, failure to\nconsider the necessity of retrieval may adversely affect overall performance.\nPrevious research has primarily focused on examining how entities influence\nretrieval models and knowledge recall in LMs, leaving other aspects relatively\nunexplored. In this work, our goal is to offer a more detailed, fact-centric\nanalysis by exploring the effects of combinations of entities and relations. To\nfacilitate this, we construct a new question answering (QA) dataset called\nWiTQA (Wikipedia Triple Question Answers). This dataset includes questions\nabout entities and relations of various popularity levels, each accompanied by\na supporting passage. Our extensive experiments with diverse LMs and retrievers\nreveal when retrieval does not consistently enhance LMs from the viewpoints of\nfact-centric popularity. Confirming earlier findings, we observe that larger\nLMs excel in recalling popular facts. However, they notably encounter\ndifficulty with infrequent entity-relation pairs compared to retrievers.\nInterestingly, they can effectively retain popular relations of less common\nentities. We demonstrate the efficacy of our finer-grained metric and insights\nthrough an adaptive retrieval system that selectively employs retrieval and\nrecall based on the frequencies of entities and relations in the question.\n","authors":["Seiji Maekawa","Hayate Iso","Sairam Gurajada","Nikita Bhutani"],"pdf_url":"https://arxiv.org/pdf/2402.13492v3.pdf","comment":"NAACL2024 (main)"},{"id":"http://arxiv.org/abs/2403.18938v1","updated":"2024-03-27T18:38:39Z","published":"2024-03-27T18:38:39Z","title":"Reshaping Free-Text Radiology Notes Into Structured Reports With\n  Generative Transformers","summary":"  BACKGROUND: Radiology reports are typically written in a free-text format,\nmaking clinical information difficult to extract and use. Recently the adoption\nof structured reporting (SR) has been recommended by various medical societies\nthanks to the advantages it offers, e.g. standardization, completeness and\ninformation retrieval. We propose a pipeline to extract information from\nfree-text radiology reports, that fits with the items of the reference SR\nregistry proposed by a national society of interventional and medical\nradiology, focusing on CT staging of patients with lymphoma. METHODS: Our work\naims to leverage the potential of Natural Language Processing (NLP) and\nTransformer-based models to deal with automatic SR registry filling. With the\navailability of 174 radiology reports, we investigate a rule-free generative\nQuestion Answering approach based on a domain-specific version of T5 (IT5). Two\nstrategies (batch-truncation and ex-post combination) are implemented to comply\nwith the model's context length limitations. Performance is evaluated in terms\nof strict accuracy, F1, and format accuracy, and compared with the widely used\nGPT-3.5 Large Language Model. A 5-point Likert scale questionnaire is used to\ncollect human-expert feedback on the similarity between medical annotations and\ngenerated answers. RESULTS: The combination of fine-tuning and batch splitting\nallows IT5 to achieve notable results; it performs on par with GPT-3.5 albeit\nits size being a thousand times smaller in terms of parameters. Human-based\nassessment scores show a high correlation (Spearman's correlation\ncoefficients>0.88, p-values<0.001) with AI performance metrics (F1) and confirm\nthe superior ability of LLMs (i.e., GPT-3.5, 175B of parameters) in generating\nplausible human-like statements.\n","authors":["Laura Bergomi","Tommaso M. Buonocore","Paolo Antonazzo","Lorenzo Alberghi","Riccardo Bellazzi","Lorenzo Preda","Chandra Bortolotto","Enea Parimbelli"],"pdf_url":"https://arxiv.org/pdf/2403.18938v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18933v1","updated":"2024-03-27T18:30:26Z","published":"2024-03-27T18:30:26Z","title":"SemEval Task 1: Semantic Textual Relatedness for African and Asian\n  Languages","summary":"  We present the first shared task on Semantic Textual Relatedness (STR). While\nearlier shared tasks primarily focused on semantic similarity, we instead\ninvestigate the broader phenomenon of semantic relatedness across 14 languages:\nAfrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian,\nKinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic, Punjabi,\nSpanish, and Telugu. These languages originate from five distinct language\nfamilies and are predominantly spoken in Africa and Asia -- regions\ncharacterised by the relatively limited availability of NLP resources. Each\ninstance in the datasets is a sentence pair associated with a score that\nrepresents the degree of semantic textual relatedness between the two\nsentences. Participating systems were asked to rank sentence pairs by their\ncloseness in meaning (i.e., their degree of semantic relatedness) in the 14\nlanguages in three main tracks: (a) supervised, (b) unsupervised, and (c)\ncrosslingual. The task attracted 163 participants. We received 70 submissions\nin total (across all tasks) from 51 different teams, and 38 system description\npapers. We report on the best-performing systems as well as the most common and\nthe most effective approaches for the three different tracks.\n","authors":["Nedjma Ousidhoum","Shamsuddeen Hassan Muhammad","Mohamed Abdalla","Idris Abdulmumin","Ibrahim Said Ahmad","Sanchit Ahuja","Alham Fikri Aji","Vladimir Araujo","Meriem Beloucif","Christine De Kock","Oumaima Hourrane","Manish Shrivastava","Thamar Solorio","Nirmal Surange","Krishnapriya Vishnubhotla","Seid Muhie Yimam","Saif M. Mohammad"],"pdf_url":"https://arxiv.org/pdf/2403.18933v1.pdf","comment":"SemEval 2024 Task Description Paper. arXiv admin note: text overlap\n  with arXiv:2402.08638"},{"id":"http://arxiv.org/abs/2403.18932v1","updated":"2024-03-27T18:22:48Z","published":"2024-03-27T18:22:48Z","title":"Measuring Political Bias in Large Language Models: What Is Said and How\n  It Is Said","summary":"  We propose to measure political bias in LLMs by analyzing both the content\nand style of their generated content regarding political issues. Existing\nbenchmarks and measures focus on gender and racial biases. However, political\nbias exists in LLMs and can lead to polarization and other harms in downstream\napplications. In order to provide transparency to users, we advocate that there\nshould be fine-grained and explainable measures of political biases generated\nby LLMs. Our proposed measure looks at different political issues such as\nreproductive rights and climate change, at both the content (the substance of\nthe generation) and the style (the lexical polarity) of such bias. We measured\nthe political bias in eleven open-sourced LLMs and showed that our proposed\nframework is easily scalable to other topics and is explainable.\n","authors":["Yejin Bang","Delong Chen","Nayeon Lee","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2403.18932v1.pdf","comment":"16 pages"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2403.18821v1","updated":"2024-03-27T17:59:56Z","published":"2024-03-27T17:59:56Z","title":"Real Acoustic Fields: An Audio-Visual Room Acoustics Dataset and\n  Benchmark","summary":"  We present a new dataset called Real Acoustic Fields (RAF) that captures real\nacoustic room data from multiple modalities. The dataset includes high-quality\nand densely captured room impulse response data paired with multi-view images,\nand precise 6DoF pose tracking data for sound emitters and listeners in the\nrooms. We used this dataset to evaluate existing methods for novel-view\nacoustic synthesis and impulse response generation which previously relied on\nsynthetic data. In our evaluation, we thoroughly assessed existing audio and\naudio-visual models against multiple criteria and proposed settings to enhance\ntheir performance on real-world data. We also conducted experiments to\ninvestigate the impact of incorporating visual data (i.e., images and depth)\ninto neural acoustic field models. Additionally, we demonstrated the\neffectiveness of a simple sim2real approach, where a model is pre-trained with\nsimulated data and fine-tuned with sparse real-world data, resulting in\nsignificant improvements in the few-shot learning approach. RAF is the first\ndataset to provide densely captured room acoustic data, making it an ideal\nresource for researchers working on audio and audio-visual neural acoustic\nfield modeling techniques. Demos and datasets are available on our project\npage: https://facebookresearch.github.io/real-acoustic-fields/\n","authors":["Ziyang Chen","Israel D. Gebru","Christian Richardt","Anurag Kumar","William Laney","Andrew Owens","Alexander Richard"],"pdf_url":"https://arxiv.org/pdf/2403.18821v1.pdf","comment":"Accepted to CVPR 2024. Project site:\n  https://facebookresearch.github.io/real-acoustic-fields/"},{"id":"http://arxiv.org/abs/2403.18820v1","updated":"2024-03-27T17:59:54Z","published":"2024-03-27T17:59:54Z","title":"MetaCap: Meta-learning Priors from Multi-View Imagery for Sparse-view\n  Human Performance Capture and Rendering","summary":"  Faithful human performance capture and free-view rendering from sparse RGB\nobservations is a long-standing problem in Vision and Graphics. The main\nchallenges are the lack of observations and the inherent ambiguities of the\nsetting, e.g. occlusions and depth ambiguity. As a result, radiance fields,\nwhich have shown great promise in capturing high-frequency appearance and\ngeometry details in dense setups, perform poorly when na\\\"ively supervising\nthem on sparse camera views, as the field simply overfits to the sparse-view\ninputs. To address this, we propose MetaCap, a method for efficient and\nhigh-quality geometry recovery and novel view synthesis given very sparse or\neven a single view of the human. Our key idea is to meta-learn the radiance\nfield weights solely from potentially sparse multi-view videos, which can serve\nas a prior when fine-tuning them on sparse imagery depicting the human. This\nprior provides a good network weight initialization, thereby effectively\naddressing ambiguities in sparse-view capture. Due to the articulated structure\nof the human body and motion-induced surface deformations, learning such a\nprior is non-trivial. Therefore, we propose to meta-learn the field weights in\na pose-canonicalized space, which reduces the spatial feature range and makes\nfeature learning more effective. Consequently, one can fine-tune our field\nparameters to quickly generalize to unseen poses, novel illumination conditions\nas well as novel and sparse (even monocular) camera views. For evaluating our\nmethod under different scenarios, we collect a new dataset, WildDynaCap, which\ncontains subjects captured in, both, a dense camera dome and in-the-wild sparse\ncamera rigs, and demonstrate superior results compared to recent\nstate-of-the-art methods on both public and WildDynaCap dataset.\n","authors":["Guoxing Sun","Rishabh Dabral","Pascal Fua","Christian Theobalt","Marc Habermann"],"pdf_url":"https://arxiv.org/pdf/2403.18820v1.pdf","comment":"Project page: https://vcai.mpi-inf.mpg.de/projects/MetaCap/"},{"id":"http://arxiv.org/abs/2403.18819v1","updated":"2024-03-27T17:59:53Z","published":"2024-03-27T17:59:53Z","title":"Benchmarking Object Detectors with COCO: A New Path Forward","summary":"  The Common Objects in Context (COCO) dataset has been instrumental in\nbenchmarking object detectors over the past decade. Like every dataset, COCO\ncontains subtle errors and imperfections stemming from its annotation\nprocedure. With the advent of high-performing models, we ask whether these\nerrors of COCO are hindering its utility in reliably benchmarking further\nprogress. In search for an answer, we inspect thousands of masks from COCO\n(2017 version) and uncover different types of errors such as imprecise mask\nboundaries, non-exhaustively annotated instances, and mislabeled masks. Due to\nthe prevalence of COCO, we choose to correct these errors to maintain\ncontinuity with prior research. We develop COCO-ReM (Refined Masks), a cleaner\nset of annotations with visibly better mask quality than COCO-2017. We evaluate\nfifty object detectors and find that models that predict visually sharper masks\nscore higher on COCO-ReM, affirming that they were being incorrectly penalized\ndue to errors in COCO-2017. Moreover, our models trained using COCO-ReM\nconverge faster and score higher than their larger variants trained using\nCOCO-2017, highlighting the importance of data quality in improving object\ndetectors. With these findings, we advocate using COCO-ReM for future object\ndetection research. Our dataset is available at https://cocorem.xyz\n","authors":["Shweta Singh","Aayan Yadav","Jitesh Jain","Humphrey Shi","Justin Johnson","Karan Desai"],"pdf_url":"https://arxiv.org/pdf/2403.18819v1.pdf","comment":"Technical report. Dataset website: https://cocorem.xyz and code:\n  https://github.com/kdexd/coco-rem"},{"id":"http://arxiv.org/abs/2403.18818v1","updated":"2024-03-27T17:59:52Z","published":"2024-03-27T17:59:52Z","title":"ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object\n  Removal and Insertion","summary":"  Diffusion models have revolutionized image editing but often generate images\nthat violate physical laws, particularly the effects of objects on the scene,\ne.g., occlusions, shadows, and reflections. By analyzing the limitations of\nself-supervised approaches, we propose a practical solution centered on a\n\\q{counterfactual} dataset. Our method involves capturing a scene before and\nafter removing a single object, while minimizing other changes. By fine-tuning\na diffusion model on this dataset, we are able to not only remove objects but\nalso their effects on the scene. However, we find that applying this approach\nfor photorealistic object insertion requires an impractically large dataset. To\ntackle this challenge, we propose bootstrap supervision; leveraging our object\nremoval model trained on a small counterfactual dataset, we synthetically\nexpand this dataset considerably. Our approach significantly outperforms prior\nmethods in photorealistic object removal and insertion, particularly at\nmodeling the effects of objects on the scene.\n","authors":["Daniel Winter","Matan Cohen","Shlomi Fruchter","Yael Pritch","Alex Rav-Acha","Yedid Hoshen"],"pdf_url":"https://arxiv.org/pdf/2403.18818v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18816v1","updated":"2024-03-27T17:59:33Z","published":"2024-03-27T17:59:33Z","title":"Garment3DGen: 3D Garment Stylization and Texture Generation","summary":"  We introduce Garment3DGen a new method to synthesize 3D garment assets from a\nbase mesh given a single input image as guidance. Our proposed approach allows\nusers to generate 3D textured clothes based on both real and synthetic images,\nsuch as those generated by text prompts. The generated assets can be directly\ndraped and simulated on human bodies. First, we leverage the recent progress of\nimage to 3D diffusion methods to generate 3D garment geometries. However, since\nthese geometries cannot be utilized directly for downstream tasks, we propose\nto use them as pseudo ground-truth and set up a mesh deformation optimization\nprocedure that deforms a base template mesh to match the generated 3D target.\nSecond, we introduce carefully designed losses that allow the input base mesh\nto freely deform towards the desired target, yet preserve mesh quality and\ntopology such that they can be simulated. Finally, a texture estimation module\ngenerates high-fidelity texture maps that are globally and locally consistent\nand faithfully capture the input guidance, allowing us to render the generated\n3D assets. With Garment3DGen users can generate the textured 3D garment of\ntheir choice without the need of artist intervention. One can provide a textual\nprompt describing the garment they desire to generate a simulation-ready 3D\nasset. We present a plethora of quantitative and qualitative comparisons on\nvarious assets both real and generated and provide use-cases of how one can\ngenerate simulation-ready 3D garments.\n","authors":["Nikolaos Sarafianos","Tuur Stuyck","Xiaoyu Xiang","Yilei Li","Jovan Popovic","Rakesh Ranjan"],"pdf_url":"https://arxiv.org/pdf/2403.18816v1.pdf","comment":"Project Page: https://nsarafianos.github.io/garment3dgen"},{"id":"http://arxiv.org/abs/2403.18814v1","updated":"2024-03-27T17:59:04Z","published":"2024-03-27T17:59:04Z","title":"Mini-Gemini: Mining the Potential of Multi-modality Vision Language\n  Models","summary":"  In this work, we introduce Mini-Gemini, a simple and effective framework\nenhancing multi-modality Vision Language Models (VLMs). Despite the\nadvancements in VLMs facilitating basic visual dialog and reasoning, a\nperformance gap persists compared to advanced models like GPT-4 and Gemini. We\ntry to narrow the gap by mining the potential of VLMs for better performance\nand any-to-any workflow from three aspects, i.e., high-resolution visual\ntokens, high-quality data, and VLM-guided generation. To enhance visual tokens,\nwe propose to utilize an additional visual encoder for high-resolution\nrefinement without increasing the visual token count. We further construct a\nhigh-quality dataset that promotes precise image comprehension and\nreasoning-based generation, expanding the operational scope of current VLMs. In\ngeneral, Mini-Gemini further mines the potential of VLMs and empowers current\nframeworks with image understanding, reasoning, and generation simultaneously.\nMini-Gemini supports a series of dense and MoE Large Language Models (LLMs)\nfrom 2B to 34B. It is demonstrated to achieve leading performance in several\nzero-shot benchmarks and even surpasses the developed private models. Code and\nmodels are available at https://github.com/dvlab-research/MiniGemini.\n","authors":["Yanwei Li","Yuechen Zhang","Chengyao Wang","Zhisheng Zhong","Yixin Chen","Ruihang Chu","Shaoteng Liu","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2403.18814v1.pdf","comment":"Code and models are available at\n  https://github.com/dvlab-research/MiniGemini"},{"id":"http://arxiv.org/abs/2403.18811v1","updated":"2024-03-27T17:57:02Z","published":"2024-03-27T17:57:02Z","title":"Duolando: Follower GPT with Off-Policy Reinforcement Learning for Dance\n  Accompaniment","summary":"  We introduce a novel task within the field of 3D dance generation, termed\ndance accompaniment, which necessitates the generation of responsive movements\nfrom a dance partner, the \"follower\", synchronized with the lead dancer's\nmovements and the underlying musical rhythm. Unlike existing solo or group\ndance generation tasks, a duet dance scenario entails a heightened degree of\ninteraction between the two participants, requiring delicate coordination in\nboth pose and position. To support this task, we first build a large-scale and\ndiverse duet interactive dance dataset, DD100, by recording about 117 minutes\nof professional dancers' performances. To address the challenges inherent in\nthis task, we propose a GPT-based model, Duolando, which autoregressively\npredicts the subsequent tokenized motion conditioned on the coordinated\ninformation of the music, the leader's and the follower's movements. To further\nenhance the GPT's capabilities of generating stable results on unseen\nconditions (music and leader motions), we devise an off-policy reinforcement\nlearning strategy that allows the model to explore viable trajectories from\nout-of-distribution samplings, guided by human-defined rewards. Based on the\ncollected dataset and proposed method, we establish a benchmark with several\ncarefully designed metrics.\n","authors":["Li Siyao","Tianpei Gu","Zhitao Yang","Zhengyu Lin","Ziwei Liu","Henghui Ding","Lei Yang","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2403.18811v1.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.18807v1","updated":"2024-03-27T17:53:30Z","published":"2024-03-27T17:53:30Z","title":"ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth\n  Estimation","summary":"  In the absence of parallax cues, a learning-based single image depth\nestimation (SIDE) model relies heavily on shading and contextual cues in the\nimage. While this simplicity is attractive, it is necessary to train such\nmodels on large and varied datasets, which are difficult to capture. It has\nbeen shown that using embeddings from pre-trained foundational models, such as\nCLIP, improves zero shot transfer in several applications. Taking inspiration\nfrom this, in our paper we explore the use of global image priors generated\nfrom a pre-trained ViT model to provide more detailed contextual information.\nWe argue that the embedding vector from a ViT model, pre-trained on a large\ndataset, captures greater relevant information for SIDE than the usual route of\ngenerating pseudo image captions, followed by CLIP based text embeddings. Based\non this idea, we propose a new SIDE model using a diffusion backbone which is\nconditioned on ViT embeddings. Our proposed design establishes a new\nstate-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of\n0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on\nKITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to\n0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model\ntrained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%)\nover NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%,\n18%, 45%, 9%) by ZoeDepth. The code is available at\nhttps://github.com/Aradhye2002/EcoDepth.\n","authors":["Suraj Patni","Aradhye Agarwal","Chetan Arora"],"pdf_url":"https://arxiv.org/pdf/2403.18807v1.pdf","comment":"Accepted at IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2024"},{"id":"http://arxiv.org/abs/2311.10319v4","updated":"2024-03-27T17:41:50Z","published":"2023-11-17T04:04:29Z","title":"Shifting to Machine Supervision: Annotation-Efficient Semi and\n  Self-Supervised Learning for Automatic Medical Image Segmentation and\n  Classification","summary":"  Advancements in clinical treatment are increasingly constrained by the\nlimitations of supervised learning techniques, which depend heavily on large\nvolumes of annotated data. The annotation process is not only costly but also\ndemands substantial time from clinical specialists. Addressing this issue, we\nintroduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging)\npipeline, a novel approach that leverages advancements in self-supervised and\nsemi-supervised learning. These techniques engage in auxiliary tasks that do\nnot require labeling, thus simplifying the scaling of machine supervision\ncompared to fully-supervised methods. Our study benchmarks these techniques on\nthree distinct medical imaging datasets to evaluate their effectiveness in\nclassification and segmentation tasks. Notably, we observed that self\nsupervised learning significantly surpassed the performance of supervised\nmethods in the classification of all evaluated datasets. Remarkably, the\nsemi-supervised approach demonstrated superior outcomes in segmentation,\noutperforming fully-supervised methods while using 50% fewer labels across all\ndatasets. In line with our commitment to contributing to the scientific\ncommunity, we have made the S4MI code openly accessible, allowing for broader\napplication and further development of these methods.\n","authors":["Pranav Singh","Raviteja Chukkapalli","Shravan Chaudhari","Luoyao Chen","Mei Chen","Jinqian Pan","Craig Smuda","Jacopo Cirrone"],"pdf_url":"https://arxiv.org/pdf/2311.10319v4.pdf","comment":"Seventeen pages (incl. references), five figures, and one table.\n  (Under Review)"},{"id":"http://arxiv.org/abs/2403.18795v1","updated":"2024-03-27T17:40:14Z","published":"2024-03-27T17:40:14Z","title":"Gamba: Marry Gaussian Splatting with Mamba for single view 3D\n  reconstruction","summary":"  We tackle the challenge of efficiently reconstructing a 3D asset from a\nsingle image with growing demands for automated 3D content creation pipelines.\nPrevious methods primarily rely on Score Distillation Sampling (SDS) and Neural\nRadiance Fields (NeRF). Despite their significant success, these approaches\nencounter practical limitations due to lengthy optimization and considerable\nmemory usage. In this report, we introduce Gamba, an end-to-end amortized 3D\nreconstruction model from single-view images, emphasizing two main insights:\n(1) 3D representation: leveraging a large number of 3D Gaussians for an\nefficient 3D Gaussian splatting process; (2) Backbone design: introducing a\nMamba-based sequential network that facilitates context-dependent reasoning and\nlinear scalability with the sequence (token) length, accommodating a\nsubstantial number of Gaussians. Gamba incorporates significant advancements in\ndata preprocessing, regularization design, and training methodologies. We\nassessed Gamba against existing optimization-based and feed-forward 3D\ngeneration approaches using the real-world scanned OmniObject3D dataset. Here,\nGamba demonstrates competitive generation capabilities, both qualitatively and\nquantitatively, while achieving remarkable speed, approximately 0.6 second on a\nsingle NVIDIA A100 GPU.\n","authors":["Qiuhong Shen","Xuanyu Yi","Zike Wu","Pan Zhou","Hanwang Zhang","Shuicheng Yan","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18791v1","updated":"2024-03-27T17:35:24Z","published":"2024-03-27T17:35:24Z","title":"Object Pose Estimation via the Aggregation of Diffusion Features","summary":"  Estimating the pose of objects from images is a crucial task of 3D scene\nunderstanding, and recent approaches have shown promising results on very large\nbenchmarks. However, these methods experience a significant performance drop\nwhen dealing with unseen objects. We believe that it results from the limited\ngeneralizability of image features. To address this problem, we have an\nin-depth analysis on the features of diffusion models, e.g. Stable Diffusion,\nwhich hold substantial potential for modeling unseen objects. Based on this\nanalysis, we then innovatively introduce these diffusion features for object\npose estimation. To achieve this, we propose three distinct architectures that\ncan effectively capture and aggregate diffusion features of different\ngranularity, greatly improving the generalizability of object pose estimation.\nOur approach outperforms the state-of-the-art methods by a considerable margin\non three popular benchmark datasets, LM, O-LM, and T-LESS. In particular, our\nmethod achieves higher accuracy than the previous best arts on unseen objects:\n98.2% vs. 93.5% on Unseen LM, 85.9% vs. 76.3% on Unseen O-LM, showing the\nstrong generalizability of our method. Our code is released at\nhttps://github.com/Tianfu18/diff-feats-pose.\n","authors":["Tianfu Wang","Guosheng Hu","Hongguang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18791v1.pdf","comment":"Accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2403.18784v1","updated":"2024-03-27T17:32:04Z","published":"2024-03-27T17:32:04Z","title":"SplatFace: Gaussian Splat Face Reconstruction Leveraging an Optimizable\n  Surface","summary":"  We present SplatFace, a novel Gaussian splatting framework designed for 3D\nhuman face reconstruction without reliance on accurate pre-determined geometry.\nOur method is designed to simultaneously deliver both high-quality novel view\nrendering and accurate 3D mesh reconstructions. We incorporate a generic 3D\nMorphable Model (3DMM) to provide a surface geometric structure, making it\npossible to reconstruct faces with a limited set of input images. We introduce\na joint optimization strategy that refines both the Gaussians and the morphable\nsurface through a synergistic non-rigid alignment process. A novel distance\nmetric, splat-to-surface, is proposed to improve alignment by considering both\nthe Gaussian position and covariance. The surface information is also utilized\nto incorporate a world-space densification process, resulting in superior\nreconstruction quality. Our experimental analysis demonstrates that the\nproposed method is competitive with both other Gaussian splatting techniques in\nnovel view synthesis and other 3D reconstruction methods in producing 3D face\nmeshes with high geometric precision.\n","authors":["Jiahao Luo","Jing Liu","James Davis"],"pdf_url":"https://arxiv.org/pdf/2403.18784v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18775v1","updated":"2024-03-27T17:23:39Z","published":"2024-03-27T17:23:39Z","title":"ImageNet-D: Benchmarking Neural Network Robustness on Diffusion\n  Synthetic Object","summary":"  We establish rigorous benchmarks for visual perception robustness. Synthetic\nimages such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific\ntype of evaluation over synthetic corruptions, backgrounds, and textures, yet\nthose robustness benchmarks are restricted in specified variations and have low\nsynthetic quality. In this work, we introduce generative model as a data source\nfor synthesizing hard images that benchmark deep models' robustness. Leveraging\ndiffusion models, we are able to generate images with more diversified\nbackgrounds, textures, and materials than any prior work, where we term this\nbenchmark as ImageNet-D. Experimental results show that ImageNet-D results in a\nsignificant accuracy drop to a range of vision models, from the standard ResNet\nvisual classifier to the latest foundation models like CLIP and MiniGPT-4,\nsignificantly reducing their accuracy by up to 60\\%. Our work suggests that\ndiffusion models can be an effective source to test vision models. The code and\ndataset are available at https://github.com/chenshuang-zhang/imagenet_d.\n","authors":["Chenshuang Zhang","Fei Pan","Junmo Kim","In So Kweon","Chengzhi Mao"],"pdf_url":"https://arxiv.org/pdf/2403.18775v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2312.01220v2","updated":"2024-03-27T17:23:16Z","published":"2023-12-02T20:11:48Z","title":"Boosting Object Detection with Zero-Shot Day-Night Domain Adaptation","summary":"  Detecting objects in low-light scenarios presents a persistent challenge, as\ndetectors trained on well-lit data exhibit significant performance degradation\non low-light data due to low visibility. Previous methods mitigate this issue\nby exploring image enhancement or object detection techniques with real\nlow-light image datasets. However, the progress is impeded by the inherent\ndifficulties about collecting and annotating low-light images. To address this\nchallenge, we propose to boost low-light object detection with zero-shot\nday-night domain adaptation, which aims to generalize a detector from well-lit\nscenarios to low-light ones without requiring real low-light data. Revisiting\nRetinex theory in the low-level vision, we first design a reflectance\nrepresentation learning module to learn Retinex-based illumination invariance\nin images with a carefully designed illumination invariance reinforcement\nstrategy. Next, an interchange-redecomposition-coherence procedure is\nintroduced to improve over the vanilla Retinex image decomposition process by\nperforming two sequential image decompositions and introducing a\nredecomposition cohering loss. Extensive experiments on ExDark, DARK FACE, and\nCODaN datasets show strong low-light generalizability of our method. Our code\nis available at https://github.com/ZPDu/DAI-Net.\n","authors":["Zhipeng Du","Miaojing Shi","Jiankang Deng"],"pdf_url":"https://arxiv.org/pdf/2312.01220v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.06054v4","updated":"2024-03-27T17:06:10Z","published":"2024-03-10T00:47:05Z","title":"Decoupled Data Consistency with Diffusion Purification for Image\n  Restoration","summary":"  Diffusion models have recently gained traction as a powerful class of deep\ngenerative priors, excelling in a wide range of image restoration tasks due to\ntheir exceptional ability to model data distributions. To solve image\nrestoration problems, many existing techniques achieve data consistency by\nincorporating additional likelihood gradient steps into the reverse sampling\nprocess of diffusion models. However, the additional gradient steps pose a\nchallenge for real-world practical applications as they incur a large\ncomputational overhead, thereby increasing inference time. They also present\nadditional difficulties when using accelerated diffusion model samplers, as the\nnumber of data consistency steps is limited by the number of reverse sampling\nsteps. In this work, we propose a novel diffusion-based image restoration\nsolver that addresses these issues by decoupling the reverse process from the\ndata consistency steps. Our method involves alternating between a\nreconstruction phase to maintain data consistency and a refinement phase that\nenforces the prior via diffusion purification. Our approach demonstrates\nversatility, making it highly adaptable for efficient problem-solving in latent\nspace. Additionally, it reduces the necessity for numerous sampling steps\nthrough the integration of consistency models. The efficacy of our approach is\nvalidated through comprehensive experiments across various image restoration\ntasks, including image denoising, deblurring, inpainting, and super-resolution.\n","authors":["Xiang Li","Soo Min Kwon","Ismail R. Alkhouri","Saiprasad Ravishankar","Qing Qu"],"pdf_url":"https://arxiv.org/pdf/2403.06054v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18762v1","updated":"2024-03-27T17:01:10Z","published":"2024-03-27T17:01:10Z","title":"ModaLink: Unifying Modalities for Efficient Image-to-PointCloud Place\n  Recognition","summary":"  Place recognition is an important task for robots and autonomous cars to\nlocalize themselves and close loops in pre-built maps. While single-modal\nsensor-based methods have shown satisfactory performance, cross-modal place\nrecognition that retrieving images from a point-cloud database remains a\nchallenging problem. Current cross-modal methods transform images into 3D\npoints using depth estimation for modality conversion, which are usually\ncomputationally intensive and need expensive labeled data for depth\nsupervision. In this work, we introduce a fast and lightweight framework to\nencode images and point clouds into place-distinctive descriptors. We propose\nan effective Field of View (FoV) transformation module to convert point clouds\ninto an analogous modality as images. This module eliminates the necessity for\ndepth estimation and helps subsequent modules achieve real-time performance. We\nfurther design a non-negative factorization-based encoder to extract mutually\nconsistent semantic features between point clouds and images. This encoder\nyields more distinctive global descriptors for retrieval. Experimental results\non the KITTI dataset show that our proposed methods achieve state-of-the-art\nperformance while running in real time. Additional evaluation on the HAOMO\ndataset covering a 17 km trajectory further shows the practical generalization\ncapabilities. We have released the implementation of our methods as open source\nat: https://github.com/haomo-ai/ModaLink.git.\n","authors":["Weidong Xie","Lun Luo","Nanfei Ye","Yi Ren","Shaoyi Du","Minhang Wang","Jintao Xu","Rui Ai","Weihao Gu","Xieyuanli Chen"],"pdf_url":"https://arxiv.org/pdf/2403.18762v1.pdf","comment":"8 pages, 11 figures, conference"},{"id":"http://arxiv.org/abs/2403.18756v1","updated":"2024-03-27T16:56:14Z","published":"2024-03-27T16:56:14Z","title":"Detection of subclinical atherosclerosis by image-based deep learning on\n  chest x-ray","summary":"  Aims. To develop a deep-learning based system for recognition of subclinical\natherosclerosis on a plain frontal chest x-ray. Methods and Results. A\ndeep-learning algorithm to predict coronary artery calcium (CAC) score (the\nAI-CAC model) was developed on 460 chest x-ray (80% training cohort, 20%\ninternal validation cohort) of primary prevention patients (58.4% male, median\nage 63 [51-74] years) with available paired chest x-ray and chest computed\ntomography (CT) indicated for any clinical reason and performed within 3\nmonths. The CAC score calculated on chest CT was used as ground truth. The\nmodel was validated on an temporally-independent cohort of 90 patients from the\nsame institution (external validation). The diagnostic accuracy of the AI-CAC\nmodel assessed by the area under the curve (AUC) was the primary outcome.\nOverall, median AI-CAC score was 35 (0-388) and 28.9% patients had no AI-CAC.\nAUC of the AI-CAC model to identify a CAC>0 was 0.90 in the internal validation\ncohort and 0.77 in the external validation cohort. Sensitivity was consistently\nabove 92% in both cohorts. In the overall cohort (n=540), among patients with\nAI-CAC=0, a single ASCVD event occurred, after 4.3 years. Patients with\nAI-CAC>0 had significantly higher Kaplan Meier estimates for ASCVD events\n(13.5% vs. 3.4%, log-rank=0.013). Conclusion. The AI-CAC model seems to\naccurately detect subclinical atherosclerosis on chest x-ray with elevated\nsensitivity, and to predict ASCVD events with elevated negative predictive\nvalue. Adoption of the AI-CAC model to refine CV risk stratification or as an\nopportunistic screening tool requires prospective evaluation.\n","authors":["Guglielmo Gallone","Francesco Iodice","Alberto Presta","Davide Tore","Ovidio de Filippo","Michele Visciano","Carlo Alberto Barbano","Alessandro Serafini","Paola Gorrini","Alessandro Bruno","Walter Grosso Marra","James Hughes","Mario Iannaccone","Paolo Fonio","Attilio Fiandrotti","Alessandro Depaoli","Marco Grangetto","Gaetano Maria de Ferrari","Fabrizio D'Ascenzo"],"pdf_url":"https://arxiv.org/pdf/2403.18756v1.pdf","comment":"Submitted to European Heart Journal - Cardiovascular Imaging Added\n  also the additional material 44 pages (30 main paper, 14 additional\n  material), 14 figures (5 main manuscript, 9 additional material)"},{"id":"http://arxiv.org/abs/2303.09817v2","updated":"2024-03-27T16:52:59Z","published":"2023-03-17T07:53:18Z","title":"Interpretable machine learning for time-to-event prediction in medicine\n  and healthcare","summary":"  Time-to-event prediction, e.g. cancer survival analysis or hospital length of\nstay, is a highly prominent machine learning task in medical and healthcare\napplications. However, only a few interpretable machine learning methods comply\nwith its challenges. To facilitate a comprehensive explanatory analysis of\nsurvival models, we formally introduce time-dependent feature effects and\nglobal feature importance explanations. We show how post-hoc interpretation\nmethods allow for finding biases in AI systems predicting length of stay using\na novel multi-modal dataset created from 1235 X-ray images with textual\nradiology reports annotated by human experts. Moreover, we evaluate cancer\nsurvival models beyond predictive performance to include the importance of\nmulti-omics feature groups based on a large-scale benchmark comprising 11\ndatasets from The Cancer Genome Atlas (TCGA). Model developers can use the\nproposed methods to debug and improve machine learning algorithms, while\nphysicians can discover disease biomarkers and assess their significance. We\nhope the contributed open data and code resources facilitate future work in the\nemerging research direction of explainable survival analysis.\n","authors":["Hubert Baniecki","Bartlomiej Sobieski","Patryk Szatkowski","Przemyslaw Bombinski","Przemyslaw Biecek"],"pdf_url":"https://arxiv.org/pdf/2303.09817v2.pdf","comment":"An extended version of an AIME 2023 paper submitted to Artificial\n  Intelligence in Medicine"},{"id":"http://arxiv.org/abs/2403.14623v2","updated":"2024-03-27T16:49:35Z","published":"2024-03-21T17:59:41Z","title":"Simplified Diffusion Schrödinger Bridge","summary":"  This paper introduces a novel theoretical simplification of the Diffusion\nSchr\\\"odinger Bridge (DSB) that facilitates its unification with Score-based\nGenerative Models (SGMs), addressing the limitations of DSB in complex data\ngeneration and enabling faster convergence and enhanced performance. By\nemploying SGMs as an initial solution for DSB, our approach capitalizes on the\nstrengths of both frameworks, ensuring a more efficient training process and\nimproving the performance of SGM. We also propose a reparameterization\ntechnique that, despite theoretical approximations, practically improves the\nnetwork's fitting capabilities. Our extensive experimental evaluations confirm\nthe effectiveness of the simplified DSB, demonstrating its significant\nimprovements. We believe the contributions of this work pave the way for\nadvanced generative modeling. The code is available at\nhttps://github.com/checkcrab/SDSB.\n","authors":["Zhicong Tang","Tiankai Hang","Shuyang Gu","Dong Chen","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2403.14623v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11107v2","updated":"2024-03-27T16:48:34Z","published":"2024-03-17T06:21:21Z","title":"Self-supervised co-salient object detection via feature correspondence\n  at multiple scales","summary":"  Our paper introduces a novel two-stage self-supervised approach for detecting\nco-occurring salient objects (CoSOD) in image groups without requiring\nsegmentation annotations. Unlike existing unsupervised methods that rely solely\non patch-level information (e.g. clustering patch descriptors) or on\ncomputation heavy off-the-shelf components for CoSOD, our lightweight model\nleverages feature correspondences at both patch and region levels,\nsignificantly improving prediction performance. In the first stage, we train a\nself-supervised network that detects co-salient regions by computing local\npatch-level feature correspondences across images. We obtain the segmentation\npredictions using confidence-based adaptive thresholding. In the next stage, we\nrefine these intermediate segmentations by eliminating the detected regions\n(within each image) whose averaged feature representations are dissimilar to\nthe foreground feature representation averaged across all the cross-attention\nmaps (from the previous stage). Extensive experiments on three CoSOD benchmark\ndatasets show that our self-supervised model outperforms the corresponding\nstate-of-the-art models by a huge margin (e.g. on the CoCA dataset, our model\nhas a 13.7% F-measure gain over the SOTA unsupervised CoSOD model). Notably,\nour self-supervised model also outperforms several recent fully supervised\nCoSOD models on the three test datasets (e.g., on the CoCA dataset, our model\nhas a 4.6% F-measure gain over a recent supervised CoSOD model).\n","authors":["Souradeep Chakraborty","Dimitris Samaras"],"pdf_url":"https://arxiv.org/pdf/2403.11107v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18734v1","updated":"2024-03-27T16:22:45Z","published":"2024-03-27T16:22:45Z","title":"A vascular synthetic model for improved aneurysm segmentation and\n  detection via Deep Neural Networks","summary":"  We hereby present a full synthetic model, able to mimic the various\nconstituents of the cerebral vascular tree: the cerebral arteries, the\nbifurcations and the intracranial aneurysms. By building this model, our goal\nwas to provide a substantial dataset of brain arteries which could be used by a\n3D Convolutional Neural Network (CNN) to either segment or detect/recognize\nvarious vascular diseases (such as artery dissection/thrombosis) or even some\nportions of the cerebral vasculature, such as the bifurcations or aneurysms. In\nthis study, we will particularly focus on Intra-Cranial Aneurysm (ICA)\ndetection and segmentation. The cerebral aneurysms most often occur on a\nparticular structure of the vascular tree named the Circle of Willis. Various\nstudies have been conducted to detect and monitor the ICAs and those based on\nDeep Learning (DL) achieve the best performances. Specifically, in this work,\nwe propose a full synthetic 3D model able to mimic the brain vasculature as\nacquired by Magnetic Resonance Angiography (MRA), and more particularly the\nTime Of Flight (TOF) principle. Among the various MRI modalities, the MRA-TOF\nallows to have a relatively good rendering of the blood vessels and is\nnon-invasive (no contrast liquid injection). Our model has been designed to\nsimultaneously mimic the arteries geometry, the ICA shape and the background\nnoise. The geometry of the vascular tree is modeled thanks to an interpolation\nwith 3D Spline functions, and the statistical properties of the background MRI\nnoise is collected from MRA acquisitions and reproduced within the model. In\nthis work, we thoroughly describe the synthetic vasculature model, we build up\na neural network designed for ICA segmentation and detection, and finally, we\ncarry out an in-depth evaluation of the performance gap gained thanks to the\nsynthetic model data augmentation.\n","authors":["Rafic Nader","Florent Autrusseau","Vincent L'Allinec","Romain Bourcier"],"pdf_url":"https://arxiv.org/pdf/2403.18734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18731v1","updated":"2024-03-27T16:21:24Z","published":"2024-03-27T16:21:24Z","title":"Enhancing Manufacturing Quality Prediction Models through the\n  Integration of Explainability Methods","summary":"  This research presents a method that utilizes explainability techniques to\namplify the performance of machine learning (ML) models in forecasting the\nquality of milling processes, as demonstrated in this paper through a\nmanufacturing use case. The methodology entails the initial training of ML\nmodels, followed by a fine-tuning phase where irrelevant features identified\nthrough explainability methods are eliminated. This procedural refinement\nresults in performance enhancements, paving the way for potential reductions in\nmanufacturing costs and a better understanding of the trained ML models. This\nstudy highlights the usefulness of explainability techniques in both explaining\nand optimizing predictive models in the manufacturing realm.\n","authors":["Dennis Gross","Helge Spieker","Arnaud Gotlieb","Ricardo Knoblauch"],"pdf_url":"https://arxiv.org/pdf/2403.18731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18730v1","updated":"2024-03-27T16:20:55Z","published":"2024-03-27T16:20:55Z","title":"Towards Image Ambient Lighting Normalization","summary":"  Lighting normalization is a crucial but underexplored restoration task with\nbroad applications. However, existing works often simplify this task within the\ncontext of shadow removal, limiting the light sources to one and\noversimplifying the scene, thus excluding complex self-shadows and restricting\nsurface classes to smooth ones. Although promising, such simplifications hinder\ngeneralizability to more realistic settings encountered in daily use. In this\npaper, we propose a new challenging task termed Ambient Lighting Normalization\n(ALN), which enables the study of interactions between shadows, unifying image\nrestoration and shadow removal in a broader context. To address the lack of\nappropriate datasets for ALN, we introduce the large-scale high-resolution\ndataset Ambient6K, comprising samples obtained from multiple light sources and\nincluding self-shadows resulting from complex geometries, which is the first of\nits kind. For benchmarking, we select various mainstream methods and rigorously\nevaluate them on Ambient6K. Additionally, we propose IFBlend, a novel strong\nbaseline that maximizes Image-Frequency joint entropy to selectively restore\nlocal areas under different lighting conditions, without relying on shadow\nlocalization priors. Experiments show that IFBlend achieves SOTA scores on\nAmbient6K and exhibits competitive performance on conventional shadow removal\nbenchmarks compared to shadow-specific models with mask priors. The dataset,\nbenchmark, and code are available at https://github.com/fvasluianu97/IFBlend.\n","authors":["Florin-Alexandru Vasluianu","Tim Seizinger","Zongwei Wu","Rakesh Ranjan","Radu Timofte"],"pdf_url":"https://arxiv.org/pdf/2403.18730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09992v3","updated":"2024-03-27T16:20:52Z","published":"2023-03-17T14:07:55Z","title":"LION: Implicit Vision Prompt Tuning","summary":"  Despite recent competitive performance across a range of vision tasks, vision\nTransformers still have an issue of heavy computational costs. Recently, vision\nprompt learning has provided an economic solution to this problem without\nfine-tuning the whole large-scale models. However, the efficiency of existing\nmodels are still far from satisfactory due to insertion of extensive prompts\nblocks and trick prompt designs. In this paper, we propose an efficient vision\nmodel named impLicit vIsion prOmpt tuNing (LION), which is motivated by deep\nimplicit models with stable memory costs for various complex tasks. In\nparticular, we merely insect two equilibrium implicit layers in two ends of the\npre-trained main backbone with parameters in the backbone frozen. Moreover, we\nprune the parameters in these two layers according to lottery hypothesis. The\nperformance obtained by our LION are promising on a wide range of datasets. In\nparticular, our LION reduces up to 11.5% of training parameter numbers while\nobtaining higher performance compared with the state-of-the-art baseline VPT,\nespecially under challenging scenes. Furthermore, we find that our proposed\nLION had a good generalization performance, making it an easy way to boost\ntransfer learning in the future.\n","authors":["Haixin Wang","Jianlong Chang","Xiao Luo","Jinan Sun","Zhouchen Lin","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2303.09992v3.pdf","comment":"Accepted by AAAI2024; 9 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.18717v1","updated":"2024-03-27T16:06:37Z","published":"2024-03-27T16:06:37Z","title":"Semi-Supervised Learning for Deep Causal Generative Models","summary":"  Developing models that can answer questions of the form \"How would $x$ change\nif $y$ had been $z$?\" is fundamental for advancing medical image analysis.\nTraining causal generative models that address such counterfactual questions,\nthough, currently requires that all relevant variables have been observed and\nthat corresponding labels are available in training data. However, clinical\ndata may not have complete records for all patients and state of the art causal\ngenerative models are unable to take full advantage of this. We thus develop,\nfor the first time, a semi-supervised deep causal generative model that\nexploits the causal relationships between variables to maximise the use of all\navailable data. We explore this in the setting where each sample is either\nfully labelled or fully unlabelled, as well as the more clinically realistic\ncase of having different labels missing for each sample. We leverage techniques\nfrom causal inference to infer missing values and subsequently generate\nrealistic counterfactuals, even for samples with incomplete labels.\n","authors":["Yasin Ibrahim","Hermione Warr","Konstantinos Kamnitsas"],"pdf_url":"https://arxiv.org/pdf/2403.18717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18715v1","updated":"2024-03-27T16:04:47Z","published":"2024-03-27T16:04:47Z","title":"Mitigating Hallucinations in Large Vision-Language Models with\n  Instruction Contrastive Decoding","summary":"  Large Vision-Language Models (LVLMs) are increasingly adept at generating\ncontextually detailed and coherent responses from visual inputs. However, their\napplication in multimodal decision-making and open-ended generation is hindered\nby a notable rate of hallucinations, where generated text inaccurately\nrepresents the visual contents. To address this issue, this paper introduces\nthe Instruction Contrastive Decoding (ICD) method, a novel approach designed to\nreduce hallucinations during LVLM inference. Our method is inspired by our\nobservation that what we call disturbance instructions significantly exacerbate\nhallucinations in multimodal fusion modules. ICD contrasts distributions from\nstandard and instruction disturbance, thereby increasing alignment uncertainty\nand effectively subtracting hallucinated concepts from the original\ndistribution. Through comprehensive experiments on discriminative benchmarks\n(POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that\nICD significantly mitigates both object-level and attribute-level\nhallucinations. Moreover, our method not only addresses hallucinations but also\nsignificantly enhances the general perception and recognition capabilities of\nLVLMs.\n","authors":["Xintong Wang","Jingheng Pan","Liang Ding","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2403.18715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18714v1","updated":"2024-03-27T16:02:00Z","published":"2024-03-27T16:02:00Z","title":"Bringing Textual Prompt to AI-Generated Image Quality Assessment","summary":"  AI-Generated Images (AGIs) have inherent multimodal nature. Unlike\ntraditional image quality assessment (IQA) on natural scenarios, AGIs quality\nassessment (AGIQA) takes the correspondence of image and its textual prompt\ninto consideration. This is coupled in the ground truth score, which confuses\nthe unimodal IQA methods. To solve this problem, we introduce IP-IQA (AGIs\nQuality Assessment via Image and Prompt), a multimodal framework for AGIQA via\ncorresponding image and prompt incorporation. Specifically, we propose a novel\nincremental pretraining task named Image2Prompt for better understanding of\nAGIs and their corresponding textual prompts. An effective and efficient\nimage-prompt fusion module, along with a novel special [QA] token, are also\napplied. Both are plug-and-play and beneficial for the cooperation of image and\nits corresponding prompt. Experiments demonstrate that our IP-IQA achieves the\nstate-of-the-art on AGIQA-1k and AGIQA-3k datasets. Code will be available.\n","authors":["Bowen Qu","Haohui Li","Wei Gao"],"pdf_url":"https://arxiv.org/pdf/2403.18714v1.pdf","comment":"6 pages, 3 figures, accepted by ICME2024"},{"id":"http://arxiv.org/abs/2403.18711v1","updated":"2024-03-27T15:58:25Z","published":"2024-03-27T15:58:25Z","title":"SAT-NGP : Unleashing Neural Graphics Primitives for Fast Relightable\n  Transient-Free 3D reconstruction from Satellite Imagery","summary":"  Current stereo-vision pipelines produce high accuracy 3D reconstruction when\nusing multiple pairs or triplets of satellite images. However, these pipelines\nare sensitive to the changes between images that can occur as a result of\nmulti-date acquisitions. Such variations are mainly due to variable shadows,\nreflexions and transient objects (cars, vegetation). To take such changes into\naccount, Neural Radiance Fields (NeRF) have recently been applied to multi-date\nsatellite imagery. However, Neural methods are very compute-intensive, taking\ndozens of hours to learn, compared with minutes for standard stereo-vision\npipelines. Following the ideas of Instant Neural Graphics Primitives we propose\nto use an efficient sampling strategy and multi-resolution hash encoding to\naccelerate the learning. Our model, Satellite Neural Graphics Primitives\n(SAT-NGP) decreases the learning time to 15 minutes while maintaining the\nquality of the 3D reconstruction.\n","authors":["Camille Billouard","Dawa Derksen","Emmanuelle Sarrazin","Bruno Vallet"],"pdf_url":"https://arxiv.org/pdf/2403.18711v1.pdf","comment":"5 pages, 3 figures, 1 table; Accepted to International Geoscience and\n  Remote Sensing Symposium (IGARSS) 2024; Code available at\n  https://github.com/Ellimac0/SAT-NGP"},{"id":"http://arxiv.org/abs/2403.18708v1","updated":"2024-03-27T15:56:42Z","published":"2024-03-27T15:56:42Z","title":"Dense Vision Transformer Compression with Few Samples","summary":"  Few-shot model compression aims to compress a large model into a more compact\none with only a tiny training set (even without labels). Block-level pruning\nhas recently emerged as a leading technique in achieving high accuracy and low\nlatency in few-shot CNN compression. But, few-shot compression for Vision\nTransformers (ViT) remains largely unexplored, which presents a new challenge.\nIn particular, the issue of sparse compression exists in traditional CNN\nfew-shot methods, which can only produce very few compressed models of\ndifferent model sizes. This paper proposes a novel framework for few-shot ViT\ncompression named DC-ViT. Instead of dropping the entire block, DC-ViT\nselectively eliminates the attention module while retaining and reusing\nportions of the MLP module. DC-ViT enables dense compression, which outputs\nnumerous compressed models that densely populate the range of model complexity.\nDC-ViT outperforms state-of-the-art few-shot compression methods by a\nsignificant margin of 10 percentage points, along with lower latency in the\ncompression of ViT and its variants.\n","authors":["Hanxiao Zhang","Yifan Zhou","Guo-Hua Wang","Jianxin Wu"],"pdf_url":"https://arxiv.org/pdf/2403.18708v1.pdf","comment":"Accepted to CVPR 2024. Note: Jianxin Wu is a contributing author for\n  the arXiv version of this paper but is not listed as an author in the CVPR\n  version due to his role as Program Chair"},{"id":"http://arxiv.org/abs/2401.15120v2","updated":"2024-03-27T15:49:52Z","published":"2024-01-26T03:44:58Z","title":"Incorporating simulated spatial context information improves the\n  effectiveness of contrastive learning models","summary":"  Visual learning often occurs in a specific context, where an agent acquires\nskills through exploration and tracking of its location in a consistent\nenvironment. The historical spatial context of the agent provides a similarity\nsignal for self-supervised contrastive learning. We present a unique approach,\ntermed Environmental Spatial Similarity (ESS), that complements existing\ncontrastive learning methods. Using images from simulated, photorealistic\nenvironments as an experimental setting, we demonstrate that ESS outperforms\ntraditional instance discrimination approaches. Moreover, sampling additional\ndata from the same environment substantially improves accuracy and provides new\naugmentations. ESS allows remarkable proficiency in room classification and\nspatial prediction tasks, especially in unfamiliar environments. This learning\nparadigm has the potential to enable rapid visual learning in agents operating\nin new environments with unique visual characteristics. Potentially\ntransformative applications span from robotics to space exploration. Our proof\nof concept demonstrates improved efficiency over methods that rely on\nextensive, disconnected datasets.\n","authors":["Lizhen Zhu","James Z. Wang","Wonseuk Lee","Brad Wyble"],"pdf_url":"https://arxiv.org/pdf/2401.15120v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12091v3","updated":"2024-03-27T15:44:25Z","published":"2023-03-21T09:07:15Z","title":"Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised\n  Learning","summary":"  Semi-supervised learning (SSL) methods assume that labeled data, unlabeled\ndata and test data are from the same distribution. Open-set semi-supervised\nlearning (Open-set SSL) considers a more practical scenario, where unlabeled\ndata and test data contain new categories (outliers) not observed in labeled\ndata (inliers). Most previous works focused on outlier detection via binary\nclassifiers, which suffer from insufficient scalability and inability to\ndistinguish different types of uncertainty. In this paper, we propose a novel\nframework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these\nlimitations. Concretely, we first introduce evidential deep learning (EDL) as\nan outlier detector to quantify different types of uncertainty, and design\ndifferent uncertainty metrics for self-training and inference. Furthermore, we\npropose a novel adaptive negative optimization strategy, making EDL more\ntailored to the unlabeled dataset containing both inliers and outliers. As\ndemonstrated empirically, our proposed method outperforms existing\nstate-of-the-art methods across four datasets.\n","authors":["Yang Yu","Danruo Deng","Furui Liu","Yueming Jin","Qi Dou","Guangyong Chen","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2303.12091v3.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2403.18690v1","updated":"2024-03-27T15:41:23Z","published":"2024-03-27T15:41:23Z","title":"Annolid: Annotate, Segment, and Track Anything You Need","summary":"  Annolid is a deep learning-based software package designed for the\nsegmentation, labeling, and tracking of research targets within video files,\nfocusing primarily on animal behavior analysis. Based on state-of-the-art\ninstance segmentation methods, Annolid now harnesses the Cutie video object\nsegmentation model to achieve resilient, markerless tracking of multiple\nanimals from single annotated frames, even in environments in which they may be\npartially or entirely concealed by environmental features or by one another.\nOur integration of Segment Anything and Grounding-DINO strategies additionally\nenables the automatic masking and segmentation of recognizable animals and\nobjects by text command, removing the need for manual annotation. Annolid's\ncomprehensive approach to object segmentation flexibly accommodates a broad\nspectrum of behavior analysis applications, enabling the classification of\ndiverse behavioral states such as freezing, digging, pup huddling, and social\ninteractions in addition to the tracking of animals and their body parts.\n","authors":["Chen Yang","Thomas A. Cleland"],"pdf_url":"https://arxiv.org/pdf/2403.18690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08479v2","updated":"2024-03-27T15:38:27Z","published":"2023-12-13T19:38:50Z","title":"Vision Transformer-Based Deep Learning for Histologic Classification of\n  Endometrial Cancer","summary":"  Endometrial cancer, the fourth most common cancer in females in the United\nStates, with the lifetime risk for developing this disease is approximately\n2.8% in women. Precise histologic evaluation and molecular classification of\nendometrial cancer is important for effective patient management and\ndetermining the best treatment modalities. This study introduces EndoNet, which\nuses convolutional neural networks for extracting histologic features and a\nvision transformer for aggregating these features and classifying slides based\non their visual characteristics into high- and low- grade. The model was\ntrained on 929 digitized hematoxylin and eosin-stained whole-slide images of\nendometrial cancer from hysterectomy cases at Dartmouth-Health. It classifies\nthese slides into low-grade (Endometroid Grades 1 and 2) and high-grade\n(endometroid carcinoma FIGO grade 3, uterine serous carcinoma, carcinosarcoma)\ncategories. EndoNet was evaluated on an internal test set of 110 patients and\nan external test set of 100 patients from the public TCGA database. The model\nachieved a weighted average F1-score of 0.91 (95% CI: 0.86-0.95) and an AUC of\n0.95 (95% CI: 0.89-0.99) on the internal test, and 0.86 (95% CI: 0.80-0.94) for\nF1-score and 0.86 (95% CI: 0.75-0.93) for AUC on the external test. Pending\nfurther validation, EndoNet has the potential to support pathologists without\nthe need of manual annotations in classifying the grades of gynecologic\npathology tumors.\n","authors":["Manu Goyal","Laura J. Tafe","James X. Feng","Kristen E. Muller","Liesbeth Hondelink","Jessica L. Bentz","Saeed Hassanpour"],"pdf_url":"https://arxiv.org/pdf/2312.08479v2.pdf","comment":"4 Tables and 3 Figures"},{"id":"http://arxiv.org/abs/2308.06098v2","updated":"2024-03-27T15:26:44Z","published":"2023-08-11T12:18:53Z","title":"Automated Construction of Time-Space Diagrams for Traffic Analysis Using\n  Street-View Video Sequence","summary":"  Time-space diagrams are essential tools for analyzing traffic patterns and\noptimizing transportation infrastructure and traffic management strategies.\nTraditional data collection methods for these diagrams have limitations in\nterms of temporal and spatial coverage. Recent advancements in camera\ntechnology have overcome these limitations and provided extensive urban data.\nIn this study, we propose an innovative approach to constructing time-space\ndiagrams by utilizing street-view video sequences captured by cameras mounted\non moving vehicles. Using the state-of-the-art YOLOv5, StrongSORT, and\nphotogrammetry techniques for distance calculation, we can infer vehicle\ntrajectories from the video data and generate time-space diagrams. To evaluate\nthe effectiveness of our proposed method, we utilized datasets from the KITTI\ncomputer vision benchmark suite. The evaluation results demonstrate that our\napproach can generate trajectories from video data, although there are some\nerrors that can be mitigated by improving the performance of the detector,\ntracker, and distance calculation components. In conclusion, the utilization of\nstreet-view video sequences captured by cameras mounted on moving vehicles,\ncombined with state-of-the-art computer vision techniques, has immense\npotential for constructing comprehensive time-space diagrams. These diagrams\noffer valuable insights into traffic patterns and contribute to the design of\ntransportation infrastructure and traffic management strategies.\n","authors":["Tanay Rastogi","Mårten Björkman"],"pdf_url":"https://arxiv.org/pdf/2308.06098v2.pdf","comment":"The paper is published in 2023 IEEE 26th International Conference on\n  Intelligent Transportation Systems (ITSC)"},{"id":"http://arxiv.org/abs/2403.18674v1","updated":"2024-03-27T15:17:10Z","published":"2024-03-27T15:17:10Z","title":"Deep Learning for Robust and Explainable Models in Computer Vision","summary":"  Recent breakthroughs in machine and deep learning (ML and DL) research have\nprovided excellent tools for leveraging enormous amounts of data and optimizing\nhuge models with millions of parameters to obtain accurate networks for image\nprocessing. These developments open up tremendous opportunities for using\nartificial intelligence (AI) in the automation and human assisted AI industry.\nHowever, as more and more models are deployed and used in practice, many\nchallenges have emerged. This thesis presents various approaches that address\nrobustness and explainability challenges for using ML and DL in practice.\n  Robustness and reliability are the critical components of any model before\ncertification and deployment in practice. Deep convolutional neural networks\n(CNNs) exhibit vulnerability to transformations of their inputs, such as\nrotation and scaling, or intentional manipulations as described in the\nadversarial attack literature. In addition, building trust in AI-based models\nrequires a better understanding of current models and developing methods that\nare more explainable and interpretable a priori.\n  This thesis presents developments in computer vision models' robustness and\nexplainability. Furthermore, this thesis offers an example of using vision\nmodels' feature response visualization (models' interpretations) to improve\nrobustness despite interpretability and robustness being seemingly unrelated in\nthe related research. Besides methodological developments for robust and\nexplainable vision models, a key message of this thesis is introducing model\ninterpretation techniques as a tool for understanding vision models and\nimproving their design and robustness. In addition to the theoretical\ndevelopments, this thesis demonstrates several applications of ML and DL in\ndifferent contexts, such as medical imaging and affective computing.\n","authors":["Mohammadreza Amirian"],"pdf_url":"https://arxiv.org/pdf/2403.18674v1.pdf","comment":"150 pages, 37 figures, 12 tables"},{"id":"http://arxiv.org/abs/2311.15803v3","updated":"2024-03-27T15:05:19Z","published":"2023-11-27T13:25:47Z","title":"SOAC: Spatio-Temporal Overlap-Aware Multi-Sensor Calibration using\n  Neural Radiance Fields","summary":"  In rapidly-evolving domains such as autonomous driving, the use of multiple\nsensors with different modalities is crucial to ensure high operational\nprecision and stability. To correctly exploit the provided information by each\nsensor in a single common frame, it is essential for these sensors to be\naccurately calibrated. In this paper, we leverage the ability of Neural\nRadiance Fields (NeRF) to represent different sensors modalities in a common\nvolumetric representation to achieve robust and accurate spatio-temporal sensor\ncalibration. By designing a partitioning approach based on the visible part of\nthe scene for each sensor, we formulate the calibration problem using only the\noverlapping areas. This strategy results in a more robust and accurate\ncalibration that is less prone to failure. We demonstrate that our approach\nworks on outdoor urban scenes by validating it on multiple established driving\ndatasets. Results show that our method is able to get better accuracy and\nrobustness compared to existing methods.\n","authors":["Quentin Herau","Nathan Piasco","Moussab Bennehar","Luis Roldão","Dzmitry Tsishkou","Cyrille Migniot","Pascal Vasseur","Cédric Demonceaux"],"pdf_url":"https://arxiv.org/pdf/2311.15803v3.pdf","comment":"Accepted at CVPR 2024. Project page: https://qherau.github.io/SOAC/"},{"id":"http://arxiv.org/abs/2403.18660v1","updated":"2024-03-27T15:03:38Z","published":"2024-03-27T15:03:38Z","title":"InstructBrush: Learning Attention-based Instruction Optimization for\n  Image Editing","summary":"  In recent years, instruction-based image editing methods have garnered\nsignificant attention in image editing. However, despite encompassing a wide\nrange of editing priors, these methods are helpless when handling editing tasks\nthat are challenging to accurately describe through language. We propose\nInstructBrush, an inversion method for instruction-based image editing methods\nto bridge this gap. It extracts editing effects from exemplar image pairs as\nediting instructions, which are further applied for image editing. Two key\ntechniques are introduced into InstructBrush, Attention-based Instruction\nOptimization and Transformation-oriented Instruction Initialization, to address\nthe limitations of the previous method in terms of inversion effects and\ninstruction generalization. To explore the ability of instruction inversion\nmethods to guide image editing in open scenarios, we establish a\nTransformationOriented Paired Benchmark (TOP-Bench), which contains a rich set\nof scenes and editing types. The creation of this benchmark paves the way for\nfurther exploration of instruction inversion. Quantitatively and qualitatively,\nour approach achieves superior performance in editing and is more semantically\nconsistent with the target editing effects.\n","authors":["Ruoyu Zhao","Qingnan Fan","Fei Kou","Shuai Qin","Hong Gu","Wei Wu","Pengcheng Xu","Mingrui Zhu","Nannan Wang","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2403.18660v1.pdf","comment":"Project Page: https://royzhao926.github.io/InstructBrush/"},{"id":"http://arxiv.org/abs/2311.12386v3","updated":"2024-03-27T15:01:44Z","published":"2023-11-21T06:55:21Z","title":"Point, Segment and Count: A Generalized Framework for Object Counting","summary":"  Class-agnostic object counting aims to count all objects in an image with\nrespect to example boxes or class names, \\emph{a.k.a} few-shot and zero-shot\ncounting. In this paper, we propose a generalized framework for both few-shot\nand zero-shot object counting based on detection. Our framework combines the\nsuperior advantages of two foundation models without compromising their\nzero-shot capability: (\\textbf{i}) SAM to segment all possible objects as mask\nproposals, and (\\textbf{ii}) CLIP to classify proposals to obtain accurate\nobject counts. However, this strategy meets the obstacles of efficiency\noverhead and the small crowded objects that cannot be localized and\ndistinguished. To address these issues, our framework, termed PseCo, follows\nthree steps: point, segment, and count. Specifically, we first propose a\nclass-agnostic object localization to provide accurate but least point prompts\nfor SAM, which consequently not only reduces computation costs but also avoids\nmissing small objects. Furthermore, we propose a generalized object\nclassification that leverages CLIP image/text embeddings as the classifier,\nfollowing a hierarchical knowledge distillation to obtain discriminative\nclassifications among hierarchical mask proposals. Extensive experimental\nresults on FSC-147, COCO, and LVIS demonstrate that PseCo achieves\nstate-of-the-art performance in both few-shot/zero-shot object\ncounting/detection. Code: https://github.com/Hzzone/PseCo\n","authors":["Zhizhong Huang","Mingliang Dai","Yi Zhang","Junping Zhang","Hongming Shan"],"pdf_url":"https://arxiv.org/pdf/2311.12386v3.pdf","comment":"Accepted by CVPR 2024. Camera ready"},{"id":"http://arxiv.org/abs/2311.17532v3","updated":"2024-03-27T15:01:22Z","published":"2023-11-29T11:10:40Z","title":"Weakly-Supervised Emotion Transition Learning for Diverse 3D Co-speech\n  Gesture Generation","summary":"  Generating vivid and emotional 3D co-speech gestures is crucial for virtual\navatar animation in human-machine interaction applications. While the existing\nmethods enable generating the gestures to follow a single emotion label, they\noverlook that long gesture sequence modeling with emotion transition is more\npractical in real scenes. In addition, the lack of large-scale available\ndatasets with emotional transition speech and corresponding 3D human gestures\nalso limits the addressing of this task. To fulfill this goal, we first\nincorporate the ChatGPT-4 and an audio inpainting approach to construct the\nhigh-fidelity emotion transition human speeches. Considering obtaining the\nrealistic 3D pose annotations corresponding to the dynamically inpainted\nemotion transition audio is extremely difficult, we propose a novel weakly\nsupervised training strategy to encourage authority gesture transitions.\nSpecifically, to enhance the coordination of transition gestures w.r.t\ndifferent emotional ones, we model the temporal association representation\nbetween two different emotional gesture sequences as style guidance and infuse\nit into the transition generation. We further devise an emotion mixture\nmechanism that provides weak supervision based on a learnable mixed emotion\nlabel for transition gestures. Last, we present a keyframe sampler to supply\neffective initial posture cues in long sequences, enabling us to generate\ndiverse gestures. Extensive experiments demonstrate that our method outperforms\nthe state-of-the-art models constructed by adapting single emotion-conditioned\ncounterparts on our newly defined emotion transition task and datasets. Our\ncode and dataset will be released on the project page:\nhttps://xingqunqi-lab.github.io/Emo-Transition-Gesture/.\n","authors":["Xingqun Qi","Jiahao Pan","Peng Li","Ruibin Yuan","Xiaowei Chi","Mengfei Li","Wenhan Luo","Wei Xue","Shanghang Zhang","Qifeng Liu","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2311.17532v3.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18649v1","updated":"2024-03-27T14:56:44Z","published":"2024-03-27T14:56:44Z","title":"Addressing Data Annotation Challenges in Multiple Sensors: A Solution\n  for Scania Collected Datasets","summary":"  Data annotation in autonomous vehicles is a critical step in the development\nof Deep Neural Network (DNN) based models or the performance evaluation of the\nperception system. This often takes the form of adding 3D bounding boxes on\ntime-sequential and registered series of point-sets captured from active\nsensors like Light Detection and Ranging (LiDAR) and Radio Detection and\nRanging (RADAR). When annotating multiple active sensors, there is a need to\nmotion compensate and translate the points to a consistent coordinate frame and\ntimestamp respectively. However, highly dynamic objects pose a unique\nchallenge, as they can appear at different timestamps in each sensor's data.\nWithout knowing the speed of the objects, their position appears to be\ndifferent in different sensor outputs. Thus, even after motion compensation,\nhighly dynamic objects are not matched from multiple sensors in the same frame,\nand human annotators struggle to add unique bounding boxes that capture all\nobjects. This article focuses on addressing this challenge, primarily within\nthe context of Scania collected datasets. The proposed solution takes a track\nof an annotated object as input and uses the Moving Horizon Estimation (MHE) to\nrobustly estimate its speed. The estimated speed profile is utilized to correct\nthe position of the annotated box and add boxes to object clusters missed by\nthe original annotation.\n","authors":["Ajinkya Khoche","Aron Asefaw","Alejandro Gonzalez","Bogdan Timus","Sina Sharif Mansouri","Patric Jensfelt"],"pdf_url":"https://arxiv.org/pdf/2403.18649v1.pdf","comment":"Accepted to European Control Conference 2024"},{"id":"http://arxiv.org/abs/2403.18637v1","updated":"2024-03-27T14:42:08Z","published":"2024-03-27T14:42:08Z","title":"Transformers-based architectures for stroke segmentation: A review","summary":"  Stroke remains a significant global health concern, necessitating precise and\nefficient diagnostic tools for timely intervention and improved patient\noutcomes. The emergence of deep learning methodologies has transformed the\nlandscape of medical image analysis. Recently, Transformers, initially designed\nfor natural language processing, have exhibited remarkable capabilities in\nvarious computer vision applications, including medical image analysis. This\ncomprehensive review aims to provide an in-depth exploration of the\ncutting-edge Transformer-based architectures applied in the context of stroke\nsegmentation. It commences with an exploration of stroke pathology, imaging\nmodalities, and the challenges associated with accurate diagnosis and\nsegmentation. Subsequently, the review delves into the fundamental ideas of\nTransformers, offering detailed insights into their architectural intricacies\nand the underlying mechanisms that empower them to effectively capture complex\nspatial information within medical images. The existing literature is\nsystematically categorized and analyzed, discussing various approaches that\nleverage Transformers for stroke segmentation. A critical assessment is\nprovided, highlighting the strengths and limitations of these methods,\nincluding considerations of performance and computational efficiency.\nAdditionally, this review explores potential avenues for future research and\ndevelopment\n","authors":["Yalda Zafari-Ghadim","Essam A. Rashed","Mohamed Mabrok"],"pdf_url":"https://arxiv.org/pdf/2403.18637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.11041v3","updated":"2024-03-27T14:29:27Z","published":"2022-04-23T10:19:58Z","title":"Learning by Erasing: Conditional Entropy based Transferable\n  Out-Of-Distribution Detection","summary":"  Out-of-distribution (OOD) detection is essential to handle the distribution\nshifts between training and test scenarios. For a new in-distribution (ID)\ndataset, existing methods require retraining to capture the dataset-specific\nfeature representation or data distribution. In this paper, we propose a deep\ngenerative models (DGM) based transferable OOD detection method, which is\nunnecessary to retrain on a new ID dataset. We design an image erasing strategy\nto equip exclusive conditional entropy distribution for each ID dataset, which\ndetermines the discrepancy of DGM's posteriori ucertainty distribution on\ndifferent ID datasets. Owing to the powerful representation capacity of\nconvolutional neural networks, the proposed model trained on complex dataset\ncan capture the above discrepancy between ID datasets without retraining and\nthus achieve transferable OOD detection. We validate the proposed method on\nfive datasets and verity that ours achieves comparable performance to the\nstate-of-the-art group based OOD detection methods that need to be retrained to\ndeploy on new ID datasets. Our code is available at\nhttps://github.com/oOHCIOo/CETOOD.\n","authors":["Meng Xing","Zhiyong Feng","Yong Su","Changjae Oh"],"pdf_url":"https://arxiv.org/pdf/2204.11041v3.pdf","comment":"update new experimental results"},{"id":"http://arxiv.org/abs/2403.18605v1","updated":"2024-03-27T14:24:30Z","published":"2024-03-27T14:24:30Z","title":"FlexEdit: Flexible and Controllable Diffusion-based Object-centric Image\n  Editing","summary":"  Our work addresses limitations seen in previous approaches for object-centric\nediting problems, such as unrealistic results due to shape discrepancies and\nlimited control in object replacement or insertion. To this end, we introduce\nFlexEdit, a flexible and controllable editing framework for objects where we\niteratively adjust latents at each denoising step using our FlexEdit block.\nInitially, we optimize latents at test time to align with specified object\nconstraints. Then, our framework employs an adaptive mask, automatically\nextracted during denoising, to protect the background while seamlessly blending\nnew content into the target image. We demonstrate the versatility of FlexEdit\nin various object editing tasks and curate an evaluation test suite with\nsamples from both real and synthetic images, along with novel evaluation\nmetrics designed for object-centric editing. We conduct extensive experiments\non different editing scenarios, demonstrating the superiority of our editing\nframework over recent advanced text-guided image editing methods. Our project\npage is published at https://flex-edit.github.io/.\n","authors":["Trong-Tung Nguyen","Duc-Anh Nguyen","Anh Tran","Cuong Pham"],"pdf_url":"https://arxiv.org/pdf/2403.18605v1.pdf","comment":"Our project page: https://flex-edit.github.io/"},{"id":"http://arxiv.org/abs/2403.18600v1","updated":"2024-03-27T14:22:40Z","published":"2024-03-27T14:22:40Z","title":"RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in\n  Instructional Videos","summary":"  Procedure Planning in instructional videos entails generating a sequence of\naction steps based on visual observations of the initial and target states.\nDespite the rapid progress in this task, there remain several critical\nchallenges to be solved: (1) Adaptive procedures: Prior works hold an\nunrealistic assumption that the number of action steps is known and fixed,\nleading to non-generalizable models in real-world scenarios where the sequence\nlength varies. (2) Temporal relation: Understanding the step temporal relation\nknowledge is essential in producing reasonable and executable plans. (3)\nAnnotation cost: Annotating instructional videos with step-level labels (i.e.,\ntimestamp) or sequence-level labels (i.e., action category) is demanding and\nlabor-intensive, limiting its generalizability to large-scale datasets.In this\nwork, we propose a new and practical setting, called adaptive procedure\nplanning in instructional videos, where the procedure length is not fixed or\npre-determined. To address these challenges we introduce Retrieval-Augmented\nPlanner (RAP) model. Specifically, for adaptive procedures, RAP adaptively\ndetermines the conclusion of actions using an auto-regressive model\narchitecture. For temporal relation, RAP establishes an external memory module\nto explicitly retrieve the most relevant state-action pairs from the training\nvideos and revises the generated procedures. To tackle high annotation cost,\nRAP utilizes a weakly-supervised learning manner to expand the training dataset\nto other task-relevant, unannotated videos by generating pseudo labels for\naction steps. Experiments on CrossTask and COIN benchmarks show the superiority\nof RAP over traditional fixed-length models, establishing it as a strong\nbaseline solution for adaptive procedure planning.\n","authors":["Ali Zare","Yulei Niu","Hammad Ayyubi","Shih-fu Chang"],"pdf_url":"https://arxiv.org/pdf/2403.18600v1.pdf","comment":"23 pages, 6 figures, 12 tables"},{"id":"http://arxiv.org/abs/2403.18593v1","updated":"2024-03-27T14:18:09Z","published":"2024-03-27T14:18:09Z","title":"Homogeneous Tokenizer Matters: Homogeneous Visual Tokenizer for Remote\n  Sensing Image Understanding","summary":"  The tokenizer, as one of the fundamental components of large models, has long\nbeen overlooked or even misunderstood in visual tasks. One key factor of the\ngreat comprehension power of the large language model is that natural language\ntokenizers utilize meaningful words or subwords as the basic elements of\nlanguage. In contrast, mainstream visual tokenizers, represented by patch-based\nmethods such as Patch Embed, rely on meaningless rectangular patches as basic\nelements of vision, which cannot serve as effectively as words or subwords in\nlanguage. Starting from the essence of the tokenizer, we defined semantically\nindependent regions (SIRs) for vision. We designed a simple HOmogeneous visual\ntOKenizer: HOOK. HOOK mainly consists of two modules: the Object Perception\nModule (OPM) and the Object Vectorization Module (OVM). To achieve homogeneity,\nthe OPM splits the image into 4*4 pixel seeds and then utilizes the attention\nmechanism to perceive SIRs. The OVM employs cross-attention to merge seeds\nwithin the same SIR. To achieve adaptability, the OVM defines a variable number\nof learnable vectors as cross-attention queries, allowing for the adjustment of\ntoken quantity. We conducted experiments on the NWPU-RESISC45, WHU-RS19\nclassification dataset, and GID5 segmentation dataset for sparse and dense\ntasks. The results demonstrate that the visual tokens obtained by HOOK\ncorrespond to individual objects, which demonstrates homogeneity. HOOK\noutperformed Patch Embed by 6\\% and 10\\% in the two tasks and achieved\nstate-of-the-art performance compared to the baselines used for comparison.\nCompared to Patch Embed, which requires more than one hundred tokens for one\nimage, HOOK requires only 6 and 8 tokens for sparse and dense tasks,\nrespectively, resulting in efficiency improvements of 1.5 to 2.8 times. The\ncode is available at https://github.com/GeoX-Lab/Hook.\n","authors":["Run Shao","Zhaoyang Zhang","Chao Tao","Yunsheng Zhang","Chengli Peng","Haifeng Li"],"pdf_url":"https://arxiv.org/pdf/2403.18593v1.pdf","comment":"20 pages, 8 figures, 6 tables"},{"id":"http://arxiv.org/abs/2403.18589v1","updated":"2024-03-27T14:12:56Z","published":"2024-03-27T14:12:56Z","title":"Users prefer Jpegli over same-sized libjpeg-turbo or MozJPEG","summary":"  We performed pairwise comparisons by human raters of JPEG images from\nMozJPEG, libjpeg-turbo and our new Jpegli encoder. When compressing images at a\nquality similar to libjpeg-turbo quality 95, the Jpegli images were 54% likely\nto be preferred over both libjpeg-turbo and MozJPEG images, but used only 2.8\nbits per pixel compared to libjpeg-turbo and MozJPEG that used 3.8 and 3.5 bits\nper pixel respectively. The raw ratings and source images are publicly\navailable for further analysis and study.\n","authors":["Martin Bruse","Luca Versari","Zoltan Szabadka","Jyrki Alakuijala"],"pdf_url":"https://arxiv.org/pdf/2403.18589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18587v1","updated":"2024-03-27T14:11:23Z","published":"2024-03-27T14:11:23Z","title":"The Impact of Uniform Inputs on Activation Sparsity and Energy-Latency\n  Attacks in Computer Vision","summary":"  Resource efficiency plays an important role for machine learning nowadays.\nThe energy and decision latency are two critical aspects to ensure a\nsustainable and practical application. Unfortunately, the energy consumption\nand decision latency are not robust against adversaries. Researchers have\nrecently demonstrated that attackers can compute and submit so-called sponge\nexamples at inference time to increase the energy consumption and decision\nlatency of neural networks. In computer vision, the proposed strategy crafts\ninputs with less activation sparsity which could otherwise be used to\naccelerate the computation. In this paper, we analyze the mechanism how these\nenergy-latency attacks reduce activation sparsity. In particular, we find that\ninput uniformity is a key enabler. A uniform image, that is, an image with\nmostly flat, uniformly colored surfaces, triggers more activations due to a\nspecific interplay of convolution, batch normalization, and ReLU activation.\nBased on these insights, we propose two new simple, yet effective strategies\nfor crafting sponge examples: sampling images from a probability distribution\nand identifying dense, yet inconspicuous inputs in natural datasets. We\nempirically examine our findings in a comprehensive evaluation with multiple\nimage classification models and show that our attack achieves the same sparsity\neffect as prior sponge-example methods, but at a fraction of computation\neffort. We also show that our sponge examples transfer between different neural\nnetworks. Finally, we discuss applications of our findings for the good by\nimproving efficiency by increasing sparsity.\n","authors":["Andreas Müller","Erwin Quiring"],"pdf_url":"https://arxiv.org/pdf/2403.18587v1.pdf","comment":"Accepted at the DLSP 2024"},{"id":"http://arxiv.org/abs/2312.07264v2","updated":"2024-03-27T14:09:10Z","published":"2023-12-12T13:44:53Z","title":"Dual Structure-Aware Image Filterings for Semi-supervised Medical Image\n  Segmentation","summary":"  Semi-supervised image segmentation has attracted great attention recently.\nThe key is how to leverage unlabeled images in the training process. Most\nmethods maintain consistent predictions of the unlabeled images under\nvariations (e.g., adding noise/perturbations, or creating alternative versions)\nin the image and/or model level. In most image-level variation, medical images\noften have prior structure information, which has not been well explored. In\nthis paper, we propose novel dual structure-aware image filterings (DSAIF) as\nthe image-level variations for semi-supervised medical image segmentation.\nMotivated by connected filtering that simplifies image via filtering in\nstructure-aware tree-based image representation, we resort to the dual contrast\ninvariant Max-tree and Min-tree representation. Specifically, we propose a\nnovel connected filtering that removes topologically equivalent nodes (i.e.\nconnected components) having no siblings in the Max/Min-tree. This results in\ntwo filtered images preserving topologically critical structure. Applying the\nproposed DSAIF to mutually supervised networks decreases the consensus of their\nerroneous predictions on unlabeled images. This helps to alleviate the\nconfirmation bias issue of overfitting to noisy pseudo labels of unlabeled\nimages, and thus effectively improves the segmentation performance. Extensive\nexperimental results on three benchmark datasets demonstrate that the proposed\nmethod significantly/consistently outperforms some state-of-the-art methods.\nThe source codes will be publicly available.\n","authors":["Yuliang Gu","Zhichao Sun","Tian Chen","Xin Xiao","Yepeng Liu","Yongchao Xu","Laurent Najman"],"pdf_url":"https://arxiv.org/pdf/2312.07264v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18575v1","updated":"2024-03-27T13:56:08Z","published":"2024-03-27T13:56:08Z","title":"HandBooster: Boosting 3D Hand-Mesh Reconstruction by Conditional\n  Synthesis and Sampling of Hand-Object Interactions","summary":"  Reconstructing 3D hand mesh robustly from a single image is very challenging,\ndue to the lack of diversity in existing real-world datasets. While data\nsynthesis helps relieve the issue, the syn-to-real gap still hinders its usage.\nIn this work, we present HandBooster, a new approach to uplift the data\ndiversity and boost the 3D hand-mesh reconstruction performance by training a\nconditional generative space on hand-object interactions and purposely sampling\nthe space to synthesize effective data samples. First, we construct versatile\ncontent-aware conditions to guide a diffusion model to produce realistic images\nwith diverse hand appearances, poses, views, and backgrounds; favorably,\naccurate 3D annotations are obtained for free. Then, we design a novel\ncondition creator based on our similarity-aware distribution sampling\nstrategies to deliberately find novel and realistic interaction poses that are\ndistinctive from the training set. Equipped with our method, several baselines\ncan be significantly improved beyond the SOTA on the HO3D and DexYCB\nbenchmarks. Our code will be released on\nhttps://github.com/hxwork/HandBooster_Pytorch.\n","authors":["Hao Xu","Haipeng Li","Yinqiao Wang","Shuaicheng Liu","Chi-Wing Fu"],"pdf_url":"https://arxiv.org/pdf/2403.18575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07636v3","updated":"2024-03-27T13:51:59Z","published":"2024-03-12T13:18:22Z","title":"Decomposing Disease Descriptions for Enhanced Pathology Detection: A\n  Multi-Aspect Vision-Language Pre-training Framework","summary":"  Medical vision language pre-training (VLP) has emerged as a frontier of\nresearch, enabling zero-shot pathological recognition by comparing the query\nimage with the textual descriptions for each disease. Due to the complex\nsemantics of biomedical texts, current methods struggle to align medical images\nwith key pathological findings in unstructured reports. This leads to the\nmisalignment with the target disease's textual representation. In this paper,\nwe introduce a novel VLP framework designed to dissect disease descriptions\ninto their fundamental aspects, leveraging prior knowledge about the visual\nmanifestations of pathologies. This is achieved by consulting a large language\nmodel and medical experts. Integrating a Transformer module, our approach\naligns an input image with the diverse elements of a disease, generating\naspect-centric image representations. By consolidating the matches from each\naspect, we improve the compatibility between an image and its associated\ndisease. Additionally, capitalizing on the aspect-oriented representations, we\npresent a dual-head Transformer tailored to process known and unknown diseases,\noptimizing the comprehensive detection efficacy. Conducting experiments on\nseven downstream datasets, ours improves the accuracy of recent methods by up\nto 8.56% and 17.0% for seen and unseen categories, respectively. Our code is\nreleased at https://github.com/HieuPhan33/MAVL.\n","authors":["Vu Minh Hieu Phan","Yutong Xie","Yuankai Qi","Lingqiao Liu","Liyang Liu","Bowen Zhang","Zhibin Liao","Qi Wu","Minh-Son To","Johan W. Verjans"],"pdf_url":"https://arxiv.org/pdf/2403.07636v3.pdf","comment":"Accepted at CVPR2024. Pre-print before final camera-ready version"},{"id":"http://arxiv.org/abs/2403.18565v1","updated":"2024-03-27T13:46:01Z","published":"2024-03-27T13:46:01Z","title":"Artifact Reduction in 3D and 4D Cone-beam Computed Tomography Images\n  with Deep Learning -- A Review","summary":"  Deep learning based approaches have been used to improve image quality in\ncone-beam computed tomography (CBCT), a medical imaging technique often used in\napplications such as image-guided radiation therapy, implant dentistry or\northopaedics. In particular, while deep learning methods have been applied to\nreduce various types of CBCT image artifacts arising from motion, metal\nobjects, or low-dose acquisition, a comprehensive review summarizing the\nsuccesses and shortcomings of these approaches, with a primary focus on the\ntype of artifacts rather than the architecture of neural networks, is lacking\nin the literature. In this review, the data generation and simulation\npipelines, and artifact reduction techniques are specifically investigated for\neach type of artifact. We provide an overview of deep learning techniques that\nhave successfully been shown to reduce artifacts in 3D, as well as in\ntime-resolved (4D) CBCT through the use of projection- and/or volume-domain\noptimizations, or by introducing neural networks directly within the CBCT\nreconstruction algorithms. Research gaps are identified to suggest avenues for\nfuture exploration. One of the key findings of this work is an observed trend\ntowards the use of generative models including GANs and score-based or\ndiffusion models, accompanied with the need for more diverse and open training\ndatasets and simulations.\n","authors":["Mohammadreza Amirian","Daniel Barco","Ivo Herzig","Frank-Peter Schilling"],"pdf_url":"https://arxiv.org/pdf/2403.18565v1.pdf","comment":"16 pages, 4 figures, 1 Table, published in IEEE Access Journal"},{"id":"http://arxiv.org/abs/2403.09700v2","updated":"2024-03-27T13:42:25Z","published":"2024-03-05T22:19:21Z","title":"Shapley Values-Powered Framework for Fair Reward Split in Content\n  Produced by GenAI","summary":"  It is evident that, currently, generative models are surpassed in quality by\nhuman professionals. However, with the advancements in Artificial Intelligence,\nthis gap will narrow, leading to scenarios where individuals who have dedicated\nyears of their lives to mastering a skill become obsolete due to their high\ncosts, which are inherently linked to the time they require to complete a task\n-- a task that AI could accomplish in minutes or seconds. To avoid future\nsocial upheavals, we must, even now, contemplate how to fairly assess the\ncontributions of such individuals in training generative models and how to\ncompensate them for the reduction or complete loss of their incomes. In this\nwork, we propose a method to structure collaboration between model developers\nand data providers. To achieve this, we employ Shapley Values to quantify the\ncontribution of artist(s) in an image generated by the Stable Diffusion-v1.5\nmodel and to equitably allocate the reward among them.\n","authors":["Alex Glinsky","Alexey Sokolsky"],"pdf_url":"https://arxiv.org/pdf/2403.09700v2.pdf","comment":"36 pages, 32 figures"},{"id":"http://arxiv.org/abs/2403.18554v1","updated":"2024-03-27T13:33:14Z","published":"2024-03-27T13:33:14Z","title":"CosalPure: Learning Concept from Group Images for Robust Co-Saliency\n  Detection","summary":"  Co-salient object detection (CoSOD) aims to identify the common and salient\n(usually in the foreground) regions across a given group of images. Although\nachieving significant progress, state-of-the-art CoSODs could be easily\naffected by some adversarial perturbations, leading to substantial accuracy\nreduction. The adversarial perturbations can mislead CoSODs but do not change\nthe high-level semantic information (e.g., concept) of the co-salient objects.\nIn this paper, we propose a novel robustness enhancement framework by first\nlearning the concept of the co-salient objects based on the input group images\nand then leveraging this concept to purify adversarial perturbations, which are\nsubsequently fed to CoSODs for robustness enhancement. Specifically, we propose\nCosalPure containing two modules, i.e., group-image concept learning and\nconcept-guided diffusion purification. For the first module, we adopt a\npre-trained text-to-image diffusion model to learn the concept of co-salient\nobjects within group images where the learned concept is robust to adversarial\nexamples. For the second module, we map the adversarial image to the latent\nspace and then perform diffusion generation by embedding the learned concept\ninto the noise prediction function as an extra condition. Our method can\neffectively alleviate the influence of the SOTA adversarial attack containing\ndifferent adversarial patterns, including exposure and noise. The extensive\nresults demonstrate that our method could enhance the robustness of CoSODs\nsignificantly.\n","authors":["Jiayi Zhu","Qing Guo","Felix Juefei-Xu","Yihao Huang","Yang Liu","Geguang Pu"],"pdf_url":"https://arxiv.org/pdf/2403.18554v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.18551v1","updated":"2024-03-27T13:31:39Z","published":"2024-03-27T13:31:39Z","title":"Attention Calibration for Disentangled Text-to-Image Personalization","summary":"  Recent thrilling progress in large-scale text-to-image (T2I) models has\nunlocked unprecedented synthesis quality of AI-generated content (AIGC)\nincluding image generation, 3D and video composition. Further, personalized\ntechniques enable appealing customized production of a novel concept given only\nseveral images as reference. However, an intriguing problem persists: Is it\npossible to capture multiple, novel concepts from one single reference image?\nIn this paper, we identify that existing approaches fail to preserve visual\nconsistency with the reference image and eliminate cross-influence from\nconcepts. To alleviate this, we propose an attention calibration mechanism to\nimprove the concept-level understanding of the T2I model. Specifically, we\nfirst introduce new learnable modifiers bound with classes to capture\nattributes of multiple concepts. Then, the classes are separated and\nstrengthened following the activation of the cross-attention operation,\nensuring comprehensive and self-contained concepts. Additionally, we suppress\nthe attention activation of different classes to mitigate mutual influence\namong concepts. Together, our proposed method, dubbed DisenDiff, can learn\ndisentangled multiple concepts from one single image and produce novel\ncustomized images with learned concepts. We demonstrate that our method\noutperforms the current state of the art in both qualitative and quantitative\nevaluations. More importantly, our proposed techniques are compatible with LoRA\nand inpainting pipelines, enabling more interactive experiences.\n","authors":["Yanbing Zhang","Mengping Yang","Qin Zhou","Zhe Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18551v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18550v1","updated":"2024-03-27T13:30:48Z","published":"2024-03-27T13:30:48Z","title":"OrCo: Towards Better Generalization via Orthogonality and Contrast for\n  Few-Shot Class-Incremental Learning","summary":"  Few-Shot Class-Incremental Learning (FSCIL) introduces a paradigm in which\nthe problem space expands with limited data. FSCIL methods inherently face the\nchallenge of catastrophic forgetting as data arrives incrementally, making\nmodels susceptible to overwriting previously acquired knowledge. Moreover,\ngiven the scarcity of labeled samples available at any given time, models may\nbe prone to overfitting and find it challenging to strike a balance between\nextensive pretraining and the limited incremental data. To address these\nchallenges, we propose the OrCo framework built on two core principles:\nfeatures' orthogonality in the representation space, and contrastive learning.\nIn particular, we improve the generalization of the embedding space by\nemploying a combination of supervised and self-supervised contrastive losses\nduring the pretraining phase. Additionally, we introduce OrCo loss to address\nchallenges arising from data limitations during incremental sessions. Through\nfeature space perturbations and orthogonality between classes, the OrCo loss\nmaximizes margins and reserves space for the following incremental data. This,\nin turn, ensures the accommodation of incoming classes in the feature space\nwithout compromising previously acquired knowledge. Our experimental results\nshowcase state-of-the-art performance across three benchmark datasets,\nincluding mini-ImageNet, CIFAR100, and CUB datasets. Code is available at\nhttps://github.com/noorahmedds/OrCo\n","authors":["Noor Ahmed","Anna Kukleva","Bernt Schiele"],"pdf_url":"https://arxiv.org/pdf/2403.18550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18548v1","updated":"2024-03-27T13:27:02Z","published":"2024-03-27T13:27:02Z","title":"A Semi-supervised Nighttime Dehazing Baseline with Spatial-Frequency\n  Aware and Realistic Brightness Constraint","summary":"  Existing research based on deep learning has extensively explored the problem\nof daytime image dehazing. However, few studies have considered the\ncharacteristics of nighttime hazy scenes. There are two distinctions between\nnighttime and daytime haze. First, there may be multiple active colored light\nsources with lower illumination intensity in nighttime scenes, which may cause\nhaze, glow and noise with localized, coupled and frequency inconsistent\ncharacteristics. Second, due to the domain discrepancy between simulated and\nreal-world data, unrealistic brightness may occur when applying a dehazing\nmodel trained on simulated data to real-world data. To address the above two\nissues, we propose a semi-supervised model for real-world nighttime dehazing.\nFirst, the spatial attention and frequency spectrum filtering are implemented\nas a spatial-frequency domain information interaction module to handle the\nfirst issue. Second, a pseudo-label-based retraining strategy and a local\nwindow-based brightness loss for semi-supervised training process is designed\nto suppress haze and glow while achieving realistic brightness. Experiments on\npublic benchmarks validate the effectiveness of the proposed method and its\nsuperiority over state-of-the-art methods. The source code and Supplementary\nMaterials are placed in the https://github.com/Xiaofeng-life/SFSNiD.\n","authors":["Xiaofeng Cong","Jie Gui","Jing Zhang","Junming Hou","Hao Shen"],"pdf_url":"https://arxiv.org/pdf/2403.18548v1.pdf","comment":"This paper is accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.18546v1","updated":"2024-03-27T13:24:58Z","published":"2024-03-27T13:24:58Z","title":"Efficient Heatmap-Guided 6-Dof Grasp Detection in Cluttered Scenes","summary":"  Fast and robust object grasping in clutter is a crucial component of\nrobotics. Most current works resort to the whole observed point cloud for 6-Dof\ngrasp generation, ignoring the guidance information excavated from global\nsemantics, thus limiting high-quality grasp generation and real-time\nperformance. In this work, we show that the widely used heatmaps are\nunderestimated in the efficiency of 6-Dof grasp generation. Therefore, we\npropose an effective local grasp generator combined with grasp heatmaps as\nguidance, which infers in a global-to-local semantic-to-point way.\nSpecifically, Gaussian encoding and the grid-based strategy are applied to\npredict grasp heatmaps as guidance to aggregate local points into graspable\nregions and provide global semantic information. Further, a novel non-uniform\nanchor sampling mechanism is designed to improve grasp accuracy and diversity.\nBenefiting from the high-efficiency encoding in the image space and focusing on\npoints in local graspable regions, our framework can perform high-quality grasp\ndetection in real-time and achieve state-of-the-art results. In addition, real\nrobot experiments demonstrate the effectiveness of our method with a success\nrate of 94% and a clutter completion rate of 100%. Our code is available at\nhttps://github.com/THU-VCLab/HGGD.\n","authors":["Siang Chen","Wei Tang","Pengwei Xie","Wenming Yang","Guijin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18546v1.pdf","comment":"Extensive results on GraspNet-1B dataset"},{"id":"http://arxiv.org/abs/2310.15081v3","updated":"2024-03-27T13:23:28Z","published":"2023-10-23T16:41:13Z","title":"E4S: Fine-grained Face Swapping via Editing With Regional GAN Inversion","summary":"  This paper proposes a novel approach to face swapping from the perspective of\nfine-grained facial editing, dubbed \"editing for swapping\" (E4S). The\ntraditional face swapping methods rely on global feature extraction and fail to\npreserve the detailed source identity. In contrast, we propose a Regional GAN\nInversion (RGI) method, which allows the explicit disentanglement of shape and\ntexture. Specifically, our E4S performs face swapping in the latent space of a\npretrained StyleGAN, where a multi-scale mask-guided encoder is applied to\nproject the texture of each facial component into regional style codes and a\nmask-guided injection module manipulating feature maps with the style codes.\nBased on this disentanglement, face swapping can be simplified as style and\nmask swapping. Besides, due to the large lighting condition gap, transferring\nthe source skin into the target image may lead to disharmony lighting. We\npropose a re-coloring network to make the swapped face maintain the target\nlighting condition while preserving the source skin. Further, to deal with the\npotential mismatch areas during mask exchange, we design a face inpainting\nmodule to refine the face shape. The extensive comparisons with\nstate-of-the-art methods demonstrate that our E4S outperforms existing methods\nin preserving texture, shape, and lighting. Our implementation is available at\nhttps://github.com/e4s2024/E4S2024.\n","authors":["Maomao Li","Ge Yuan","Cairong Wang","Zhian Liu","Yong Zhang","Yongwei Nie","Jue Wang","Dong Xu"],"pdf_url":"https://arxiv.org/pdf/2310.15081v3.pdf","comment":"Project Page: https://e4s2024.github.io/ ;. arXiv admin note: text\n  overlap with arXiv:2211.14068"},{"id":"http://arxiv.org/abs/2403.18525v1","updated":"2024-03-27T12:59:44Z","published":"2024-03-27T12:59:44Z","title":"Language Plays a Pivotal Role in the Object-Attribute Compositional\n  Generalization of CLIP","summary":"  Vision-language models, such as CLIP, have shown promising\nOut-of-Distribution (OoD) generalization under various types of distribution\nshifts. Recent studies attempted to investigate the leading cause of this\ncapability. In this work, we follow the same path, but focus on a specific type\nof OoD data - images with novel compositions of attribute-object pairs - and\nstudy whether such models can successfully classify those images into\ncomposition classes. We carefully designed an authentic image test dataset\ncalled ImageNet-AO, consisting of attributes for objects that are unlikely\nencountered in the CLIP training sets. We found that CLIPs trained with large\ndatasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude\nimprovement in effective compositional OoD generalization compared to both\nsupervised models and CLIPs trained with smaller datasets, such as CC-12M and\nYFCC-15M. Our results provide evidence that the scale and diversity of training\ndata and language supervision play a key role in unlocking the compositional\ngeneralization abilities of vision-language models.\n","authors":["Reza Abbasi","Mohammad Samiei","Mohammad Hossein Rohban","Mahdieh Soleymani Baghshah"],"pdf_url":"https://arxiv.org/pdf/2403.18525v1.pdf","comment":"Oral accepted at OODCV 2023(http://www.ood-cv.org)"},{"id":"http://arxiv.org/abs/2403.18514v1","updated":"2024-03-27T12:44:57Z","published":"2024-03-27T12:44:57Z","title":"CT-3DFlow : Leveraging 3D Normalizing Flows for Unsupervised Detection\n  of Pathological Pulmonary CT scans","summary":"  Unsupervised pathology detection can be implemented by training a model on\nhealthy data only and measuring the deviation from the training set upon\ninference, for example with CNN-based feature extraction and one-class\nclassifiers, or reconstruction-score-based methods such as AEs, GANs and\nDiffusion models. Normalizing Flows (NF) have the ability to directly learn the\nprobability distribution of training examples through an invertible\narchitecture. We leverage this property in a novel 3D NF-based model named\nCT-3DFlow, specifically tailored for patient-level pulmonary pathology\ndetection in chest CT data. Our model is trained unsupervised on healthy 3D\npulmonary CT patches, and detects deviations from its log-likelihood\ndistribution as anomalies. We aggregate patches-level likelihood values from a\npatient's CT scan to provide a patient-level 'normal'/'abnormal' prediction.\nOut-of-distribution detection performance is evaluated using expert annotations\non a separate chest CT test dataset, outperforming other state-of-the-art\nmethods.\n","authors":["Aissam Djahnine","Alexandre Popoff","Emilien Jupin-Delevaux","Vincent Cottin","Olivier Nempont","Loic Boussel"],"pdf_url":"https://arxiv.org/pdf/2403.18514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.04344v3","updated":"2024-03-27T12:44:55Z","published":"2023-06-07T11:18:53Z","title":"ViDA: Homeostatic Visual Domain Adapter for Continual Test Time\n  Adaptation","summary":"  Since real-world machine systems are running in non-stationary environments,\nContinual Test-Time Adaptation (CTTA) task is proposed to adapt the pre-trained\nmodel to continually changing target domains. Recently, existing methods mainly\nfocus on model-based adaptation, which aims to leverage a self-training manner\nto extract the target domain knowledge. However, pseudo labels can be noisy and\nthe updated model parameters are unreliable under dynamic data distributions,\nleading to error accumulation and catastrophic forgetting in the continual\nadaptation process. To tackle these challenges and maintain the model\nplasticity, we design a Visual Domain Adapter (ViDA) for CTTA, explicitly\nhandling both domain-specific and domain-shared knowledge. Specifically, we\nfirst comprehensively explore the different domain representations of the\nadapters with trainable high-rank or low-rank embedding spaces. Then we inject\nViDAs into the pre-trained model, which leverages high-rank and low-rank\nfeatures to adapt the current domain distribution and maintain the continual\ndomain-shared knowledge, respectively. To exploit the low-rank and high-rank\nViDAs more effectively, we further propose a Homeostatic Knowledge Allotment\n(HKA) strategy, which adaptively combines different knowledge from each ViDA.\nExtensive experiments conducted on four widely used benchmarks demonstrate that\nour proposed method achieves state-of-the-art performance in both\nclassification and segmentation CTTA tasks. Note that, our method can be\nregarded as a novel transfer paradigm for large-scale models, delivering\npromising results in adaptation to continually changing distributions. Project\npage: https://sites.google.com/view/iclr2024-vida/home.\n","authors":["Jiaming Liu","Senqiao Yang","Peidong Jia","Renrui Zhang","Ming Lu","Yandong Guo","Wei Xue","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2306.04344v3.pdf","comment":"Accepted by ICLR2024"},{"id":"http://arxiv.org/abs/2403.18512v1","updated":"2024-03-27T12:41:30Z","published":"2024-03-27T12:41:30Z","title":"ParCo: Part-Coordinating Text-to-Motion Synthesis","summary":"  We study a challenging task: text-to-motion synthesis, aiming to generate\nmotions that align with textual descriptions and exhibit coordinated movements.\nCurrently, the part-based methods introduce part partition into the motion\nsynthesis process to achieve finer-grained generation. However, these methods\nencounter challenges such as the lack of coordination between different part\nmotions and difficulties for networks to understand part concepts. Moreover,\nintroducing finer-grained part concepts poses computational complexity\nchallenges. In this paper, we propose Part-Coordinating Text-to-Motion\nSynthesis (ParCo), endowed with enhanced capabilities for understanding part\nmotions and communication among different part motion generators, ensuring a\ncoordinated and fined-grained motion synthesis. Specifically, we discretize\nwhole-body motion into multiple part motions to establish the prior concept of\ndifferent parts. Afterward, we employ multiple lightweight generators designed\nto synthesize different part motions and coordinate them through our part\ncoordination module. Our approach demonstrates superior performance on common\nbenchmarks with economic computations, including HumanML3D and KIT-ML,\nproviding substantial evidence of its effectiveness. Code is available at\nhttps://github.com/qrzou/ParCo .\n","authors":["Qiran Zou","Shangyuan Yuan","Shian Du","Yu Wang","Chang Liu","Yi Xu","Jie Chen","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2403.18512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16516v2","updated":"2024-03-27T12:32:31Z","published":"2024-03-25T08:00:43Z","title":"Visually Guided Generative Text-Layout Pre-training for Document\n  Intelligence","summary":"  Prior study shows that pre-training techniques can boost the performance of\nvisual document understanding (VDU), which typically requires models to gain\nabilities to perceive and reason both document texts and layouts (e.g.,\nlocations of texts and table-cells). To this end, we propose visually guided\ngenerative text-layout pre-training, named ViTLP. Given a document image, the\nmodel optimizes hierarchical language and layout modeling objectives to\ngenerate the interleaved text and layout sequence. In addition, to address the\nlimitation of processing long documents by Transformers, we introduce a\nstraightforward yet effective multi-segment generative pre-training scheme,\nfacilitating ViTLP to process word-intensive documents of any length. ViTLP can\nfunction as a native OCR model to localize and recognize texts of document\nimages. Besides, ViTLP can be effectively applied to various downstream VDU\ntasks. Extensive experiments show that ViTLP achieves competitive performance\nover existing baselines on benchmark VDU tasks, including information\nextraction, document classification, and document question answering.\n","authors":["Zhiming Mao","Haoli Bai","Lu Hou","Jiansheng Wei","Xin Jiang","Qun Liu","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2403.16516v2.pdf","comment":"Accepted to NAACL 2024 main conference. The first version of this\n  paper was submitted to OpenReview\n  (https://openreview.net/forum?id=ARtBIBAmNR) in June 2023"},{"id":"http://arxiv.org/abs/2312.06358v2","updated":"2024-03-27T12:24:29Z","published":"2023-12-11T13:05:54Z","title":"Intraoperative 2D/3D Image Registration via Differentiable X-ray\n  Rendering","summary":"  Surgical decisions are informed by aligning rapid portable 2D intraoperative\nimages (e.g., X-rays) to a high-fidelity 3D preoperative reference scan (e.g.,\nCT). 2D/3D image registration often fails in practice: conventional\noptimization methods are prohibitively slow and susceptible to local minima,\nwhile neural networks trained on small datasets fail on new patients or require\nimpractical landmark supervision. We present DiffPose, a self-supervised\napproach that leverages patient-specific simulation and differentiable\nphysics-based rendering to achieve accurate 2D/3D registration without relying\non manually labeled data. Preoperatively, a CNN is trained to regress the pose\nof a randomly oriented synthetic X-ray rendered from the preoperative CT. The\nCNN then initializes rapid intraoperative test-time optimization that uses the\ndifferentiable X-ray renderer to refine the solution. Our work further proposes\nseveral geometrically principled methods for sampling camera poses from\n$\\mathbf{SE}(3)$, for sparse differentiable rendering, and for driving\nregistration in the tangent space $\\mathfrak{se}(3)$ with geodesic and\nmultiscale locality-sensitive losses. DiffPose achieves sub-millimeter accuracy\nacross surgical datasets at intraoperative speeds, improving upon existing\nunsupervised methods by an order of magnitude and even outperforming supervised\nbaselines. Our code is available at https://github.com/eigenvivek/DiffPose.\n","authors":["Vivek Gopalakrishnan","Neel Dey","Polina Golland"],"pdf_url":"https://arxiv.org/pdf/2312.06358v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18501v1","updated":"2024-03-27T12:24:20Z","published":"2024-03-27T12:24:20Z","title":"HEMIT: H&E to Multiplex-immunohistochemistry Image Translation with\n  Dual-Branch Pix2pix Generator","summary":"  Computational analysis of multiplexed immunofluorescence histology data is\nemerging as an important method for understanding the tumour micro-environment\nin cancer. This work presents HEMIT, a dataset designed for translating\nHematoxylin and Eosin (H&E) sections to multiplex-immunohistochemistry (mIHC)\nimages, featuring DAPI, CD3, and panCK markers. Distinctively, HEMIT's mIHC\nimages are multi-component and cellular-level aligned with H&E, enriching\nsupervised stain translation tasks. To our knowledge, HEMIT is the first\npublicly available cellular-level aligned dataset that enables H&E to\nmulti-target mIHC image translation. This dataset provides the computer vision\ncommunity with a valuable resource to develop novel computational methods which\nhave the potential to gain new insights from H&E slide archives.\n  We also propose a new dual-branch generator architecture, using residual\nConvolutional Neural Networks (CNNs) and Swin Transformers which achieves\nbetter translation outcomes than other popular algorithms. When evaluated on\nHEMIT, it outperforms pix2pixHD, pix2pix, U-Net, and ResNet, achieving the\nhighest overall score on key metrics including the Structural Similarity Index\nMeasure (SSIM), Pearson correlation score (R), and Peak signal-to-noise Ratio\n(PSNR). Additionally, downstream analysis has been used to further validate the\nquality of the generated mIHC images. These results set a new benchmark in the\nfield of stain translation tasks.\n","authors":["Chang Bian","Beth Philips","Tim Cootes","Martin Fergie"],"pdf_url":"https://arxiv.org/pdf/2403.18501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04698v3","updated":"2024-03-27T12:24:17Z","published":"2023-11-08T14:10:19Z","title":"Challenging Common Paradigms in Multi-Task Learning","summary":"  While multi-task learning (MTL) has gained significant attention in recent\nyears, its underlying mechanisms remain poorly understood. Recent methods did\nnot yield consistent performance improvements over single task learning (STL)\nbaselines, underscoring the importance of gaining more profound insights about\nchallenges specific to MTL. In our study, we challenge paradigms in MTL in the\ncontext of STL: First, the impact of the choice of optimizer has only been\nmildly investigated in MTL. We show the pivotal role of common STL tools such\nas the Adam optimizer in MTL empirically in various experiments. To further\ninvestigate Adam's effectiveness, we theoretical derive a partial loss-scale\ninvariance under mild assumptions. Second, the notion of gradient conflicts has\noften been phrased as a specific problem in MTL. We delve into the role of\ngradient conflicts in MTL and compare it to STL. For angular gradient alignment\nwe find no evidence that this is a unique problem in MTL. We emphasize\ndifferences in gradient magnitude as the main distinguishing factor. Lastly, we\ncompare the transferability of features learned through MTL and STL on common\nimage corruptions, and find light evidence that MTL can lead to superior\ntransferability. Overall, we find surprising similarities between STL and MTL\nsuggesting to consider methods from both fields in a broader context.\n","authors":["Cathrin Elich","Lukas Kirchdorfer","Jan M. Köhler","Lukas Schott"],"pdf_url":"https://arxiv.org/pdf/2311.04698v3.pdf","comment":"-"},{"id":"http://arxiv.org/abs/2403.18495v1","updated":"2024-03-27T12:15:22Z","published":"2024-03-27T12:15:22Z","title":"Direct mineral content prediction from drill core images via transfer\n  learning","summary":"  Deep subsurface exploration is important for mining, oil and gas industries,\nas well as in the assessment of geological units for the disposal of chemical\nor nuclear waste, or the viability of geothermal energy systems. Typically,\ndetailed examinations of subsurface formations or units are performed on\ncuttings or core materials extracted during drilling campaigns, as well as on\ngeophysical borehole data, which provide detailed information about the\npetrophysical properties of the rocks. Depending on the volume of rock samples\nand the analytical program, the laboratory analysis and diagnostics can be very\ntime-consuming. This study investigates the potential of utilizing machine\nlearning, specifically convolutional neural networks (CNN), to assess the\nlithology and mineral content solely from analysis of drill core images, aiming\nto support and expedite the subsurface geological exploration. The paper\noutlines a comprehensive methodology, encompassing data preprocessing, machine\nlearning methods, and transfer learning techniques. The outcome reveals a\nremarkable 96.7% accuracy in the classification of drill core segments into\ndistinct formation classes. Furthermore, a CNN model was trained for the\nevaluation of mineral content using a learning data set from multidimensional\nlog analysis data (silicate, total clay, carbonate). When benchmarked against\nlaboratory XRD measurements on samples from the cores, both the advanced\nmultidimensional log analysis model and the neural network approach developed\nhere provide equally good performance. This work demonstrates that deep\nlearning and particularly transfer learning can support extracting\npetrophysical properties, including mineral content and formation\nclassification, from drill core images, thus offering a road map for enhancing\nmodel performance and data set quality in image-based analysis of drill cores.\n","authors":["Romana Boiger","Sergey V. Churakov","Ignacio Ballester Llagaria","Georg Kosakowski","Raphael Wüst","Nikolaos I. Prasianakis"],"pdf_url":"https://arxiv.org/pdf/2403.18495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.02203v5","updated":"2024-03-27T12:12:45Z","published":"2023-07-05T10:54:50Z","title":"Neural Fields for Interactive Visualization of Statistical Dependencies\n  in 3D Simulation Ensembles","summary":"  We present the first neural network that has learned to compactly represent\nand can efficiently reconstruct the statistical dependencies between the values\nof physical variables at different spatial locations in large 3D simulation\nensembles. Going beyond linear dependencies, we consider mutual information as\na measure of non-linear dependence. We demonstrate learning and reconstruction\nwith a large weather forecast ensemble comprising 1000 members, each storing\nmultiple physical variables at a 250 x 352 x 20 simulation grid. By\ncircumventing compute-intensive statistical estimators at runtime, we\ndemonstrate significantly reduced memory and computation requirements for\nreconstructing the major dependence structures. This enables embedding the\nestimator into a GPU-accelerated direct volume renderer and interactively\nvisualizing all mutual dependencies for a selected domain point.\n","authors":["Fatemeh Farokhmanesh","Kevin Höhlein","Christoph Neuhauser","Tobias Necker","Martin Weissmann","Takemasa Miyoshi","Rüdiger Westermann"],"pdf_url":"https://arxiv.org/pdf/2307.02203v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18493v1","updated":"2024-03-27T12:08:41Z","published":"2024-03-27T12:08:41Z","title":"VersaT2I: Improving Text-to-Image Models with Versatile Reward","summary":"  Recent text-to-image (T2I) models have benefited from large-scale and\nhigh-quality data, demonstrating impressive performance. However, these T2I\nmodels still struggle to produce images that are aesthetically pleasing,\ngeometrically accurate, faithful to text, and of good low-level quality. We\npresent VersaT2I, a versatile training framework that can boost the performance\nwith multiple rewards of any T2I model. We decompose the quality of the image\ninto several aspects such as aesthetics, text-image alignment, geometry,\nlow-level quality, etc. Then, for every quality aspect, we select high-quality\nimages in this aspect generated by the model as the training set to finetune\nthe T2I model using the Low-Rank Adaptation (LoRA). Furthermore, we introduce a\ngating function to combine multiple quality aspects, which can avoid conflicts\nbetween different quality aspects. Our method is easy to extend and does not\nrequire any manual annotation, reinforcement learning, or model architecture\nchanges. Extensive experiments demonstrate that VersaT2I outperforms the\nbaseline methods across various quality criteria.\n","authors":["Jianshu Guo","Wenhao Chai","Jie Deng","Hsiang-Wei Huang","Tian Ye","Yichen Xu","Jiawei Zhang","Jenq-Neng Hwang","Gaoang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18490v1","updated":"2024-03-27T12:05:22Z","published":"2024-03-27T12:05:22Z","title":"I2CKD : Intra- and Inter-Class Knowledge Distillation for Semantic\n  Segmentation","summary":"  This paper proposes a new knowledge distillation method tailored for image\nsemantic segmentation, termed Intra- and Inter-Class Knowledge Distillation\n(I2CKD). The focus of this method is on capturing and transferring knowledge\nbetween the intermediate layers of teacher (cumbersome model) and student\n(compact model). For knowledge extraction, we exploit class prototypes derived\nfrom feature maps. To facilitate knowledge transfer, we employ a triplet loss\nin order to minimize intra-class variances and maximize inter-class variances\nbetween teacher and student prototypes. Consequently, I2CKD enables the student\nto better mimic the feature representation of the teacher for each class,\nthereby enhancing the segmentation performance of the compact network.\nExtensive experiments on three segmentation datasets, i.e., Cityscapes, Pascal\nVOC and CamVid, using various teacher-student network pairs demonstrate the\neffectiveness of the proposed method.\n","authors":["Ayoub Karine","Thibault Napoléon","Maher Jridi"],"pdf_url":"https://arxiv.org/pdf/2403.18490v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16943v2","updated":"2024-03-27T11:46:36Z","published":"2023-12-28T10:40:11Z","title":"SAR-Net: Multi-scale Direction-aware SAR Network via Global Information\n  Fusion","summary":"  Deep learning has driven significant progress in object detection using\nSynthetic Aperture Radar (SAR) imagery. Existing methods, while achieving\npromising results, often struggle to effectively integrate local and global\ninformation, particularly direction-aware features. This paper proposes\nSAR-Net, a novel framework specifically designed for global fusion of\ndirection-aware information in SAR object detection. SAR-Net leverages two key\ninnovations: the Unity Compensation Mechanism (UCM) and the Direction-aware\nAttention Module (DAM). UCM facilitates the establishment of complementary\nrelationships among features across different scales, enabling efficient global\ninformation fusion. Among them, Multi-scale Alignment Module (MAM) and distinct\nMulti-level Fusion Module (MFM) enhance feature integration by capturing both\ntexture detail and semantic information. Then, Multi-feature Embedding Module\n(MEM) feeds back global features into the primary branches, further improving\ninformation transmission. Additionally, DAM, through bidirectional attention\npolymerization, captures direction-aware information, effectively eliminating\nbackground interference. Extensive experiments demonstrate the effectiveness of\nSAR-Net, achieving state-of-the-art results on aircraft (SAR-AIRcraft-1.0) and\nship datasets (SSDD, HRSID), confirming its generalization capability and\nrobustness.\n","authors":["Mingxiang Cao","Jie Lei","Weiying Xie","Jiaqing Zhang","Daixun Li","Yunsong Li"],"pdf_url":"https://arxiv.org/pdf/2312.16943v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18476v1","updated":"2024-03-27T11:45:08Z","published":"2024-03-27T11:45:08Z","title":"Modeling uncertainty for Gaussian Splatting","summary":"  We present Stochastic Gaussian Splatting (SGS): the first framework for\nuncertainty estimation using Gaussian Splatting (GS). GS recently advanced the\nnovel-view synthesis field by achieving impressive reconstruction quality at a\nfraction of the computational cost of Neural Radiance Fields (NeRF). However,\ncontrary to the latter, it still lacks the ability to provide information about\nthe confidence associated with their outputs. To address this limitation, in\nthis paper, we introduce a Variational Inference-based approach that seamlessly\nintegrates uncertainty prediction into the common rendering pipeline of GS.\nAdditionally, we introduce the Area Under Sparsification Error (AUSE) as a new\nterm in the loss function, enabling optimization of uncertainty estimation\nalongside image reconstruction. Experimental results on the LLFF dataset\ndemonstrate that our method outperforms existing approaches in terms of both\nimage rendering quality and uncertainty estimation accuracy. Overall, our\nframework equips practitioners with valuable insights into the reliability of\nsynthesized views, facilitating safer decision-making in real-world\napplications.\n","authors":["Luca Savant","Diego Valsesia","Enrico Magli"],"pdf_url":"https://arxiv.org/pdf/2403.18476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12028v2","updated":"2024-03-27T11:43:28Z","published":"2023-11-20T18:59:51Z","title":"Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose\n  Estimation","summary":"  Transformers have been successfully applied in the field of video-based 3D\nhuman pose estimation. However, the high computational costs of these video\npose transformers (VPTs) make them impractical on resource-constrained devices.\nIn this paper, we present a plug-and-play pruning-and-recovering framework,\ncalled Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose\nestimation from videos. Our HoT begins with pruning pose tokens of redundant\nframes and ends with recovering full-length tokens, resulting in a few pose\ntokens in the intermediate transformer blocks and thus improving the model\nefficiency. To effectively achieve this, we propose a token pruning cluster\n(TPC) that dynamically selects a few representative tokens with high semantic\ndiversity while eliminating the redundancy of video frames. In addition, we\ndevelop a token recovering attention (TRA) to restore the detailed\nspatio-temporal information based on the selected tokens, thereby expanding the\nnetwork output to the original full-length temporal resolution for fast\ninference. Extensive experiments on two benchmark datasets (i.e., Human3.6M and\nMPI-INF-3DHP) demonstrate that our method can achieve both high efficiency and\nestimation accuracy compared to the original VPT models. For instance, applying\nto MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPs\nwithout sacrificing accuracy and nearly 40% FLOPs with only 0.2% accuracy drop,\nrespectively. Code and models are available at\nhttps://github.com/NationalGAILab/HoT.\n","authors":["Wenhao Li","Mengyuan Liu","Hong Liu","Pichao Wang","Jialun Cai","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2311.12028v2.pdf","comment":"Accepted by CVPR 2024, Open Sourced"},{"id":"http://arxiv.org/abs/2403.18471v1","updated":"2024-03-27T11:32:44Z","published":"2024-03-27T11:32:44Z","title":"DiffusionFace: Towards a Comprehensive Dataset for Diffusion-Based Face\n  Forgery Analysis","summary":"  The rapid progress in deep learning has given rise to hyper-realistic facial\nforgery methods, leading to concerns related to misinformation and security\nrisks. Existing face forgery datasets have limitations in generating\nhigh-quality facial images and addressing the challenges posed by evolving\ngenerative techniques. To combat this, we present DiffusionFace, the first\ndiffusion-based face forgery dataset, covering various forgery categories,\nincluding unconditional and Text Guide facial image generation, Img2Img,\nInpaint, and Diffusion-based facial exchange algorithms. Our DiffusionFace\ndataset stands out with its extensive collection of 11 diffusion models and the\nhigh-quality of the generated images, providing essential metadata and a\nreal-world internet-sourced forgery facial image dataset for evaluation.\nAdditionally, we provide an in-depth analysis of the data and introduce\npractical evaluation protocols to rigorously assess discriminative models'\neffectiveness in detecting counterfeit facial images, aiming to enhance\nsecurity in facial image authentication processes. The dataset is available for\ndownload at \\url{https://github.com/Rapisurazurite/DiffFace}.\n","authors":["Zhongxi Chen","Ke Sun","Ziyin Zhou","Xianming Lin","Xiaoshuai Sun","Liujuan Cao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2403.18471v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18469v1","updated":"2024-03-27T11:28:57Z","published":"2024-03-27T11:28:57Z","title":"Density-guided Translator Boosts Synthetic-to-Real Unsupervised Domain\n  Adaptive Segmentation of 3D Point Clouds","summary":"  3D synthetic-to-real unsupervised domain adaptive segmentation is crucial to\nannotating new domains. Self-training is a competitive approach for this task,\nbut its performance is limited by different sensor sampling patterns (i.e.,\nvariations in point density) and incomplete training strategies. In this work,\nwe propose a density-guided translator (DGT), which translates point density\nbetween domains, and integrates it into a two-stage self-training pipeline\nnamed DGT-ST. First, in contrast to existing works that simultaneously conduct\ndata generation and feature/output alignment within unstable adversarial\ntraining, we employ the non-learnable DGT to bridge the domain gap at the input\nlevel. Second, to provide a well-initialized model for self-training, we\npropose a category-level adversarial network in stage one that utilizes the\nprototype to prevent negative transfer. Finally, by leveraging the designs\nabove, a domain-mixed self-training method with source-aware consistency loss\nis proposed in stage two to narrow the domain gap further. Experiments on two\nsynthetic-to-real segmentation tasks (SynLiDAR $\\rightarrow$ semanticKITTI and\nSynLiDAR $\\rightarrow$ semanticPOSS) demonstrate that DGT-ST outperforms\nstate-of-the-art methods, achieving 9.4$\\%$ and 4.3$\\%$ mIoU improvements,\nrespectively. Code is available at \\url{https://github.com/yuan-zm/DGT-ST}.\n","authors":["Zhimin Yuan","Wankang Zeng","Yanfei Su","Weiquan Liu","Ming Cheng","Yulan Guo","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18469v1.pdf","comment":"CVPR2024"},{"id":"http://arxiv.org/abs/2403.18468v1","updated":"2024-03-27T11:28:32Z","published":"2024-03-27T11:28:32Z","title":"Deep Learning Segmentation and Classification of Red Blood Cells Using a\n  Large Multi-Scanner Dataset","summary":"  Digital pathology has recently been revolutionized by advancements in\nartificial intelligence, deep learning, and high-performance computing. With\nits advanced tools, digital pathology can help improve and speed up the\ndiagnostic process, reduce human errors, and streamline the reporting step. In\nthis paper, we report a new large red blood cell (RBC) image dataset and\npropose a two-stage deep learning framework for RBC image segmentation and\nclassification. The dataset is a highly diverse dataset of more than 100K RBCs\ncontaining eight different classes. The dataset, which is considerably larger\nthan any publicly available hematopathology dataset, was labeled independently\nby two hematopathologists who also manually created masks for RBC cell\nsegmentation. Subsequently, in the proposed framework, first, a U-Net model was\ntrained to achieve automatic RBC image segmentation. Second, an EfficientNetB0\nmodel was trained to classify RBC images into one of the eight classes using a\ntransfer learning approach with a 5X2 cross-validation scheme. An IoU of 98.03%\nand an average classification accuracy of 96.5% were attained on the test set.\nMoreover, we have performed experimental comparisons against several prominent\nCNN models. These comparisons show the superiority of the proposed model with a\ngood balance between performance and computational cost.\n","authors":["Mohamed Elmanna","Ahmed Elsafty","Yomna Ahmed","Muhammad Rushdi","Ahmed Morsy"],"pdf_url":"https://arxiv.org/pdf/2403.18468v1.pdf","comment":"15 pages, 12 figures, 8 tables"},{"id":"http://arxiv.org/abs/2403.18461v1","updated":"2024-03-27T11:19:34Z","published":"2024-03-27T11:19:34Z","title":"DiffStyler: Diffusion-based Localized Image Style Transfer","summary":"  Image style transfer aims to imbue digital imagery with the distinctive\nattributes of style targets, such as colors, brushstrokes, shapes, whilst\nconcurrently preserving the semantic integrity of the content. Despite the\nadvancements in arbitrary style transfer methods, a prevalent challenge remains\nthe delicate equilibrium between content semantics and style attributes. Recent\ndevelopments in large-scale text-to-image diffusion models have heralded\nunprecedented synthesis capabilities, albeit at the expense of relying on\nextensive and often imprecise textual descriptions to delineate artistic\nstyles. Addressing these limitations, this paper introduces DiffStyler, a novel\napproach that facilitates efficient and precise arbitrary image style transfer.\nDiffStyler lies the utilization of a text-to-image Stable Diffusion model-based\nLoRA to encapsulate the essence of style targets. This approach, coupled with\nstrategic cross-LoRA feature and attention injection, guides the style transfer\nprocess. The foundation of our methodology is rooted in the observation that\nLoRA maintains the spatial feature consistency of UNet, a discovery that\nfurther inspired the development of a mask-wise style transfer technique. This\ntechnique employs masks extracted through a pre-trained FastSAM model,\nutilizing mask prompts to facilitate feature fusion during the denoising\nprocess, thereby enabling localized style transfer that preserves the original\nimage's unaffected regions. Moreover, our approach accommodates multiple style\ntargets through the use of corresponding masks. Through extensive\nexperimentation, we demonstrate that DiffStyler surpasses previous methods in\nachieving a more harmonious balance between content preservation and style\nintegration.\n","authors":["Shaoxu Li"],"pdf_url":"https://arxiv.org/pdf/2403.18461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10522v4","updated":"2024-03-27T11:18:51Z","published":"2023-11-17T13:43:43Z","title":"Enhancing Object Coherence in Layout-to-Image Synthesis","summary":"  Layout-to-image synthesis is an emerging technique in conditional image\ngeneration. It aims to generate complex scenes, where users require fine\ncontrol over the layout of the objects in a scene. However, it remains\nchallenging to control the object coherence, including semantic coherence\n(e.g., the cat looks at the flowers or not) and physical coherence (e.g., the\nhand and the racket should not be misaligned). In this paper, we propose a\nnovel diffusion model with effective global semantic fusion (GSF) and\nself-similarity feature enhancement modules to guide the object coherence for\nthis task. For semantic coherence, we argue that the image caption contains\nrich information for defining the semantic relationship within the objects in\nthe images. Instead of simply employing cross-attention between captions and\ngenerated images, which addresses the highly relevant layout restriction and\nsemantic coherence separately and thus leads to unsatisfying results shown in\nour experiments, we develop GSF to fuse the supervision from the layout\nrestriction and semantic coherence requirement and exploit it to guide the\nimage synthesis process. Moreover, to improve the physical coherence, we\ndevelop a Self-similarity Coherence Attention (SCA) module to explicitly\nintegrate local contextual physical coherence into each pixel's generation\nprocess. Specifically, we adopt a self-similarity map to encode the coherence\nrestrictions and employ it to extract coherent features from text embedding.\nThrough visualization of our self-similarity map, we explore the essence of\nSCA, revealing that its effectiveness is not only in capturing reliable\nphysical coherence patterns but also in enhancing complex texture generation.\nExtensive experiments demonstrate the superiority of our proposed method in\nboth image generation quality and controllability.\n","authors":["Yibin Wang","Weizhong Zhang","Jianwei Zheng","Cheng Jin"],"pdf_url":"https://arxiv.org/pdf/2311.10522v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18454v1","updated":"2024-03-27T11:13:20Z","published":"2024-03-27T11:13:20Z","title":"Scaling Vision-and-Language Navigation With Offline RL","summary":"  The study of vision-and-language navigation (VLN) has typically relied on\nexpert trajectories, which may not always be available in real-world situations\ndue to the significant effort required to collect them. On the other hand,\nexisting approaches to training VLN agents that go beyond available expert data\ninvolve data augmentations or online exploration which can be tedious and\nrisky. In contrast, it is easy to access large repositories of suboptimal\noffline trajectories. Inspired by research in offline reinforcement learning\n(ORL), we introduce a new problem setup of VLN-ORL which studies VLN using\nsuboptimal demonstration data. We introduce a simple and effective\nreward-conditioned approach that can account for dataset suboptimality for\ntraining VLN agents, as well as benchmarks to evaluate progress and promote\nresearch in this area. We empirically study various noise models for\ncharacterizing dataset suboptimality among other unique challenges in VLN-ORL\nand instantiate it for the VLN$\\circlearrowright$BERT and MTVM architectures in\nthe R2R and RxR environments. Our experiments demonstrate that the proposed\nreward-conditioned approach leads to significant performance improvements, even\nin complex and intricate environments.\n","authors":["Valay Bundele","Mahesh Bhupati","Biplab Banerjee","Aditya Grover"],"pdf_url":"https://arxiv.org/pdf/2403.18454v1.pdf","comment":"Published in Transactions on Machine Learning Research (04/2024)"},{"id":"http://arxiv.org/abs/2403.18452v1","updated":"2024-03-27T11:11:08Z","published":"2024-03-27T11:11:08Z","title":"SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model","summary":"  There are five types of trajectory prediction tasks: deterministic,\nstochastic, domain adaptation, momentary observation, and few-shot. These\nassociated tasks are defined by various factors, such as the length of input\npaths, data split and pre-processing methods. Interestingly, even though they\ncommonly take sequential coordinates of observations as input and infer future\npaths in the same coordinates as output, designing specialized architectures\nfor each task is still necessary. For the other task, generality issues can\nlead to sub-optimal performances. In this paper, we propose SingularTrajectory,\na diffusion-based universal trajectory prediction framework to reduce the\nperformance gap across the five tasks. The core of SingularTrajectory is to\nunify a variety of human dynamics representations on the associated tasks. To\ndo this, we first build a Singular space to project all types of motion\npatterns from each task into one embedding space. We next propose an adaptive\nanchor working in the Singular space. Unlike traditional fixed anchor methods\nthat sometimes yield unacceptable paths, our adaptive anchor enables correct\nanchors, which are put into a wrong location, based on a traversability map.\nFinally, we adopt a diffusion-based predictor to further enhance the prototype\npaths using a cascaded denoising process. Our unified framework ensures the\ngenerality across various benchmark settings such as input modality, and\ntrajectory lengths. Extensive experiments on five public benchmarks demonstrate\nthat SingularTrajectory substantially outperforms existing models, highlighting\nits effectiveness in estimating general dynamics of human movements. Code is\npublicly available at https://github.com/inhwanbae/SingularTrajectory .\n","authors":["Inhwan Bae","Young-Jae Park","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2403.18452v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18447v1","updated":"2024-03-27T11:06:44Z","published":"2024-03-27T11:06:44Z","title":"Can Language Beat Numerical Regression? Language-Based Multimodal\n  Trajectory Prediction","summary":"  Language models have demonstrated impressive ability in context understanding\nand generative performance. Inspired by the recent success of language\nfoundation models, in this paper, we propose LMTraj (Language-based Multimodal\nTrajectory predictor), which recasts the trajectory prediction task into a sort\nof question-answering problem. Departing from traditional numerical regression\nmodels, which treat the trajectory coordinate sequence as continuous signals,\nwe consider them as discrete signals like text prompts. Specially, we first\ntransform an input space for the trajectory coordinate into the natural\nlanguage space. Here, the entire time-series trajectories of pedestrians are\nconverted into a text prompt, and scene images are described as text\ninformation through image captioning. The transformed numerical and image data\nare then wrapped into the question-answering template for use in a language\nmodel. Next, to guide the language model in understanding and reasoning\nhigh-level knowledge, such as scene context and social relationships between\npedestrians, we introduce an auxiliary multi-task question and answering. We\nthen train a numerical tokenizer with the prompt data. We encourage the\ntokenizer to separate the integer and decimal parts well, and leverage it to\ncapture correlations between the consecutive numbers in the language model.\nLastly, we train the language model using the numerical tokenizer and all of\nthe question-answer prompts. Here, we propose a beam-search-based most-likely\nprediction and a temperature-based multimodal prediction to implement both\ndeterministic and stochastic inferences. Applying our LMTraj, we show that the\nlanguage-based model can be a powerful pedestrian trajectory predictor, and\noutperforms existing numerical-based predictor methods. Code is publicly\navailable at https://github.com/inhwanbae/LMTrajectory .\n","authors":["Inhwan Bae","Junoh Lee","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2403.18447v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18443v1","updated":"2024-03-27T11:00:33Z","published":"2024-03-27T11:00:33Z","title":"$\\mathrm{F^2Depth}$: Self-supervised Indoor Monocular Depth Estimation\n  via Optical Flow Consistency and Feature Map Synthesis","summary":"  Self-supervised monocular depth estimation methods have been increasingly\ngiven much attention due to the benefit of not requiring large, labelled\ndatasets. Such self-supervised methods require high-quality salient features\nand consequently suffer from severe performance drop for indoor scenes, where\nlow-textured regions dominant in the scenes are almost indiscriminative. To\naddress the issue, we propose a self-supervised indoor monocular depth\nestimation framework called $\\mathrm{F^2Depth}$. A self-supervised optical flow\nestimation network is introduced to supervise depth learning. To improve\noptical flow estimation performance in low-textured areas, only some patches of\npoints with more discriminative features are adopted for finetuning based on\nour well-designed patch-based photometric loss. The finetuned optical flow\nestimation network generates high-accuracy optical flow as a supervisory signal\nfor depth estimation. Correspondingly, an optical flow consistency loss is\ndesigned. Multi-scale feature maps produced by finetuned optical flow\nestimation network perform warping to compute feature map synthesis loss as\nanother supervisory signal for depth learning. Experimental results on the NYU\nDepth V2 dataset demonstrate the effectiveness of the framework and our\nproposed losses. To evaluate the generalization ability of our\n$\\mathrm{F^2Depth}$, we collect a Campus Indoor depth dataset composed of\napproximately 1500 points selected from 99 images in 18 scenes. Zero-shot\ngeneralization experiments on 7-Scenes dataset and Campus Indoor achieve\n$\\delta_1$ accuracy of 75.8% and 76.0% respectively. The accuracy results show\nthat our model can generalize well to monocular images captured in unknown\nindoor scenes.\n","authors":["Xiaotong Guo","Huijie Zhao","Shuwei Shao","Xudong Li","Baochang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.18443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.17126v2","updated":"2024-03-27T10:50:54Z","published":"2022-11-30T16:03:24Z","title":"BEVUDA: Multi-geometric Space Alignments for Domain Adaptive BEV 3D\n  Object Detection","summary":"  Vision-centric bird-eye-view (BEV) perception has shown promising potential\nin autonomous driving. Recent works mainly focus on improving efficiency or\naccuracy but neglect the challenges when facing environment changing, resulting\nin severe degradation of transfer performance. For BEV perception, we figure\nout the significant domain gaps existing in typical real-world cross-domain\nscenarios and comprehensively solve the Domain Adaption (DA) problem for\nmulti-view 3D object detection. Since BEV perception approaches are complicated\nand contain several components, the domain shift accumulation on multiple\ngeometric spaces (i.e., 2D, 3D Voxel, BEV) makes BEV DA even challenging. In\nthis paper, we propose a Multi-space Alignment Teacher-Student (MATS) framework\nto ease the domain shift accumulation, which consists of a Depth-Aware Teacher\n(DAT) and a Geometric-space Aligned Student (GAS) model. DAT tactfully combines\ntarget lidar and reliable depth prediction to construct depth-aware\ninformation, extracting target domain-specific knowledge in Voxel and BEV\nfeature spaces. It then transfers the sufficient domain knowledge of multiple\nspaces to the student model. In order to jointly alleviate the domain shift,\nGAS projects multi-geometric space features to a shared geometric embedding\nspace and decreases data distribution distance between two domains. To verify\nthe effectiveness of our method, we conduct BEV 3D object detection experiments\non three cross-domain scenarios and achieve state-of-the-art performance.\n","authors":["Jiaming Liu","Rongyu Zhang","Xiaoqi Li","Xiaowei Chi","Zehui Chen","Ming Lu","Yandong Guo","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2211.17126v2.pdf","comment":"Accepted by ICRA2024"},{"id":"http://arxiv.org/abs/2403.18442v1","updated":"2024-03-27T10:50:24Z","published":"2024-03-27T10:50:24Z","title":"Backpropagation-free Network for 3D Test-time Adaptation","summary":"  Real-world systems often encounter new data over time, which leads to\nexperiencing target domain shifts. Existing Test-Time Adaptation (TTA) methods\ntend to apply computationally heavy and memory-intensive backpropagation-based\napproaches to handle this. Here, we propose a novel method that uses a\nbackpropagation-free approach for TTA for the specific case of 3D data. Our\nmodel uses a two-stream architecture to maintain knowledge about the source\ndomain as well as complementary target-domain-specific information. The\nbackpropagation-free property of our model helps address the well-known\nforgetting problem and mitigates the error accumulation issue. The proposed\nmethod also eliminates the need for the usually noisy process of\npseudo-labeling and reliance on costly self-supervised training. Moreover, our\nmethod leverages subspace learning, effectively reducing the distribution\nvariance between the two domains. Furthermore, the source-domain-specific and\nthe target-domain-specific streams are aligned using a novel entropy-based\nadaptive fusion strategy. Extensive experiments on popular benchmarks\ndemonstrate the effectiveness of our method. The code will be available at\nhttps://github.com/abie-e/BFTT3D.\n","authors":["Yanshuo Wang","Ali Cheraghian","Zeeshan Hayder","Jie Hong","Sameera Ramasinghe","Shafin Rahman","David Ahmedt-Aristizabal","Xuesong Li","Lars Petersson","Mehrtash Harandi"],"pdf_url":"https://arxiv.org/pdf/2403.18442v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2311.18113v2","updated":"2024-03-27T10:46:59Z","published":"2023-11-29T21:58:41Z","title":"Back to 3D: Few-Shot 3D Keypoint Detection with Back-Projected 2D\n  Features","summary":"  With the immense growth of dataset sizes and computing resources in recent\nyears, so-called foundation models have become popular in NLP and vision tasks.\nIn this work, we propose to explore foundation models for the task of keypoint\ndetection on 3D shapes. A unique characteristic of keypoint detection is that\nit requires semantic and geometric awareness while demanding high localization\naccuracy. To address this problem, we propose, first, to back-project features\nfrom large pre-trained 2D vision models onto 3D shapes and employ them for this\ntask. We show that we obtain robust 3D features that contain rich semantic\ninformation and analyze multiple candidate features stemming from different 2D\nfoundation models. Second, we employ a keypoint candidate optimization module\nwhich aims to match the average observed distribution of keypoints on the shape\nand is guided by the back-projected features. The resulting approach achieves a\nnew state of the art for few-shot keypoint detection on the KeyPointNet\ndataset, almost doubling the performance of the previous best methods.\n","authors":["Thomas Wimmer","Peter Wonka","Maks Ovsjanikov"],"pdf_url":"https://arxiv.org/pdf/2311.18113v2.pdf","comment":"Accepted to CVPR 2024, Project page:\n  https://wimmerth.github.io/back-to-3d.html"},{"id":"http://arxiv.org/abs/2401.08742v2","updated":"2024-03-27T10:33:02Z","published":"2024-01-16T18:58:36Z","title":"Fast Dynamic 3D Object Generation from a Single-view Video","summary":"  Generating dynamic 3D object from a single-view video is challenging due to\nthe lack of 4D labeled data. Extending image-to-3D pipelines by transferring\noff-the-shelf image generation models such as score distillation sampling,\nexisting methods tend to be slow and expensive to scale due to the need for\nback-propagating the information-limited supervision signals through a large\npretrained model. To address this, we propose an efficient video-to-4D object\ngeneration framework called Efficient4D. It generates high-quality\nspacetime-consistent images under different camera views, and then uses them as\nlabeled data to directly train a novel 4D Gaussian splatting model with\nexplicit point cloud geometry, enabling real-time rendering under continuous\ncamera trajectories. Extensive experiments on synthetic and real videos show\nthat Efficient4D offers a remarkable 20-fold increase in speed when compared to\nprior art alternatives while preserving the quality of novel view synthesis.\nFor example, Efficient4D takes only 6 mins to model a dynamic object, vs 120\nmins by Consistent4D.\n","authors":["Zijie Pan","Zeyu Yang","Xiatian Zhu","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.08742v2.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2403.18425v1","updated":"2024-03-27T10:26:42Z","published":"2024-03-27T10:26:42Z","title":"U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models","summary":"  Diffusion models have demonstrated remarkable performance in text-to-image\nsynthesis, producing realistic and high resolution images that faithfully\nadhere to the corresponding text-prompts. Despite their great success, they\nstill fall behind in sketch-to-image synthesis tasks, where in addition to\ntext-prompts, the spatial layout of the generated images has to closely follow\nthe outlines of certain reference sketches. Employing an MLP latent edge\npredictor to guide the spatial layout of the synthesized image by predicting\nedge maps at each denoising step has been recently proposed. Despite yielding\npromising results, the pixel-wise operation of the MLP does not take into\naccount the spatial layout as a whole, and demands numerous denoising\niterations to produce satisfactory images, leading to time inefficiency. To\nthis end, we introduce U-Sketch, a framework featuring a U-Net type latent edge\npredictor, which is capable of efficiently capturing both local and global\nfeatures, as well as spatial correlations between pixels. Moreover, we propose\nthe addition of a sketch simplification network that offers the user the choice\nof preprocessing and simplifying input sketches for enhanced outputs. The\nexperimental results, corroborated by user feedback, demonstrate that our\nproposed U-Net latent edge predictor leads to more realistic results, that are\nbetter aligned with the spatial outlines of the reference sketches, while\ndrastically reducing the number of required denoising steps and, consequently,\nthe overall execution time.\n","authors":["Ilias Mitsouras","Eleftherios Tsonis","Paraskevi Tzouveli","Athanasios Voulodimos"],"pdf_url":"https://arxiv.org/pdf/2403.18425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15098v2","updated":"2024-03-27T10:26:23Z","published":"2024-03-22T10:36:50Z","title":"UniTraj: A Unified Framework for Scalable Vehicle Trajectory Prediction","summary":"  Vehicle trajectory prediction has increasingly relied on data-driven\nsolutions, but their ability to scale to different data domains and the impact\nof larger dataset sizes on their generalization remain under-explored. While\nthese questions can be studied by employing multiple datasets, it is\nchallenging due to several discrepancies, e.g., in data formats, map\nresolution, and semantic annotation types. To address these challenges, we\nintroduce UniTraj, a comprehensive framework that unifies various datasets,\nmodels, and evaluation criteria, presenting new opportunities for the vehicle\ntrajectory prediction field. In particular, using UniTraj, we conduct extensive\nexperiments and find that model performance significantly drops when\ntransferred to other datasets. However, enlarging data size and diversity can\nsubstantially improve performance, leading to a new state-of-the-art result for\nthe nuScenes dataset. We provide insights into dataset characteristics to\nexplain these findings. The code can be found here:\nhttps://github.com/vita-epfl/UniTraj\n","authors":["Lan Feng","Mohammadhossein Bahari","Kaouther Messaoud Ben Amor","Éloi Zablocki","Matthieu Cord","Alexandre Alahi"],"pdf_url":"https://arxiv.org/pdf/2403.15098v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12359v2","updated":"2024-03-27T10:18:04Z","published":"2023-12-19T17:40:27Z","title":"CLIP-DINOiser: Teaching CLIP a few DINO tricks for open-vocabulary\n  semantic segmentation","summary":"  The popular CLIP model displays impressive zero-shot capabilities thanks to\nits seamless interaction with arbitrary text prompts. However, its lack of\nspatial awareness makes it unsuitable for dense computer vision tasks, e.g.,\nsemantic segmentation, without an additional fine-tuning step that often uses\nannotations and can potentially suppress its original open-vocabulary\nproperties. Meanwhile, self-supervised representation methods have demonstrated\ngood localization properties without human-made annotations nor explicit\nsupervision. In this work, we take the best of both worlds and propose an\nopen-vocabulary semantic segmentation method, which does not require any\nannotations. We propose to locally improve dense MaskCLIP features, which are\ncomputed with a simple modification of CLIP's last pooling layer, by\nintegrating localization priors extracted from self-supervised features. By\ndoing so, we greatly improve the performance of MaskCLIP and produce smooth\noutputs. Moreover, we show that the used self-supervised feature properties can\ndirectly be learnt from CLIP features. Our method CLIP-DINOiser needs only a\nsingle forward pass of CLIP and two light convolutional layers at inference, no\nextra supervision nor extra memory and reaches state-of-the-art results on\nchallenging and fine-grained benchmarks such as COCO, Pascal Context,\nCityscapes and ADE20k. The code to reproduce our results is available at\nhttps://github.com/wysoczanska/clip_dinoiser.\n","authors":["Monika Wysoczańska","Oriane Siméoni","Michaël Ramamonjisoa","Andrei Bursuc","Tomasz Trzciński","Patrick Pérez"],"pdf_url":"https://arxiv.org/pdf/2312.12359v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12480v2","updated":"2024-03-27T10:12:32Z","published":"2023-12-19T15:34:52Z","title":"Continual-MAE: Adaptive Distribution Masked Autoencoders for Continual\n  Test-Time Adaptation","summary":"  Continual Test-Time Adaptation (CTTA) is proposed to migrate a source\npre-trained model to continually changing target distributions, addressing\nreal-world dynamism. Existing CTTA methods mainly rely on entropy minimization\nor teacher-student pseudo-labeling schemes for knowledge extraction in\nunlabeled target domains. However, dynamic data distributions cause\nmiscalibrated predictions and noisy pseudo-labels in existing self-supervised\nlearning methods, hindering the effective mitigation of error accumulation and\ncatastrophic forgetting problems during the continual adaptation process. To\ntackle these issues, we propose a continual self-supervised method, Adaptive\nDistribution Masked Autoencoders (ADMA), which enhances the extraction of\ntarget domain knowledge while mitigating the accumulation of distribution\nshifts. Specifically, we propose a Distribution-aware Masking (DaM) mechanism\nto adaptively sample masked positions, followed by establishing consistency\nconstraints between the masked target samples and the original target samples.\nAdditionally, for masked tokens, we utilize an efficient decoder to reconstruct\na hand-crafted feature descriptor (e.g., Histograms of Oriented Gradients),\nleveraging its invariant properties to boost task-relevant representations.\nThrough conducting extensive experiments on four widely recognized benchmarks,\nour proposed method attains state-of-the-art performance in both classification\nand segmentation CTTA tasks. Our project page:\nhttps://sites.google.com/view/continual-mae/home.\n","authors":["Jiaming Liu","Ran Xu","Senqiao Yang","Renrui Zhang","Qizhe Zhang","Zehui Chen","Yandong Guo","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.12480v2.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.18417v1","updated":"2024-03-27T10:09:38Z","published":"2024-03-27T10:09:38Z","title":"ECNet: Effective Controllable Text-to-Image Diffusion Models","summary":"  The conditional text-to-image diffusion models have garnered significant\nattention in recent years. However, the precision of these models is often\ncompromised mainly for two reasons, ambiguous condition input and inadequate\ncondition guidance over single denoising loss. To address the challenges, we\nintroduce two innovative solutions. Firstly, we propose a Spatial Guidance\nInjector (SGI) which enhances conditional detail by encoding text inputs with\nprecise annotation information. This method directly tackles the issue of\nambiguous control inputs by providing clear, annotated guidance to the model.\nSecondly, to overcome the issue of limited conditional supervision, we\nintroduce Diffusion Consistency Loss (DCL), which applies supervision on the\ndenoised latent code at any given time step. This encourages consistency\nbetween the latent code at each time step and the input signal, thereby\nenhancing the robustness and accuracy of the output. The combination of SGI and\nDCL results in our Effective Controllable Network (ECNet), which offers a more\naccurate controllable end-to-end text-to-image generation framework with a more\nprecise conditioning input and stronger controllable supervision. We validate\nour approach through extensive experiments on generation under various\nconditions, such as human body skeletons, facial landmarks, and sketches of\ngeneral objects. The results consistently demonstrate that our method\nsignificantly enhances the controllability and robustness of the generated\nimages, outperforming existing state-of-the-art controllable text-to-image\nmodels.\n","authors":["Sicheng Li","Keqiang Sun","Zhixin Lai","Xiaoshi Wu","Feng Qiu","Haoran Xie","Kazunori Miyata","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2403.18417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06075v2","updated":"2024-03-27T09:51:15Z","published":"2023-09-12T09:12:37Z","title":"A2V: A Semi-Supervised Domain Adaptation Framework for Brain Vessel\n  Segmentation via Two-Phase Training Angiography-to-Venography Translation","summary":"  We present a semi-supervised domain adaptation framework for brain vessel\nsegmentation from different image modalities. Existing state-of-the-art methods\nfocus on a single modality, despite the wide range of available cerebrovascular\nimaging techniques. This can lead to significant distribution shifts that\nnegatively impact the generalization across modalities. By relying on annotated\nangiographies and a limited number of annotated venographies, our framework\naccomplishes image-to-image translation and semantic segmentation, leveraging a\ndisentangled and semantically rich latent space to represent heterogeneous data\nand perform image-level adaptation from source to target domains. Moreover, we\nreduce the typical complexity of cycle-based architectures and minimize the use\nof adversarial training, which allows us to build an efficient and intuitive\nmodel with stable training. We evaluate our method on magnetic resonance\nangiographies and venographies. While achieving state-of-the-art performance in\nthe source domain, our method attains a Dice score coefficient in the target\ndomain that is only 8.9% lower, highlighting its promising potential for robust\ncerebrovascular image segmentation across different modalities.\n","authors":["Francesco Galati","Daniele Falcetta","Rosa Cortese","Barbara Casolla","Ferran Prados","Ninon Burgos","Maria A. Zuluaga"],"pdf_url":"https://arxiv.org/pdf/2309.06075v2.pdf","comment":"Accepted at the 34th British Machine Vision Conference (BMVC)"},{"id":"http://arxiv.org/abs/2403.18407v1","updated":"2024-03-27T09:49:37Z","published":"2024-03-27T09:49:37Z","title":"A Channel-ensemble Approach: Unbiased and Low-variance Pseudo-labels is\n  Critical for Semi-supervised Classification","summary":"  Semi-supervised learning (SSL) is a practical challenge in computer vision.\nPseudo-label (PL) methods, e.g., FixMatch and FreeMatch, obtain the State Of\nThe Art (SOTA) performances in SSL. These approaches employ a\nthreshold-to-pseudo-label (T2L) process to generate PLs by truncating the\nconfidence scores of unlabeled data predicted by the self-training method.\nHowever, self-trained models typically yield biased and high-variance\npredictions, especially in the scenarios when a little labeled data are\nsupplied. To address this issue, we propose a lightweight channel-based\nensemble method to effectively consolidate multiple inferior PLs into the\ntheoretically guaranteed unbiased and low-variance one. Importantly, our\napproach can be readily extended to any SSL framework, such as FixMatch or\nFreeMatch. Experimental results demonstrate that our method significantly\noutperforms state-of-the-art techniques on CIFAR10/100 in terms of\neffectiveness and efficiency.\n","authors":["Jiaqi Wu","Junbiao Pang","Baochang Zhang","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2403.18407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18406v1","updated":"2024-03-27T09:48:23Z","published":"2024-03-27T09:48:23Z","title":"An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering\n  Using a VLM","summary":"  Stimulated by the sophisticated reasoning capabilities of recent Large\nLanguage Models (LLMs), a variety of strategies for bridging video modality\nhave been devised. A prominent strategy involves Video Language Models\n(VideoLMs), which train a learnable interface with video data to connect\nadvanced vision encoders with LLMs. Recently, an alternative strategy has\nsurfaced, employing readily available foundation models, such as VideoLMs and\nLLMs, across multiple stages for modality bridging. In this study, we introduce\na simple yet novel strategy where only a single Vision Language Model (VLM) is\nutilized. Our starting point is the plain insight that a video comprises a\nseries of images, or frames, interwoven with temporal information. The essence\nof video comprehension lies in adeptly managing the temporal aspects along with\nthe spatial details of each frame. Initially, we transform a video into a\nsingle composite image by arranging multiple frames in a grid layout. The\nresulting single image is termed as an image grid. This format, while\nmaintaining the appearance of a solitary image, effectively retains temporal\ninformation within the grid structure. Therefore, the image grid approach\nenables direct application of a single high-performance VLM without\nnecessitating any video-data training. Our extensive experimental analysis\nacross ten zero-shot video question answering benchmarks, including five\nopen-ended and five multiple-choice benchmarks, reveals that the proposed Image\nGrid Vision Language Model (IG-VLM) surpasses the existing methods in nine out\nof ten benchmarks.\n","authors":["Wonkyun Kim","Changin Choi","Wonseok Lee","Wonjong Rhee"],"pdf_url":"https://arxiv.org/pdf/2403.18406v1.pdf","comment":"Our code is available at https://github.com/imagegridworth/IG-VLM"},{"id":"http://arxiv.org/abs/2403.05262v2","updated":"2024-03-27T09:43:41Z","published":"2024-03-08T12:35:07Z","title":"Debiasing Multimodal Large Language Models","summary":"  In the realms of computer vision and natural language processing, Large\nVision-Language Models (LVLMs) have become indispensable tools, proficient in\ngenerating textual descriptions based on visual inputs. Despite their\nadvancements, our investigation reveals a noteworthy bias in the generated\ncontent, where the output is primarily influenced by the underlying Large\nLanguage Models (LLMs) prior rather than the input image. Our empirical\nexperiments underscore the persistence of this bias, as LVLMs often provide\nconfident answers even in the absence of relevant images or given incongruent\nvisual input. To rectify these biases and redirect the model's focus toward\nvision information, we introduce two simple, training-free strategies. Firstly,\nfor tasks such as classification or multi-choice question-answering (QA), we\npropose a ``calibration'' step through affine transformation to adjust the\noutput distribution. This ``Post-Hoc debias'' approach ensures uniform scores\nfor each answer when the image is absent, serving as an effective\nregularization technique to alleviate the influence of LLM priors. For more\nintricate open-ended generation tasks, we extend this method to ``Debias\nsampling'', drawing inspirations from contrastive decoding methods.\nFurthermore, our investigation sheds light on the instability of LVLMs across\nvarious decoding configurations. Through systematic exploration of different\nsettings, we significantly enhance performance, surpassing reported results and\nraising concerns about the fairness of existing evaluations. Comprehensive\nexperiments substantiate the effectiveness of our proposed strategies in\nmitigating biases. These strategies not only prove beneficial in minimizing\nhallucinations but also contribute to the generation of more helpful and\nprecise illustrations.\n","authors":["Yi-Fan Zhang","Weichen Yu","Qingsong Wen","Xue Wang","Zhang Zhang","Liang Wang","Rong Jin","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2403.05262v2.pdf","comment":"38 pages, 17 figures"},{"id":"http://arxiv.org/abs/2401.01647v2","updated":"2024-03-27T09:39:41Z","published":"2024-01-03T09:46:43Z","title":"SIGNeRF: Scene Integrated Generation for Neural Radiance Fields","summary":"  Advances in image diffusion models have recently led to notable improvements\nin the generation of high-quality images. In combination with Neural Radiance\nFields (NeRFs), they enabled new opportunities in 3D generation. However, most\ngenerative 3D approaches are object-centric and applying them to editing\nexisting photorealistic scenes is not trivial. We propose SIGNeRF, a novel\napproach for fast and controllable NeRF scene editing and scene-integrated\nobject generation. A new generative update strategy ensures 3D consistency\nacross the edited images, without requiring iterative optimization. We find\nthat depth-conditioned diffusion models inherently possess the capability to\ngenerate 3D consistent views by requesting a grid of images instead of single\nviews. Based on these insights, we introduce a multi-view reference sheet of\nmodified images. Our method updates an image collection consistently based on\nthe reference sheet and refines the original NeRF with the newly generated\nimage set in one go. By exploiting the depth conditioning mechanism of the\nimage diffusion model, we gain fine control over the spatial location of the\nedit and enforce shape guidance by a selected region or an external mesh.\n","authors":["Jan-Niklas Dihlmann","Andreas Engelhardt","Hendrik Lensch"],"pdf_url":"https://arxiv.org/pdf/2401.01647v2.pdf","comment":"Project Page: https://signerf.jdihlmann.com"},{"id":"http://arxiv.org/abs/2403.18397v1","updated":"2024-03-27T09:35:56Z","published":"2024-03-27T09:35:56Z","title":"Colour and Brush Stroke Pattern Recognition in Abstract Art using\n  Modified Deep Convolutional Generative Adversarial Networks","summary":"  Abstract Art is an immensely popular, discussed form of art that often has\nthe ability to depict the emotions of an artist. Many researchers have made\nattempts to study abstract art in the form of edge detection, brush stroke and\nemotion recognition algorithms using machine and deep learning. This papers\ndescribes the study of a wide distribution of abstract paintings using\nGenerative Adversarial Neural Networks(GAN). GANs have the ability to learn and\nreproduce a distribution enabling researchers and scientists to effectively\nexplore and study the generated image space. However, the challenge lies in\ndeveloping an efficient GAN architecture that overcomes common training\npitfalls. This paper addresses this challenge by introducing a modified-DCGAN\n(mDCGAN) specifically designed for high-quality artwork generation. The\napproach involves a thorough exploration of the modifications made, delving\ninto the intricate workings of DCGANs, optimisation techniques, and\nregularisation methods aimed at improving stability and realism in art\ngeneration enabling effective study of generated patterns. The proposed mDCGAN\nincorporates meticulous adjustments in layer configurations and architectural\nchoices, offering tailored solutions to the unique demands of art generation\nwhile effectively combating issues like mode collapse and gradient vanishing.\nFurther this paper explores the generated latent space by performing random\nwalks to understand vector relationships between brush strokes and colours in\nthe abstract art space and a statistical analysis of unstable outputs after a\ncertain period of GAN training and compare its significant difference. These\nfindings validate the effectiveness of the proposed approach, emphasising its\npotential to revolutionise the field of digital art generation and digital art\necosystem.\n","authors":["Srinitish Srinivasan","Varenya Pathak"],"pdf_url":"https://arxiv.org/pdf/2403.18397v1.pdf","comment":"28 pages, 5 tables, 7 figures"},{"id":"http://arxiv.org/abs/2403.11656v2","updated":"2024-03-27T09:34:44Z","published":"2024-03-18T10:53:00Z","title":"LocalStyleFool: Regional Video Style Transfer Attack Using Segment\n  Anything Model","summary":"  Previous work has shown that well-crafted adversarial perturbations can\nthreaten the security of video recognition systems. Attackers can invade such\nmodels with a low query budget when the perturbations are semantic-invariant,\nsuch as StyleFool. Despite the query efficiency, the naturalness of the minutia\nareas still requires amelioration, since StyleFool leverages style transfer to\nall pixels in each frame. To close the gap, we propose LocalStyleFool, an\nimproved black-box video adversarial attack that superimposes regional\nstyle-transfer-based perturbations on videos. Benefiting from the popularity\nand scalably usability of Segment Anything Model (SAM), we first extract\ndifferent regions according to semantic information and then track them through\nthe video stream to maintain the temporal consistency. Then, we add\nstyle-transfer-based perturbations to several regions selected based on the\nassociative criterion of transfer-based gradient information and regional area.\nPerturbation fine adjustment is followed to make stylized videos adversarial.\nWe demonstrate that LocalStyleFool can improve both intra-frame and inter-frame\nnaturalness through a human-assessed survey, while maintaining competitive\nfooling rate and query efficiency. Successful experiments on the\nhigh-resolution dataset also showcase that scrupulous segmentation of SAM helps\nto improve the scalability of adversarial attacks under high-resolution data.\n","authors":["Yuxin Cao","Jinghao Li","Xi Xiao","Derui Wang","Minhui Xue","Hao Ge","Wei Liu","Guangwu Hu"],"pdf_url":"https://arxiv.org/pdf/2403.11656v2.pdf","comment":"Accepted to 2024 IEEE Security and Privacy Workshops (SPW)"},{"id":"http://arxiv.org/abs/2403.18388v1","updated":"2024-03-27T09:25:20Z","published":"2024-03-27T09:25:20Z","title":"FTBC: Forward Temporal Bias Correction for Optimizing ANN-SNN Conversion","summary":"  Spiking Neural Networks (SNNs) offer a promising avenue for energy-efficient\ncomputing compared with Artificial Neural Networks (ANNs), closely mirroring\nbiological neural processes. However, this potential comes with inherent\nchallenges in directly training SNNs through spatio-temporal backpropagation --\nstemming from the temporal dynamics of spiking neurons and their discrete\nsignal processing -- which necessitates alternative ways of training, most\nnotably through ANN-SNN conversion. In this work, we introduce a lightweight\nForward Temporal Bias Correction (FTBC) technique, aimed at enhancing\nconversion accuracy without the computational overhead. We ground our method on\nprovided theoretical findings that through proper temporal bias calibration the\nexpected error of ANN-SNN conversion can be reduced to be zero after each time\nstep. We further propose a heuristic algorithm for finding the temporal bias\nonly in the forward pass, thus eliminating the computational burden of\nbackpropagation and we evaluate our method on CIFAR-10/100 and ImageNet\ndatasets, achieving a notable increase in accuracy on all datasets. Codes are\nreleased at a GitHub repository.\n","authors":["Xiaofeng Wu","Velibor Bojkovic","Bin Gu","Kun Suo","Kai Zou"],"pdf_url":"https://arxiv.org/pdf/2403.18388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06733v3","updated":"2024-03-27T09:24:56Z","published":"2023-12-11T10:43:28Z","title":"TULIP: Transformer for Upsampling of LiDAR Point Cloud","summary":"  LiDAR Upsampling is a challenging task for the perception systems of robots\nand autonomous vehicles, due to the sparse and irregular structure of\nlarge-scale scene contexts. Recent works propose to solve this problem by\nconverting LiDAR data from 3D Euclidean space into an image super-resolution\nproblem in 2D image space. Although their methods can generate high-resolution\nrange images with fine-grained details, the resulting 3D point clouds often\nblur out details and predict invalid points. In this paper, we propose TULIP, a\nnew method to reconstruct high-resolution LiDAR point clouds from\nlow-resolution LiDAR input. We also follow a range image-based approach but\nspecifically modify the patch and window geometries of a Swin-Transformer-based\nnetwork to better fit the characteristics of range images. We conducted several\nexperiments on three public real-world and simulated datasets. TULIP\noutperforms state-of-the-art methods in all relevant metrics and generates\nrobust and more realistic point clouds than prior works.\n","authors":["Bin Yang","Patrick Pfreundschuh","Roland Siegwart","Marco Hutter","Peyman Moghadam","Vaishakh Patil"],"pdf_url":"https://arxiv.org/pdf/2312.06733v3.pdf","comment":"The paper was accepted by CVPR20224"},{"id":"http://arxiv.org/abs/2403.05218v2","updated":"2024-03-27T09:21:42Z","published":"2024-03-08T11:09:46Z","title":"3D Face Reconstruction Using A Spectral-Based Graph Convolution Encoder","summary":"  Monocular 3D face reconstruction plays a crucial role in avatar generation,\nwith significant demand in web-related applications such as generating virtual\nfinancial advisors in FinTech. Current reconstruction methods predominantly\nrely on deep learning techniques and employ 2D self-supervision as a means to\nguide model learning. However, these methods encounter challenges in capturing\nthe comprehensive 3D structural information of the face due to the utilization\nof 2D images for model training purposes. To overcome this limitation and\nenhance the reconstruction of 3D structural features, we propose an innovative\napproach that integrates existing 2D features with 3D features to guide the\nmodel learning process. Specifically, we introduce the 3D-ID Loss, which\nleverages the high-dimensional structure features extracted from a\nSpectral-Based Graph Convolution Encoder applied to the facial mesh. This\napproach surpasses the sole reliance on the 3D information provided by the\nfacial mesh vertices coordinates. Our model is trained using 2D-3D data pairs\nfrom a combination of datasets and achieves state-of-the-art performance on the\nNoW benchmark.\n","authors":["Haoxin Xu","Zezheng Zhao","Yuxin Cao","Chunyu Chen","Hao Ge","Ziyao Liu"],"pdf_url":"https://arxiv.org/pdf/2403.05218v2.pdf","comment":"4 pages, 3 figures. Accepted to WWW 2024"},{"id":"http://arxiv.org/abs/2403.18383v1","updated":"2024-03-27T09:21:07Z","published":"2024-03-27T09:21:07Z","title":"Generative Multi-modal Models are Good Class-Incremental Learners","summary":"  In class-incremental learning (CIL) scenarios, the phenomenon of catastrophic\nforgetting caused by the classifier's bias towards the current task has long\nposed a significant challenge. It is mainly caused by the characteristic of\ndiscriminative models. With the growing popularity of the generative\nmulti-modal models, we would explore replacing discriminative models with\ngenerative ones for CIL. However, transitioning from discriminative to\ngenerative models requires addressing two key challenges. The primary challenge\nlies in transferring the generated textual information into the classification\nof distinct categories. Additionally, it requires formulating the task of CIL\nwithin a generative framework. To this end, we propose a novel generative\nmulti-modal model (GMM) framework for class-incremental learning. Our approach\ndirectly generates labels for images using an adapted generative model. After\nobtaining the detailed text, we use a text encoder to extract text features and\nemploy feature matching to determine the most similar label as the\nclassification prediction. In the conventional CIL settings, we achieve\nsignificantly better results in long-sequence task scenarios. Under the\nFew-shot CIL setting, we have improved by at least 14\\% accuracy over all the\ncurrent state-of-the-art methods with significantly less forgetting. Our code\nis available at \\url{https://github.com/DoubleClass/GMM}.\n","authors":["Xusheng Cao","Haori Lu","Linlan Huang","Xialei Liu","Ming-Ming Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.18383v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2401.17879v2","updated":"2024-03-27T09:17:14Z","published":"2024-01-31T14:36:49Z","title":"AEROBLADE: Training-Free Detection of Latent Diffusion Images Using\n  Autoencoder Reconstruction Error","summary":"  With recent text-to-image models, anyone can generate deceptively realistic\nimages with arbitrary contents, fueling the growing threat of visual\ndisinformation. A key enabler for generating high-resolution images with low\ncomputational cost has been the development of latent diffusion models (LDMs).\nIn contrast to conventional diffusion models, LDMs perform the denoising\nprocess in the low-dimensional latent space of a pre-trained autoencoder (AE)\ninstead of the high-dimensional image space. Despite their relevance, the\nforensic analysis of LDMs is still in its infancy. In this work we propose\nAEROBLADE, a novel detection method which exploits an inherent component of\nLDMs: the AE used to transform images between image and latent space. We find\nthat generated images can be more accurately reconstructed by the AE than real\nimages, allowing for a simple detection approach based on the reconstruction\nerror. Most importantly, our method is easy to implement and does not require\nany training, yet nearly matches the performance of detectors that rely on\nextensive training. We empirically demonstrate that AEROBLADE is effective\nagainst state-of-the-art LDMs, including Stable Diffusion and Midjourney.\nBeyond detection, our approach allows for the qualitative analysis of images,\nwhich can be leveraged for identifying inpainted regions. We release our code\nand data at https://github.com/jonasricker/aeroblade .\n","authors":["Jonas Ricker","Denis Lukovnikov","Asja Fischer"],"pdf_url":"https://arxiv.org/pdf/2401.17879v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.00174v2","updated":"2024-03-27T09:13:19Z","published":"2024-02-29T22:58:13Z","title":"A citizen science toolkit to collect human perceptions of urban\n  environments using open street view images","summary":"  Street View-level Imagery (SVI) is a valuable data source for studies (e.g.,\nenvironmental assessments, green space identification or land cover\nclassification). While commercial SVI is available, such providers commonly\nrestrict copying or reuse in ways necessary for research. Open SVI datasets are\nreadily available from less restrictive sources, such as Mapillary, but due to\nthe heterogeneity of the images, these require substantial preprocessing,\nfiltering, and careful quality checks. We present an efficient method for\nautomated downloading, processing, cropping, and filtering open SVI, to be used\nin a survey of human perceptions of the streets portrayed in these images. We\ndemonstrate our open-source reusable SVI preparation and smartphone-friendly\nperception-survey software with Amsterdam (Netherlands) as the case study.\nUsing a citizen science approach, we collected from 331 people 22,637 ratings\nabout their perceptions for various criteria. We have published our software in\na public repository for future re-use and reproducibility.\n","authors":["Matthew Danish","SM Labib","Britta Ricker","Marco Helbich"],"pdf_url":"https://arxiv.org/pdf/2403.00174v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18373v1","updated":"2024-03-27T09:10:01Z","published":"2024-03-27T09:10:01Z","title":"BAM: Box Abstraction Monitors for Real-time OoD Detection in Object\n  Detection","summary":"  Out-of-distribution (OoD) detection techniques for deep neural networks\n(DNNs) become crucial thanks to their filtering of abnormal inputs, especially\nwhen DNNs are used in safety-critical applications and interact with an open\nand dynamic environment. Nevertheless, integrating OoD detection into\nstate-of-the-art (SOTA) object detection DNNs poses significant challenges,\npartly due to the complexity introduced by the SOTA OoD construction methods,\nwhich require the modification of DNN architecture and the introduction of\ncomplex loss functions. This paper proposes a simple, yet surprisingly\neffective, method that requires neither retraining nor architectural change in\nobject detection DNN, called Box Abstraction-based Monitors (BAM). The novelty\nof BAM stems from using a finite union of convex box abstractions to capture\nthe learned features of objects for in-distribution (ID) data, and an important\nobservation that features from OoD data are more likely to fall outside of\nthese boxes. The union of convex regions within the feature space allows the\nformation of non-convex and interpretable decision boundaries, overcoming the\nlimitations of VOS-like detectors without sacrificing real-time performance.\nExperiments integrating BAM into Faster R-CNN-based object detection DNNs\ndemonstrate a considerably improved performance against SOTA OoD detection\ntechniques.\n","authors":["Changshun Wu","Weicheng He","Chih-Hong Cheng","Xiaowei Huang","Saddek Bensalem"],"pdf_url":"https://arxiv.org/pdf/2403.18373v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17905v2","updated":"2024-03-27T09:07:02Z","published":"2024-03-26T17:45:06Z","title":"Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2","summary":"  We propose a new approach for non-Cartesian magnetic resonance image\nreconstruction. While unrolled architectures provide robustness via\ndata-consistency layers, embedding measurement operators in Deep Neural Network\n(DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP)\napproaches, where the denoising DNNs are blind to the measurement setting, are\nnot affected by this limitation and have also proven effective, but their\nhighly iterative nature also affects scalability. To address this scalability\nchallenge, we leverage the \"Residual-to-Residual DNN series for high-Dynamic\nrange imaging (R2D2)\" approach recently introduced in astronomical imaging.\nR2D2's reconstruction is formed as a series of residual images, iteratively\nestimated as outputs of DNNs taking the previous iteration's image estimate and\nassociated data residual as inputs. The method can be interpreted as a learned\nversion of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation,\nconsidering radial k-space sampling acquisition sequences. Our preliminary\nresults suggest that R2D2 achieves: (i) suboptimal performance compared to its\nunrolled incarnation R2D2-Net, which is however non-scalable due to the\nnecessary embedding of NUFFT-based data-consistency layers; (ii) superior\nreconstruction quality to a scalable version of R2D2-Net embedding an FFT-based\napproximation for data consistency; (iii) superior reconstruction quality to\nPnP, while only requiring few iterations.\n","authors":["Yiwei Chen","Chao Tang","Amir Aghabiglou","Chung San Chu","Yves Wiaux"],"pdf_url":"https://arxiv.org/pdf/2403.17905v2.pdf","comment":"submitted to IEEE EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2403.18370v1","updated":"2024-03-27T09:06:36Z","published":"2024-03-27T09:06:36Z","title":"Ship in Sight: Diffusion Models for Ship-Image Super Resolution","summary":"  In recent years, remarkable advancements have been achieved in the field of\nimage generation, primarily driven by the escalating demand for high-quality\noutcomes across various image generation subtasks, such as inpainting,\ndenoising, and super resolution. A major effort is devoted to exploring the\napplication of super-resolution techniques to enhance the quality of\nlow-resolution images. In this context, our method explores in depth the\nproblem of ship image super resolution, which is crucial for coastal and port\nsurveillance. We investigate the opportunity given by the growing interest in\ntext-to-image diffusion models, taking advantage of the prior knowledge that\nsuch foundation models have already learned. In particular, we present a\ndiffusion-model-based architecture that leverages text conditioning during\ntraining while being class-aware, to best preserve the crucial details of the\nships during the generation of the super-resoluted image. Since the specificity\nof this task and the scarcity availability of off-the-shelf data, we also\nintroduce a large labeled ship dataset scraped from online ship images, mostly\nfrom ShipSpotting\\footnote{\\url{www.shipspotting.com}} website. Our method\nachieves more robust results than other deep learning models previously\nemployed for super resolution, as proven by the multiple experiments performed.\nMoreover, we investigate how this model can benefit downstream tasks, such as\nclassification and object detection, thus emphasizing practical implementation\nin a real-world scenario. Experimental results show flexibility, reliability,\nand impressive performance of the proposed framework over state-of-the-art\nmethods for different tasks. The code is available at:\nhttps://github.com/LuigiSigillo/ShipinSight .\n","authors":["Luigi Sigillo","Riccardo Fosco Gramaccioni","Alessandro Nicolosi","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2403.18370v1.pdf","comment":"Accepted at 2024 International Joint Conference on Neural Networks\n  (IJCNN)"},{"id":"http://arxiv.org/abs/2312.10114v2","updated":"2024-03-27T09:00:54Z","published":"2023-12-15T09:49:21Z","title":"FoMo-Bench: a multi-modal, multi-scale and multi-task Forest Monitoring\n  Benchmark for remote sensing foundation models","summary":"  Forests are an essential part of Earth's ecosystems and natural systems, as\nwell as providing services on which humanity depends, yet they are rapidly\nchanging as a result of land use decisions and climate change. Understanding\nand mitigating negative effects requires parsing data on forests at global\nscale from a broad array of sensory modalities, and recently many such problems\nhave been approached using machine learning algorithms for remote sensing. To\ndate, forest-monitoring problems have largely been addressed in isolation.\nInspired by the rise of foundation models for computer vision and remote\nsensing, we here present the first unified Forest Monitoring Benchmark\n(FoMo-Bench). FoMo-Bench consists of 15 diverse datasets encompassing\nsatellite, aerial, and inventory data, covering a variety of geographical\nregions, and including multispectral, red-green-blue, synthetic aperture radar\n(SAR) and LiDAR data with various temporal, spatial and spectral resolutions.\nFoMo-Bench includes multiple types of forest-monitoring tasks, spanning\nclassification, segmentation, and object detection. To further enhance the\ndiversity of tasks and geographies represented in FoMo-Bench, we introduce a\nnovel global dataset, TalloS, combining satellite imagery with ground-based\nannotations for tree species classification, encompassing 1,000+ categories\nacross multiple hierarchical taxonomic levels (species, genus, family).\nFinally, we propose FoMo-Net, a baseline foundation model with the capacity to\nprocess any combination of commonly used spectral bands in remote sensing,\nacross diverse ground sampling distances and geographical locations worldwide.\nThis work aims to inspire research collaborations between machine learning and\nforest biology researchers in exploring scalable multi-modal and multi-task\nmodels for forest monitoring. All code and data will be made publicly\navailable.\n","authors":["Nikolaos Ioannis Bountos","Arthur Ouaknine","David Rolnick"],"pdf_url":"https://arxiv.org/pdf/2312.10114v2.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2402.19473v2","updated":"2024-03-27T09:00:25Z","published":"2024-02-29T18:59:01Z","title":"Retrieval-Augmented Generation for AI-Generated Content: A Survey","summary":"  The development of Artificial Intelligence Generated Content (AIGC) has been\nfacilitated by advancements in model algorithms, the increasing scale of\nfoundation models, and the availability of ample high-quality datasets. While\nAIGC has achieved remarkable performance, it still faces several challenges,\nsuch as the difficulty of maintaining up-to-date and long-tail knowledge, the\nrisk of data leakage, and the high costs associated with training and\ninference. Retrieval-Augmented Generation(RAG) has recently emerged as a\nparadigm to address such challenges. In particular, RAG introduces the\ninformation retrieval process, which enhances the generation process by\nretrieving relevant objects from available data stores, leading to higher\naccuracy and better robustness. In this paper, we comprehensively review\nexisting efforts that integrate RAG technique into AIGC scenarios. We first\nclassify RAG foundations according to how the retriever augments the generator,\ndistilling the fundamental abstractions of the augmentation methodologies for\nvarious retrievers and generators. This unified perspective encompasses all RAG\nscenarios, illuminating advancements and pivotal technologies that help with\npotential future progress. We also summarize additional enhancements methods\nfor RAG, facilitating effective engineering and implementation of RAG systems.\nThen from another view, we survey on practical applications of RAG across\ndifferent modalities and tasks, offering valuable references for researchers\nand practitioners. Furthermore, we introduce the benchmarks for RAG, discuss\nthe limitations of current RAG systems, and suggest potential directions for\nfuture research.Project Repo: https://github.com/hymie122/RAG-Survey.\n","authors":["Penghao Zhao","Hailin Zhang","Qinhan Yu","Zhengren Wang","Yunteng Geng","Fangcheng Fu","Ling Yang","Wentao Zhang","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2402.19473v2.pdf","comment":"Citing 380 papers, 36 pages, 16 figures. Project:\n  https://github.com/hymie122/RAG-Survey"},{"id":"http://arxiv.org/abs/2310.03325v2","updated":"2024-03-27T08:54:35Z","published":"2023-10-05T05:41:21Z","title":"Learning Concept-Based Causal Transition and Symbolic Reasoning for\n  Visual Planning","summary":"  Visual planning simulates how humans make decisions to achieve desired goals\nin the form of searching for visual causal transitions between an initial\nvisual state and a final visual goal state. It has become increasingly\nimportant in egocentric vision with its advantages in guiding agents to perform\ndaily tasks in complex environments. In this paper, we propose an interpretable\nand generalizable visual planning framework consisting of i) a novel\nSubstitution-based Concept Learner (SCL) that abstracts visual inputs into\ndisentangled concept representations, ii) symbol abstraction and reasoning that\nperforms task planning via the self-learned symbols, and iii) a Visual Causal\nTransition model (ViCT) that grounds visual causal transitions to semantically\nsimilar real-world actions. Given an initial state, we perform goal-conditioned\nvisual planning with a symbolic reasoning method fueled by the learned\nrepresentations and causal transitions to reach the goal state. To verify the\neffectiveness of the proposed model, we collect a large-scale visual planning\ndataset based on AI2-THOR, dubbed as CCTP. Extensive experiments on this\nchallenging dataset demonstrate the superior performance of our method in\nvisual task planning. Empirically, we show that our framework can generalize to\nunseen task trajectories, unseen object categories, and real-world data.\nFurther details of this work are provided at\nhttps://fqyqc.github.io/ConTranPlan/.\n","authors":["Yilue Qian","Peiyu Yu","Ying Nian Wu","Yao Su","Wei Wang","Lifeng Fan"],"pdf_url":"https://arxiv.org/pdf/2310.03325v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15837v2","updated":"2024-03-27T08:54:06Z","published":"2024-03-23T13:24:31Z","title":"Centered Masking for Language-Image Pre-Training","summary":"  We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel,\nstraightforward, and effective technique for masking image patches during\npre-training of a vision-language model. GLIP builds on Fast Language-Image\nPre-Training (FLIP), which randomly masks image patches while training a CLIP\nmodel. GLIP replaces random masking with centered masking, that uses a Gaussian\ndistribution and is inspired by the importance of image patches at the center\nof the image. GLIP retains the same computational savings as FLIP, while\nimproving performance across a range of downstream datasets and tasks, as\ndemonstrated by our experimental results. We show the benefits of GLIP to be\neasy to obtain, requiring no delicate tuning of the Gaussian, and also\napplicable to data sets containing images without an obvious center focus.\n","authors":["Mingliang Liang","Martha Larson"],"pdf_url":"https://arxiv.org/pdf/2403.15837v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18361v1","updated":"2024-03-27T08:53:13Z","published":"2024-03-27T08:53:13Z","title":"ViTAR: Vision Transformer with Any Resolution","summary":"  his paper tackles a significant challenge faced by Vision Transformers\n(ViTs): their constrained scalability across different image resolutions.\nTypically, ViTs experience a performance decline when processing resolutions\ndifferent from those seen during training. Our work introduces two key\ninnovations to address this issue. Firstly, we propose a novel module for\ndynamic resolution adjustment, designed with a single Transformer block,\nspecifically to achieve highly efficient incremental token integration.\nSecondly, we introduce fuzzy positional encoding in the Vision Transformer to\nprovide consistent positional awareness across multiple resolutions, thereby\npreventing overfitting to any single training resolution. Our resulting model,\nViTAR (Vision Transformer with Any Resolution), demonstrates impressive\nadaptability, achieving 83.3\\% top-1 accuracy at a 1120x1120 resolution and\n80.4\\% accuracy at a 4032x4032 resolution, all while reducing computational\ncosts. ViTAR also shows strong performance in downstream tasks such as instance\nand semantic segmentation and can easily combined with self-supervised learning\ntechniques like Masked AutoEncoder. Our work provides a cost-effective solution\nfor enhancing the resolution scalability of ViTs, paving the way for more\nversatile and efficient high-resolution image processing.\n","authors":["Qihang Fan","Quanzeng You","Xiaotian Han","Yongfei Liu","Yunzhe Tao","Huaibo Huang","Ran He","Hongxia Yang"],"pdf_url":"https://arxiv.org/pdf/2403.18361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18360v1","updated":"2024-03-27T08:52:44Z","published":"2024-03-27T08:52:44Z","title":"Learning CNN on ViT: A Hybrid Model to Explicitly Class-specific\n  Boundaries for Domain Adaptation","summary":"  Most domain adaptation (DA) methods are based on either a convolutional\nneural networks (CNNs) or a vision transformers (ViTs). They align the\ndistribution differences between domains as encoders without considering their\nunique characteristics. For instance, ViT excels in accuracy due to its\nsuperior ability to capture global representations, while CNN has an advantage\nin capturing local representations. This fact has led us to design a hybrid\nmethod to fully take advantage of both ViT and CNN, called Explicitly\nClass-specific Boundaries (ECB). ECB learns CNN on ViT to combine their\ndistinct strengths. In particular, we leverage ViT's properties to explicitly\nfind class-specific decision boundaries by maximizing the discrepancy between\nthe outputs of the two classifiers to detect target samples far from the source\nsupport. In contrast, the CNN encoder clusters target features based on the\npreviously defined class-specific boundaries by minimizing the discrepancy\nbetween the probabilities of the two classifiers. Finally, ViT and CNN mutually\nexchange knowledge to improve the quality of pseudo labels and reduce the\nknowledge discrepancies of these models. Compared to conventional DA methods,\nour ECB achieves superior performance, which verifies its effectiveness in this\nhybrid model. The project website can be found\nhttps://dotrannhattuong.github.io/ECB/website/.\n","authors":["Ba Hung Ngo","Nhat-Tuong Do-Tran","Tuan-Ngoc Nguyen","Hae-Gon Jeon","Tae Jong Choi"],"pdf_url":"https://arxiv.org/pdf/2403.18360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18356v1","updated":"2024-03-27T08:48:47Z","published":"2024-03-27T08:48:47Z","title":"MonoHair: High-Fidelity Hair Modeling from a Monocular Video","summary":"  Undoubtedly, high-fidelity 3D hair is crucial for achieving realism, artistic\nexpression, and immersion in computer graphics. While existing 3D hair modeling\nmethods have achieved impressive performance, the challenge of achieving\nhigh-quality hair reconstruction persists: they either require strict capture\nconditions, making practical applications difficult, or heavily rely on learned\nprior data, obscuring fine-grained details in images. To address these\nchallenges, we propose MonoHair,a generic framework to achieve high-fidelity\nhair reconstruction from a monocular video, without specific requirements for\nenvironments. Our approach bifurcates the hair modeling process into two main\nstages: precise exterior reconstruction and interior structure inference. The\nexterior is meticulously crafted using our Patch-based Multi-View Optimization\n(PMVO). This method strategically collects and integrates hair information from\nmultiple views, independent of prior data, to produce a high-fidelity exterior\n3D line map. This map not only captures intricate details but also facilitates\nthe inference of the hair's inner structure. For the interior, we employ a\ndata-driven, multi-view 3D hair reconstruction method. This method utilizes 2D\nstructural renderings derived from the reconstructed exterior, mirroring the\nsynthetic 2D inputs used during training. This alignment effectively bridges\nthe domain gap between our training data and real-world data, thereby enhancing\nthe accuracy and reliability of our interior structure inference. Lastly, we\ngenerate a strand model and resolve the directional ambiguity by our hair\ngrowth algorithm. Our experiments demonstrate that our method exhibits\nrobustness across diverse hairstyles and achieves state-of-the-art performance.\nFor more results, please refer to our project page\nhttps://keyuwu-cs.github.io/MonoHair/.\n","authors":["Keyu Wu","Lingchen Yang","Zhiyi Kuang","Yao Feng","Xutao Han","Yuefan Shen","Hongbo Fu","Kun Zhou","Youyi Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.18356v1.pdf","comment":"Accepted by IEEE CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18351v1","updated":"2024-03-27T08:42:47Z","published":"2024-03-27T08:42:47Z","title":"Generating Diverse Agricultural Data for Vision-Based Farming\n  Applications","summary":"  We present a specialized procedural model for generating synthetic\nagricultural scenes, focusing on soybean crops, along with various weeds. This\nmodel is capable of simulating distinct growth stages of these plants, diverse\nsoil conditions, and randomized field arrangements under varying lighting\nconditions. The integration of real-world textures and environmental factors\ninto the procedural generation process enhances the photorealism and\napplicability of the synthetic data. Our dataset includes 12,000 images with\nsemantic labels, offering a comprehensive resource for computer vision tasks in\nprecision agriculture, such as semantic segmentation for autonomous weed\ncontrol. We validate our model's effectiveness by comparing the synthetic data\nagainst real agricultural images, demonstrating its potential to significantly\naugment training data for machine learning models in agriculture. This approach\nnot only provides a cost-effective solution for generating high-quality,\ndiverse data but also addresses specific needs in agricultural vision tasks\nthat are not fully covered by general-purpose models.\n","authors":["Mikolaj Cieslak","Umabharathi Govindarajan","Alejandro Garcia","Anuradha Chandrashekar","Torsten Hädrich","Aleksander Mendoza-Drosik","Dominik L. Michels","Sören Pirk","Chia-Chun Fu","Wojciech Pałubicki"],"pdf_url":"https://arxiv.org/pdf/2403.18351v1.pdf","comment":"10 pages, 8 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.18347v1","updated":"2024-03-27T08:38:56Z","published":"2024-03-27T08:38:56Z","title":"A Quantum Fuzzy-based Approach for Real-Time Detection of Solar Coronal\n  Holes","summary":"  The detection and analysis of the solar coronal holes (CHs) is an important\nfield of study in the domain of solar physics. Mainly, it is required for the\nproper prediction of the geomagnetic storms which directly or indirectly affect\nvarious space and ground-based systems. For the detection of CHs till date, the\nsolar scientist depends on manual hand-drawn approaches. However, with the\nadvancement of image processing technologies, some automated image segmentation\nmethods have been used for the detection of CHs. In-spite of this, fast and\naccurate detection of CHs are till a major issues. Here in this work, a novel\nquantum computing-based fast fuzzy c-mean technique has been developed for fast\ndetection of the CHs region. The task has been carried out in two stages, in\nfirst stage the solar image has been segmented using a quantum computing based\nfast fuzzy c-mean (QCFFCM) and in the later stage the CHs has been extracted\nout from the segmented image based on image morphological operation. In the\nwork, quantum computing has been used to optimize the cost function of the fast\nfuzzy c-mean (FFCM) algorithm, where quantum approximate optimization algorithm\n(QAOA) has been used to optimize the quadratic part of the cost function. The\nproposed method has been tested for 193 \\AA{} SDO/AIA full-disk solar image\ndatasets and has been compared with the existing techniques. The outcome shows\nthe comparable performance of the proposed method with the existing one within\na very lesser time.\n","authors":["Sanmoy Bandyopadhyay","Suman Kundu"],"pdf_url":"https://arxiv.org/pdf/2403.18347v1.pdf","comment":"14 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.18346v1","updated":"2024-03-27T08:38:49Z","published":"2024-03-27T08:38:49Z","title":"Quantifying and Mitigating Unimodal Biases in Multimodal Large Language\n  Models: A Causal Perspective","summary":"  Recent advancements in Large Language Models (LLMs) have facilitated the\ndevelopment of Multimodal LLMs (MLLMs). Despite their impressive capabilities,\nMLLMs often suffer from an over-reliance on unimodal biases (e.g., language\nbias and vision bias), leading to incorrect answers in complex multimodal\ntasks. To investigate this issue, we propose a causal framework to interpret\nthe biases in Visual Question Answering (VQA) problems. Within our framework,\nwe devise a causal graph to elucidate the predictions of MLLMs on VQA problems,\nand assess the causal effect of biases through an in-depth causal analysis.\nMotivated by the causal graph, we introduce a novel MORE dataset, consisting of\n12,000 VQA instances. This dataset is designed to challenge MLLMs' abilities,\nnecessitating multi-hop reasoning and the surmounting of unimodal biases.\nFurthermore, we propose two strategies to mitigate unimodal biases and enhance\nMLLMs' reasoning capabilities, including a Decompose-Verify-Answer (DeVA)\nframework for limited-access MLLMs and the refinement of open-source MLLMs\nthrough fine-tuning. Extensive quantitative and qualitative experiments offer\nvaluable insights for future research.\n","authors":["Meiqi Chen","Yixin Cao","Yan Zhang","Chaochao Lu"],"pdf_url":"https://arxiv.org/pdf/2403.18346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18342v1","updated":"2024-03-27T08:32:48Z","published":"2024-03-27T08:32:48Z","title":"Learning Inclusion Matching for Animation Paint Bucket Colorization","summary":"  Colorizing line art is a pivotal task in the production of hand-drawn cel\nanimation. This typically involves digital painters using a paint bucket tool\nto manually color each segment enclosed by lines, based on RGB values\npredetermined by a color designer. This frame-by-frame process is both arduous\nand time-intensive. Current automated methods mainly focus on segment matching.\nThis technique migrates colors from a reference to the target frame by aligning\nfeatures within line-enclosed segments across frames. However, issues like\nocclusion and wrinkles in animations often disrupt these direct\ncorrespondences, leading to mismatches. In this work, we introduce a new\nlearning-based inclusion matching pipeline, which directs the network to\ncomprehend the inclusion relationships between segments rather than relying\nsolely on direct visual correspondences. Our method features a two-stage\npipeline that integrates a coarse color warping module with an inclusion\nmatching module, enabling more nuanced and accurate colorization. To facilitate\nthe training of our network, we also develope a unique dataset, referred to as\nPaintBucket-Character. This dataset includes rendered line arts alongside their\ncolorized counterparts, featuring various 3D characters. Extensive experiments\ndemonstrate the effectiveness and superiority of our method over existing\ntechniques.\n","authors":["Yuekun Dai","Shangchen Zhou","Qinyue Li","Chongyi Li","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2403.18342v1.pdf","comment":"accepted to CVPR 2024. Project Page:\n  https://ykdai.github.io/projects/InclusionMatching"},{"id":"http://arxiv.org/abs/2403.18339v1","updated":"2024-03-27T08:28:14Z","published":"2024-03-27T08:28:14Z","title":"H2ASeg: Hierarchical Adaptive Interaction and Weighting Network for\n  Tumor Segmentation in PET/CT Images","summary":"  Positron emission tomography (PET) combined with computed tomography (CT)\nimaging is routinely used in cancer diagnosis and prognosis by providing\ncomplementary information. Automatically segmenting tumors in PET/CT images can\nsignificantly improve examination efficiency. Traditional multi-modal\nsegmentation solutions mainly rely on concatenation operations for modality\nfusion, which fail to effectively model the non-linear dependencies between PET\nand CT modalities. Recent studies have investigated various approaches to\noptimize the fusion of modality-specific features for enhancing joint\nrepresentations. However, modality-specific encoders used in these methods\noperate independently, inadequately leveraging the synergistic relationships\ninherent in PET and CT modalities, for example, the complementarity between\nsemantics and structure. To address these issues, we propose a Hierarchical\nAdaptive Interaction and Weighting Network termed H2ASeg to explore the\nintrinsic cross-modal correlations and transfer potential complementary\ninformation. Specifically, we design a Modality-Cooperative Spatial Attention\n(MCSA) module that performs intra- and inter-modal interactions globally and\nlocally. Additionally, a Target-Aware Modality Weighting (TAMW) module is\ndeveloped to highlight tumor-related features within multi-modal features,\nthereby refining tumor segmentation. By embedding these modules across\ndifferent layers, H2ASeg can hierarchically model cross-modal correlations,\nenabling a nuanced understanding of both semantic and structural tumor\nfeatures. Extensive experiments demonstrate the superiority of H2ASeg,\noutperforming state-of-the-art methods on AutoPet-II and Hecktor2022\nbenchmarks. The code is released at https://github.com/G14nTDo4/H2ASeg.\n","authors":["Jinpeng Lu","Jingyun Chen","Linghan Cai","Songhan Jiang","Yongbing Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.18339v1.pdf","comment":"10 pages,4 figures"},{"id":"http://arxiv.org/abs/2403.17301v2","updated":"2024-03-27T08:23:09Z","published":"2024-03-26T01:06:47Z","title":"Physical 3D Adversarial Attacks against Monocular Depth Estimation in\n  Autonomous Driving","summary":"  Deep learning-based monocular depth estimation (MDE), extensively applied in\nautonomous driving, is known to be vulnerable to adversarial attacks. Previous\nphysical attacks against MDE models rely on 2D adversarial patches, so they\nonly affect a small, localized region in the MDE map but fail under various\nviewpoints. To address these limitations, we propose 3D Depth Fool\n(3D$^2$Fool), the first 3D texture-based adversarial attack against MDE models.\n3D$^2$Fool is specifically optimized to generate 3D adversarial textures\nagnostic to model types of vehicles and to have improved robustness in bad\nweather conditions, such as rain and fog. Experimental results validate the\nsuperior performance of our 3D$^2$Fool across various scenarios, including\nvehicles, MDE models, weather conditions, and viewpoints. Real-world\nexperiments with printed 3D textures on physical vehicle models further\ndemonstrate that our 3D$^2$Fool can cause an MDE error of over 10 meters.\n","authors":["Junhao Zheng","Chenhao Lin","Jiahao Sun","Zhengyu Zhao","Qian Li","Chao Shen"],"pdf_url":"https://arxiv.org/pdf/2403.17301v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2306.02928v2","updated":"2024-03-27T08:21:17Z","published":"2023-06-05T14:45:38Z","title":"Weakly-Supervised Conditional Embedding for Referred Visual Search","summary":"  This paper introduces a new challenge for image similarity search in the\ncontext of fashion, addressing the inherent ambiguity in this domain stemming\nfrom complex images. We present Referred Visual Search (RVS), a task allowing\nusers to define more precisely the desired similarity, following recent\ninterest in the industry. We release a new large public dataset,\nLAION-RVS-Fashion, consisting of 272k fashion products with 842k images\nextracted from LAION, designed explicitly for this task. However, unlike\ntraditional visual search methods in the industry, we demonstrate that superior\nperformance can be achieved by bypassing explicit object detection and adopting\nweakly-supervised conditional contrastive learning on image tuples. Our method\nis lightweight and demonstrates robustness, reaching Recall at one superior to\nstrong detection-based baselines against 2M distractors. Code, data and models\nare available at https://www.github.com/Simon-Lepage/CondViT-LRVSF .\n","authors":["Simon Lepage","Jérémie Mary","David Picard"],"pdf_url":"https://arxiv.org/pdf/2306.02928v2.pdf","comment":"28 pages, 13 figures, 5 tables"},{"id":"http://arxiv.org/abs/2403.18334v1","updated":"2024-03-27T08:16:33Z","published":"2024-03-27T08:16:33Z","title":"DODA: Diffusion for Object-detection Domain Adaptation in Agriculture","summary":"  The diverse and high-quality content generated by recent generative models\ndemonstrates the great potential of using synthetic data to train downstream\nmodels. However, in vision, especially in objection detection, related areas\nare not fully explored, the synthetic images are merely used to balance the\nlong tails of existing datasets, and the accuracy of the generated labels is\nlow, the full potential of generative models has not been exploited. In this\npaper, we propose DODA, a data synthesizer that can generate high-quality\nobject detection data for new domains in agriculture. Specifically, we improve\nthe controllability of layout-to-image through encoding layout as an image,\nthereby improving the quality of labels, and use a visual encoder to provide\nvisual clues for the diffusion model to decouple visual features from the\ndiffusion model, and empowering the model the ability to generate data in new\ndomains. On the Global Wheat Head Detection (GWHD) Dataset, which is the\nlargest dataset in agriculture and contains diverse domains, using the data\nsynthesized by DODA improves the performance of the object detector by\n12.74-17.76 AP$_{50}$ in the domain that was significantly shifted from the\ntraining data.\n","authors":["Shuai Xiang","Pieter M. Blok","James Burridge","Haozhou Wang","Wei Guo"],"pdf_url":"https://arxiv.org/pdf/2403.18334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18330v1","updated":"2024-03-27T08:11:25Z","published":"2024-03-27T08:11:25Z","title":"Tracking-Assisted Object Detection with Event Cameras","summary":"  Event-based object detection has recently garnered attention in the computer\nvision community due to the exceptional properties of event cameras, such as\nhigh dynamic range and no motion blur. However, feature asynchronism and\nsparsity cause invisible objects due to no relative motion to the camera,\nposing a significant challenge in the task. Prior works have studied various\nmemory mechanisms to preserve as many features as possible at the current time,\nguided by temporal clues. While these implicit-learned memories retain some\nshort-term information, they still struggle to preserve long-term features\neffectively. In this paper, we consider those invisible objects as\npseudo-occluded objects and aim to reveal their features. Firstly, we introduce\nvisibility attribute of objects and contribute an auto-labeling algorithm to\nappend additional visibility labels on an existing event camera dataset.\nSecondly, we exploit tracking strategies for pseudo-occluded objects to\nmaintain their permanence and retain their bounding boxes, even when features\nhave not been available for a very long time. These strategies can be treated\nas an explicit-learned memory guided by the tracking objective to record the\ndisplacements of objects across frames. Lastly, we propose a spatio-temporal\nfeature aggregation module to enrich the latent features and a consistency loss\nto increase the robustness of the overall pipeline. We conduct comprehensive\nexperiments to verify our method's effectiveness where still objects are\nretained but real occluded objects are discarded. The results demonstrate that\n(1) the additional visibility labels can assist in supervised training, and (2)\nour method outperforms state-of-the-art approaches with a significant\nimprovement of 7.9% absolute mAP.\n","authors":["Ting-Kang Yen","Igor Morawski","Shusil Dangi","Kai He","Chung-Yi Lin","Jia-Fong Yeh","Hung-Ting Su","Winston Hsu"],"pdf_url":"https://arxiv.org/pdf/2403.18330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18328v1","updated":"2024-03-27T08:09:04Z","published":"2024-03-27T08:09:04Z","title":"PIPNet3D: Interpretable Detection of Alzheimer in MRI Scans","summary":"  Information from neuroimaging examinations (CT, MRI) is increasingly used to\nsupport diagnoses of dementia, e.g., Alzheimer's disease. While current\nclinical practice is mainly based on visual inspection and feature engineering,\nDeep Learning approaches can be used to automate the analysis and to discover\nnew image-biomarkers. Part-prototype neural networks (PP-NN) are an alternative\nto standard blackbox models, and have shown promising results in general\ncomputer vision. PP-NN's base their reasoning on prototypical image regions\nthat are learned fully unsupervised, and combined with a simple-to-understand\ndecision layer. We present PIPNet3D, a PP-NN for volumetric images. We apply\nPIPNet3D to the clinical case study of Alzheimer's Disease diagnosis from\nstructural Magnetic Resonance Imaging (sMRI). We assess the quality of\nprototypes under a systematic evaluation framework, propose new metrics to\nevaluate brain prototypes and perform an evaluation with domain experts. Our\nresults show that PIPNet3D is an interpretable, compact model for Alzheimer's\ndiagnosis with its reasoning well aligned to medical domain knowledge. Notably,\nPIPNet3D achieves the same accuracy as its blackbox counterpart; and removing\nthe remaining clinically irrelevant prototypes from its decision process does\nnot decrease predictive performance.\n","authors":["Lisa Anita De Santi","Jörg Schlötterer","Michael Scheschenja","Joel Wessendorf","Meike Nauta","Vincenzo Positano","Christin Seifert"],"pdf_url":"https://arxiv.org/pdf/2403.18328v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10030v2","updated":"2024-03-27T07:52:10Z","published":"2024-03-15T05:30:29Z","title":"Multi-criteria Token Fusion with One-step-ahead Attention for Efficient\n  Vision Transformers","summary":"  Vision Transformer (ViT) has emerged as a prominent backbone for computer\nvision. For more efficient ViTs, recent works lessen the quadratic cost of the\nself-attention layer by pruning or fusing the redundant tokens. However, these\nworks faced the speed-accuracy trade-off caused by the loss of information.\nHere, we argue that token fusion needs to consider diverse relations between\ntokens to minimize information loss. In this paper, we propose a Multi-criteria\nToken Fusion (MCTF), that gradually fuses the tokens based on multi-criteria\n(e.g., similarity, informativeness, and size of fused tokens). Further, we\nutilize the one-step-ahead attention, which is the improved approach to capture\nthe informativeness of the tokens. By training the model equipped with MCTF\nusing a token reduction consistency, we achieve the best speed-accuracy\ntrade-off in the image classification (ImageNet1K). Experimental results prove\nthat MCTF consistently surpasses the previous reduction methods with and\nwithout training. Specifically, DeiT-T and DeiT-S with MCTF reduce FLOPs by\nabout 44% while improving the performance (+0.5%, and +0.3%) over the base\nmodel, respectively. We also demonstrate the applicability of MCTF in various\nVision Transformers (e.g., T2T-ViT, LV-ViT), achieving at least 31% speedup\nwithout performance degradation. Code is available at\nhttps://github.com/mlvlab/MCTF.\n","authors":["Sanghyeok Lee","Joonmyung Choi","Hyunwoo J. Kim"],"pdf_url":"https://arxiv.org/pdf/2403.10030v2.pdf","comment":"Conference on Computer Vision and Pattern Recognition (CVPR), 2024"},{"id":"http://arxiv.org/abs/2403.18321v1","updated":"2024-03-27T07:50:45Z","published":"2024-03-27T07:50:45Z","title":"Implementation of the Principal Component Analysis onto High-Performance\n  Computer Facilities for Hyperspectral Dimensionality Reduction: Results and\n  Comparisons","summary":"  Dimensionality reduction represents a critical preprocessing step in order to\nincrease the efficiency and the performance of many hyperspectral imaging\nalgorithms. However, dimensionality reduction algorithms, such as the Principal\nComponent Analysis (PCA), suffer from their computationally demanding nature,\nbecoming advisable for their implementation onto high-performance computer\narchitectures for applications under strict latency constraints. This work\npresents the implementation of the PCA algorithm onto two different\nhigh-performance devices, namely, an NVIDIA Graphics Processing Unit (GPU) and\na Kalray manycore, uncovering a highly valuable set of tips and tricks in order\nto take full advantage of the inherent parallelism of these high-performance\ncomputing platforms, and hence, reducing the time that is required to process a\ngiven hyperspectral image. Moreover, the achieved results obtained with\ndifferent hyperspectral images have been compared with the ones that were\nobtained with a field programmable gate array (FPGA)-based implementation of\nthe PCA algorithm that has been recently published, providing, for the first\ntime in the literature, a comprehensive analysis in order to highlight the pros\nand cons of each option.\n","authors":["E. Martel","R. Lazcano","J. Lopez","D. Madroñal","R. Salvador","S. Lopez","E. Juarez","R. Guerra","C. Sanz","R. Sarmiento"],"pdf_url":"https://arxiv.org/pdf/2403.18321v1.pdf","comment":"30 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.18318v1","updated":"2024-03-27T07:40:51Z","published":"2024-03-27T07:40:51Z","title":"Uncertainty-Aware SAR ATR: Defending Against Adversarial Attacks via\n  Bayesian Neural Networks","summary":"  Adversarial attacks have demonstrated the vulnerability of Machine Learning\n(ML) image classifiers in Synthetic Aperture Radar (SAR) Automatic Target\nRecognition (ATR) systems. An adversarial attack can deceive the classifier\ninto making incorrect predictions by perturbing the input SAR images, for\nexample, with a few scatterers attached to the on-ground objects. Therefore, it\nis critical to develop robust SAR ATR systems that can detect potential\nadversarial attacks by leveraging the inherent uncertainty in ML classifiers,\nthereby effectively alerting human decision-makers. In this paper, we propose a\nnovel uncertainty-aware SAR ATR for detecting adversarial attacks.\nSpecifically, we leverage the capability of Bayesian Neural Networks (BNNs) in\nperforming image classification with quantified epistemic uncertainty to\nmeasure the confidence for each input SAR image. By evaluating the uncertainty,\nour method alerts when the input SAR image is likely to be adversarially\ngenerated. Simultaneously, we also generate visual explanations that reveal the\nspecific regions in the SAR image where the adversarial scatterers are likely\nto to be present, thus aiding human decision-making with hints of evidence of\nadversarial attacks. Experiments on the MSTAR dataset demonstrate that our\napproach can identify over 80% adversarial SAR images with fewer than 20% false\nalarms, and our visual explanations can identify up to over 90% of scatterers\nin an adversarial SAR image.\n","authors":["Tian Ye","Rajgopal Kannan","Viktor Prasanna","Carl Busart"],"pdf_url":"https://arxiv.org/pdf/2403.18318v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08251v2","updated":"2024-03-27T07:33:42Z","published":"2022-12-16T02:43:52Z","title":"Task-Adaptive Saliency Guidance for Exemplar-free Class Incremental\n  Learning","summary":"  Exemplar-free Class Incremental Learning (EFCIL) aims to sequentially learn\ntasks with access only to data from the current one. EFCIL is of interest\nbecause it mitigates concerns about privacy and long-term storage of data,\nwhile at the same time alleviating the problem of catastrophic forgetting in\nincremental learning. In this work, we introduce task-adaptive saliency for\nEFCIL and propose a new framework, which we call Task-Adaptive Saliency\nSupervision (TASS), for mitigating the negative effects of saliency drift\nbetween different tasks. We first apply boundary-guided saliency to maintain\ntask adaptivity and \\textit{plasticity} on model attention. Besides, we\nintroduce task-agnostic low-level signals as auxiliary supervision to increase\nthe \\textit{stability} of model attention. Finally, we introduce a module for\ninjecting and recovering saliency noise to increase the robustness of saliency\npreservation. Our experiments demonstrate that our method can better preserve\nsaliency maps across tasks and achieve state-of-the-art results on the\nCIFAR-100, Tiny-ImageNet, and ImageNet-Subset EFCIL benchmarks. Code is\navailable at \\url{https://github.com/scok30/tass}.\n","authors":["Xialei Liu","Jiang-Tian Zhai","Andrew D. Bagdanov","Ke Li","Ming-Ming Cheng"],"pdf_url":"https://arxiv.org/pdf/2212.08251v2.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2307.09136v2","updated":"2024-03-27T07:16:28Z","published":"2023-07-18T10:34:21Z","title":"The Effects of Mixed Sample Data Augmentation are Class Dependent","summary":"  Mixed Sample Data Augmentation (MSDA) techniques, such as Mixup, CutMix, and\nPuzzleMix, have been widely acknowledged for enhancing performance in a variety\nof tasks. A previous study reported the class dependency of traditional data\naugmentation (DA), where certain classes benefit disproportionately compared to\nothers. This paper reveals a class dependent effect of MSDA, where some classes\nexperience improved performance while others experience degraded performance.\nThis research addresses the issue of class dependency in MSDA and proposes an\nalgorithm to mitigate it. The approach involves training on a mixture of MSDA\nand non-MSDA data, which not only mitigates the negative impact on the affected\nclasses, but also improves overall accuracy. Furthermore, we provide in-depth\nanalysis and discussion of why MSDA introduced class dependencies and which\nclasses are most likely to have them.\n","authors":["Haeil Lee","Hansang Lee","Junmo Kim"],"pdf_url":"https://arxiv.org/pdf/2307.09136v2.pdf","comment":"21 pages, 18 figures, Overall Revision"},{"id":"http://arxiv.org/abs/2402.18920v5","updated":"2024-03-27T07:16:21Z","published":"2024-02-29T07:26:23Z","title":"Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation","summary":"  Although 3D shape matching and interpolation are highly interrelated, they\nare often studied separately and applied sequentially to relate different 3D\nshapes, thus resulting in sub-optimal performance. In this work we present a\nunified framework to predict both point-wise correspondences and shape\ninterpolation between 3D shapes. To this end, we combine the deep functional\nmap framework with classical surface deformation models to map shapes in both\nspectral and spatial domains. On the one hand, by incorporating spatial maps,\nour method obtains more accurate and smooth point-wise correspondences compared\nto previous functional map methods for shape matching. On the other hand, by\nintroducing spectral maps, our method gets rid of commonly used but\ncomputationally expensive geodesic distance constraints that are only valid for\nnear-isometric shape deformations. Furthermore, we propose a novel test-time\nadaptation scheme to capture both pose-dominant and shape-dominant\ndeformations. Using different challenging datasets, we demonstrate that our\nmethod outperforms previous state-of-the-art methods for both shape matching\nand interpolation, even compared to supervised approaches.\n","authors":["Dongliang Cao","Marvin Eisenberger","Nafie El Amrani","Daniel Cremers","Florian Bernard"],"pdf_url":"https://arxiv.org/pdf/2402.18920v5.pdf","comment":"accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2308.13356v3","updated":"2024-03-27T07:12:09Z","published":"2023-08-25T13:05:06Z","title":"CEIMVEN: An Approach of Cutting Edge Implementation of Modified Versions\n  of EfficientNet (V1-V2) Architecture for Breast Cancer Detection and\n  Classification from Ultrasound Images","summary":"  Undoubtedly breast cancer identifies itself as one of the most widespread and\nterrifying cancers across the globe. Millions of women are getting affected\neach year from it. Breast cancer remains the major one for being the reason of\nlargest number of demise of women. In the recent time of research, Medical\nImage Computing and Processing has been playing a significant role for\ndetecting and classifying breast cancers from ultrasound images and mammograms,\nalong with the celestial touch of deep neural networks. In this research, we\nfocused mostly on our rigorous implementations and iterative result analysis of\ndifferent cutting-edge modified versions of EfficientNet architectures namely\nEfficientNet-V1 (b0-b7) and EfficientNet-V2 (b0-b3) with ultrasound image,\nnamed as CEIMVEN. We utilized transfer learning approach here for using the\npre-trained models of EfficientNet versions. We activated the hyper-parameter\ntuning procedures, added fully connected layers, discarded the unprecedented\noutliers and recorded the accuracy results from our custom modified\nEfficientNet architectures. Our deep learning model training approach was\nrelated to both identifying the cancer affected areas with region of interest\n(ROI) techniques and multiple classifications (benign, malignant and normal).\nThe approximate testing accuracies we got from the modified versions of\nEfficientNet-V1 (b0- 99.15%, b1- 98.58%, b2- 98.43%, b3- 98.01%, b4- 98.86%,\nb5- 97.72%, b6- 97.72%, b7- 98.72%) and EfficientNet-V2 (b0- 99.29%, b1-\n99.01%, b2- 98.72%, b3- 99.43%) are showing very bright future and strong\npotentials of deep learning approach for the successful detection and\nclassification of breast cancers from the ultrasound images at a very early\nstage. The code for this research is available here:\nhttps://github.com/ac005sheekar/CEIMVEN-Breast.\n","authors":["Sheekar Banerjee","Md. Kamrul Hasan Monir"],"pdf_url":"https://arxiv.org/pdf/2308.13356v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18301v1","updated":"2024-03-27T06:55:23Z","published":"2024-03-27T06:55:23Z","title":"Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives","summary":"  The rise in internet usage has led to the generation of massive amounts of\ndata, resulting in the adoption of various supervised and semi-supervised\nmachine learning algorithms, which can effectively utilize the colossal amount\nof data to train models. However, before deploying these models in the real\nworld, these must be strictly evaluated on performance measures like worst-case\nrecall and satisfy constraints such as fairness. We find that current\nstate-of-the-art empirical techniques offer sub-optimal performance on these\npractical, non-decomposable performance objectives. On the other hand, the\ntheoretical techniques necessitate training a new model from scratch for each\nperformance objective. To bridge the gap, we propose SelMix, a selective\nmixup-based inexpensive fine-tuning technique for pre-trained models, to\noptimize for the desired objective. The core idea of our framework is to\ndetermine a sampling distribution to perform a mixup of features between\nsamples from particular classes such that it optimizes the given objective. We\ncomprehensively evaluate our technique against the existing empirical and\ntheoretically principled methods on standard benchmark datasets for imbalanced\nclassification. We find that proposed SelMix fine-tuning significantly improves\nthe performance for various practical non-decomposable objectives across\nbenchmarks.\n","authors":["Shrinivas Ramasubramanian","Harsh Rangwani","Sho Takemori","Kunal Samanta","Yuhei Umeda","Venkatesh Babu Radhakrishnan"],"pdf_url":"https://arxiv.org/pdf/2403.18301v1.pdf","comment":"ICLR 2024 SpotLight"},{"id":"http://arxiv.org/abs/2403.07392v3","updated":"2024-03-27T06:44:13Z","published":"2024-03-12T07:59:41Z","title":"ViT-CoMer: Vision Transformer with Convolutional Multi-scale Feature\n  Interaction for Dense Predictions","summary":"  Although Vision Transformer (ViT) has achieved significant success in\ncomputer vision, it does not perform well in dense prediction tasks due to the\nlack of inner-patch information interaction and the limited diversity of\nfeature scale. Most existing studies are devoted to designing vision-specific\ntransformers to solve the above problems, which introduce additional\npre-training costs. Therefore, we present a plain, pre-training-free, and\nfeature-enhanced ViT backbone with Convolutional Multi-scale feature\ninteraction, named ViT-CoMer, which facilitates bidirectional interaction\nbetween CNN and transformer. Compared to the state-of-the-art, ViT-CoMer has\nthe following advantages: (1) We inject spatial pyramid multi-receptive field\nconvolutional features into the ViT architecture, which effectively alleviates\nthe problems of limited local information interaction and single-feature\nrepresentation in ViT. (2) We propose a simple and efficient CNN-Transformer\nbidirectional fusion interaction module that performs multi-scale fusion across\nhierarchical features, which is beneficial for handling dense prediction tasks.\n(3) We evaluate the performance of ViT-CoMer across various dense prediction\ntasks, different frameworks, and multiple advanced pre-training. Notably, our\nViT-CoMer-L achieves 64.3% AP on COCO val2017 without extra training data, and\n62.1% mIoU on ADE20K val, both of which are comparable to state-of-the-art\nmethods. We hope ViT-CoMer can serve as a new backbone for dense prediction\ntasks to facilitate future research. The code will be released at\nhttps://github.com/Traffic-X/ViT-CoMer.\n","authors":["Chunlong Xia","Xinliang Wang","Feng Lv","Xin Hao","Yifeng Shi"],"pdf_url":"https://arxiv.org/pdf/2403.07392v3.pdf","comment":"CVPR2024"},{"id":"http://arxiv.org/abs/2403.18294v1","updated":"2024-03-27T06:40:26Z","published":"2024-03-27T06:40:26Z","title":"Multi-scale Unified Network for Image Classification","summary":"  Convolutional Neural Networks (CNNs) have advanced significantly in visual\nrepresentation learning and recognition. However, they face notable challenges\nin performance and computational efficiency when dealing with real-world,\nmulti-scale image inputs. Conventional methods rescale all input images into a\nfixed size, wherein a larger fixed size favors performance but rescaling small\nsize images to a larger size incurs digitization noise and increased\ncomputation cost. In this work, we carry out a comprehensive, layer-wise\ninvestigation of CNN models in response to scale variation, based on Centered\nKernel Alignment (CKA) analysis. The observations reveal lower layers are more\nsensitive to input image scale variations than high-level layers. Inspired by\nthis insight, we propose Multi-scale Unified Network (MUSN) consisting of\nmulti-scale subnets, a unified network, and scale-invariant constraint. Our\nmethod divides the shallow layers into multi-scale subnets to enable feature\nextraction from multi-scale inputs, and the low-level features are unified in\ndeep layers for extracting high-level semantic features. A scale-invariant\nconstraint is posed to maintain feature consistency across different scales.\nExtensive experiments on ImageNet and other scale-diverse datasets, demonstrate\nthat MSUN achieves significant improvements in both model performance and\ncomputational efficiency. Particularly, MSUN yields an accuracy increase up to\n44.53% and diminishes FLOPs by 7.01-16.13% in multi-scale scenarios.\n","authors":["Wenzhuo Liu","Fei Zhu","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2403.18294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18293v1","updated":"2024-03-27T06:37:51Z","published":"2024-03-27T06:37:51Z","title":"Efficient Test-Time Adaptation of Vision-Language Models","summary":"  Test-time adaptation with pre-trained vision-language models has attracted\nincreasing attention for tackling distribution shifts during the test time.\nThough prior studies have achieved very promising performance, they involve\nintensive computation which is severely unaligned with test-time adaptation. We\ndesign TDA, a training-free dynamic adapter that enables effective and\nefficient test-time adaptation with vision-language models. TDA works with a\nlightweight key-value cache that maintains a dynamic queue with few-shot pseudo\nlabels as values and the corresponding test-sample features as keys. Leveraging\nthe key-value cache, TDA allows adapting to test data gradually via progressive\npseudo label refinement which is super-efficient without incurring any\nbackpropagation. In addition, we introduce negative pseudo labeling that\nalleviates the adverse impact of pseudo label noises by assigning pseudo labels\nto certain negative classes when the model is uncertain about its pseudo label\npredictions. Extensive experiments over two benchmarks demonstrate TDA's\nsuperior effectiveness and efficiency as compared with the state-of-the-art.\nThe code has been released in \\url{https://kdiaaa.github.io/tda/}.\n","authors":["Adilbek Karmanov","Dayan Guan","Shijian Lu","Abdulmotaleb El Saddik","Eric Xing"],"pdf_url":"https://arxiv.org/pdf/2403.18293v1.pdf","comment":"Accepted to CVPR 2024. The code has been released in\n  \\url{https://kdiaaa.github.io/tda/}"},{"id":"http://arxiv.org/abs/2403.18291v1","updated":"2024-03-27T06:28:19Z","published":"2024-03-27T06:28:19Z","title":"Towards Non-Exemplar Semi-Supervised Class-Incremental Learning","summary":"  Deep neural networks perform remarkably well in close-world scenarios.\nHowever, novel classes emerged continually in real applications, making it\nnecessary to learn incrementally. Class-incremental learning (CIL) aims to\ngradually recognize new classes while maintaining the discriminability of old\nones. Existing CIL methods have two limitations: a heavy reliance on preserving\nold data for forgetting mitigation and the need for vast labeled data for\nknowledge adaptation. To overcome these issues, we propose a non-exemplar\nsemi-supervised CIL framework with contrastive learning and semi-supervised\nincremental prototype classifier (Semi-IPC). On the one hand, contrastive\nlearning helps the model learn rich representations, easing the trade-off\nbetween learning representations of new classes and forgetting that of old\nclasses. On the other hand, Semi-IPC learns a prototype for each class with\nunsupervised regularization, enabling the model to incrementally learn from\npartially labeled new data while maintaining the knowledge of old classes.\nExperiments on benchmark datasets demonstrate the strong performance of our\nmethod: without storing any old samples and only using less than 1% of labels,\nSemi-IPC outperforms advanced exemplar-based methods. We hope our work offers\nnew insights for future CIL research. The code will be made publicly available.\n","authors":["Wenzhuo Liu","Fei Zhu","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2403.18291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15864v2","updated":"2024-03-27T06:26:09Z","published":"2023-11-27T14:32:33Z","title":"InterControl: Generate Human Motion Interactions by Controlling Every\n  Joint","summary":"  Text-conditioned human motion synthesis has made remarkable progress with the\nemergence of diffusion models in recent research. However, the majority of\nthese motion diffusion models are primarily designed for a single character and\noverlook multi-human interactions. In our approach, we strive to explore this\nproblem by synthesizing human motion with interactions for a group of\ncharacters of any size. The key aspect of our approach is the adaptation of\nhuman-wise interactions as pairs of human joints that can be either in contact\nor separated by a desired distance. In contrast to existing methods that\nnecessitate training motion generation models on multi-human motion datasets\nwith a fixed number of characters, our approach inherently possesses the\nflexibility to model human interactions involving an arbitrary number of\nindividuals, thereby transcending the limitations imposed by the training data.\nWe introduce a novel controllable motion generation method, InterControl, to\nencourage the synthesized motions maintaining the desired distance between\njoint pairs. It consists of a motion controller and an inverse kinematics\nguidance module that realistically and accurately aligns the joints of\nsynthesized characters to the desired location. Furthermore, we demonstrate\nthat the distance between joint pairs for human-wise interactions can be\ngenerated using an off-the-shelf Large Language Model (LLM). Experimental\nresults highlight the capability of our framework to generate interactions with\nmultiple human characters and its potential to work with off-the-shelf\nphysics-based character simulators.\n","authors":["Zhenzhi Wang","Jingbo Wang","Yixuan Li","Dahua Lin","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2311.15864v2.pdf","comment":"Generate human interactions with only single-person data via joint\n  contact pairs, code https://github.com/zhenzhiwang/intercontrol"},{"id":"http://arxiv.org/abs/2403.18282v1","updated":"2024-03-27T06:18:40Z","published":"2024-03-27T06:18:40Z","title":"SGDM: Static-Guided Dynamic Module Make Stronger Visual Models","summary":"  The spatial attention mechanism has been widely used to improve object\ndetection performance. However, its operation is currently limited to static\nconvolutions lacking content-adaptive features. This paper innovatively\napproaches from the perspective of dynamic convolution. We propose Razor\nDynamic Convolution (RDConv) to address thetwo flaws in dynamic weight\nconvolution, making it hard to implement in spatial mechanism: 1) it is\ncomputation-heavy; 2) when generating weights, spatial information is\ndisregarded. Firstly, by using Razor Operation to generate certain features, we\nvastly reduce the parameters of the entire dynamic convolution operation.\nSecondly, we added a spatial branch inside RDConv to generate convolutional\nkernel parameters with richer spatial information. Embedding dynamic\nconvolution will also bring the problem of sensitivity to high-frequency noise.\nWe propose the Static-Guided Dynamic Module (SGDM) to address this limitation.\nBy using SGDM, we utilize a set of asymmetric static convolution kernel\nparameters to guide the construction of dynamic convolution. We introduce the\nmechanism of shared weights in static convolution to solve the problem of\ndynamic convolution being sensitive to high-frequency noise. Extensive\nexperiments illustrate that multiple different object detection backbones\nequipped with SGDM achieve a highly competitive boost in performance(e.g., +4%\nmAP with YOLOv5n on VOC and +1.7% mAP with YOLOv8n on COCO) with negligible\nparameter increase(i.e., +0.33M on YOLOv5n and +0.19M on YOLOv8n).\n","authors":["Wenjie Xing","Zhenchao Cui","Jing Qi"],"pdf_url":"https://arxiv.org/pdf/2403.18282v1.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.18281v1","updated":"2024-03-27T06:17:21Z","published":"2024-03-27T06:17:21Z","title":"AIR-HLoc: Adaptive Image Retrieval for Efficient Visual Localisation","summary":"  State-of-the-art (SOTA) hierarchical localisation pipelines (HLoc) rely on\nimage retrieval (IR) techniques to establish 2D-3D correspondences by selecting\nthe $k$ most similar images from a reference image database for a given query\nimage. Although higher values of $k$ enhance localisation robustness, the\ncomputational cost for feature matching increases linearly with $k$. In this\npaper, we observe that queries that are the most similar to images in the\ndatabase result in a higher proportion of feature matches and, thus, more\naccurate positioning. Thus, a small number of images is sufficient for queries\nvery similar to images in the reference database. We then propose a novel\napproach, AIR-HLoc, which divides query images into different localisation\ndifficulty levels based on their similarity to the reference image database. We\nconsider an image with high similarity to the reference image as an easy query\nand an image with low similarity as a hard query. Easy queries show a limited\nimprovement in accuracy when increasing $k$. Conversely, higher values of $k$\nsignificantly improve accuracy for hard queries. Given the limited improvement\nin accuracy when increasing $k$ for easy queries and the significant\nimprovement for hard queries, we adapt the value of $k$ to the query's\ndifficulty level. Therefore, AIR-HLoc optimizes processing time by adaptively\nassigning different values of $k$ based on the similarity between the query and\nreference images without losing accuracy. Our extensive experiments on the\nCambridge Landmarks, 7Scenes, and Aachen Day-Night-v1.1 datasets demonstrate\nour algorithm's efficacy, reducing 30\\%, 26\\%, and 11\\% in computational\noverhead while maintaining SOTA accuracy compared to HLoc with fixed image\nretrieval.\n","authors":["Changkun Liu","Huajian Huang","Zhengyang Ma","Tristan Braud"],"pdf_url":"https://arxiv.org/pdf/2403.18281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07711v2","updated":"2024-03-27T06:02:38Z","published":"2024-03-12T14:53:56Z","title":"SSM Meets Video Diffusion Models: Efficient Video Generation with\n  Structured State Spaces","summary":"  Given the remarkable achievements in image generation through diffusion\nmodels, the research community has shown increasing interest in extending these\nmodels to video generation. Recent diffusion models for video generation have\npredominantly utilized attention layers to extract temporal features. However,\nattention layers are limited by their memory consumption, which increases\nquadratically with the length of the sequence. This limitation presents\nsignificant challenges when attempting to generate longer video sequences using\ndiffusion models. To overcome this challenge, we propose leveraging state-space\nmodels (SSMs). SSMs have recently gained attention as viable alternatives due\nto their linear memory consumption relative to sequence length. In the\nexperiments, we first evaluate our SSM-based model with UCF101, a standard\nbenchmark of video generation. In addition, to investigate the potential of\nSSMs for longer video generation, we perform an experiment using the MineRL\nNavigate dataset, varying the number of frames to 64, 200, and 400. In these\nsettings, our SSM-based model can considerably save memory consumption for\nlonger sequences, while maintaining competitive FVD scores to the\nattention-based models. Our codes are available at\nhttps://github.com/shim0114/SSM-Meets-Video-Diffusion-Models.\n","authors":["Yuta Oshima","Shohei Taniguchi","Masahiro Suzuki","Yutaka Matsuo"],"pdf_url":"https://arxiv.org/pdf/2403.07711v2.pdf","comment":"Accepted as workshop paper at ICLR 2024"},{"id":"http://arxiv.org/abs/2303.08231v3","updated":"2024-03-27T06:00:18Z","published":"2023-03-14T20:55:27Z","title":"Rotation-Invariant Transformer for Point Cloud Matching","summary":"  The intrinsic rotation invariance lies at the core of matching point clouds\nwith handcrafted descriptors. However, it is widely despised by recent deep\nmatchers that obtain the rotation invariance extrinsically via data\naugmentation. As the finite number of augmented rotations can never span the\ncontinuous SO(3) space, these methods usually show instability when facing\nrotations that are rarely seen. To this end, we introduce RoITr, a\nRotation-Invariant Transformer to cope with the pose variations in the point\ncloud matching task. We contribute both on the local and global levels.\nStarting from the local level, we introduce an attention mechanism embedded\nwith Point Pair Feature (PPF)-based coordinates to describe the pose-invariant\ngeometry, upon which a novel attention-based encoder-decoder architecture is\nconstructed. We further propose a global transformer with rotation-invariant\ncross-frame spatial awareness learned by the self-attention mechanism, which\nsignificantly improves the feature distinctiveness and makes the model robust\nwith respect to the low overlap. Experiments are conducted on both the rigid\nand non-rigid public benchmarks, where RoITr outperforms all the\nstate-of-the-art models by a considerable margin in the low-overlapping\nscenarios. Especially when the rotations are enlarged on the challenging\n3DLoMatch benchmark, RoITr surpasses the existing methods by at least 13 and 5\npercentage points in terms of Inlier Ratio and Registration Recall,\nrespectively.\n","authors":["Hao Yu","Zheng Qin","Ji Hou","Mahdi Saleh","Dongsheng Li","Benjamin Busam","Slobodan Ilic"],"pdf_url":"https://arxiv.org/pdf/2303.08231v3.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2403.18274v1","updated":"2024-03-27T05:57:45Z","published":"2024-03-27T05:57:45Z","title":"DVLO: Deep Visual-LiDAR Odometry with Local-to-Global Feature Fusion and\n  Bi-Directional Structure Alignment","summary":"  Information inside visual and LiDAR data is well complementary derived from\nthe fine-grained texture of images and massive geometric information in point\nclouds. However, it remains challenging to explore effective visual-LiDAR\nfusion, mainly due to the intrinsic data structure inconsistency between two\nmodalities: Images are regular and dense, but LiDAR points are unordered and\nsparse. To address the problem, we propose a local-to-global fusion network\nwith bi-directional structure alignment. To obtain locally fused features, we\nproject points onto image plane as cluster centers and cluster image pixels\naround each center. Image pixels are pre-organized as pseudo points for\nimage-to-point structure alignment. Then, we convert points to pseudo images by\ncylindrical projection (point-to-image structure alignment) and perform\nadaptive global feature fusion between point features with local fused\nfeatures. Our method achieves state-of-the-art performance on KITTI odometry\nand FlyingThings3D scene flow datasets compared to both single-modal and\nmulti-modal methods. Codes will be released later.\n","authors":["Jiuming Liu","Dong Zhuo","Zhiheng Feng","Siting Zhu","Chensheng Peng","Zhe Liu","Hesheng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18271v1","updated":"2024-03-27T05:55:16Z","published":"2024-03-27T05:55:16Z","title":"Unleashing the Potential of SAM for Medical Adaptation via Hierarchical\n  Decoding","summary":"  The Segment Anything Model (SAM) has garnered significant attention for its\nversatile segmentation abilities and intuitive prompt-based interface. However,\nits application in medical imaging presents challenges, requiring either\nsubstantial training costs and extensive medical datasets for full model\nfine-tuning or high-quality prompts for optimal performance. This paper\nintroduces H-SAM: a prompt-free adaptation of SAM tailored for efficient\nfine-tuning of medical images via a two-stage hierarchical decoding procedure.\nIn the initial stage, H-SAM employs SAM's original decoder to generate a prior\nprobabilistic mask, guiding a more intricate decoding process in the second\nstage. Specifically, we propose two key designs: 1) A class-balanced,\nmask-guided self-attention mechanism addressing the unbalanced label\ndistribution, enhancing image embedding; 2) A learnable mask cross-attention\nmechanism spatially modulating the interplay among different image regions\nbased on the prior mask. Moreover, the inclusion of a hierarchical pixel\ndecoder in H-SAM enhances its proficiency in capturing fine-grained and\nlocalized details. This approach enables SAM to effectively integrate learned\nmedical priors, facilitating enhanced adaptation for medical image segmentation\nwith limited samples. Our H-SAM demonstrates a 4.78% improvement in average\nDice compared to existing prompt-free SAM variants for multi-organ segmentation\nusing only 10% of 2D slices. Notably, without using any unlabeled data, H-SAM\neven outperforms state-of-the-art semi-supervised models relying on extensive\nunlabeled training data across various medical datasets. Our code is available\nat https://github.com/Cccccczh404/H-SAM.\n","authors":["Zhiheng Cheng","Qingyue Wei","Hongru Zhu","Yan Wang","Liangqiong Qu","Wei Shao","Yuyin Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.18271v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18270v1","updated":"2024-03-27T05:52:39Z","published":"2024-03-27T05:52:39Z","title":"Image Deraining via Self-supervised Reinforcement Learning","summary":"  The quality of images captured outdoors is often affected by the weather. One\nfactor that interferes with sight is rain, which can obstruct the view of\nobservers and computer vision applications that rely on those images. The work\naims to recover rain images by removing rain streaks via Self-supervised\nReinforcement Learning (RL) for image deraining (SRL-Derain). We locate rain\nstreak pixels from the input rain image via dictionary learning and use\npixel-wise RL agents to take multiple inpainting actions to remove rain\nprogressively. To our knowledge, this work is the first attempt where\nself-supervised RL is applied to image deraining. Experimental results on\nseveral benchmark image-deraining datasets show that the proposed SRL-Derain\nperforms favorably against state-of-the-art few-shot and self-supervised\nderaining and denoising methods.\n","authors":["He-Hao Liao","Yan-Tsung Peng","Wen-Tao Chu","Ping-Chun Hsieh","Chung-Chi Tsai"],"pdf_url":"https://arxiv.org/pdf/2403.18270v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18266v1","updated":"2024-03-27T05:38:48Z","published":"2024-03-27T05:38:48Z","title":"Branch-Tuning: Balancing Stability and Plasticity for Continual\n  Self-Supervised Learning","summary":"  Self-supervised learning (SSL) has emerged as an effective paradigm for\nderiving general representations from vast amounts of unlabeled data. However,\nas real-world applications continually integrate new content, the high\ncomputational and resource demands of SSL necessitate continual learning rather\nthan complete retraining. This poses a challenge in striking a balance between\nstability and plasticity when adapting to new information. In this paper, we\nemploy Centered Kernel Alignment for quantitatively analyzing model stability\nand plasticity, revealing the critical roles of batch normalization layers for\nstability and convolutional layers for plasticity. Motivated by this, we\npropose Branch-tuning, an efficient and straightforward method that achieves a\nbalance between stability and plasticity in continual SSL. Branch-tuning\nconsists of branch expansion and compression, and can be easily applied to\nvarious SSL methods without the need of modifying the original methods,\nretaining old data or models. We validate our method through incremental\nexperiments on various benchmark datasets, demonstrating its effectiveness and\npractical value in real-world scenarios. We hope our work offers new insights\nfor future continual self-supervised learning research. The code will be made\npublicly available.\n","authors":["Wenzhuo Liu","Fei Zhu","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2403.18266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03532v2","updated":"2024-03-27T05:28:55Z","published":"2024-03-06T08:18:02Z","title":"Extend Your Own Correspondences: Unsupervised Distant Point Cloud\n  Registration by Progressive Distance Extension","summary":"  Registration of point clouds collected from a pair of distant vehicles\nprovides a comprehensive and accurate 3D view of the driving scenario, which is\nvital for driving safety related applications, yet existing literature suffers\nfrom the expensive pose label acquisition and the deficiency to generalize to\nnew data distributions. In this paper, we propose EYOC, an unsupervised distant\npoint cloud registration method that adapts to new point cloud distributions on\nthe fly, requiring no global pose labels. The core idea of EYOC is to train a\nfeature extractor in a progressive fashion, where in each round, the feature\nextractor, trained with near point cloud pairs, can label slightly farther\npoint cloud pairs, enabling self-supervision on such far point cloud pairs.\nThis process continues until the derived extractor can be used to register\ndistant point clouds. Particularly, to enable high-fidelity correspondence\nlabel generation, we devise an effective spatial filtering scheme to select the\nmost representative correspondences to register a point cloud pair, and then\nutilize the aligned point clouds to discover more correct correspondences.\nExperiments show that EYOC can achieve comparable performance with\nstate-of-the-art supervised methods at a lower training cost. Moreover, it\noutwits supervised methods regarding generalization performance on new data\ndistributions.\n","authors":["Quan Liu","Hongzi Zhu","Zhenxi Wang","Yunsong Zhou","Shan Chang","Minyi Guo"],"pdf_url":"https://arxiv.org/pdf/2403.03532v2.pdf","comment":"In Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR), 2024"},{"id":"http://arxiv.org/abs/2402.02561v2","updated":"2024-03-27T05:23:40Z","published":"2024-02-04T16:27:37Z","title":"Foundation Model Makes Clustering A Better Initialization For Cold-Start\n  Active Learning","summary":"  Active learning selects the most informative samples from the unlabelled\ndataset to annotate in the context of a limited annotation budget. While\nnumerous methods have been proposed for subsequent sample selection based on an\ninitialized model, scant attention has been paid to the indispensable phase of\nactive learning: selecting samples for model cold-start initialization. Most of\nthe previous studies resort to random sampling or naive clustering. However,\nrandom sampling is prone to fluctuation, and naive clustering suffers from\nconvergence speed, particularly when dealing with high-dimensional data such as\nimaging data. In this work, we propose to integrate foundation models with\nclustering methods to select samples for cold-start active learning\ninitialization. Foundation models refer to those trained on massive datasets by\nthe self-supervised paradigm and capable of generating informative and\ncompacted embeddings for various downstream tasks. Leveraging these embeddings\nto replace raw features such as pixel values, clustering quickly converges and\nidentifies better initial samples. For a comprehensive comparison, we included\na classic ImageNet-supervised model to acquire embeddings. Experiments on two\nclinical tasks of image classification and segmentation demonstrated that\nfoundation model-based clustering efficiently pinpointed informative initial\nsamples, leading to models showcasing enhanced performance than the baseline\nmethods. We envisage that this study provides an effective paradigm for future\ncold-start active learning.\n","authors":["Han Yuan","Chuan Hong"],"pdf_url":"https://arxiv.org/pdf/2402.02561v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17456v3","updated":"2024-03-27T05:22:18Z","published":"2023-11-29T08:56:24Z","title":"DifFlow3D: Toward Robust Uncertainty-Aware Scene Flow Estimation with\n  Iterative Diffusion-Based Refinement","summary":"  Scene flow estimation, which aims to predict per-point 3D displacements of\ndynamic scenes, is a fundamental task in the computer vision field. However,\nprevious works commonly suffer from unreliable correlation caused by locally\nconstrained searching ranges, and struggle with accumulated inaccuracy arising\nfrom the coarse-to-fine structure. To alleviate these problems, we propose a\nnovel uncertainty-aware scene flow estimation network (DifFlow3D) with the\ndiffusion probabilistic model. Iterative diffusion-based refinement is designed\nto enhance the correlation robustness and resilience to challenging cases, e.g.\ndynamics, noisy inputs, repetitive patterns, etc. To restrain the generation\ndiversity, three key flow-related features are leveraged as conditions in our\ndiffusion model. Furthermore, we also develop an uncertainty estimation module\nwithin diffusion to evaluate the reliability of estimated scene flow. Our\nDifFlow3D achieves state-of-the-art performance, with 24.0% and 29.1% EPE3D\nreduction respectively on FlyingThings3D and KITTI 2015 datasets. Notably, our\nmethod achieves an unprecedented millimeter-level accuracy (0.0078m in EPE3D)\non the KITTI dataset. Additionally, our diffusion-based refinement paradigm can\nbe readily integrated as a plug-and-play module into existing scene flow\nnetworks, significantly increasing their estimation accuracy. Codes are\nreleased at https://github.com/IRMVLab/DifFlow3D.\n","authors":["Jiuming Liu","Guangming Wang","Weicai Ye","Chaokang Jiang","Jinru Han","Zhe Liu","Guofeng Zhang","Dalong Du","Hesheng Wang"],"pdf_url":"https://arxiv.org/pdf/2311.17456v3.pdf","comment":"Camera-ready version of CVPR 2024. Codes are released at\n  https://github.com/IRMVLab/DifFlow3D"},{"id":"http://arxiv.org/abs/2403.18260v1","updated":"2024-03-27T05:22:06Z","published":"2024-03-27T05:22:06Z","title":"Toward Interactive Regional Understanding in Vision-Large Language\n  Models","summary":"  Recent Vision-Language Pre-training (VLP) models have demonstrated\nsignificant advancements. Nevertheless, these models heavily rely on image-text\npairs that capture only coarse and global information of an image, leading to a\nlimitation in their regional understanding ability. In this work, we introduce\n\\textbf{RegionVLM}, equipped with explicit regional modeling capabilities,\nallowing them to understand user-indicated image regions. To achieve this, we\ndesign a simple yet innovative architecture, requiring no modifications to the\nmodel architecture or objective function. Additionally, we leverage a dataset\nthat contains a novel source of information, namely Localized Narratives, which\nhas been overlooked in previous VLP research. Our experiments demonstrate that\nour single generalist model not only achieves an interactive dialogue system\nbut also exhibits superior performance on various zero-shot region\nunderstanding tasks, without compromising its ability for global image\nunderstanding.\n","authors":["Jungbeom Lee","Sanghyuk Chun","Sangdoo Yun"],"pdf_url":"https://arxiv.org/pdf/2403.18260v1.pdf","comment":"NAACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2209.02200v3","updated":"2024-03-27T05:16:02Z","published":"2022-09-06T03:42:18Z","title":"Task-wise Sampling Convolutions for Arbitrary-Oriented Object Detection\n  in Aerial Images","summary":"  Arbitrary-oriented object detection (AOOD) has been widely applied to locate\nand classify objects with diverse orientations in remote sensing images.\nHowever, the inconsistent features for the localization and classification\ntasks in AOOD models may lead to ambiguity and low-quality object predictions,\nwhich constrains the detection performance. In this article, an AOOD method\ncalled task-wise sampling convolutions (TS-Conv) is proposed. TS-Conv\nadaptively samples task-wise features from respective sensitive regions and\nmaps these features together in alignment to guide a dynamic label assignment\nfor better predictions. Specifically, sampling positions of the localization\nconvolution in TS-Conv are supervised by the oriented bounding box (OBB)\nprediction associated with spatial coordinates, while sampling positions and\nconvolutional kernel of the classification convolution are designed to be\nadaptively adjusted according to different orientations for improving the\norientation robustness of features. Furthermore, a dynamic\ntask-consistent-aware label assignment (DTLA) strategy is developed to select\noptimal candidate positions and assign labels dynamically according to ranked\ntask-aware scores obtained from TS-Conv. Extensive experiments on several\npublic datasets covering multiple scenes, multimodal images, and multiple\ncategories of objects demonstrate the effectiveness, scalability, and superior\nperformance of the proposed TS-Conv.\n","authors":["Zhanchao Huang","Wei Li","Xiang-Gen Xia","Hao Wang","Ran Tao"],"pdf_url":"https://arxiv.org/pdf/2209.02200v3.pdf","comment":"15 pages, 13 figures, 11 tables"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2403.18766v1","updated":"2024-03-27T17:05:03Z","published":"2024-03-27T17:05:03Z","title":"Superior Parallel Big Data Clustering through Competitive Stochastic\n  Sample Size Optimization in Big-means","summary":"  This paper introduces a novel K-means clustering algorithm, an advancement on\nthe conventional Big-means methodology. The proposed method efficiently\nintegrates parallel processing, stochastic sampling, and competitive\noptimization to create a scalable variant designed for big data applications.\nIt addresses scalability and computation time challenges typically faced with\ntraditional techniques. The algorithm adjusts sample sizes dynamically for each\nworker during execution, optimizing performance. Data from these sample sizes\nare continually analyzed, facilitating the identification of the most efficient\nconfiguration. By incorporating a competitive element among workers using\ndifferent sample sizes, efficiency within the Big-means algorithm is further\nstimulated. In essence, the algorithm balances computational time and\nclustering quality by employing a stochastic, competitive sampling strategy in\na parallel computing setting.\n","authors":["Rustam Mussabayev","Ravil Mussabayev"],"pdf_url":"https://arxiv.org/pdf/2403.18766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18684v1","updated":"2024-03-27T15:27:36Z","published":"2024-03-27T15:27:36Z","title":"Scaling Laws For Dense Retrieval","summary":"  Scaling up neural models has yielded significant advancements in a wide array\nof tasks, particularly in language generation. Previous studies have found that\nthe performance of neural models frequently adheres to predictable scaling\nlaws, correlated with factors such as training set size and model size. This\ninsight is invaluable, especially as large-scale experiments grow increasingly\nresource-intensive. Yet, such scaling law has not been fully explored in dense\nretrieval due to the discrete nature of retrieval metrics and complex\nrelationships between training data and model sizes in retrieval tasks. In this\nstudy, we investigate whether the performance of dense retrieval models follows\nthe scaling law as other neural models. We propose to use contrastive\nlog-likelihood as the evaluation metric and conduct extensive experiments with\ndense retrieval models implemented with different numbers of parameters and\ntrained with different amounts of annotated data. Results indicate that, under\nour settings, the performance of dense retrieval models follows a precise\npower-law scaling related to the model size and the number of annotations.\nAdditionally, we examine scaling with prevalent data augmentation methods to\nassess the impact of annotation quality, and apply the scaling law to find the\nbest resource allocation strategy under a budget constraint. We believe that\nthese insights will significantly contribute to understanding the scaling\neffect of dense retrieval models and offer meaningful guidance for future\nresearch endeavors.\n","authors":["Yan Fang","Jingtao Zhan","Qingyao Ai","Jiaxin Mao","Weihang Su","Jia Chen","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.18684v1.pdf","comment":"Accepted at SIGIR 2024"},{"id":"http://arxiv.org/abs/2403.18667v1","updated":"2024-03-27T15:11:00Z","published":"2024-03-27T15:11:00Z","title":"Improving Content Recommendation: Knowledge Graph-Based Semantic\n  Contrastive Learning for Diversity and Cold-Start Users","summary":"  Addressing the challenges related to data sparsity, cold-start problems, and\ndiversity in recommendation systems is both crucial and demanding. Many current\nsolutions leverage knowledge graphs to tackle these issues by combining both\nitem-based and user-item collaborative signals. A common trend in these\napproaches focuses on improving ranking performance at the cost of escalating\nmodel complexity, reducing diversity, and complicating the task. It is\nessential to provide recommendations that are both personalized and diverse,\nrather than solely relying on achieving high rank-based performance, such as\nClick-through Rate, Recall, etc. In this paper, we propose a hybrid multi-task\nlearning approach, training on user-item and item-item interactions. We apply\nitem-based contrastive learning on descriptive text, sampling positive and\nnegative pairs based on item metadata. Our approach allows the model to better\nunderstand the relationships between entities within the knowledge graph by\nutilizing semantic information from text. It leads to more accurate, relevant,\nand diverse user recommendations and a benefit that extends even to cold-start\nusers who have few interactions with items. We perform extensive experiments on\ntwo widely used datasets to validate the effectiveness of our approach. Our\nfindings demonstrate that jointly training user-item interactions and\nitem-based signals using synopsis text is highly effective. Furthermore, our\nresults provide evidence that item-based contrastive learning enhances the\nquality of entity embeddings, as indicated by metrics such as uniformity and\nalignment.\n","authors":["Yejin Kim","Scott Rome","Kevin Foley","Mayur Nankani","Rimon Melamed","Javier Morales","Abhay Yadav","Maria Peifer","Sardar Hamidian","H. Howie Huang"],"pdf_url":"https://arxiv.org/pdf/2403.18667v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18628v1","updated":"2024-03-27T14:37:01Z","published":"2024-03-27T14:37:01Z","title":"To Recommend or Not: Recommendability Identification in Conversations\n  with Pre-trained Language Models","summary":"  Most current recommender systems primarily focus on what to recommend,\nassuming users always require personalized recommendations. However, with the\nwidely spread of ChatGPT and other chatbots, a more crucial problem in the\ncontext of conversational systems is how to minimize user disruption when we\nprovide recommendation services for users. While previous research has\nextensively explored different user intents in dialogue systems, fewer efforts\nare made to investigate whether recommendations should be provided. In this\npaper, we formally define the recommendability identification problem, which\naims to determine whether recommendations are necessary in a specific scenario.\nFirst, we propose and define the recommendability identification task, which\ninvestigates the need for recommendations in the current conversational\ncontext. A new dataset is constructed. Subsequently, we discuss and evaluate\nthe feasibility of leveraging pre-trained language models (PLMs) for\nrecommendability identification. Finally, through comparative experiments, we\ndemonstrate that directly employing PLMs with zero-shot results falls short of\nmeeting the task requirements. Besides, fine-tuning or utilizing soft prompt\ntechniques yields comparable results to traditional classification methods. Our\nwork is the first to study recommendability before recommendation and provides\npreliminary ways to make it a fundamental component of the future\nrecommendation system.\n","authors":["Zhefan Wang","Weizhi Ma","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.18628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18623v1","updated":"2024-03-27T14:34:22Z","published":"2024-03-27T14:34:22Z","title":"Antitrust, Amazon, and Algorithmic Auditing","summary":"  In digital markets, antitrust law and special regulations aim to ensure that\nmarkets remain competitive despite the dominating role that digital platforms\nplay today in everyone's life. Unlike traditional markets, market participant\nbehavior is easily observable in these markets. We present a series of\nempirical investigations into the extent to which Amazon engages in practices\nthat are typically described as self-preferencing. We discuss how the computer\nscience tools used in this paper can be used in a regulatory environment that\nis based on algorithmic auditing and requires regulating digital markets at\nscale.\n","authors":["Abhisek Dash","Abhijnan Chakraborty","Saptarshi Ghosh","Animesh Mukherjee","Jens Frankenreiter","Stefan Bechtold","Krishna P. Gummadi"],"pdf_url":"https://arxiv.org/pdf/2403.18623v1.pdf","comment":"The paper has been accepted to appear at Journal of Institutional and\n  Theoretical Economics (JITE) 2024"},{"id":"http://arxiv.org/abs/2403.18604v1","updated":"2024-03-27T14:24:28Z","published":"2024-03-27T14:24:28Z","title":"Modeling Sustainable City Trips: Integrating CO2 Emissions, Popularity,\n  and Seasonality into Tourism Recommender Systems","summary":"  In an era of information overload and complex decision-making processes,\nRecommender Systems (RS) have emerged as indispensable tools across diverse\ndomains, particularly travel and tourism. These systems simplify trip planning\nby offering personalized recommendations that consider individual preferences\nand address broader challenges like seasonality, travel regulations, and\ncapacity constraints. The intricacies of the tourism domain, characterized by\nmultiple stakeholders, including consumers, item providers, platforms, and\nsociety, underscore the complexity of achieving balance among diverse\ninterests. Although previous research has focused on fairness in Tourism\nRecommender Systems (TRS) from a multistakeholder perspective, limited work has\nfocused on generating sustainable recommendations.\n  Our paper introduces a novel approach for assigning a sustainability\nindicator (SF index) for city trips accessible from the users' starting point,\nintegrating Co2e analysis, destination popularity, and seasonal demand. Our\nmethodology involves comprehensive data gathering on transportation modes and\nemissions, complemented by analyses of destination popularity and seasonal\ndemand. A user study validates our index, showcasing its practicality and\nefficacy in providing well-rounded and sustainable city trip recommendations.\nOur findings contribute significantly to the evolution of responsible tourism\nstrategies, harmonizing the interests of tourists, local communities, and the\nenvironment while paving the way for future research in responsible and\nequitable tourism practices.\n","authors":["Ashmi Banerjee","Tunar Mahmudov","Emil Adler","Fitri Nur Aisyah","Wolfgang Wörndl"],"pdf_url":"https://arxiv.org/pdf/2403.18604v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12997v3","updated":"2024-03-27T13:59:57Z","published":"2024-02-20T13:25:16Z","title":"Towards Trustworthy Reranking: A Simple yet Effective Abstention\n  Mechanism","summary":"  Neural Information Retrieval (NIR) has significantly improved upon\nheuristic-based IR systems. Yet, failures remain frequent, the models used\noften being unable to retrieve documents relevant to the user's query. We\naddress this challenge by proposing a lightweight abstention mechanism tailored\nfor real-world constraints, with particular emphasis placed on the reranking\nphase. We introduce a protocol for evaluating abstention strategies in a\nblack-box scenario, demonstrating their efficacy, and propose a simple yet\neffective data-driven mechanism. We provide open-source code for experiment\nreplication and abstention implementation, fostering wider adoption and\napplication in diverse contexts.\n","authors":["Hippolyte Gisserot-Boukhlef","Manuel Faysse","Emmanuel Malherbe","Céline Hudelot","Pierre Colombo"],"pdf_url":"https://arxiv.org/pdf/2402.12997v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18536v1","updated":"2024-03-27T13:12:41Z","published":"2024-03-27T13:12:41Z","title":"A Novel Behavior-Based Recommendation System for E-commerce","summary":"  The majority of existing recommender systems rely on user ratings, which are\nlimited by the lack of user collaboration and the sparsity problem. To address\nthese issues, this study proposes a behavior-based recommender system that\nleverages customers' natural behaviors, such as browsing and clicking, on\ne-commerce platforms. The proposed recommendation system involves clustering\nactive customers, determining neighborhoods, collecting similar users,\ncalculating product reputation based on similar users, and recommending\nhigh-reputation products. To overcome the complexity of customer behaviors and\ntraditional clustering methods, an unsupervised clustering approach based on\nproduct categories is developed to enhance the recommendation methodology. This\nstudy makes notable contributions in several aspects. Firstly, a groundbreaking\nbehavior-based recommendation methodology is developed, incorporating customer\nbehavior to generate accurate and tailored recommendations leading to improved\ncustomer satisfaction and engagement. Secondly, an original unsupervised\nclustering method, focusing on product categories, enables more precise\nclustering and facilitates accurate recommendations. Finally, an approach to\ndetermine neighborhoods for active customers within clusters is established,\nensuring grouping of customers with similar behavioral patterns to enhance\nrecommendation accuracy and relevance. The proposed recommendation methodology\nand clustering method contribute to improved recommendation performance,\noffering valuable insights for researchers and practitioners in the field of\ne-commerce recommendation systems. Additionally, the proposed method\noutperforms benchmark methods in experiments conducted using a behavior dataset\nfrom the well-known e-commerce site Alibaba.\n","authors":["Reza Barzegar Nozari","Mahdi Divsalar","Sepehr Akbarzadeh Abkenar","Mohammadreza Fadavi Amiri","Ali Divsalar"],"pdf_url":"https://arxiv.org/pdf/2403.18536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18480v1","updated":"2024-03-27T11:49:58Z","published":"2024-03-27T11:49:58Z","title":"Enhanced Generative Recommendation via Content and Collaboration\n  Integration","summary":"  Generative recommendation has emerged as a promising paradigm aimed at\naugmenting recommender systems with recent advancements in generative\nartificial intelligence. This task has been formulated as a\nsequence-to-sequence generation process, wherein the input sequence encompasses\ndata pertaining to the user's previously interacted items, and the output\nsequence denotes the generative identifier for the suggested item. However,\nexisting generative recommendation approaches still encounter challenges in (i)\neffectively integrating user-item collaborative signals and item content\ninformation within a unified generative framework, and (ii) executing an\nefficient alignment between content information and collaborative signals.\n  In this paper, we introduce content-based collaborative generation for\nrecommender systems, denoted as ColaRec. To capture collaborative signals, the\ngenerative item identifiers are derived from a pretrained collaborative\nfiltering model, while the user is represented through the aggregation of\ninteracted items' content. Subsequently, the aggregated textual description of\nitems is fed into a language model to encapsulate content information. This\nintegration enables ColaRec to amalgamate collaborative signals and content\ninformation within an end-to-end framework. Regarding the alignment, we propose\nan item indexing task to facilitate the mapping between the content-based\nsemantic space and the interaction-based collaborative space. Additionally, a\ncontrastive loss is introduced to ensure that items with similar collaborative\nGIDs possess comparable content representations, thereby enhancing alignment.\nTo validate the efficacy of ColaRec, we conduct experiments on three benchmark\ndatasets. Empirical results substantiate the superior performance of ColaRec.\n","authors":["Yidan Wang","Zhaochun Ren","Weiwei Sun","Jiyuan Yang","Zhixiang Liang","Xin Chen","Ruobing Xie","Su Yan","Xu Zhang","Pengjie Ren","Zhumin Chen","Xin Xin"],"pdf_url":"https://arxiv.org/pdf/2403.18480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18479v1","updated":"2024-03-27T11:49:55Z","published":"2024-03-27T11:49:55Z","title":"Lightweight Embeddings for Graph Collaborative Filtering","summary":"  Graph neural networks (GNNs) are currently one of the most performant\ncollaborative filtering methods. Meanwhile, owing to the use of an embedding\ntable to represent each user/item as a distinct vector, GNN-based recommenders\nhave inherited the long-standing defect of parameter inefficiency. As a common\npractice for scalable embeddings, parameter sharing enables the use of fewer\nembedding vectors (i.e., meta-embeddings). When assigning meta-embeddings, most\nexisting methods are a heuristically designed, predefined mapping from each\nuser's/item's ID to the corresponding meta-embedding indexes, thus simplifying\nthe optimization problem into learning only the meta-embeddings. However, in\nthe context of GNN-based collaborative filtering, such a fixed mapping omits\nthe semantic correlations between entities that are evident in the user-item\ninteraction graph, leading to suboptimal recommendation performance. To this\nend, we propose Lightweight Embeddings for Graph Collaborative Filtering\n(LEGCF), a parameter-efficient embedding framework dedicated to GNN-based\nrecommenders. LEGCF innovatively introduces an assignment matrix as an extra\nlearnable component on top of meta-embeddings. To jointly optimize these two\nheavily entangled components, aside from learning the meta-embeddings by\nminimizing the recommendation loss, LEGCF further performs efficient assignment\nupdate by enforcing a novel semantic similarity constraint and finding its\nclosed-form solution based on matrix pseudo-inverse. The meta-embeddings and\nassignment matrix are alternately updated, where the latter is sparsified on\nthe fly to ensure negligible storage overhead. Extensive experiments on three\nbenchmark datasets have verified LEGCF's smallest trade-off between size and\nperformance, with consistent accuracy gain over state-of-the-art baselines. The\ncodebase of LEGCF is available in https://github.com/xurong-liang/LEGCF.\n","authors":["Xurong Liang","Tong Chen","Lizhen Cui","Yang Wang","Meng Wang","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2403.18479v1.pdf","comment":"Accepted by SIGIR '24"},{"id":"http://arxiv.org/abs/2311.07838v3","updated":"2024-03-27T11:36:46Z","published":"2023-11-14T01:38:02Z","title":"LLatrieval: LLM-Verified Retrieval for Verifiable Generation","summary":"  Verifiable generation aims to let the large language model (LLM) generate\ntext with supporting documents, which enables the user to flexibly verify the\nanswer and makes the LLM's output more reliable. Retrieval plays a crucial role\nin verifiable generation. Specifically, the retrieved documents not only\nsupplement knowledge to help the LLM generate correct answers, but also serve\nas supporting evidence for the user to verify the LLM's output. However, the\nwidely used retrievers become the bottleneck of the entire pipeline and limit\nthe overall performance. Their capabilities are usually inferior to LLMs since\nthey often have much fewer parameters than the large language model and have\nnot been demonstrated to scale well to the size of LLMs. If the retriever does\nnot correctly find the supporting documents, the LLM can not generate the\ncorrect and verifiable answer, which overshadows the LLM's remarkable\nabilities. To address these limitations, we propose \\LLatrieval (Large Language\nModel Verified Retrieval), where the LLM updates the retrieval result until it\nverifies that the retrieved documents can sufficiently support answering the\nquestion. Thus, the LLM can iteratively provide feedback to retrieval and\nfacilitate the retrieval result to fully support verifiable generation.\nExperiments show that LLatrieval significantly outperforms extensive baselines\nand achieves state-of-the-art results.\n","authors":["Xiaonan Li","Changtai Zhu","Linyang Li","Zhangyue Yin","Tianxiang Sun","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2311.07838v3.pdf","comment":"Accepted by NAACL 2024 (Main Conference)"},{"id":"http://arxiv.org/abs/2403.18462v1","updated":"2024-03-27T11:20:48Z","published":"2024-03-27T11:20:48Z","title":"Decoy Effect In Search Interaction: Understanding User Behavior and\n  Measuring System Vulnerability","summary":"  This study examines the decoy effect's underexplored influence on user search\ninteractions and methods for measuring information retrieval (IR) systems'\nvulnerability to this effect. It explores how decoy results alter users'\ninteractions on search engine result pages, focusing on metrics like\nclick-through likelihood, browsing time, and perceived document usefulness. By\nanalyzing user interaction logs from multiple datasets, the study demonstrates\nthat decoy results significantly affect users' behavior and perceptions.\nFurthermore, it investigates how different levels of task difficulty and user\nknowledge modify the decoy effect's impact, finding that easier tasks and lower\nknowledge levels lead to higher engagement with target documents. In terms of\nIR system evaluation, the study introduces the DEJA-VU metric to assess\nsystems' susceptibility to the decoy effect, testing it on specific retrieval\ntasks. The results show differences in systems' effectiveness and\nvulnerability, contributing to our understanding of cognitive biases in search\nbehavior and suggesting pathways for creating more balanced and bias-aware IR\nevaluations.\n","authors":["Nuo Chen","Jiqun Liu","Hanpei Fang","Yuankai Luo","Tetsuya Sakai","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2403.18462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18435v1","updated":"2024-03-27T10:40:14Z","published":"2024-03-27T10:40:14Z","title":"DELTA: Pre-train a Discriminative Encoder for Legal Case Retrieval via\n  Structural Word Alignment","summary":"  Recent research demonstrates the effectiveness of using pre-trained language\nmodels for legal case retrieval. Most of the existing works focus on improving\nthe representation ability for the contextualized embedding of the [CLS] token\nand calculate relevance using textual semantic similarity. However, in the\nlegal domain, textual semantic similarity does not always imply that the cases\nare relevant enough. Instead, relevance in legal cases primarily depends on the\nsimilarity of key facts that impact the final judgment. Without proper\ntreatments, the discriminative ability of learned representations could be\nlimited since legal cases are lengthy and contain numerous non-key facts. To\nthis end, we introduce DELTA, a discriminative model designed for legal case\nretrieval. The basic idea involves pinpointing key facts in legal cases and\npulling the contextualized embedding of the [CLS] token closer to the key facts\nwhile pushing away from the non-key facts, which can warm up the case embedding\nspace in an unsupervised manner. To be specific, this study brings the word\nalignment mechanism to the contextual masked auto-encoder. First, we leverage\nshallow decoders to create information bottlenecks, aiming to enhance the\nrepresentation ability. Second, we employ the deep decoder to enable\ntranslation between different structures, with the goal of pinpointing key\nfacts to enhance discriminative ability. Comprehensive experiments conducted on\npublicly available legal benchmarks show that our approach can outperform\nexisting state-of-the-art methods in legal case retrieval. It provides a new\nperspective on the in-depth understanding and processing of legal case\ndocuments.\n","authors":["Haitao Li","Qingyao Ai","Xinyan Han","Jia Chen","Qian Dong","Yiqun Liu","Chong Chen","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2403.18435v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2403.06747v5","updated":"2024-03-27T10:32:29Z","published":"2024-03-11T14:13:41Z","title":"MetaSplit: Meta-Split Network for Limited-Stock Product Recommendation","summary":"  Compared to business-to-consumer (B2C) e-commerce systems,\nconsumer-to-consumer (C2C) e-commerce platforms usually encounter the\nlimited-stock problem, that is, a product can only be sold one time in a C2C\nsystem. This poses several unique challenges for click-through rate (CTR)\nprediction. Due to limited user interactions for each product (i.e. item), the\ncorresponding item embedding in the CTR model may not easily converge. This\nmakes the conventional sequence modeling based approaches cannot effectively\nutilize user history information since historical user behaviors contain a\nmixture of items with different volume of stocks. Particularly, the attention\nmechanism in a sequence model tends to assign higher score to products with\nmore accumulated user interactions, making limited-stock products being ignored\nand contribute less to the final output. To this end, we propose the Meta-Split\nNetwork (MSNet) to split user history sequence regarding to the volume of stock\nfor each product, and adopt differentiated modeling approaches for different\nsequences. As for the limited-stock products, a meta-learning approach is\napplied to address the problem of inconvergence, which is achieved by designing\nmeta scaling and shifting networks with ID and side information. In addition,\ntraditional approach can hardly update item embedding once the product is\nconsumed. Thereby, we propose an auxiliary loss that makes the parameters\nupdatable even when the product is no longer in distribution. To the best of\nour knowledge, this is the first solution addressing the recommendation of\nlimited-stock product. Experimental results on the production dataset and\nonline A/B testing demonstrate the effectiveness of our proposed method.\n","authors":["Wenhao Wu","Jialiang Zhou","Ailong He","Shuguang Han","Jufeng Chen","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.06747v5.pdf","comment":"Accepted at WWW 2024. This work has already been deployed on the\n  Xianyu platform in Alibaba. The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2403.18348v1","updated":"2024-03-27T08:39:42Z","published":"2024-03-27T08:39:42Z","title":"Sequential Recommendation with Latent Relations based on Large Language\n  Model","summary":"  Sequential recommender systems predict items that may interest users by\nmodeling their preferences based on historical interactions. Traditional\nsequential recommendation methods rely on capturing implicit collaborative\nfiltering signals among items. Recent relation-aware sequential recommendation\nmodels have achieved promising performance by explicitly incorporating item\nrelations into the modeling of user historical sequences, where most relations\nare extracted from knowledge graphs. However, existing methods rely on manually\npredefined relations and suffer the sparsity issue, limiting the generalization\nability in diverse scenarios with varied item relations. In this paper, we\npropose a novel relation-aware sequential recommendation framework with Latent\nRelation Discovery (LRD). Different from previous relation-aware models that\nrely on predefined rules, we propose to leverage the Large Language Model (LLM)\nto provide new types of relations and connections between items. The motivation\nis that LLM contains abundant world knowledge, which can be adopted to mine\nlatent relations of items for recommendation. Specifically, inspired by that\nhumans can describe relations between items using natural language, LRD\nharnesses the LLM that has demonstrated human-like knowledge to obtain language\nknowledge representations of items. These representations are fed into a latent\nrelation discovery module based on the discrete state variational autoencoder\n(DVAE). Then the self-supervised relation discovery tasks and recommendation\ntasks are jointly optimized. Experimental results on multiple public datasets\ndemonstrate our proposed latent relations discovery method can be incorporated\nwith existing relation-aware sequential recommendation models and significantly\nimprove the performance. Further analysis experiments indicate the\neffectiveness and reliability of the discovered latent relations.\n","authors":["Shenghao Yang","Weizhi Ma","Peijie Sun","Qingyao Ai","Yiqun Liu","Mingchen Cai","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.18348v1.pdf","comment":"Accepted by SIGIR 2024"},{"id":"http://arxiv.org/abs/2403.18325v1","updated":"2024-03-27T08:06:56Z","published":"2024-03-27T08:06:56Z","title":"Common Sense Enhanced Knowledge-based Recommendation with Large Language\n  Model","summary":"  Knowledge-based recommendation models effectively alleviate the data sparsity\nissue leveraging the side information in the knowledge graph, and have achieved\nconsiderable performance. Nevertheless, the knowledge graphs used in previous\nwork, namely metadata-based knowledge graphs, are usually constructed based on\nthe attributes of items and co-occurring relations (e.g., also buy), in which\nthe former provides limited information and the latter relies on sufficient\ninteraction data and still suffers from cold start issue. Common sense, as a\nform of knowledge with generality and universality, can be used as a supplement\nto the metadata-based knowledge graph and provides a new perspective for\nmodeling users' preferences. Recently, benefiting from the emergent world\nknowledge of the large language model, efficient acquisition of common sense\nhas become possible. In this paper, we propose a novel knowledge-based\nrecommendation framework incorporating common sense, CSRec, which can be\nflexibly coupled to existing knowledge-based methods. Considering the challenge\nof the knowledge gap between the common sense-based knowledge graph and\nmetadata-based knowledge graph, we propose a knowledge fusion approach based on\nmutual information maximization theory. Experimental results on public datasets\ndemonstrate that our approach significantly improves the performance of\nexisting knowledge-based recommendation models.\n","authors":["Shenghao Yang","Weizhi Ma","Peijie Sun","Min Zhang","Qingyao Ai","Yiqun Liu","Mingchen Cai"],"pdf_url":"https://arxiv.org/pdf/2403.18325v1.pdf","comment":"Accepted by DASFAA 2024"},{"id":"http://arxiv.org/abs/2403.18317v1","updated":"2024-03-27T07:40:05Z","published":"2024-03-27T07:40:05Z","title":"A Situation-aware Enhancer for Personalized Recommendation","summary":"  When users interact with Recommender Systems (RecSys), current situations,\nsuch as time, location, and environment, significantly influence their\npreferences. Situations serve as the background for interactions, where\nrelationships between users and items evolve with situation changes. However,\nexisting RecSys treat situations, users, and items on the same level. They can\nonly model the relations between situations and users/items respectively,\nrather than the dynamic impact of situations on user-item associations (i.e.,\nuser preferences). In this paper, we provide a new perspective that takes\nsituations as the preconditions for users' interactions. This perspective\nallows us to separate situations from user/item representations, and capture\nsituations' influences over the user-item relationship, offering a more\ncomprehensive understanding of situations. Based on it, we propose a novel\nSituation-Aware Recommender Enhancer (SARE), a pluggable module to integrate\nsituations into various existing RecSys. Since users' perception of situations\nand situations' impact on preferences are both personalized, SARE includes a\nPersonalized Situation Fusion (PSF) and a User-Conditioned Preference Encoder\n(UCPE) to model the perception and impact of situations, respectively. We\nconduct experiments of applying SARE on seven backbones in various settings on\ntwo real-world datasets. Experimental results indicate that SARE improves the\nrecommendation performances significantly compared with backbones and SOTA\nsituation-aware baselines.\n","authors":["Jiayu Li","Peijie Sun","Chumeng Jiang","Weizhi Ma","Qingyao Ai","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.18317v1.pdf","comment":"Accepted at the International Conference on Database Systems for\n  Advanced Applications (DASFAA 2024)"},{"id":"http://arxiv.org/abs/2402.18355v2","updated":"2024-03-27T07:22:40Z","published":"2024-02-28T14:26:52Z","title":"COPR -- Efficient, large-scale log storage and retrieval","summary":"  Modern, large scale monitoring systems have to process and store vast amounts\nof log data in near real-time. At query time the systems have to find relevant\nlogs based on the content of the log message using support structures that can\nscale to these amounts of data while still being efficient to use. We present\nour novel Compressed Probabilistic Retrieval algorithm (COPR), capable of\nanswering Multi-Set Multi-Membership-Queries, that can be used as an\nalternative to existing indexing structures for streamed log data. In our\nexperiments, COPR required up to 93% less storage space than the tested\nstate-of-the-art inverted index and had up to four orders of magnitude less\nfalse-positives than the tested state-of-the-art membership sketch.\nAdditionally, COPR achieved up to 250 times higher query throughput than the\ntested inverted index and up to 240 times higher query throughput than the\ntested membership sketch.\n","authors":["Julian Reichinger","Thomas Krismayer","Jan Rellermeyer"],"pdf_url":"https://arxiv.org/pdf/2402.18355v2.pdf","comment":"14 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.18305v1","updated":"2024-03-27T06:59:39Z","published":"2024-03-27T06:59:39Z","title":"A Recommender System for NFT Collectibles with Item Feature","summary":"  Recommender systems have been actively studied and applied in various domains\nto deal with information overload. Although there are numerous studies on\nrecommender systems for movies, music, and e-commerce, comparatively less\nattention has been paid to the recommender system for NFTs despite the\ncontinuous growth of the NFT market. This paper presents a recommender system\nfor NFTs that utilizes a variety of data sources, from NFT transaction records\nto external item features, to generate precise recommendations that cater to\nindividual preferences. We develop a data-efficient graph-based recommender\nsystem to efficiently capture the complex relationship between each item and\nusers and generate node(item) embeddings which incorporate both node feature\ninformation and graph structure. Furthermore, we exploit inputs beyond\nuser-item interactions, such as image feature, text feature, and price feature.\nNumerical experiments verify the performance of the graph-based recommender\nsystem improves significantly after utilizing all types of item features as\nside information, thereby outperforming all other baselines.\n","authors":["Minjoo Choi","Seonmi Kim","Yejin Kim","Youngbin Lee","Joohwan Hong","Yongjae Lee"],"pdf_url":"https://arxiv.org/pdf/2403.18305v1.pdf","comment":"Presented at the AAAI 2023 Bridge on AI for Financial Services\n  (https://sites.google.com/view/aaai-ai-fin/home)"},{"id":"http://arxiv.org/abs/2403.17421v2","updated":"2024-03-27T06:28:53Z","published":"2024-03-26T06:34:23Z","title":"MA4DIV: Multi-Agent Reinforcement Learning for Search Result\n  Diversification","summary":"  The objective of search result diversification (SRD) is to ensure that\nselected documents cover as many different subtopics as possible. Existing\nmethods primarily utilize a paradigm of \"greedy selection\", i.e., selecting one\ndocument with the highest diversity score at a time. These approaches tend to\nbe inefficient and are easily trapped in a suboptimal state. In addition, some\nother methods aim to approximately optimize the diversity metric, such as\n$\\alpha$-NDCG, but the results still remain suboptimal. To address these\nchallenges, we introduce Multi-Agent reinforcement learning (MARL) for search\nresult DIVersity, which called MA4DIV. In this approach, each document is an\nagent and the search result diversification is modeled as a cooperative task\namong multiple agents. This approach allows for directly optimizing the\ndiversity metrics, such as $\\alpha$-NDCG, while achieving high training\nefficiency. We conducted preliminary experiments on public TREC datasets to\ndemonstrate the effectiveness and potential of MA4DIV. Considering the limited\nnumber of queries in public TREC datasets, we construct a large-scale dataset\nfrom industry sources and show that MA4DIV achieves substantial improvements in\nboth effectiveness and efficiency than existing baselines on a industrial scale\ndataset.\n","authors":["Yiqun Chen","Jiaxin Mao","Yi Zhang","Dehong Ma","Long Xia","Jun Fan","Daiting Shi","Zhicong Cheng","Simiu Gu","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2403.17421v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18280v1","updated":"2024-03-27T06:16:14Z","published":"2024-03-27T06:16:14Z","title":"Improving Out-of-Vocabulary Handling in Recommendation Systems","summary":"  Recommendation systems (RS) are an increasingly relevant area for both\nacademic and industry researchers, given their widespread impact on the daily\nonline experiences of billions of users. One common issue in real RS is the\ncold-start problem, where users and items may not contain enough information to\nproduce high-quality recommendations. This work focuses on a complementary\nproblem: recommending new users and items unseen (out-of-vocabulary, or OOV) at\ntraining time. This setting is known as the inductive setting and is especially\nproblematic for factorization-based models, which rely on encoding only those\nusers/items seen at training time with fixed parameter vectors. Many existing\nsolutions applied in practice are often naive, such as assigning OOV\nusers/items to random buckets. In this work, we tackle this problem and propose\napproaches that better leverage available user/item features to improve OOV\nhandling at the embedding table level. We discuss general-purpose plug-and-play\napproaches that are easily applicable to most RS models and improve inductive\nperformance without negatively impacting transductive model performance. We\nextensively evaluate 9 OOV embedding methods on 5 models across 4 datasets\n(spanning different domains). One of these datasets is a proprietary production\ndataset from a prominent RS employed by a large social platform serving\nhundreds of millions of daily active users. In our experiments, we find that\nseveral proposed methods that exploit feature similarity using LSH consistently\noutperform alternatives on most model-dataset combinations, with the best\nmethod showing a mean improvement of 3.74% over the industry standard baseline\nin inductive performance. We release our code and hope our work helps\npractitioners make more informed decisions when handling OOV for their RS and\nfurther inspires academic research into improving OOV support in RS.\n","authors":["William Shiao","Mingxuan Ju","Zhichun Guo","Xin Chen","Evangelos Papalexakis","Tong Zhao","Neil Shah","Yozen Liu"],"pdf_url":"https://arxiv.org/pdf/2403.18280v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.18276v1","updated":"2024-03-27T06:07:05Z","published":"2024-03-27T06:07:05Z","title":"RankMamba, Benchmarking Mamba's Document Ranking Performance in the Era\n  of Transformers","summary":"  Transformer structure has achieved great success in multiple applied machine\nlearning communities, such as natural language processing (NLP), computer\nvision (CV) and information retrieval (IR). Transformer architecture's core\nmechanism -- attention requires $O(n^2)$ time complexity in training and $O(n)$\ntime complexity in inference. Many works have been proposed to improve the\nattention mechanism's scalability, such as Flash Attention and Multi-query\nAttention. A different line of work aims to design new mechanisms to replace\nattention. Recently, a notable model structure -- Mamba, which is based on\nstate space models, has achieved transformer-equivalent performance in multiple\nsequence modeling tasks.\n  In this work, we examine \\mamba's efficacy through the lens of a classical IR\ntask -- document ranking. A reranker model takes a query and a document as\ninput, and predicts a scalar relevance score. This task demands the language\nmodel's ability to comprehend lengthy contextual inputs and to capture the\ninteraction between query and document tokens. We find that (1) Mamba models\nachieve competitive performance compared to transformer-based models with the\nsame training recipe; (2) but also have a lower training throughput in\ncomparison to efficient transformer implementations such as flash attention. We\nhope this study can serve as a starting point to explore Mamba models in other\nclassical IR tasks. Our code implementation and trained checkpoints are made\npublic to facilitate\nreproducibility.\\footnote{https://github.com/zhichaoxu-shufe/RankMamba}.\n","authors":["Zhichao Xu"],"pdf_url":"https://arxiv.org/pdf/2403.18276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18227v1","updated":"2024-03-27T03:31:05Z","published":"2024-03-27T03:31:05Z","title":"One Backpropagation in Two Tower Recommendation Models","summary":"  Recent years have witnessed extensive researches on developing two tower\nrecommendation models for relieving information overload. Four building modules\ncan be identified in such models, namely, user-item encoding, negative\nsampling, loss computing and back-propagation updating. To the best of our\nknowledge, existing algorithms have researched only on the first three modules,\nyet neglecting the backpropagation module. They all adopt a kind of two\nbackpropagation strategy, which are based on an implicit assumption of equally\ntreating users and items in the training phase. In this paper, we challenge\nsuch an equal training assumption and propose a novel one backpropagation\nupdating strategy, which keeps the normal gradient backpropagation for the item\nencoding tower, but cuts off the backpropagation for the user encoding tower.\nInstead, we propose a moving-aggregation updating strategy to update a user\nencoding in each training epoch. Except the proposed backpropagation updating\nmodule, we implement the other three modules with the most straightforward\nchoices. Experiments on four public datasets validate the effectiveness and\nefficiency of our model in terms of improved recommendation performance and\nreduced computation overload over the state-of-the-art competitors.\n","authors":["Erjia Chen","Bang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18227v1.pdf","comment":"9 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.16915v3","updated":"2024-03-27T01:53:36Z","published":"2024-03-25T16:32:50Z","title":"Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language\n  Models","summary":"  Fine-tuning in information retrieval systems using pre-trained language\nmodels (PLM-based IR) requires learning query representations and\nquery-document relations, in addition to downstream task-specific learning.\nThis study introduces coarse-tuning as an intermediate learning stage that\nbridges pre-training and fine-tuning. By learning query representations and\nquery-document relations in coarse-tuning, we aim to reduce the load of\nfine-tuning and improve the learning effect of downstream IR tasks. We propose\nQuery-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the\nappropriateness of query-document pairs. Evaluation experiments show that the\nproposed method significantly improves MRR and/or nDCG@5 in four ad-hoc\ndocument retrieval datasets. Furthermore, the results of the query prediction\ntask suggested that coarse-tuning facilitated learning of query representation\nand query-document relations.\n","authors":["Atsushi Keyaki","Ribeka Keyaki"],"pdf_url":"https://arxiv.org/pdf/2403.16915v3.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18183v1","updated":"2024-03-27T01:21:48Z","published":"2024-03-27T01:21:48Z","title":"Can AI Models Appreciate Document Aesthetics? An Exploration of\n  Legibility and Layout Quality in Relation to Prediction Confidence","summary":"  A well-designed document communicates not only through its words but also\nthrough its visual eloquence. Authors utilize aesthetic elements such as\ncolors, fonts, graphics, and layouts to shape the perception of information.\nThoughtful document design, informed by psychological insights, enhances both\nthe visual appeal and the comprehension of the content. While state-of-the-art\ndocument AI models demonstrate the benefits of incorporating layout and image\ndata, it remains unclear whether the nuances of document aesthetics are\neffectively captured. To bridge the gap between human cognition and AI\ninterpretation of aesthetic elements, we formulated hypotheses concerning AI\nbehavior in document understanding tasks, specifically anchored in document\ndesign principles. With a focus on legibility and layout quality, we tested\nfour aspects of aesthetic effects: noise, font-size contrast, alignment, and\ncomplexity, on model confidence using correlational analysis. The results and\nobservations highlight the value of model analysis rooted in document design\ntheories. Our work serves as a trailhead for further studies and we advocate\nfor continued research in this topic to deepen our understanding of how AI\ninterprets document aesthetics.\n","authors":["Hsiu-Wei Yang","Abhinav Agrawal","Pavlos Fragkogiannis","Shubham Nitin Mulay"],"pdf_url":"https://arxiv.org/pdf/2403.18183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18173v1","updated":"2024-03-27T01:01:09Z","published":"2024-03-27T01:01:09Z","title":"LLMs in HCI Data Work: Bridging the Gap Between Information Retrieval\n  and Responsible Research Practices","summary":"  Efficient and accurate information extraction from scientific papers is\nsignificant in the rapidly developing human-computer interaction research in\nthe literature review process. Our paper introduces and analyses a new\ninformation retrieval system using state-of-the-art Large Language Models\n(LLMs) in combination with structured text analysis techniques to extract\nexperimental data from HCI literature, emphasizing key elements. Then We\nanalyze the challenges and risks of using LLMs in the world of research. We\nperformed a comprehensive analysis on our conducted dataset, which contained\nthe specified information of 300 CHI 2020-2022 papers, to evaluate the\nperformance of the two large language models, GPT-3.5 (text-davinci-003) and\nLlama-2-70b, paired with structured text analysis techniques. The GPT-3.5 model\ngains an accuracy of 58\\% and a mean absolute error of 7.00. In contrast, the\nLlama2 model indicates an accuracy of 56\\% with a mean absolute error of 7.63.\nThe ability to answer questions was also included in the system in order to\nwork with streamlined data. By evaluating the risks and opportunities presented\nby LLMs, our work contributes to the ongoing dialogue on establishing\nmethodological validity and ethical guidelines for LLM use in HCI data work.\n","authors":["Neda Taghizadeh Serajeh","Iman Mohammadi","Vittorio Fuccella","Mattia De Rosa"],"pdf_url":"https://arxiv.org/pdf/2403.18173v1.pdf","comment":"5 pages, CHI2024 Workshop on LLMs as Research Tools: Applications and\n  Evaluations in HCI Data Work"},{"id":"http://arxiv.org/abs/2204.11970v3","updated":"2024-03-27T22:02:30Z","published":"2022-04-25T21:20:27Z","title":"Visual Acuity Prediction on Real-Life Patient Data Using a Machine\n  Learning Based Multistage System","summary":"  In ophthalmology, intravitreal operative medication therapy (IVOM) is a\nwidespread treatment for diseases related to the age-related macular\ndegeneration (AMD), the diabetic macular edema (DME), as well as the retinal\nvein occlusion (RVO). However, in real-world settings, patients often suffer\nfrom loss of vision on time scales of years despite therapy, whereas the\nprediction of the visual acuity (VA) and the earliest possible detection of\ndeterioration under real-life conditions is challenging due to heterogeneous\nand incomplete data. In this contribution, we present a workflow for the\ndevelopment of a research-compatible data corpus fusing different IT systems of\nthe department of ophthalmology of a German maximum care hospital. The\nextensive data corpus allows predictive statements of the expected progression\nof a patient and his or her VA in each of the three diseases. For the disease\nAMD, we found out a significant deterioration of the visual acuity over time.\nWithin our proposed multistage system, we subsequently classify the VA\nprogression into the three groups of therapy \"winners\", \"stabilizers\", and\n\"losers\" (WSL classification scheme). Our OCT biomarker classification using an\nensemble of deep neural networks results in a classification accuracy\n(F1-score) of over 98 %, enabling us to complete incomplete OCT documentations\nwhile allowing us to exploit them for a more precise VA modelling process. Our\nVA prediction requires at least four VA examinations and optionally OCT\nbiomarkers from the same time period to predict the VA progression within a\nforecasted time frame, whereas our prediction is currently restricted to IVOM /\nno therapy. We achieve a final prediction accuracy of 69 % in macro average\nF1-score, while being in the same range as the ophthalmologists with 57.8 and\n50 +- 10.7 % F1-score.\n","authors":["Tobias Schlosser","Frederik Beuth","Trixy Meyer","Arunodhayan Sampath Kumar","Gabriel Stolze","Olga Furashova","Katrin Engelmann","Danny Kowerko"],"pdf_url":"https://arxiv.org/pdf/2204.11970v3.pdf","comment":"Preprint for journal Scientific Reports (Springer)"},{"id":"http://arxiv.org/abs/2403.12984v2","updated":"2024-03-27T21:51:03Z","published":"2024-03-03T11:09:32Z","title":"When SMILES have Language: Drug Classification using Text Classification\n  Methods on Drug SMILES Strings","summary":"  Complex chemical structures, like drugs, are usually defined by SMILES\nstrings as a sequence of molecules and bonds. These SMILES strings are used in\ndifferent complex machine learning-based drug-related research and\nrepresentation works. Escaping from complex representation, in this work, we\npose a single question: What if we treat drug SMILES as conventional sentences\nand engage in text classification for drug classification? Our experiments\naffirm the possibility with very competitive scores. The study explores the\nnotion of viewing each atom and bond as sentence components, employing basic\nNLP methods to categorize drug types, proving that complex problems can also be\nsolved with simpler perspectives. The data and code are available here:\nhttps://github.com/azminewasi/Drug-Classification-NLP.\n","authors":["Azmine Toushik Wasi","Šerbetar Karlo","Raima Islam","Taki Hasan Rafi","Dong-Kyu Chae"],"pdf_url":"https://arxiv.org/pdf/2403.12984v2.pdf","comment":"7 pages, 2 figures, 5 tables, Accepted (invited to present) to the\n  The Second Tiny Papers Track at ICLR 2024\n  (https://openreview.net/forum?id=VUYCyH8fCw)"},{"id":"http://arxiv.org/abs/2403.17210v2","updated":"2024-03-27T21:47:49Z","published":"2024-03-25T21:37:31Z","title":"CADGL: Context-Aware Deep Graph Learning for Predicting Drug-Drug\n  Interactions","summary":"  Examining Drug-Drug Interactions (DDIs) is a pivotal element in the process\nof drug development. DDIs occur when one drug's properties are affected by the\ninclusion of other drugs. Detecting favorable DDIs has the potential to pave\nthe way for creating and advancing innovative medications applicable in\npractical settings. However, existing DDI prediction models continue to face\nchallenges related to generalization in extreme cases, robust feature\nextraction, and real-life application possibilities. We aim to address these\nchallenges by leveraging the effectiveness of context-aware deep graph learning\nby introducing a novel framework named CADGL. Based on a customized variational\ngraph autoencoder (VGAE), we capture critical structural and physio-chemical\ninformation using two context preprocessors for feature extraction from two\ndifferent perspectives: local neighborhood and molecular context, in a\nheterogeneous graphical structure. Our customized VGAE consists of a graph\nencoder, a latent information encoder, and an MLP decoder. CADGL surpasses\nother state-of-the-art DDI prediction models, excelling in predicting\nclinically valuable novel DDIs, supported by rigorous case studies.\n","authors":["Azmine Toushik Wasi","Taki Hasan Rafi","Raima Islam","Serbetar Karlo","Dong-Kyu Chae"],"pdf_url":"https://arxiv.org/pdf/2403.17210v2.pdf","comment":"8 Pages, 4 Figures; In review"},{"id":"http://arxiv.org/abs/2403.19021v1","updated":"2024-03-27T21:22:37Z","published":"2024-03-27T21:22:37Z","title":"Towards LLM-RecSys Alignment with Textual ID Learning","summary":"  Generative recommendation based on Large Language Models (LLMs) have\ntransformed the traditional ranking-based recommendation style into a\ntext-to-text generation paradigm. However, in contrast to standard NLP tasks\nthat inherently operate on human vocabulary, current research in generative\nrecommendations struggles to effectively encode recommendation items within the\ntext-to-text framework using concise yet meaningful ID representations. To\nbetter align LLMs with recommendation needs, we propose IDGen, representing\neach item as a unique, concise, semantically rich, platform-agnostic textual ID\nusing human language tokens. This is achieved by training a textual ID\ngenerator alongside the LLM-based recommender, enabling seamless integration of\npersonalized recommendations into natural language generation. Notably, as user\nhistory is expressed in natural language and decoupled from the original\ndataset, our approach suggests the potential for a foundational generative\nrecommendation model. Experiments show that our framework consistently\nsurpasses existing models in sequential recommendation under standard\nexperimental setting. Then, we explore the possibility of training a foundation\nrecommendation model with the proposed method on data collected from 19\ndifferent datasets and tested its recommendation performance on 6 unseen\ndatasets across different platforms under a completely zero-shot setting. The\nresults show that the zero-shot performance of the pre-trained foundation model\nis comparable to or even better than some traditional recommendation models\nbased on supervised training, showing the potential of the IDGen paradigm\nserving as the foundation model for generative recommendation. Code and data\nare open-sourced at https://github.com/agiresearch/IDGenRec.\n","authors":["Juntao Tan","Shuyuan Xu","Wenyue Hua","Yingqiang Ge","Zelong Li","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.19021v1.pdf","comment":"Accepted in SIGIR 2024"},{"id":"http://arxiv.org/abs/2403.10667v2","updated":"2024-03-27T21:11:19Z","published":"2024-03-15T20:21:31Z","title":"Towards Unified Multi-Modal Personalization: Large Vision-Language\n  Models for Generative Recommendation and Beyond","summary":"  Developing a universal model that can effectively harness heterogeneous\nresources and respond to a wide range of personalized needs has been a\nlongstanding community aspiration. Our daily choices, especially in domains\nlike fashion and retail, are substantially shaped by multi-modal data, such as\npictures and textual descriptions. These modalities not only offer intuitive\nguidance but also cater to personalized user preferences. However, the\npredominant personalization approaches mainly focus on the ID or text-based\nrecommendation problem, failing to comprehend the information spanning various\ntasks or modalities. In this paper, our goal is to establish a Unified paradigm\nfor Multi-modal Personalization systems (UniMP), which effectively leverages\nmulti-modal data while eliminating the complexities associated with task- and\nmodality-specific customization. We argue that the advancements in foundational\ngenerative modeling have provided the flexibility and effectiveness necessary\nto achieve the objective. In light of this, we develop a generic and extensible\npersonalization generative framework, that can handle a wide range of\npersonalized needs including item recommendation, product search, preference\nprediction, explanation generation, and further user-guided image generation.\nOur methodology enhances the capabilities of foundational language models for\npersonalized tasks by seamlessly ingesting interleaved cross-modal user history\ninformation, ensuring a more precise and customized experience for users. To\ntrain and evaluate the proposed multi-modal personalized tasks, we also\nintroduce a novel and comprehensive benchmark covering a variety of user\nrequirements. Our experiments on the real-world benchmark showcase the model's\npotential, outperforming competitive methods specialized for each task.\n","authors":["Tianxin Wei","Bowen Jin","Ruirui Li","Hansi Zeng","Zhengyang Wang","Jianhui Sun","Qingyu Yin","Hanqing Lu","Suhang Wang","Jingrui He","Xianfeng Tang"],"pdf_url":"https://arxiv.org/pdf/2403.10667v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.18962v1","updated":"2024-03-27T19:12:24Z","published":"2024-03-27T19:12:24Z","title":"High Recall, Small Data: The Challenges of Within-System Evaluation in a\n  Live Legal Search System","summary":"  This paper illustrates some challenges of common ranking evaluation methods\nfor legal information retrieval (IR). We show these challenges with log data\nfrom a live legal search system and two user studies. We provide an overview of\naspects of legal IR, and the implications of these aspects for the expected\nchallenges of common evaluation methods: test collections based on explicit and\nimplicit feedback, user surveys, and A/B testing. Next, we illustrate the\nchallenges of common evaluation methods using data from a live, commercial,\nlegal search engine. We specifically focus on methods for monitoring the\neffectiveness of (continuous) changes to document ranking by a single IR system\nover time. We show how the combination of characteristics in legal IR systems\nand limited user data can lead to challenges that cause the common evaluation\nmethods discussed to be sub-optimal. In our future work we will therefore focus\non less common evaluation methods, such as cost-based evaluation models.\n","authors":["Gineke Wiggers","Suzan Verberne","Arjen de Vries","Roel van der Burg"],"pdf_url":"https://arxiv.org/pdf/2403.18962v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19716v1","updated":"2024-03-27T17:41:16Z","published":"2024-03-27T17:41:16Z","title":"Capability-aware Prompt Reformulation Learning for Text-to-Image\n  Generation","summary":"  Text-to-image generation systems have emerged as revolutionary tools in the\nrealm of artistic creation, offering unprecedented ease in transforming textual\nprompts into visual art. However, the efficacy of these systems is intricately\nlinked to the quality of user-provided prompts, which often poses a challenge\nto users unfamiliar with prompt crafting. This paper addresses this challenge\nby leveraging user reformulation data from interaction logs to develop an\nautomatic prompt reformulation model. Our in-depth analysis of these logs\nreveals that user prompt reformulation is heavily dependent on the individual\nuser's capability, resulting in significant variance in the quality of\nreformulation pairs. To effectively use this data for training, we introduce\nthe Capability-aware Prompt Reformulation (CAPR) framework. CAPR innovatively\nintegrates user capability into the reformulation process through two key\ncomponents: the Conditional Reformulation Model (CRM) and Configurable\nCapability Features (CCF). CRM reformulates prompts according to a specified\nuser capability, as represented by CCF. The CCF, in turn, offers the\nflexibility to tune and guide the CRM's behavior. This enables CAPR to\neffectively learn diverse reformulation strategies across various user\ncapacities and to simulate high-capability user reformulation during inference.\nExtensive experiments on standard text-to-image generation benchmarks showcase\nCAPR's superior performance over existing baselines and its remarkable\nrobustness on unseen systems. Furthermore, comprehensive analyses validate the\neffectiveness of different components. CAPR can facilitate user-friendly\ninteraction with text-to-image systems and make advanced artistic creation more\nachievable for a broader range of users.\n","authors":["Jingtao Zhan","Qingyao Ai","Yiqun Liu","Jia Chen","Shaoping Ma"],"pdf_url":"https://arxiv.org/pdf/2403.19716v1.pdf","comment":"Accepted at SIGIR 2024"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2403.18807v1","updated":"2024-03-27T17:53:30Z","published":"2024-03-27T17:53:30Z","title":"ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth\n  Estimation","summary":"  In the absence of parallax cues, a learning-based single image depth\nestimation (SIDE) model relies heavily on shading and contextual cues in the\nimage. While this simplicity is attractive, it is necessary to train such\nmodels on large and varied datasets, which are difficult to capture. It has\nbeen shown that using embeddings from pre-trained foundational models, such as\nCLIP, improves zero shot transfer in several applications. Taking inspiration\nfrom this, in our paper we explore the use of global image priors generated\nfrom a pre-trained ViT model to provide more detailed contextual information.\nWe argue that the embedding vector from a ViT model, pre-trained on a large\ndataset, captures greater relevant information for SIDE than the usual route of\ngenerating pseudo image captions, followed by CLIP based text embeddings. Based\non this idea, we propose a new SIDE model using a diffusion backbone which is\nconditioned on ViT embeddings. Our proposed design establishes a new\nstate-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of\n0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on\nKITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to\n0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model\ntrained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%)\nover NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%,\n18%, 45%, 9%) by ZoeDepth. The code is available at\nhttps://github.com/Aradhye2002/EcoDepth.\n","authors":["Suraj Patni","Aradhye Agarwal","Chetan Arora"],"pdf_url":"https://arxiv.org/pdf/2403.18807v1.pdf","comment":"Accepted at IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2024"},{"id":"http://arxiv.org/abs/2403.18802v1","updated":"2024-03-27T17:48:55Z","published":"2024-03-27T17:48:55Z","title":"Long-form factuality in large language models","summary":"  Large language models (LLMs) often generate content that contains factual\nerrors when responding to fact-seeking prompts on open-ended topics. To\nbenchmark a model's long-form factuality in open domains, we first use GPT-4 to\ngenerate LongFact, a prompt set comprising thousands of questions spanning 38\ntopics. We then propose that LLM agents can be used as automated evaluators for\nlong-form factuality through a method which we call Search-Augmented Factuality\nEvaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into\na set of individual facts and to evaluate the accuracy of each fact using a\nmulti-step reasoning process comprising sending search queries to Google Search\nand determining whether a fact is supported by the search results. Furthermore,\nwe propose extending F1 score as an aggregated metric for long-form factuality.\nTo do so, we balance the percentage of supported facts in a response\n(precision) with the percentage of provided facts relative to a hyperparameter\nrepresenting a user's preferred response length (recall).\n  Empirically, we demonstrate that LLM agents can achieve superhuman rating\nperformance - on a set of ~16k individual facts, SAFE agrees with crowdsourced\nhuman annotators 72% of the time, and on a random subset of 100 disagreement\ncases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times\ncheaper than human annotators. We also benchmark thirteen language models on\nLongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding\nthat larger language models generally achieve better long-form factuality.\nLongFact, SAFE, and all experimental code are available at\nhttps://github.com/google-deepmind/long-form-factuality.\n","authors":["Jerry Wei","Chengrun Yang","Xinying Song","Yifeng Lu","Nathan Hu","Dustin Tran","Daiyi Peng","Ruibo Liu","Da Huang","Cosmo Du","Quoc V. Le"],"pdf_url":"https://arxiv.org/pdf/2403.18802v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13525v2","updated":"2024-03-27T17:47:56Z","published":"2023-05-22T22:41:49Z","title":"A 4D Hybrid Algorithm to Scale Parallel Training to Thousands of GPUs","summary":"  Large communication costs are a critical bottleneck in training\nstate-of-the-art neural networks on distributed systems. This paper introduces\nAxoNN, a novel four-dimensional (4D) parallelization approach, inspired by\nAgarwal's algorithm for matrix multiplication, for parallelizing tensor\ncomputations in deep learning, AxoNN employs two key strategies to minimize\ncommunication overhead. First, we optimize communication by overlapping\nexpensive collective operations (reduce-scatter, all-gather, all-reduce) with\ncomputations. Our experiments with a 20-billion parameter transformer model\ndemonstrate that these optimizations deliver nearly 53\\% improvement. Second,\nwe present an analytical model to assist users in identifying\ncommunication-minimizing configurations within the vast search space defined by\nour 4D algorithm. This model empowers practitioners by simplifying the tuning\nprocess for their specific training workloads. When training an 80-billion\nparameter model on 1024 GPUs of Perlmutter, AxoNN surpasses Megatron-LM, a\nstate-of-the-art framework, by a significant 26%. Additionally, it achieves 57%\nof the theoretical peak FLOP/s.\n","authors":["Siddharth Singh","Prajwal Singhania","Aditya K. Ranjan","Zack Sating","Abhinav Bhatele"],"pdf_url":"https://arxiv.org/pdf/2305.13525v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.13483v4","updated":"2024-03-27T17:38:27Z","published":"2023-02-27T02:42:27Z","title":"CrystalBox: Future-Based Explanations for Input-Driven Deep RL Systems","summary":"  We present CrystalBox, a novel, model-agnostic, posthoc explainability\nframework for Deep Reinforcement Learning (DRL) controllers in the large family\nof input-driven environments which includes computer systems. We combine the\nnatural decomposability of reward functions in input-driven environments with\nthe explanatory power of decomposed returns. We propose an efficient algorithm\nto generate future-based explanations across both discrete and continuous\ncontrol environments. Using applications such as adaptive bitrate streaming and\ncongestion control, we demonstrate CrystalBox's capability to generate\nhigh-fidelity explanations. We further illustrate its higher utility across\nthree practical use cases: contrastive explanations, network observability, and\nguided reward design, as opposed to prior explainability techniques that\nidentify salient features.\n","authors":["Sagar Patel","Sangeetha Abdu Jyothi","Nina Narodytska"],"pdf_url":"https://arxiv.org/pdf/2302.13483v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18775v1","updated":"2024-03-27T17:23:39Z","published":"2024-03-27T17:23:39Z","title":"ImageNet-D: Benchmarking Neural Network Robustness on Diffusion\n  Synthetic Object","summary":"  We establish rigorous benchmarks for visual perception robustness. Synthetic\nimages such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific\ntype of evaluation over synthetic corruptions, backgrounds, and textures, yet\nthose robustness benchmarks are restricted in specified variations and have low\nsynthetic quality. In this work, we introduce generative model as a data source\nfor synthesizing hard images that benchmark deep models' robustness. Leveraging\ndiffusion models, we are able to generate images with more diversified\nbackgrounds, textures, and materials than any prior work, where we term this\nbenchmark as ImageNet-D. Experimental results show that ImageNet-D results in a\nsignificant accuracy drop to a range of vision models, from the standard ResNet\nvisual classifier to the latest foundation models like CLIP and MiniGPT-4,\nsignificantly reducing their accuracy by up to 60\\%. Our work suggests that\ndiffusion models can be an effective source to test vision models. The code and\ndataset are available at https://github.com/chenshuang-zhang/imagenet_d.\n","authors":["Chenshuang Zhang","Fei Pan","Junmo Kim","In So Kweon","Chengzhi Mao"],"pdf_url":"https://arxiv.org/pdf/2403.18775v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2309.04381v2","updated":"2024-03-27T17:07:47Z","published":"2023-09-08T15:23:40Z","title":"Generalization Bounds: Perspectives from Information Theory and\n  PAC-Bayes","summary":"  A fundamental question in theoretical machine learning is generalization.\nOver the past decades, the PAC-Bayesian approach has been established as a\nflexible framework to address the generalization capabilities of machine\nlearning algorithms, and design new ones. Recently, it has garnered increased\ninterest due to its potential applicability for a variety of learning\nalgorithms, including deep neural networks. In parallel, an\ninformation-theoretic view of generalization has developed, wherein the\nrelation between generalization and various information measures has been\nestablished. This framework is intimately connected to the PAC-Bayesian\napproach, and a number of results have been independently discovered in both\nstrands. In this monograph, we highlight this strong connection and present a\nunified treatment of PAC-Bayesian and information-theoretic generalization\nbounds. We present techniques and results that the two perspectives have in\ncommon, and discuss the approaches and interpretations that differ. In\nparticular, we demonstrate how many proofs in the area share a modular\nstructure, through which the underlying ideas can be intuited. We pay special\nattention to the conditional mutual information (CMI) framework; analytical\nstudies of the information complexity of learning algorithms; and the\napplication of the proposed methods to deep learning. This monograph is\nintended to provide a comprehensive introduction to information-theoretic\ngeneralization bounds and their connection to PAC-Bayes, serving as a\nfoundation from which the most recent developments are accessible. It is aimed\nbroadly towards researchers with an interest in generalization and theoretical\nmachine learning.\n","authors":["Fredrik Hellström","Giuseppe Durisi","Benjamin Guedj","Maxim Raginsky"],"pdf_url":"https://arxiv.org/pdf/2309.04381v2.pdf","comment":"228 pages"},{"id":"http://arxiv.org/abs/2403.06054v4","updated":"2024-03-27T17:06:10Z","published":"2024-03-10T00:47:05Z","title":"Decoupled Data Consistency with Diffusion Purification for Image\n  Restoration","summary":"  Diffusion models have recently gained traction as a powerful class of deep\ngenerative priors, excelling in a wide range of image restoration tasks due to\ntheir exceptional ability to model data distributions. To solve image\nrestoration problems, many existing techniques achieve data consistency by\nincorporating additional likelihood gradient steps into the reverse sampling\nprocess of diffusion models. However, the additional gradient steps pose a\nchallenge for real-world practical applications as they incur a large\ncomputational overhead, thereby increasing inference time. They also present\nadditional difficulties when using accelerated diffusion model samplers, as the\nnumber of data consistency steps is limited by the number of reverse sampling\nsteps. In this work, we propose a novel diffusion-based image restoration\nsolver that addresses these issues by decoupling the reverse process from the\ndata consistency steps. Our method involves alternating between a\nreconstruction phase to maintain data consistency and a refinement phase that\nenforces the prior via diffusion purification. Our approach demonstrates\nversatility, making it highly adaptable for efficient problem-solving in latent\nspace. Additionally, it reduces the necessity for numerous sampling steps\nthrough the integration of consistency models. The efficacy of our approach is\nvalidated through comprehensive experiments across various image restoration\ntasks, including image denoising, deblurring, inpainting, and super-resolution.\n","authors":["Xiang Li","Soo Min Kwon","Ismail R. Alkhouri","Saiprasad Ravishankar","Qing Qu"],"pdf_url":"https://arxiv.org/pdf/2403.06054v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18766v1","updated":"2024-03-27T17:05:03Z","published":"2024-03-27T17:05:03Z","title":"Superior Parallel Big Data Clustering through Competitive Stochastic\n  Sample Size Optimization in Big-means","summary":"  This paper introduces a novel K-means clustering algorithm, an advancement on\nthe conventional Big-means methodology. The proposed method efficiently\nintegrates parallel processing, stochastic sampling, and competitive\noptimization to create a scalable variant designed for big data applications.\nIt addresses scalability and computation time challenges typically faced with\ntraditional techniques. The algorithm adjusts sample sizes dynamically for each\nworker during execution, optimizing performance. Data from these sample sizes\nare continually analyzed, facilitating the identification of the most efficient\nconfiguration. By incorporating a competitive element among workers using\ndifferent sample sizes, efficiency within the Big-means algorithm is further\nstimulated. In essence, the algorithm balances computational time and\nclustering quality by employing a stochastic, competitive sampling strategy in\na parallel computing setting.\n","authors":["Rustam Mussabayev","Ravil Mussabayev"],"pdf_url":"https://arxiv.org/pdf/2403.18766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18765v1","updated":"2024-03-27T17:03:31Z","published":"2024-03-27T17:03:31Z","title":"CaT: Constraints as Terminations for Legged Locomotion Reinforcement\n  Learning","summary":"  Deep Reinforcement Learning (RL) has demonstrated impressive results in\nsolving complex robotic tasks such as quadruped locomotion. Yet, current\nsolvers fail to produce efficient policies respecting hard constraints. In this\nwork, we advocate for integrating constraints into robot learning and present\nConstraints as Terminations (CaT), a novel constrained RL algorithm. Departing\nfrom classical constrained RL formulations, we reformulate constraints through\nstochastic terminations during policy learning: any violation of a constraint\ntriggers a probability of terminating potential future rewards the RL agent\ncould attain. We propose an algorithmic approach to this formulation, by\nminimally modifying widely used off-the-shelf RL algorithms in robot learning\n(such as Proximal Policy Optimization). Our approach leads to excellent\nconstraint adherence without introducing undue complexity and computational\noverhead, thus mitigating barriers to broader adoption. Through empirical\nevaluation on the real quadruped robot Solo crossing challenging obstacles, we\ndemonstrate that CaT provides a compelling solution for incorporating\nconstraints into RL frameworks. Videos and code are available at\nhttps://constraints-as-terminations.github.io.\n","authors":["Elliot Chane-Sane","Pierre-Alexandre Leziart","Thomas Flayols","Olivier Stasse","Philippe Souères","Nicolas Mansard"],"pdf_url":"https://arxiv.org/pdf/2403.18765v1.pdf","comment":"Project webpage: https://constraints-as-terminations.github.io"},{"id":"http://arxiv.org/abs/2311.01483v3","updated":"2024-03-27T16:56:23Z","published":"2023-11-02T14:47:06Z","title":"FedSN: A Novel Federated Learning Framework over LEO Satellite Networks","summary":"  Recently, a large number of Low Earth Orbit (LEO) satellites have been\nlaunched and deployed successfully in space by commercial companies, such as\nSpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve\nnot only for communication but also for various machine learning applications,\nsuch as space modulation recognition, remote sensing image classification, etc.\nHowever, the ground station (GS) may be incapable of downloading such a large\nvolume of raw sensing data for centralized model training due to the limited\ncontact time with LEO satellites (e.g. 5 minutes). Therefore, federated\nlearning (FL) has emerged as the promising solution to address this problem via\non-device training. Unfortunately, to enable FL on LEO satellites, we still\nface three critical challenges that are i) heterogeneous computing and memory\ncapabilities, ii) limited uplink rate, and iii) model staleness. To this end,\nwe propose FedSN as a general FL framework to tackle the above challenges, and\nfully explore data diversity on LEO satellites. Specifically, we first present\na novel sub-structure scheme to enable heterogeneous local model training\nconsidering different computing, memory, and communication constraints on LEO\nsatellites. Additionally, we propose a pseudo-synchronous model aggregation\nstrategy to dynamically schedule model aggregation for compensating model\nstaleness. To further demonstrate the effectiveness of the FedSN, we evaluate\nit using space modulation recognition and remote sensing image classification\ntasks by leveraging the data from real-world satellite networks. Extensive\nexperimental results demonstrate that FedSN framework achieves higher accuracy,\nlower computing, and communication overhead than the state-of-the-art\nbenchmarks and the effectiveness of each components in FedSN.\n","authors":["Zheng Lin","Zhe Chen","Zihan Fang","Xianhao Chen","Xiong Wang","Yue Gao"],"pdf_url":"https://arxiv.org/pdf/2311.01483v3.pdf","comment":"14 pages, 17 figures"},{"id":"http://arxiv.org/abs/2403.18756v1","updated":"2024-03-27T16:56:14Z","published":"2024-03-27T16:56:14Z","title":"Detection of subclinical atherosclerosis by image-based deep learning on\n  chest x-ray","summary":"  Aims. To develop a deep-learning based system for recognition of subclinical\natherosclerosis on a plain frontal chest x-ray. Methods and Results. A\ndeep-learning algorithm to predict coronary artery calcium (CAC) score (the\nAI-CAC model) was developed on 460 chest x-ray (80% training cohort, 20%\ninternal validation cohort) of primary prevention patients (58.4% male, median\nage 63 [51-74] years) with available paired chest x-ray and chest computed\ntomography (CT) indicated for any clinical reason and performed within 3\nmonths. The CAC score calculated on chest CT was used as ground truth. The\nmodel was validated on an temporally-independent cohort of 90 patients from the\nsame institution (external validation). The diagnostic accuracy of the AI-CAC\nmodel assessed by the area under the curve (AUC) was the primary outcome.\nOverall, median AI-CAC score was 35 (0-388) and 28.9% patients had no AI-CAC.\nAUC of the AI-CAC model to identify a CAC>0 was 0.90 in the internal validation\ncohort and 0.77 in the external validation cohort. Sensitivity was consistently\nabove 92% in both cohorts. In the overall cohort (n=540), among patients with\nAI-CAC=0, a single ASCVD event occurred, after 4.3 years. Patients with\nAI-CAC>0 had significantly higher Kaplan Meier estimates for ASCVD events\n(13.5% vs. 3.4%, log-rank=0.013). Conclusion. The AI-CAC model seems to\naccurately detect subclinical atherosclerosis on chest x-ray with elevated\nsensitivity, and to predict ASCVD events with elevated negative predictive\nvalue. Adoption of the AI-CAC model to refine CV risk stratification or as an\nopportunistic screening tool requires prospective evaluation.\n","authors":["Guglielmo Gallone","Francesco Iodice","Alberto Presta","Davide Tore","Ovidio de Filippo","Michele Visciano","Carlo Alberto Barbano","Alessandro Serafini","Paola Gorrini","Alessandro Bruno","Walter Grosso Marra","James Hughes","Mario Iannaccone","Paolo Fonio","Attilio Fiandrotti","Alessandro Depaoli","Marco Grangetto","Gaetano Maria de Ferrari","Fabrizio D'Ascenzo"],"pdf_url":"https://arxiv.org/pdf/2403.18756v1.pdf","comment":"Submitted to European Heart Journal - Cardiovascular Imaging Added\n  also the additional material 44 pages (30 main paper, 14 additional\n  material), 14 figures (5 main manuscript, 9 additional material)"},{"id":"http://arxiv.org/abs/2403.14623v2","updated":"2024-03-27T16:49:35Z","published":"2024-03-21T17:59:41Z","title":"Simplified Diffusion Schrödinger Bridge","summary":"  This paper introduces a novel theoretical simplification of the Diffusion\nSchr\\\"odinger Bridge (DSB) that facilitates its unification with Score-based\nGenerative Models (SGMs), addressing the limitations of DSB in complex data\ngeneration and enabling faster convergence and enhanced performance. By\nemploying SGMs as an initial solution for DSB, our approach capitalizes on the\nstrengths of both frameworks, ensuring a more efficient training process and\nimproving the performance of SGM. We also propose a reparameterization\ntechnique that, despite theoretical approximations, practically improves the\nnetwork's fitting capabilities. Our extensive experimental evaluations confirm\nthe effectiveness of the simplified DSB, demonstrating its significant\nimprovements. We believe the contributions of this work pave the way for\nadvanced generative modeling. The code is available at\nhttps://github.com/checkcrab/SDSB.\n","authors":["Zhicong Tang","Tiankai Hang","Shuyang Gu","Dong Chen","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2403.14623v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.03683v2","updated":"2024-03-27T16:44:22Z","published":"2023-11-07T03:19:16Z","title":"Preventing Arbitrarily High Confidence on Far-Away Data in\n  Point-Estimated Discriminative Neural Networks","summary":"  Discriminatively trained, deterministic neural networks are the de facto\nchoice for classification problems. However, even though they achieve\nstate-of-the-art results on in-domain test sets, they tend to be overconfident\non out-of-distribution (OOD) data. For instance, ReLU networks - a popular\nclass of neural network architectures - have been shown to almost always yield\nhigh confidence predictions when the test data are far away from the training\nset, even when they are trained with OOD data. We overcome this problem by\nadding a term to the output of the neural network that corresponds to the logit\nof an extra class, that we design to dominate the logits of the original\nclasses as we move away from the training data.This technique provably prevents\narbitrarily high confidence on far-away test data while maintaining a simple\ndiscriminative point-estimate training. Evaluation on various benchmarks\ndemonstrates strong performance against competitive baselines on both far-away\nand realistic OOD data.\n","authors":["Ahmad Rashid","Serena Hacker","Guojun Zhang","Agustinus Kristiadi","Pascal Poupart"],"pdf_url":"https://arxiv.org/pdf/2311.03683v2.pdf","comment":"Accepted at AISTATS 2024"},{"id":"http://arxiv.org/abs/2403.18742v1","updated":"2024-03-27T16:39:28Z","published":"2024-03-27T16:39:28Z","title":"Understanding the Learning Dynamics of Alignment with Human Feedback","summary":"  Aligning large language models (LLMs) with human intentions has become a\ncritical task for safely deploying models in real-world systems. While existing\nalignment approaches have seen empirical success, theoretically understanding\nhow these methods affect model behavior remains an open question. Our work\nprovides an initial attempt to theoretically analyze the learning dynamics of\nhuman preference alignment. We formally show how the distribution of preference\ndatasets influences the rate of model updates and provide rigorous guarantees\non the training accuracy. Our theory also reveals an intricate phenomenon where\nthe optimization is prone to prioritizing certain behaviors with higher\npreference distinguishability. We empirically validate our findings on\ncontemporary LLMs and alignment tasks, reinforcing our theoretical insights and\nshedding light on considerations for future alignment approaches. Disclaimer:\nThis paper contains potentially offensive text; reader discretion is advised.\n","authors":["Shawn Im","Yixuan Li"],"pdf_url":"https://arxiv.org/pdf/2403.18742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18739v1","updated":"2024-03-27T16:32:32Z","published":"2024-03-27T16:32:32Z","title":"Usage-Specific Survival Modeling Based on Operational Data and Neural\n  Networks","summary":"  Accurate predictions of when a component will fail are crucial when planning\nmaintenance, and by modeling the distribution of these failure times, survival\nmodels have shown to be particularly useful in this context. The presented\nmethodology is based on conventional neural network-based survival models that\nare trained using data that is continuously gathered and stored at specific\ntimes, called snapshots. An important property of this type of training data is\nthat it can contain more than one snapshot from a specific individual which\nresults in that standard maximum likelihood training can not be directly\napplied since the data is not independent. However, the papers show that if the\ndata is in a specific format where all snapshot times are the same for all\nindividuals, called homogeneously sampled, maximum likelihood training can be\napplied and produce desirable results. In many cases, the data is not\nhomogeneously sampled and in this case, it is proposed to resample the data to\nmake it homogeneously sampled. How densely the dataset is sampled turns out to\nbe an important parameter; it should be chosen large enough to produce good\nresults, but this also increases the size of the dataset which makes training\nslow. To reduce the number of samples needed during training, the paper also\nproposes a technique to, instead of resampling the dataset once before the\ntraining starts, randomly resample the dataset at the start of each epoch\nduring the training. The proposed methodology is evaluated on both a simulated\ndataset and an experimental dataset of starter battery failures. The results\nshow that if the data is homogeneously sampled the methodology works as\nintended and produces accurate survival models. The results also show that\nrandomly resampling the dataset on each epoch is an effective way to reduce the\nsize of the training data.\n","authors":["Olov Holmer","Mattias Krysander","Erik Frisk"],"pdf_url":"https://arxiv.org/pdf/2403.18739v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2403.18735v1","updated":"2024-03-27T16:24:26Z","published":"2024-03-27T16:24:26Z","title":"Nonlinear model reduction for operator learning","summary":"  Operator learning provides methods to approximate mappings between\ninfinite-dimensional function spaces. Deep operator networks (DeepONets) are a\nnotable architecture in this field. Recently, an extension of DeepONet based on\nmodel reduction and neural networks, proper orthogonal decomposition\n(POD)-DeepONet, has been able to outperform other architectures in terms of\naccuracy for several benchmark tests. We extend this idea towards nonlinear\nmodel order reduction by proposing an efficient framework that combines neural\nnetworks with kernel principal component analysis (KPCA) for operator learning.\nOur results demonstrate the superior performance of KPCA-DeepONet over\nPOD-DeepONet.\n","authors":["Hamidreza Eivazi","Stefan Wittek","Andreas Rausch"],"pdf_url":"https://arxiv.org/pdf/2403.18735v1.pdf","comment":"Published as a Tiny Paper at ICLR 2024 (Notable)"},{"id":"http://arxiv.org/abs/2403.18731v1","updated":"2024-03-27T16:21:24Z","published":"2024-03-27T16:21:24Z","title":"Enhancing Manufacturing Quality Prediction Models through the\n  Integration of Explainability Methods","summary":"  This research presents a method that utilizes explainability techniques to\namplify the performance of machine learning (ML) models in forecasting the\nquality of milling processes, as demonstrated in this paper through a\nmanufacturing use case. The methodology entails the initial training of ML\nmodels, followed by a fine-tuning phase where irrelevant features identified\nthrough explainability methods are eliminated. This procedural refinement\nresults in performance enhancements, paving the way for potential reductions in\nmanufacturing costs and a better understanding of the trained ML models. This\nstudy highlights the usefulness of explainability techniques in both explaining\nand optimizing predictive models in the manufacturing realm.\n","authors":["Dennis Gross","Helge Spieker","Arnaud Gotlieb","Ricardo Knoblauch"],"pdf_url":"https://arxiv.org/pdf/2403.18731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03100v2","updated":"2024-03-27T16:14:34Z","published":"2024-03-05T16:35:25Z","title":"NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and\n  Diffusion Models","summary":"  While recent large-scale text-to-speech (TTS) models have achieved\nsignificant progress, they still fall short in speech quality, similarity, and\nprosody. Considering speech intricately encompasses various attributes (e.g.,\ncontent, prosody, timbre, and acoustic details) that pose significant\nchallenges for generation, a natural idea is to factorize speech into\nindividual subspaces representing different attributes and generate them\nindividually. Motivated by it, we propose NaturalSpeech 3, a TTS system with\nnovel factorized diffusion models to generate natural speech in a zero-shot\nway. Specifically, 1) we design a neural codec with factorized vector\nquantization (FVQ) to disentangle speech waveform into subspaces of content,\nprosody, timbre, and acoustic details; 2) we propose a factorized diffusion\nmodel to generate attributes in each subspace following its corresponding\nprompt. With this factorization design, NaturalSpeech 3 can effectively and\nefficiently model intricate speech with disentangled subspaces in a\ndivide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the\nstate-of-the-art TTS systems on quality, similarity, prosody, and\nintelligibility, and achieves on-par quality with human recordings.\nFurthermore, we achieve better performance by scaling to 1B parameters and 200K\nhours of training data.\n","authors":["Zeqian Ju","Yuancheng Wang","Kai Shen","Xu Tan","Detai Xin","Dongchao Yang","Yanqing Liu","Yichong Leng","Kaitao Song","Siliang Tang","Zhizheng Wu","Tao Qin","Xiang-Yang Li","Wei Ye","Shikun Zhang","Jiang Bian","Lei He","Jinyu Li","Sheng Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.03100v2.pdf","comment":"Achieving human-level quality and naturalness on multi-speaker\n  datasets (e.g., LibriSpeech) in a zero-shot way"},{"id":"http://arxiv.org/abs/2402.07868v2","updated":"2024-03-27T16:12:43Z","published":"2024-02-12T18:29:17Z","title":"Nesting Particle Filters for Experimental Design in Dynamical Systems","summary":"  In this paper, we propose a novel approach to Bayesian experimental design\nfor non-exchangeable data that formulates it as risk-sensitive policy\noptimization. We develop the Inside-Out SMC$^2$ algorithm, a nested sequential\nMonte Carlo technique to infer optimal designs, and embed it into a particle\nMarkov chain Monte Carlo framework to perform gradient-based policy\namortization. Our approach is distinct from other amortized experimental design\ntechniques, as it does not rely on contrastive estimators. Numerical validation\non a set of dynamical systems showcases the efficacy of our method in\ncomparison to other state-of-the-art strategies.\n","authors":["Sahel Iqbal","Adrien Corenflos","Simo Särkkä","Hany Abdulsamad"],"pdf_url":"https://arxiv.org/pdf/2402.07868v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11798v3","updated":"2024-03-27T16:12:18Z","published":"2023-09-21T06:04:06Z","title":"A Comprehensive Review of Community Detection in Graphs","summary":"  The study of complex networks has significantly advanced our understanding of\ncommunity structures which serves as a crucial feature of real-world graphs.\nDetecting communities in graphs is a challenging problem with applications in\nsociology, biology, and computer science. Despite the efforts of an\ninterdisciplinary community of scientists, a satisfactory solution to this\nproblem has not yet been achieved. This review article delves into the topic of\ncommunity detection in graphs, which serves as a thorough exposition of various\ncommunity detection methods from perspectives of modularity-based method,\nspectral clustering, probabilistic modelling, and deep learning. Along with the\nmethods, a new community detection method designed by us is also presented.\nAdditionally, the performance of these methods on the datasets with and without\nground truth is compared. In conclusion, this comprehensive review provides a\ndeep understanding of community detection in graphs.\n","authors":["Jiakang Li","Songning Lai","Zhihao Shuai","Yuan Tan","Yifan Jia","Mianyang Yu","Zichen Song","Xiaokang Peng","Ziyang Xu","Yongxin Ni","Haifeng Qiu","Jiayu Yang","Yutong Liu","Yonggang Lu"],"pdf_url":"https://arxiv.org/pdf/2309.11798v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18717v1","updated":"2024-03-27T16:06:37Z","published":"2024-03-27T16:06:37Z","title":"Semi-Supervised Learning for Deep Causal Generative Models","summary":"  Developing models that can answer questions of the form \"How would $x$ change\nif $y$ had been $z$?\" is fundamental for advancing medical image analysis.\nTraining causal generative models that address such counterfactual questions,\nthough, currently requires that all relevant variables have been observed and\nthat corresponding labels are available in training data. However, clinical\ndata may not have complete records for all patients and state of the art causal\ngenerative models are unable to take full advantage of this. We thus develop,\nfor the first time, a semi-supervised deep causal generative model that\nexploits the causal relationships between variables to maximise the use of all\navailable data. We explore this in the setting where each sample is either\nfully labelled or fully unlabelled, as well as the more clinically realistic\ncase of having different labels missing for each sample. We leverage techniques\nfrom causal inference to infer missing values and subsequently generate\nrealistic counterfactuals, even for samples with incomplete labels.\n","authors":["Yasin Ibrahim","Hermione Warr","Konstantinos Kamnitsas"],"pdf_url":"https://arxiv.org/pdf/2403.18717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.07494v3","updated":"2024-03-27T16:06:34Z","published":"2024-01-15T06:26:53Z","title":"Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering\n  Tasks","summary":"  Computational efficiency and non-adversarial robustness are critical factors\nin real-world engineering applications. Yet, conventional neural networks often\nfall short in addressing both simultaneously, or even separately. Drawing\ninsights from natural physical systems and existing literature, it is known\nthat an input convex architecture enhances computational efficiency, while a\nLipschitz-constrained architecture bolsters non-adversarial robustness. By\nleveraging the strengths of convexity and Lipschitz continuity, we develop a\nnovel network architecture, termed Input Convex Lipschitz Recurrent Neural\nNetworks. This model is explicitly designed for fast and robust\noptimization-based tasks and outperforms existing recurrent units across a\nspectrum of engineering tasks in terms of computational efficiency and\nnon-adversarial robustness, including real-world solar irradiance prediction\nfor Solar PV system planning at LHT Holdings in Singapore and real-time Model\nPredictive Control optimization for a nonlinear chemical reactor.\n","authors":["Zihao Wang","P S Pravin","Zhe Wu"],"pdf_url":"https://arxiv.org/pdf/2401.07494v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.03123v3","updated":"2024-03-27T16:03:32Z","published":"2023-04-13T16:01:28Z","title":"ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and\n  Ethics) Evaluation: A Review","summary":"  ChatGPT is another large language model (LLM) vastly available for the\nconsumers on their devices but due to its performance and ability to converse\neffectively, it has gained a huge popularity amongst research as well as\nindustrial community. Recently, many studies have been published to show the\neffectiveness, efficiency, integration, and sentiments of chatGPT and other\nLLMs. In contrast, this study focuses on the important aspects that are mostly\noverlooked, i.e. sustainability, privacy, digital divide, and ethics and\nsuggests that not only chatGPT but every subsequent entry in the category of\nconversational bots should undergo Sustainability, PrivAcy, Digital divide, and\nEthics (SPADE) evaluation. This paper discusses in detail the issues and\nconcerns raised over chatGPT in line with aforementioned characteristics. We\nalso discuss the recent EU AI Act briefly in accordance with the SPADE\nevaluation. We support our hypothesis by some preliminary data collection and\nvisualizations along with hypothesized facts. We also suggest mitigations and\nrecommendations for each of the concerns. Furthermore, we also suggest some\npolicies and recommendations for EU AI policy act concerning ethics, digital\ndivide, and sustainability.\n","authors":["Sunder Ali Khowaja","Parus Khuwaja","Kapal Dev","Weizheng Wang","Lewis Nkenyereye"],"pdf_url":"https://arxiv.org/pdf/2305.03123v3.pdf","comment":"29 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.17878v2","updated":"2024-03-27T16:01:00Z","published":"2024-03-26T17:10:15Z","title":"Empowering Data Mesh with Federated Learning","summary":"  The evolution of data architecture has seen the rise of data lakes, aiming to\nsolve the bottlenecks of data management and promote intelligent\ndecision-making. However, this centralized architecture is limited by the\nproliferation of data sources and the growing demand for timely analysis and\nprocessing. A new data paradigm, Data Mesh, is proposed to overcome these\nchallenges. Data Mesh treats domains as a first-class concern by distributing\nthe data ownership from the central team to each data domain, while keeping the\nfederated governance to monitor domains and their data products. Many\nmulti-million dollar organizations like Paypal, Netflix, and Zalando have\nalready transformed their data analysis pipelines based on this new\narchitecture. In this decentralized architecture where data is locally\npreserved by each domain team, traditional centralized machine learning is\nincapable of conducting effective analysis across multiple domains, especially\nfor security-sensitive organizations. To this end, we introduce a pioneering\napproach that incorporates Federated Learning into Data Mesh. To the best of\nour knowledge, this is the first open-source applied work that represents a\ncritical advancement toward the integration of federated learning methods into\nthe Data Mesh paradigm, underscoring the promising prospects for\nprivacy-preserving and decentralized data analysis strategies within Data Mesh\narchitecture.\n","authors":["Haoyuan Li","Salman Toor"],"pdf_url":"https://arxiv.org/pdf/2403.17878v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18710v1","updated":"2024-03-27T15:57:42Z","published":"2024-03-27T15:57:42Z","title":"Deep Learning for Traffic Flow Prediction using Cellular Automata-based\n  Model and CNN-LSTM architecture","summary":"  Recent works have attempted to use deep learning to predict future states of\ntraffic flow, but have met with mixed results. These approaches face two key\nchallenges. First, training deep learning neural networks requires large\namounts of training data which are not yet easily available for traffic flow\nsystems. Second, even when data is available, the neural networks require\naccess to historical data that covers most possible traffic flow dynamics to\nsuccessfully predict future traffic states. Specifically, these deep learning\napproaches do not fully leverage domain-knowledge about traffic flow dynamics,\ndespite a significant existing knowledge-base. In this work, we propose to\nsolve both issues using a Convolutional Neural Network (CNNs) with Long Short\nTerm Memory (LSTM) deep learning architecture to successfully predict traffic\nflow, while leveraging a cellular automata-based statistical mechanics model of\ntraffic flow to generate training and test data. Another major contribution of\nthis paper is the insight that training data for a large traffic system can\nactually be sampled from the simulations of a much smaller traffic system. This\nis achieved through observing that the normalized energy distribution of the\nstatistical mechanics model is scale invariant, which significantly eases the\nburden of data generation for large scale traffic systems. The resulting\nsimulations indicate good agreement between the predicted and the true traffic\nflow dynamics.\n","authors":["Zhaohui Yang","Kshitij Jerath"],"pdf_url":"https://arxiv.org/pdf/2403.18710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18705v1","updated":"2024-03-27T15:54:55Z","published":"2024-03-27T15:54:55Z","title":"Conditional Wasserstein Distances with Applications in Bayesian OT Flow\n  Matching","summary":"  In inverse problems, many conditional generative models approximate the\nposterior measure by minimizing a distance between the joint measure and its\nlearned approximation. While this approach also controls the distance between\nthe posterior measures in the case of the Kullback--Leibler divergence, this is\nin general not hold true for the Wasserstein distance. In this paper, we\nintroduce a conditional Wasserstein distance via a set of restricted couplings\nthat equals the expected Wasserstein distance of the posteriors. Interestingly,\nthe dual formulation of the conditional Wasserstein-1 flow resembles losses in\nthe conditional Wasserstein GAN literature in a quite natural way. We derive\ntheoretical properties of the conditional Wasserstein distance, characterize\nthe corresponding geodesics and velocity fields as well as the flow ODEs.\nSubsequently, we propose to approximate the velocity fields by relaxing the\nconditional Wasserstein distance. Based on this, we propose an extension of OT\nFlow Matching for solving Bayesian inverse problems and demonstrate its\nnumerical advantages on an inverse problem and class-conditional image\ngeneration.\n","authors":["Jannis Chemseddine","Paul Hagemann","Christian Wald","Gabriele Steidl"],"pdf_url":"https://arxiv.org/pdf/2403.18705v1.pdf","comment":"This paper supersedes arXiv:2310.13433"},{"id":"http://arxiv.org/abs/2403.18703v1","updated":"2024-03-27T15:52:54Z","published":"2024-03-27T15:52:54Z","title":"Fpga-Based Neural Thrust Controller for UAVs","summary":"  The advent of unmanned aerial vehicles (UAVs) has improved a variety of\nfields by providing a versatile, cost-effective and accessible platform for\nimplementing state-of-the-art algorithms. To accomplish a broader range of\ntasks, there is a growing need for enhanced on-board computing to cope with\nincreasing complexity and dynamic environmental conditions. Recent advances\nhave seen the application of Deep Neural Networks (DNNs), particularly in\ncombination with Reinforcement Learning (RL), to improve the adaptability and\nperformance of UAVs, especially in unknown environments. However, the\ncomputational requirements of DNNs pose a challenge to the limited computing\nresources available on many UAVs. This work explores the use of Field\nProgrammable Gate Arrays (FPGAs) as a viable solution to this challenge,\noffering flexibility, high performance, energy and time efficiency. We propose\na novel hardware board equipped with an Artix-7 FPGA for a popular open-source\nmicro-UAV platform. We successfully validate its functionality by implementing\nan RL-based low-level controller using real-world experiments.\n","authors":["Sharif Azem","David Scheunert","Mengguang Li","Jonas Gehrunger","Kai Cui","Christian Hochberger","Heinz Koepp"],"pdf_url":"https://arxiv.org/pdf/2403.18703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11800v3","updated":"2024-03-27T15:48:29Z","published":"2024-02-19T03:08:02Z","title":"Stochastic Approximation with Delayed Updates: Finite-Time Rates under\n  Markovian Sampling","summary":"  Motivated by applications in large-scale and multi-agent reinforcement\nlearning, we study the non-asymptotic performance of stochastic approximation\n(SA) schemes with delayed updates under Markovian sampling. While the effect of\ndelays has been extensively studied for optimization, the manner in which they\ninteract with the underlying Markov process to shape the finite-time\nperformance of SA remains poorly understood. In this context, our first main\ncontribution is to show that under time-varying bounded delays, the delayed SA\nupdate rule guarantees exponentially fast convergence of the \\emph{last\niterate} to a ball around the SA operator's fixed point. Notably, our bound is\n\\emph{tight} in its dependence on both the maximum delay $\\tau_{max}$, and the\nmixing time $\\tau_{mix}$. To achieve this tight bound, we develop a novel\ninductive proof technique that, unlike various existing delayed-optimization\nanalyses, relies on establishing uniform boundedness of the iterates. As such,\nour proof may be of independent interest. Next, to mitigate the impact of the\nmaximum delay on the convergence rate, we provide the first finite-time\nanalysis of a delay-adaptive SA scheme under Markovian sampling. In particular,\nwe show that the exponent of convergence of this scheme gets scaled down by\n$\\tau_{avg}$, as opposed to $\\tau_{max}$ for the vanilla delayed SA rule; here,\n$\\tau_{avg}$ denotes the average delay across all iterations. Moreover, the\nadaptive scheme requires no prior knowledge of the delay sequence for step-size\ntuning. Our theoretical findings shed light on the finite-time effects of\ndelays for a broad class of algorithms, including TD learning, Q-learning, and\nstochastic gradient descent under Markovian sampling.\n","authors":["Arman Adibi","Nicolo Dal Fabbro","Luca Schenato","Sanjeev Kulkarni","H. Vincent Poor","George J. Pappas","Hamed Hassani","Aritra Mitra"],"pdf_url":"https://arxiv.org/pdf/2402.11800v3.pdf","comment":"Accepted to the 27th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2024!"},{"id":"http://arxiv.org/abs/2403.18699v1","updated":"2024-03-27T15:48:16Z","published":"2024-03-27T15:48:16Z","title":"Contrastive Learning with Orthonormal Anchors (CLOA)","summary":"  This study focuses on addressing the instability issues prevalent in\ncontrastive learning, specifically examining the InfoNCE loss function and its\nderivatives. We reveal a critical observation that these loss functions exhibit\na restrictive behavior, leading to a convergence phenomenon where embeddings\ntend to merge into a singular point. This \"over-fusion\" effect detrimentally\naffects classification accuracy in subsequent supervised-learning tasks.\nThrough theoretical analysis, we demonstrate that embeddings, when equalized or\nconfined to a rank-1 linear subspace, represent a local minimum for InfoNCE. In\nresponse to this challenge, our research introduces an innovative strategy that\nleverages the same or fewer labeled data than typically used in the fine-tuning\nphase. The loss we proposed, Orthonormal Anchor Regression Loss, is designed to\ndisentangle embedding clusters, significantly enhancing the distinctiveness of\neach embedding while simultaneously ensuring their aggregation into dense,\nwell-defined clusters. Our method demonstrates remarkable improvements with\njust a fraction of the conventional label requirements, as evidenced by our\nresults on CIFAR10 and CIFAR100 datasets.\n","authors":["Huanran Li","Daniel Pimentel-Alarcón"],"pdf_url":"https://arxiv.org/pdf/2403.18699v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.12091v3","updated":"2024-03-27T15:44:25Z","published":"2023-03-21T09:07:15Z","title":"Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised\n  Learning","summary":"  Semi-supervised learning (SSL) methods assume that labeled data, unlabeled\ndata and test data are from the same distribution. Open-set semi-supervised\nlearning (Open-set SSL) considers a more practical scenario, where unlabeled\ndata and test data contain new categories (outliers) not observed in labeled\ndata (inliers). Most previous works focused on outlier detection via binary\nclassifiers, which suffer from insufficient scalability and inability to\ndistinguish different types of uncertainty. In this paper, we propose a novel\nframework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these\nlimitations. Concretely, we first introduce evidential deep learning (EDL) as\nan outlier detector to quantify different types of uncertainty, and design\ndifferent uncertainty metrics for self-training and inference. Furthermore, we\npropose a novel adaptive negative optimization strategy, making EDL more\ntailored to the unlabeled dataset containing both inliers and outliers. As\ndemonstrated empirically, our proposed method outperforms existing\nstate-of-the-art methods across four datasets.\n","authors":["Yang Yu","Danruo Deng","Furui Liu","Yueming Jin","Qi Dou","Guangyong Chen","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2303.12091v3.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2403.18687v1","updated":"2024-03-27T15:34:27Z","published":"2024-03-27T15:34:27Z","title":"InceptionTime vs. Wavelet -- A comparison for time series classification","summary":"  Neural networks were used to classify infrasound data. Two different\napproaches were compared. One based on the direct classification of time series\ndata, using a custom implementation of the InceptionTime network. For the other\napproach, we generated 2D images of the wavelet transformation of the signals,\nwhich were subsequently classified using a ResNet implementation. Choosing\nappropriate hyperparameter settings, both achieve a classification accuracy of\nabove 90 %, with the direct approach reaching 95.2 %.\n","authors":["Daniel Klenkert","Daniel Schaeffer","Julian Stauch"],"pdf_url":"https://arxiv.org/pdf/2403.18687v1.pdf","comment":"4 pages, 1 figure"},{"id":"http://arxiv.org/abs/2403.18685v1","updated":"2024-03-27T15:29:08Z","published":"2024-03-27T15:29:08Z","title":"Representatividad Muestral en la Incertidumbre Simétrica Multivariada\n  para la Selección de Atributos","summary":"  In this work, we analyze the behavior of the multivariate symmetric\nuncertainty (MSU) measure through the use of statistical simulation techniques\nunder various mixes of informative and non-informative randomly generated\nfeatures. Experiments show how the number of attributes, their cardinalities,\nand the sample size affect the MSU. In this thesis, through observation of\nresults, it is proposed an heuristic condition that preserves good quality in\nthe MSU under different combinations of these three factors, providing a new\nuseful criterion to help drive the process of dimension reduction.\n  --\n  En el presente trabajo hemos analizado el comportamiento de una versi\\'on\nmultivariada de la incertidumbre sim\\'etrica a trav\\'es de t\\'ecnicas de\nsimulaci\\'on estad\\'isticas sobre varias combinaciones de atributos\ninformativos y no-informativos generados de forma aleatoria. Los experimentos\nmuestran como el n\\'umero de atributos, sus cardinalidades y el tama\\~no\nmuestral afectan al MSU como medida. En esta tesis, mediante la observaci\\'on\nde resultados hemos propuesto una condici\\'on que preserva una buena calidad en\nel MSU bajo diferentes combinaciones de los tres factores mencionados, lo cual\nprovee un nuevo y valioso criterio para llevar a cabo el proceso de reducci\\'on\nde dimensionalidad.\n","authors":["Gustavo Sosa-Cabrera"],"pdf_url":"https://arxiv.org/pdf/2403.18685v1.pdf","comment":"52 pages, in Spanish. Advisors: Miguel Garc\\'ia-Torres, Santiago\n  G\\'omez-Guerrero, Christian E. Schaerer Serra"},{"id":"http://arxiv.org/abs/2403.18681v1","updated":"2024-03-27T15:24:54Z","published":"2024-03-27T15:24:54Z","title":"TransFusion: Contrastive Learning with Transformers","summary":"  This paper proposes a novel framework, TransFusion, designed to make the\nprocess of contrastive learning more analytical and explainable. TransFusion\nconsists of attention blocks whose softmax being replaced by ReLU, and its\nfinal block's weighted-sum operation is truncated to leave the adjacency matrix\nas the output. The model is trained by minimizing the Jensen-Shannon Divergence\nbetween its output and the target affinity matrix, which indicates whether each\npair of samples belongs to the same or different classes. The main contribution\nof TransFusion lies in defining a theoretical limit for answering two\nfundamental questions in the field: the maximum level of data augmentation and\nthe minimum batch size required for effective contrastive learning.\nFurthermore, experimental results indicate that TransFusion successfully\nextracts features that isolate clusters from complex real-world data, leading\nto improved classification accuracy in downstream tasks.\n","authors":["Huanran Li","Daniel Pimentel-Alarcón"],"pdf_url":"https://arxiv.org/pdf/2403.18681v1.pdf","comment":"17 pages, 4 figures,"},{"id":"http://arxiv.org/abs/2403.18680v1","updated":"2024-03-27T15:22:16Z","published":"2024-03-27T15:22:16Z","title":"NL-ITI: Optimizing Probing and Intervention for Improvement of ITI\n  Method","summary":"  Large Language Models (LLM) are prone to returning false information. It\nconstitutes one of major challenges in the AI field. In our work, we explore\nparadigm introduced by Inference-Time-Intervention (ITI). In first stage, it\nidentifies attention heads, which contain the highest amount of desired type of\nknowledge (e.g., truthful). Afterwards, during inference, LLM activations are\nshifted for chosen subset of attention heads. We further improved the ITI\nframework by introducing a nonlinear probing and multi-token intervention -\nNon-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice\nbenchmarks, including TruthfulQA, on which we report around 14% MC1 metric\nimprovement with respect to the baseline ITI results. NL-ITI achieves also\nencouraging results on other testsets - on Business Ethics subdomain of MMLU,\naround 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI\nperforms better while being less invasive in the behavior of LLM at the same\ntime (as measured by Kullback-Leibler divergence).\n","authors":["Jakub Hoscilowicz","Adam Wiacek","Jan Chojnacki","Adam Cieslak","Leszek Michon","Vitalii Urbanevych","Artur Janicki"],"pdf_url":"https://arxiv.org/pdf/2403.18680v1.pdf","comment":"Code is available at https://github.com/Samsung/NL-ITI"},{"id":"http://arxiv.org/abs/2403.17143v2","updated":"2024-03-27T15:15:16Z","published":"2024-03-25T19:40:26Z","title":"Guided Distant Supervision for Multilingual Relation Extraction Data:\n  Adapting to a New Language","summary":"  Relation extraction is essential for extracting and understanding\nbiographical information in the context of digital humanities and related\nsubjects. There is a growing interest in the community to build datasets\ncapable of training machine learning models to extract relationships. However,\nannotating such datasets can be expensive and time-consuming, in addition to\nbeing limited to English. This paper applies guided distant supervision to\ncreate a large biographical relationship extraction dataset for German. Our\ndataset, composed of more than 80,000 instances for nine relationship types, is\nthe largest biographical German relationship extraction dataset. We also create\na manually annotated dataset with 2000 instances to evaluate the models and\nrelease it together with the dataset compiled using guided distant supervision.\nWe train several state-of-the-art machine learning models on the automatically\ncreated dataset and release them as well. Furthermore, we experiment with\nmultilingual and cross-lingual experiments that could benefit many low-resource\nlanguages.\n","authors":["Alistair Plum","Tharindu Ranasinghe","Christoph Purschke"],"pdf_url":"https://arxiv.org/pdf/2403.17143v2.pdf","comment":"Accepted to LREC-COLING 2024 (The 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation)"},{"id":"http://arxiv.org/abs/2403.18671v1","updated":"2024-03-27T15:15:14Z","published":"2024-03-27T15:15:14Z","title":"Fact Checking Beyond Training Set","summary":"  Evaluating the veracity of everyday claims is time consuming and in some\ncases requires domain expertise. We empirically demonstrate that the commonly\nused fact checking pipeline, known as the retriever-reader, suffers from\nperformance deterioration when it is trained on the labeled data from one\ndomain and used in another domain. Afterwards, we delve into each component of\nthe pipeline and propose novel algorithms to address this problem. We propose\nan adversarial algorithm to make the retriever component robust against\ndistribution shift. Our core idea is to initially train a bi-encoder on the\nlabeled source data, and then, to adversarially train two separate document and\nclaim encoders using unlabeled target data. We then focus on the reader\ncomponent and propose to train it such that it is insensitive towards the order\nof claims and evidence documents. Our empirical evaluations support the\nhypothesis that such a reader shows a higher robustness against distribution\nshift. To our knowledge, there is no publicly available multi-topic fact\nchecking dataset. Thus, we propose a simple automatic method to re-purpose two\nwell-known fact checking datasets. We then construct eight fact checking\nscenarios from these datasets, and compare our model to a set of strong\nbaseline models, including recent domain adaptation models that use GPT4 for\ngenerating synthetic data.\n","authors":["Payam Karisani","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2403.18671v1.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.18668v1","updated":"2024-03-27T15:11:07Z","published":"2024-03-27T15:11:07Z","title":"Aiming for Relevance","summary":"  Vital signs are crucial in intensive care units (ICUs). They are used to\ntrack the patient's state and to identify clinically significant changes.\nPredicting vital sign trajectories is valuable for early detection of adverse\nevents. However, conventional machine learning metrics like RMSE often fail to\ncapture the true clinical relevance of such predictions. We introduce novel\nvital sign prediction performance metrics that align with clinical contexts,\nfocusing on deviations from clinical norms, overall trends, and trend\ndeviations. These metrics are derived from empirical utility curves obtained in\na previous study through interviews with ICU clinicians. We validate the\nmetrics' usefulness using simulated and real clinical datasets (MIMIC and\neICU). Furthermore, we employ these metrics as loss functions for neural\nnetworks, resulting in models that excel in predicting clinically significant\nevents. This research paves the way for clinically relevant machine learning\nmodel evaluation and optimization, promising to improve ICU patient care. 10\npages, 9 figures.\n","authors":["Bar Eini Porat","Danny Eytan","Uri Shalit"],"pdf_url":"https://arxiv.org/pdf/2403.18668v1.pdf","comment":"10 pages, 9 figures, AMIA Informatics 2024"},{"id":"http://arxiv.org/abs/2403.18664v1","updated":"2024-03-27T15:08:00Z","published":"2024-03-27T15:08:00Z","title":"Neural Network-Based Piecewise Survival Models","summary":"  In this paper, a family of neural network-based survival models is presented.\nThe models are specified based on piecewise definitions of the hazard function\nand the density function on a partitioning of the time; both constant and\nlinear piecewise definitions are presented, resulting in a family of four\nmodels. The models can be seen as an extension of the commonly used\ndiscrete-time and piecewise exponential models and thereby add flexibility to\nthis set of standard models. Using a simulated dataset the models are shown to\nperform well compared to the highly expressive, state-of-the-art energy-based\nmodel, while only requiring a fraction of the computation time.\n","authors":["Olov Holmer","Erik Frisk","Mattias Krysander"],"pdf_url":"https://arxiv.org/pdf/2403.18664v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2403.13374v3","updated":"2024-03-27T14:57:54Z","published":"2024-03-20T08:15:08Z","title":"Byzantine-resilient Federated Learning With Adaptivity to Data\n  Heterogeneity","summary":"  This paper deals with federated learning (FL) in the presence of malicious\nByzantine attacks and data heterogeneity. A novel Robust Average Gradient\nAlgorithm (RAGA) is proposed, which leverages the geometric median for\naggregation and can freely select the round number for local updating.\nDifferent from most existing resilient approaches, which perform convergence\nanalysis based on strongly-convex loss function or homogeneously distributed\ndataset, we conduct convergence analysis for not only strongly-convex but also\nnon-convex loss function over heterogeneous dataset. According to our\ntheoretical analysis, as long as the fraction of dataset from malicious users\nis less than half, RAGA can achieve convergence at rate\n$\\mathcal{O}({1}/{T^{2/3- \\delta}})$ where $T$ is the iteration number and\n$\\delta \\in (0, 2/3)$ for non-convex loss function, and at linear rate for\nstrongly-convex loss function. Moreover, stationary point or global optimal\nsolution is proved to obtainable as data heterogeneity vanishes. Experimental\nresults corroborate the robustness of RAGA to Byzantine attacks and verifies\nthe advantage of RAGA over baselines on convergence performance under various\nintensity of Byzantine attacks, for heterogeneous dataset.\n","authors":["Shiyuan Zuo","Xingrun Yan","Rongfei Fan","Han Hu","Hangguan Shan","Tony Q. S. Quek"],"pdf_url":"https://arxiv.org/pdf/2403.13374v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17251v2","updated":"2024-03-27T14:48:48Z","published":"2023-03-30T09:29:53Z","title":"Demystifying Misconceptions in Social Bots Research","summary":"  Research on social bots aims at advancing knowledge and providing solutions\nto one of the most debated forms of online manipulation. Yet, social bot\nresearch is plagued by widespread biases, hyped results, and misconceptions\nthat set the stage for ambiguities, unrealistic expectations, and seemingly\nirreconcilable findings. Overcoming such issues is instrumental towards\nensuring reliable solutions and reaffirming the validity of the scientific\nmethod. In this contribution, we review some recent results in social bots\nresearch, highlighting and revising factual errors as well as methodological\nand conceptual biases. More importantly, we demystify common misconceptions,\naddressing fundamental points on how social bots research is discussed. Our\nanalysis surfaces the need to discuss research about online disinformation and\nmanipulation in a rigorous, unbiased, and responsible way. This article\nbolsters such effort by identifying and refuting common fallacious arguments\nused by both proponents and opponents of social bots research, as well as\nproviding directions toward sound methodologies for future research in the\nfield.\n","authors":["Stefano Cresci","Kai-Cheng Yang","Angelo Spognardi","Roberto Di Pietro","Filippo Menczer","Marinella Petrocchi"],"pdf_url":"https://arxiv.org/pdf/2303.17251v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12882v2","updated":"2024-03-27T14:47:41Z","published":"2023-08-23T17:42:00Z","title":"LCANets++: Robust Audio Classification using Multi-layer Neural Networks\n  with Lateral Competition","summary":"  Audio classification aims at recognizing audio signals, including speech\ncommands or sound events. However, current audio classifiers are susceptible to\nperturbations and adversarial attacks. In addition, real-world audio\nclassification tasks often suffer from limited labeled data. To help bridge\nthese gaps, previous work developed neuro-inspired convolutional neural\nnetworks (CNNs) with sparse coding via the Locally Competitive Algorithm (LCA)\nin the first layer (i.e., LCANets) for computer vision. LCANets learn in a\ncombination of supervised and unsupervised learning, reducing dependency on\nlabeled samples. Motivated by the fact that auditory cortex is also sparse, we\nextend LCANets to audio recognition tasks and introduce LCANets++, which are\nCNNs that perform sparse coding in multiple layers via LCA. We demonstrate that\nLCANets++ are more robust than standard CNNs and LCANets against perturbations,\ne.g., background noise, as well as black-box and white-box attacks, e.g.,\nevasion and fast gradient sign (FGSM) attacks.\n","authors":["Sayanton V. Dibbo","Juston S. Moore","Garrett T. Kenyon","Michael A. Teti"],"pdf_url":"https://arxiv.org/pdf/2308.12882v2.pdf","comment":"Accepted at 2024 IEEE International Conference on Acoustics, Speech\n  and Signal Processing Workshops (ICASSPW)"},{"id":"http://arxiv.org/abs/2403.18637v1","updated":"2024-03-27T14:42:08Z","published":"2024-03-27T14:42:08Z","title":"Transformers-based architectures for stroke segmentation: A review","summary":"  Stroke remains a significant global health concern, necessitating precise and\nefficient diagnostic tools for timely intervention and improved patient\noutcomes. The emergence of deep learning methodologies has transformed the\nlandscape of medical image analysis. Recently, Transformers, initially designed\nfor natural language processing, have exhibited remarkable capabilities in\nvarious computer vision applications, including medical image analysis. This\ncomprehensive review aims to provide an in-depth exploration of the\ncutting-edge Transformer-based architectures applied in the context of stroke\nsegmentation. It commences with an exploration of stroke pathology, imaging\nmodalities, and the challenges associated with accurate diagnosis and\nsegmentation. Subsequently, the review delves into the fundamental ideas of\nTransformers, offering detailed insights into their architectural intricacies\nand the underlying mechanisms that empower them to effectively capture complex\nspatial information within medical images. The existing literature is\nsystematically categorized and analyzed, discussing various approaches that\nleverage Transformers for stroke segmentation. A critical assessment is\nprovided, highlighting the strengths and limitations of these methods,\nincluding considerations of performance and computational efficiency.\nAdditionally, this review explores potential avenues for future research and\ndevelopment\n","authors":["Yalda Zafari-Ghadim","Essam A. Rashed","Mohamed Mabrok"],"pdf_url":"https://arxiv.org/pdf/2403.18637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18635v1","updated":"2024-03-27T14:40:25Z","published":"2024-03-27T14:40:25Z","title":"Fusion approaches for emotion recognition from speech using acoustic and\n  text-based features","summary":"  In this paper, we study different approaches for classifying emotions from\nspeech using acoustic and text-based features. We propose to obtain\ncontextualized word embeddings with BERT to represent the information contained\nin speech transcriptions and show that this results in better performance than\nusing Glove embeddings. We also propose and compare different strategies to\ncombine the audio and text modalities, evaluating them on IEMOCAP and\nMSP-PODCAST datasets. We find that fusing acoustic and text-based systems is\nbeneficial on both datasets, though only subtle differences are observed across\nthe evaluated fusion approaches. Finally, for IEMOCAP, we show the large effect\nthat the criteria used to define the cross-validation folds have on results. In\nparticular, the standard way of creating folds for this dataset results in a\nhighly optimistic estimation of performance for the text-based system,\nsuggesting that some previous works may overestimate the advantage of\nincorporating transcriptions.\n","authors":["Leonardo Pepino","Pablo Riera","Luciana Ferrer","Agustin Gravano"],"pdf_url":"https://arxiv.org/pdf/2403.18635v1.pdf","comment":"5 pages. Accepted in ICASSP 2020"},{"id":"http://arxiv.org/abs/2403.18631v1","updated":"2024-03-27T14:38:02Z","published":"2024-03-27T14:38:02Z","title":"First Experiences with the Identification of People at Risk for Diabetes\n  in Argentina using Machine Learning Techniques","summary":"  Detecting Type 2 Diabetes (T2D) and Prediabetes (PD) is a real challenge for\nmedicine due to the absence of pathogenic symptoms and the lack of known\nassociated risk factors. Even though some proposals for machine learning models\nenable the identification of people at risk, the nature of the condition makes\nit so that a model suitable for one population may not necessarily be suitable\nfor another. In this article, the development and assessment of predictive\nmodels to identify people at risk for T2D and PD specifically in Argentina are\ndiscussed. First, the database was thoroughly preprocessed and three specific\ndatasets were generated considering a compromise between the number of records\nand the amount of available variables. After applying 5 different\nclassification models, the results obtained show that a very good performance\nwas observed for two datasets with some of these models. In particular, RF, DT,\nand ANN demonstrated great classification power, with good values for the\nmetrics under consideration. Given the lack of this type of tool in Argentina,\nthis work represents the first step towards the development of more\nsophisticated models.\n","authors":["Enzo Rucci","Gonzalo Tittarelli","Franco Ronchetti","Jorge F. Elgart","Laura Lanzarini","Juan José Gagliardino"],"pdf_url":"https://arxiv.org/pdf/2403.18631v1.pdf","comment":"Accepted for publication in Computer Science - CACIC 2023"},{"id":"http://arxiv.org/abs/2403.16451v3","updated":"2024-03-27T14:36:21Z","published":"2024-03-25T06:30:54Z","title":"DeepMachining: Online Prediction of Machining Errors of Lathe Machines","summary":"  We describe DeepMachining, a deep learning-based AI system for online\nprediction of machining errors of lathe machine operations. We have built and\nevaluated DeepMachining based on manufacturing data from factories.\nSpecifically, we first pretrain a deep learning model for a given lathe\nmachine's operations to learn the salient features of machining states. Then,\nwe fine-tune the pretrained model to adapt to specific machining tasks. We\ndemonstrate that DeepMachining achieves high prediction accuracy for multiple\ntasks that involve different workpieces and cutting tools. To the best of our\nknowledge, this work is one of the first factory experiments using pre-trained\ndeep-learning models to predict machining errors of lathe machines.\n","authors":["Xiang-Li Lu","Hwai-Jung Hsu","Che-Wei Chou","H. T. Kung","Chen-Hsin Lee"],"pdf_url":"https://arxiv.org/pdf/2403.16451v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18613v1","updated":"2024-03-27T14:28:44Z","published":"2024-03-27T14:28:44Z","title":"Scalable Lipschitz Estimation for CNNs","summary":"  Estimating the Lipschitz constant of deep neural networks is of growing\ninterest as it is useful for informing on generalisability and adversarial\nrobustness. Convolutional neural networks (CNNs) in particular, underpin much\nof the recent success in computer vision related applications. However,\nalthough existing methods for estimating the Lipschitz constant can be tight,\nthey have limited scalability when applied to CNNs. To tackle this, we propose\na novel method to accelerate Lipschitz constant estimation for CNNs. The core\nidea is to divide a large convolutional block via a joint layer and width-wise\npartition, into a collection of smaller blocks. We prove an upper-bound on the\nLipschitz constant of the larger block in terms of the Lipschitz constants of\nthe smaller blocks. Through varying the partition factor, the resulting method\ncan be adjusted to prioritise either accuracy or scalability and permits\nparallelisation. We demonstrate an enhanced scalability and comparable accuracy\nto existing baselines through a range of experiments.\n","authors":["Yusuf Sulehman","Tingting Mu"],"pdf_url":"https://arxiv.org/pdf/2403.18613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18597v1","updated":"2024-03-27T14:20:11Z","published":"2024-03-27T14:20:11Z","title":"Heterogeneous Peridynamic Neural Operators: Discover Biotissue\n  Constitutive Law and Microstructure From Digital Image Correlation\n  Measurements","summary":"  Human tissues are highly organized structures with specific collagen fiber\narrangements varying from point to point. The effects of such heterogeneity\nplay an important role for tissue function, and hence it is of critical to\ndiscover and understand the distribution of such fiber orientations from\nexperimental measurements, such as the digital image correlation data. To this\nend, we introduce the heterogeneous peridynamic neural operator (HeteroPNO)\napproach, for data-driven constitutive modeling of heterogeneous anisotropic\nmaterials. The goal is to learn both a nonlocal constitutive law together with\nthe material microstructure, in the form of a heterogeneous fiber orientation\nfield, from loading field-displacement field measurements. To this end, we\npropose a two-phase learning approach. Firstly, we learn a homogeneous\nconstitutive law in the form of a neural network-based kernel function and a\nnonlocal bond force, to capture complex homogeneous material responses from\ndata. Then, in the second phase we reinitialize the learnt bond force and the\nkernel function, and training them together with a fiber orientation field for\neach material point. Owing to the state-based peridynamic skeleton, our\nHeteroPNO-learned material models are objective and have the balance of linear\nand angular momentum guaranteed. Moreover, the effects from heterogeneity and\nnonlinear constitutive relationship are captured by the kernel function and the\nbond force respectively, enabling physical interpretability. As a result, our\nHeteroPNO architecture can learn a constitutive model for a biological tissue\nwith anisotropic heterogeneous response undergoing large deformation regime.\nMoreover, the framework is capable to provide displacement and stress field\npredictions for new and unseen loading instances.\n","authors":["Siavash Jafarzadeh","Stewart Silling","Lu Zhang","Colton Ross","Chung-Hao Lee","S. M. Rakibur Rahman","Shuodao Wang","Yue Yu"],"pdf_url":"https://arxiv.org/pdf/2403.18597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18587v1","updated":"2024-03-27T14:11:23Z","published":"2024-03-27T14:11:23Z","title":"The Impact of Uniform Inputs on Activation Sparsity and Energy-Latency\n  Attacks in Computer Vision","summary":"  Resource efficiency plays an important role for machine learning nowadays.\nThe energy and decision latency are two critical aspects to ensure a\nsustainable and practical application. Unfortunately, the energy consumption\nand decision latency are not robust against adversaries. Researchers have\nrecently demonstrated that attackers can compute and submit so-called sponge\nexamples at inference time to increase the energy consumption and decision\nlatency of neural networks. In computer vision, the proposed strategy crafts\ninputs with less activation sparsity which could otherwise be used to\naccelerate the computation. In this paper, we analyze the mechanism how these\nenergy-latency attacks reduce activation sparsity. In particular, we find that\ninput uniformity is a key enabler. A uniform image, that is, an image with\nmostly flat, uniformly colored surfaces, triggers more activations due to a\nspecific interplay of convolution, batch normalization, and ReLU activation.\nBased on these insights, we propose two new simple, yet effective strategies\nfor crafting sponge examples: sampling images from a probability distribution\nand identifying dense, yet inconspicuous inputs in natural datasets. We\nempirically examine our findings in a comprehensive evaluation with multiple\nimage classification models and show that our attack achieves the same sparsity\neffect as prior sponge-example methods, but at a fraction of computation\neffort. We also show that our sponge examples transfer between different neural\nnetworks. Finally, we discuss applications of our findings for the good by\nimproving efficiency by increasing sparsity.\n","authors":["Andreas Müller","Erwin Quiring"],"pdf_url":"https://arxiv.org/pdf/2403.18587v1.pdf","comment":"Accepted at the DLSP 2024"},{"id":"http://arxiv.org/abs/2403.18582v1","updated":"2024-03-27T14:03:41Z","published":"2024-03-27T14:03:41Z","title":"One flow to correct them all: improving simulations in high-energy\n  physics with a single normalising flow and a switch","summary":"  Simulated events are key ingredients in almost all high-energy physics\nanalyses. However, imperfections in the simulation can lead to sizeable\ndifferences between the observed data and simulated events. The effects of such\nmismodelling on relevant observables must be corrected either effectively via\nscale factors, with weights or by modifying the distributions of the\nobservables and their correlations. We introduce a correction method that\ntransforms one multidimensional distribution (simulation) into another one\n(data) using a simple architecture based on a single normalising flow with a\nboolean condition. We demonstrate the effectiveness of the method on a\nphysics-inspired toy dataset with non-trivial mismodelling of several\nobservables and their correlations.\n","authors":["Caio Cesar Daumann","Mauro Donega","Johannes Erdmann","Massimiliano Galli","Jan Lukas Späh","Davide Valsecchi"],"pdf_url":"https://arxiv.org/pdf/2403.18582v1.pdf","comment":"19 pages, 12 figures"},{"id":"http://arxiv.org/abs/2306.09459v3","updated":"2024-03-27T14:02:58Z","published":"2023-06-15T19:29:08Z","title":"Recurrent Action Transformer with Memory","summary":"  Recently, the use of transformers in offline reinforcement learning has\nbecome a rapidly developing area. This is due to their ability to treat the\nagent's trajectory in the environment as a sequence, thereby reducing the\npolicy learning problem to sequence modeling. In environments where the agent's\ndecisions depend on past events, it is essential to capture both the event\nitself and the decision point in the context of the model. However, the\nquadratic complexity of the attention mechanism limits the potential for\ncontext expansion. One solution to this problem is to enhance transformers with\nmemory mechanisms. In this paper, we propose the Recurrent Action Transformer\nwith Memory (RATE) - a model that incorporates recurrent memory. To evaluate\nour model, we conducted extensive experiments on both memory-intensive\nenvironments (VizDoom-Two-Color, T-Maze) and classic Atari games and MuJoCo\ncontrol environments. The results show that the use of memory can significantly\nimprove performance in memory-intensive environments while maintaining or\nimproving results in classic environments. We hope that our findings will\nstimulate research on memory mechanisms for transformers applicable to offline\nreinforcement learning.\n","authors":["Alexey Staroverov","Egor Cherepanov","Dmitry Yudin","Alexey K. Kovalev","Aleksandr I. Panov"],"pdf_url":"https://arxiv.org/pdf/2306.09459v3.pdf","comment":"15 pages, 11 figures"},{"id":"http://arxiv.org/abs/2309.11427v2","updated":"2024-03-27T14:02:57Z","published":"2023-09-20T16:01:45Z","title":"Generative Pre-Training of Time-Series Data for Unsupervised Fault\n  Detection in Semiconductor Manufacturing","summary":"  This paper introduces TRACE-GPT, which stands for Time-seRies\nAnomaly-detection with Convolutional Embedding and Generative Pre-trained\nTransformers. TRACE-GPT is designed to pre-train univariate time-series sensor\ndata and detect faults on unlabeled datasets in semiconductor manufacturing. In\nsemiconductor industry, classifying abnormal time-series sensor data from\nnormal data is important because it is directly related to wafer defect.\nHowever, small, unlabeled, and even mixed training data without enough\nanomalies make classification tasks difficult. In this research, we capture\nfeatures of time-series data with temporal convolutional embedding and\nGenerative Pre-trained Transformer (GPT) to classify abnormal sequences from\nnormal sequences using cross entropy loss. We prove that our model shows better\nperformance than previous unsupervised models with both an open dataset, the\nUniversity of California Riverside (UCR) time-series classification archive,\nand the process log of our Chemical Vapor Deposition (CVD) equipment. Our model\nhas the highest F1 score at Equal Error Rate (EER) across all datasets and is\nonly 0.026 below the supervised state-of-the-art baseline on the open dataset.\n","authors":["Sewoong Lee","JinKyou Choi","Min Su Kim"],"pdf_url":"https://arxiv.org/pdf/2309.11427v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18579v1","updated":"2024-03-27T13:59:09Z","published":"2024-03-27T13:59:09Z","title":"On Optimizing Hyperparameters for Quantum Neural Networks","summary":"  The increasing capabilities of Machine Learning (ML) models go hand in hand\nwith an immense amount of data and computational power required for training.\nTherefore, training is usually outsourced into HPC facilities, where we have\nstarted to experience limits in scaling conventional HPC hardware, as theorized\nby Moore's law. Despite heavy parallelization and optimization efforts, current\nstate-of-the-art ML models require weeks for training, which is associated with\nan enormous $CO_2$ footprint. Quantum Computing, and specifically Quantum\nMachine Learning (QML), can offer significant theoretical speed-ups and\nenhanced expressive power. However, training QML models requires tuning various\nhyperparameters, which is a nontrivial task and suboptimal choices can highly\naffect the trainability and performance of the models. In this study, we\nidentify the most impactful hyperparameters and collect data about the\nperformance of QML models. We compare different configurations and provide\nresearchers with performance data and concrete suggestions for hyperparameter\nselection.\n","authors":["Sabrina Herbst","Vincenzo De Maio","Ivona Brandic"],"pdf_url":"https://arxiv.org/pdf/2403.18579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18578v1","updated":"2024-03-27T13:59:05Z","published":"2024-03-27T13:59:05Z","title":"SteinGen: Generating Fidelitous and Diverse Graph Samples","summary":"  Generating graphs that preserve characteristic structures while promoting\nsample diversity can be challenging, especially when the number of graph\nobservations is small. Here, we tackle the problem of graph generation from\nonly one observed graph. The classical approach of graph generation from\nparametric models relies on the estimation of parameters, which can be\ninconsistent or expensive to compute due to intractable normalisation\nconstants. Generative modelling based on machine learning techniques to\ngenerate high-quality graph samples avoids parameter estimation but usually\nrequires abundant training samples. Our proposed generating procedure,\nSteinGen, which is phrased in the setting of graphs as realisations of\nexponential random graph models, combines ideas from Stein's method and MCMC by\nemploying Markovian dynamics which are based on a Stein operator for the target\nmodel. SteinGen uses the Glauber dynamics associated with an estimated Stein\noperator to generate a sample, and re-estimates the Stein operator from the\nsample after every sampling step. We show that on a class of exponential random\ngraph models this novel \"estimation and re-estimation\" generation strategy\nyields high distributional similarity (high fidelity) to the original data,\ncombined with high sample diversity.\n","authors":["Gesine Reinert","Wenkai Xu"],"pdf_url":"https://arxiv.org/pdf/2403.18578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09283v3","updated":"2024-03-27T13:55:14Z","published":"2024-02-14T16:14:03Z","title":"Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey","summary":"  Large Language Models (LLMs) are now commonplace in conversation\napplications. However, their risks of misuse for generating harmful responses\nhave raised serious societal concerns and spurred recent research on LLM\nconversation safety. Therefore, in this survey, we provide a comprehensive\noverview of recent studies, covering three critical aspects of LLM conversation\nsafety: attacks, defenses, and evaluations. Our goal is to provide a structured\nsummary that enhances understanding of LLM conversation safety and encourages\nfurther investigation into this important subject. For easy reference, we have\ncategorized all the studies mentioned in this survey according to our taxonomy,\navailable at: https://github.com/niconi19/LLM-conversation-safety.\n","authors":["Zhichen Dong","Zhanhui Zhou","Chao Yang","Jing Shao","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2402.09283v3.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2403.18570v1","updated":"2024-03-27T13:51:26Z","published":"2024-03-27T13:51:26Z","title":"Physics-Informed Graph Neural Networks for Water Distribution Systems","summary":"  Water distribution systems (WDS) are an integral part of critical\ninfrastructure which is pivotal to urban development. As 70% of the world's\npopulation will likely live in urban environments in 2050, efficient simulation\nand planning tools for WDS play a crucial role in reaching UN's sustainable\ndevelopmental goal (SDG) 6 - \"Clean water and sanitation for all\". In this\nrealm, we propose a novel and efficient machine learning emulator, more\nprecisely, a physics-informed deep learning (DL) model, for hydraulic state\nestimation in WDS. Using a recursive approach, our model only needs a few graph\nconvolutional neural network (GCN) layers and employs an innovative algorithm\nbased on message passing. Unlike conventional machine learning tasks, the model\nuses hydraulic principles to infer two additional hydraulic state features in\nthe process of reconstructing the available ground truth feature in an\nunsupervised manner. To the best of our knowledge, this is the first DL\napproach to emulate the popular hydraulic simulator EPANET, utilizing no\nadditional information. Like most DL models and unlike the hydraulic simulator,\nour model demonstrates vastly faster emulation times that do not increase\ndrastically with the size of the WDS. Moreover, we achieve high accuracy on the\nground truth and very similar results compared to the hydraulic simulator as\ndemonstrated through experiments on five real-world WDS datasets.\n","authors":["Inaam Ashraf","Janine Strotherm","Luca Hermes","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2403.18570v1.pdf","comment":"Extended version of the paper with the same title published at\n  Proceedings of the AAAI Conference on Artificial Intelligence 2024"},{"id":"http://arxiv.org/abs/2403.18569v1","updated":"2024-03-27T13:50:13Z","published":"2024-03-27T13:50:13Z","title":"PDNNet: PDN-Aware GNN-CNN Heterogeneous Network for Dynamic IR Drop\n  Prediction","summary":"  IR drop on the power delivery network (PDN) is closely related to PDN's\nconfiguration and cell current consumption. As the integrated circuit (IC)\ndesign is growing larger, dynamic IR drop simulation becomes computationally\nunaffordable and machine learning based IR drop prediction has been explored as\na promising solution. Although CNN-based methods have been adapted to IR drop\nprediction task in several works, the shortcomings of overlooking PDN\nconfiguration is non-negligible. In this paper, we consider not only how to\nproperly represent cell-PDN relation, but also how to model IR drop following\nits physical nature in the feature aggregation procedure. Thus, we propose a\nnovel graph structure, PDNGraph, to unify the representations of the PDN\nstructure and the fine-grained cell-PDN relation. We further propose a\ndual-branch heterogeneous network, PDNNet, incorporating two parallel GNN-CNN\nbranches to favorably capture the above features during the learning process.\nSeveral key designs are presented to make the dynamic IR drop prediction highly\neffective and interpretable. We are the first work to apply graph structure to\ndeep-learning based dynamic IR drop prediction method. Experiments show that\nPDNNet outperforms the state-of-the-art CNN-based methods by up to 39.3%\nreduction in prediction error and achieves 545x speedup compared to the\ncommercial tool, which demonstrates the superiority of our method.\n","authors":["Yuxiang Zhao","Zhuomin Chai","Xun Jiang","Yibo Lin","Runsheng Wang","Ru Huang"],"pdf_url":"https://arxiv.org/pdf/2403.18569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12370v2","updated":"2024-03-27T13:44:21Z","published":"2023-10-18T22:34:32Z","title":"No-Regret Learning in Bilateral Trade via Global Budget Balance","summary":"  Bilateral trade models the problem of intermediating between two rational\nagents -- a seller and a buyer -- both characterized by a private valuation for\nan item they want to trade. We study the online learning version of the\nproblem, in which at each time step a new seller and buyer arrive and the\nlearner has to set prices for them without any knowledge about their\n(adversarially generated) valuations.\n  In this setting, known impossibility results rule out the existence of\nno-regret algorithms when budget balanced has to be enforced at each time step.\nIn this paper, we introduce the notion of \\emph{global budget balance}, which\nonly requires the learner to fulfill budget balance over the entire time\nhorizon. Under this natural relaxation, we provide the first no-regret\nalgorithms for adversarial bilateral trade under various feedback models.\nFirst, we show that in the full-feedback model, the learner can guarantee\n$\\tilde O(\\sqrt{T})$ regret against the best fixed prices in hindsight, and\nthat this bound is optimal up to poly-logarithmic terms. Second, we provide a\nlearning algorithm guaranteeing a $\\tilde O(T^{3/4})$ regret upper bound with\none-bit feedback, which we complement with a $\\Omega(T^{5/7})$ lower bound that\nholds even in the two-bit feedback model. Finally, we introduce and analyze an\nalternative benchmark that is provably stronger than the best fixed prices in\nhindsight and is inspired by the literature on bandits with knapsacks.\n","authors":["Martino Bernasconi","Matteo Castiglioni","Andrea Celli","Federico Fusco"],"pdf_url":"https://arxiv.org/pdf/2310.12370v2.pdf","comment":"Accepted at STOC 2024"},{"id":"http://arxiv.org/abs/2403.18560v1","updated":"2024-03-27T13:42:14Z","published":"2024-03-27T13:42:14Z","title":"Noise-Robust Keyword Spotting through Self-supervised Pretraining","summary":"  Voice assistants are now widely available, and to activate them a keyword\nspotting (KWS) algorithm is used. Modern KWS systems are mainly trained using\nsupervised learning methods and require a large amount of labelled data to\nachieve a good performance. Leveraging unlabelled data through self-supervised\nlearning (SSL) has been shown to increase the accuracy in clean conditions.\nThis paper explores how SSL pretraining such as Data2Vec can be used to enhance\nthe robustness of KWS models in noisy conditions, which is under-explored.\n  Models of three different sizes are pretrained using different pretraining\napproaches and then fine-tuned for KWS. These models are then tested and\ncompared to models trained using two baseline supervised learning methods, one\nbeing standard training using clean data and the other one being multi-style\ntraining (MTR). The results show that pretraining and fine-tuning on clean data\nis superior to supervised learning on clean data across all testing conditions,\nand superior to supervised MTR for testing conditions of SNR above 5 dB. This\nindicates that pretraining alone can increase the model's robustness. Finally,\nit is found that using noisy data for pretraining models, especially with the\nData2Vec-denoising approach, significantly enhances the robustness of KWS\nmodels in noisy conditions.\n","authors":["Jacob Mørk","Holger Severin Bovbjerg","Gergely Kiss","Zheng-Hua Tan"],"pdf_url":"https://arxiv.org/pdf/2403.18560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06712v2","updated":"2024-03-27T13:38:35Z","published":"2024-01-12T17:26:51Z","title":"Few-Shot Detection of Machine-Generated Text using Style Representations","summary":"  The advent of instruction-tuned language models that convincingly mimic human\nwriting poses a significant risk of abuse. However, such abuse may be\ncounteracted with the ability to detect whether a piece of text was composed by\na language model rather than a human author. Some previous approaches to this\nproblem have relied on supervised methods by training on corpora of confirmed\nhuman- and machine- written documents. Unfortunately, model under-specification\nposes an unavoidable challenge for neural network-based detectors, making them\nbrittle in the face of data shifts, such as the release of newer language\nmodels producing still more fluent text than the models used to train the\ndetectors. Other approaches require access to the models that may have\ngenerated a document in question, which is often impractical. In light of these\nchallenges, we pursue a fundamentally different approach not relying on samples\nfrom language models of concern at training time. Instead, we propose to\nleverage representations of writing style estimated from human-authored text.\nIndeed, we find that features effective at distinguishing among human authors\nare also effective at distinguishing human from machine authors, including\nstate-of-the-art large language models like Llama-2, ChatGPT, and GPT-4.\nFurthermore, given a handful of examples composed by each of several specific\nlanguage models of interest, our approach affords the ability to predict which\nmodel generated a given document. The code and data to reproduce our\nexperiments are available at\nhttps://github.com/LLNL/LUAR/tree/main/fewshot_iclr2024.\n","authors":["Rafael Rivera Soto","Kailin Koch","Aleem Khan","Barry Chen","Marcus Bishop","Nicholas Andrews"],"pdf_url":"https://arxiv.org/pdf/2401.06712v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00117v4","updated":"2024-03-27T13:38:00Z","published":"2023-09-29T20:11:15Z","title":"ABScribe: Rapid Exploration & Organization of Multiple Writing\n  Variations in Human-AI Co-Writing Tasks using Large Language Models","summary":"  Exploring alternative ideas by rewriting text is integral to the writing\nprocess. State-of-the-art Large Language Models (LLMs) can simplify writing\nvariation generation. However, current interfaces pose challenges for\nsimultaneous consideration of multiple variations: creating new variations\nwithout overwriting text can be difficult, and pasting them sequentially can\nclutter documents, increasing workload and disrupting writers' flow. To tackle\nthis, we present ABScribe, an interface that supports rapid, yet visually\nstructured, exploration and organization of writing variations in human-AI\nco-writing tasks. With ABScribe, users can swiftly modify variations using LLM\nprompts, which are auto-converted into reusable buttons. Variations are stored\nadjacently within text fields for rapid in-place comparisons using mouse-over\ninteractions on a popup toolbar. Our user study with 12 writers shows that\nABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances\nuser perceptions of the revision process (d = 2.41, p < 0.001) compared to a\npopular baseline workflow, and provides insights into how writers explore\nvariations using LLMs.\n","authors":["Mohi Reza","Nathan Laundry","Ilya Musabirov","Peter Dushniku","Zhi Yuan \"Michael\" Yu","Kashish Mittal","Tovi Grossman","Michael Liut","Anastasia Kuzminykh","Joseph Jay Williams"],"pdf_url":"https://arxiv.org/pdf/2310.00117v4.pdf","comment":"CHI 2024"},{"id":"http://arxiv.org/abs/2403.18542v1","updated":"2024-03-27T13:22:38Z","published":"2024-03-27T13:22:38Z","title":"Attention-aware semantic relevance predicting Chinese sentence reading","summary":"  In recent years, several influential computational models and metrics have\nbeen proposed to predict how humans comprehend and process sentence. One\nparticularly promising approach is contextual semantic similarity. Inspired by\nthe attention algorithm in Transformer and human memory mechanisms, this study\nproposes an ``attention-aware'' approach for computing contextual semantic\nrelevance. This new approach takes into account the different contributions of\ncontextual parts and the expectation effect, allowing it to incorporate\ncontextual information fully. The attention-aware approach also facilitates the\nsimulation of existing reading models and evaluate them. The resulting\n``attention-aware'' metrics of semantic relevance can more accurately predict\nfixation durations in Chinese reading tasks recorded in an eye-tracking corpus\nthan those calculated by existing approaches. The study's findings further\nprovide strong support for the presence of semantic preview benefits in Chinese\nnaturalistic reading. Furthermore, the attention-aware metrics of semantic\nrelevance, being memory-based, possess high interpretability from both\nlinguistic and cognitive standpoints, making them a valuable computational tool\nfor modeling eye-movements in reading and further gaining insight into the\nprocess of language comprehension. Our approach underscores the potential of\nthese metrics to advance our comprehension of how humans understand and process\nlanguage, ultimately leading to a better understanding of language\ncomprehension and processing.\n","authors":["Kun Sun"],"pdf_url":"https://arxiv.org/pdf/2403.18542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18540v1","updated":"2024-03-27T13:17:15Z","published":"2024-03-27T13:17:15Z","title":"skscope: Fast Sparsity-Constrained Optimization in Python","summary":"  Applying iterative solvers on sparsity-constrained optimization (SCO)\nrequires tedious mathematical deduction and careful programming/debugging that\nhinders these solvers' broad impact. In the paper, the library skscope is\nintroduced to overcome such an obstacle. With skscope, users can solve the SCO\nby just programming the objective function. The convenience of skscope is\ndemonstrated through two examples in the paper, where sparse linear regression\nand trend filtering are addressed with just four lines of code. More\nimportantly, skscope's efficient implementation allows state-of-the-art solvers\nto quickly attain the sparse solution regardless of the high dimensionality of\nparameter space. Numerical experiments reveal the available solvers in skscope\ncan achieve up to 80x speedup on the competing relaxation solutions obtained\nvia the benchmarked convex solver. skscope is published on the Python Package\nIndex (PyPI) and Conda, and its source code is available at:\nhttps://github.com/abess-team/skscope.\n","authors":["Zezhi Wang","Jin Zhu","Peng Chen","Huiyang Peng","Xiaoke Zhang","Anran Wang","Yu Zheng","Junxian Zhu","Xueqin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18540v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2403.18539v1","updated":"2024-03-27T13:14:29Z","published":"2024-03-27T13:14:29Z","title":"Safe and Robust Reinforcement-Learning: Principles and Practice","summary":"  Reinforcement Learning (RL) has shown remarkable success in solving\nrelatively complex tasks, yet the deployment of RL systems in real-world\nscenarios poses significant challenges related to safety and robustness. This\npaper aims to identify and further understand those challenges thorough the\nexploration of the main dimensions of the safe and robust RL landscape,\nencompassing algorithmic, ethical, and practical considerations. We conduct a\ncomprehensive review of methodologies and open problems that summarizes the\nefforts in recent years to address the inherent risks associated with RL\napplications.\n  After discussing and proposing definitions for both safe and robust RL, the\npaper categorizes existing research works into different algorithmic approaches\nthat enhance the safety and robustness of RL agents. We examine techniques such\nas uncertainty estimation, optimisation methodologies, exploration-exploitation\ntrade-offs, and adversarial training. Environmental factors, including\nsim-to-real transfer and domain adaptation, are also scrutinized to understand\nhow RL systems can adapt to diverse and dynamic surroundings. Moreover, human\ninvolvement is an integral ingredient of the analysis, acknowledging the broad\nset of roles that humans can take in this context.\n  Importantly, to aid practitioners in navigating the complexities of safe and\nrobust RL implementation, this paper introduces a practical checklist derived\nfrom the synthesized literature. The checklist encompasses critical aspects of\nalgorithm design, training environment considerations, and ethical guidelines.\nIt will serve as a resource for developers and policymakers alike to ensure the\nresponsible deployment of RL systems in many application domains.\n","authors":["Taku Yamagata","Raul Santos-Rodriguez"],"pdf_url":"https://arxiv.org/pdf/2403.18539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18535v1","updated":"2024-03-27T13:11:34Z","published":"2024-03-27T13:11:34Z","title":"Theoretical Bound-Guided Hierarchical VAE for Neural Image Codecs","summary":"  Recent studies reveal a significant theoretical link between variational\nautoencoders (VAEs) and rate-distortion theory, notably in utilizing VAEs to\nestimate the theoretical upper bound of the information rate-distortion\nfunction of images. Such estimated theoretical bounds substantially exceed the\nperformance of existing neural image codecs (NICs). To narrow this gap, we\npropose a theoretical bound-guided hierarchical VAE (BG-VAE) for NIC. The\nproposed BG-VAE leverages the theoretical bound to guide the NIC model towards\nenhanced performance. We implement the BG-VAE using Hierarchical VAEs and\ndemonstrate its effectiveness through extensive experiments. Along with\nadvanced neural network blocks, we provide a versatile, variable-rate NIC that\noutperforms existing methods when considering both rate-distortion performance\nand computational complexity. The code is available at BG-VAE.\n","authors":["Yichi Zhang","Zhihao Duan","Yuning Huang","Fengqing Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.18535v1.pdf","comment":"2024 IEEE International Conference on Multimedia and Expo (ICME2024)"},{"id":"http://arxiv.org/abs/2403.18525v1","updated":"2024-03-27T12:59:44Z","published":"2024-03-27T12:59:44Z","title":"Language Plays a Pivotal Role in the Object-Attribute Compositional\n  Generalization of CLIP","summary":"  Vision-language models, such as CLIP, have shown promising\nOut-of-Distribution (OoD) generalization under various types of distribution\nshifts. Recent studies attempted to investigate the leading cause of this\ncapability. In this work, we follow the same path, but focus on a specific type\nof OoD data - images with novel compositions of attribute-object pairs - and\nstudy whether such models can successfully classify those images into\ncomposition classes. We carefully designed an authentic image test dataset\ncalled ImageNet-AO, consisting of attributes for objects that are unlikely\nencountered in the CLIP training sets. We found that CLIPs trained with large\ndatasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude\nimprovement in effective compositional OoD generalization compared to both\nsupervised models and CLIPs trained with smaller datasets, such as CC-12M and\nYFCC-15M. Our results provide evidence that the scale and diversity of training\ndata and language supervision play a key role in unlocking the compositional\ngeneralization abilities of vision-language models.\n","authors":["Reza Abbasi","Mohammad Samiei","Mohammad Hossein Rohban","Mahdieh Soleymani Baghshah"],"pdf_url":"https://arxiv.org/pdf/2403.18525v1.pdf","comment":"Oral accepted at OODCV 2023(http://www.ood-cv.org)"},{"id":"http://arxiv.org/abs/2303.10365v3","updated":"2024-03-27T12:53:12Z","published":"2023-03-18T08:48:16Z","title":"CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label\n  Learning","summary":"  Partial-label learning (PLL) is an important weakly supervised learning\nproblem, which allows each training example to have a candidate label set\ninstead of a single ground-truth label. Identification-based methods have been\nwidely explored to tackle label ambiguity issues in PLL, which regard the true\nlabel as a latent variable to be identified. However, identifying the true\nlabels accurately and completely remains challenging, causing noise in pseudo\nlabels during model training. In this paper, we propose a new method called\nCroSel, which leverages historical predictions from the model to identify true\nlabels for most training examples. First, we introduce a cross selection\nstrategy, which enables two deep models to select true labels of partially\nlabeled data for each other. Besides, we propose a novel consistency\nregularization term called co-mix to avoid sample waste and tiny noise caused\nby false selection. In this way, CroSel can pick out the true labels of most\nexamples with high precision. Extensive experiments demonstrate the superiority\nof CroSel, which consistently outperforms previous state-of-the-art methods on\nbenchmark datasets. Additionally, our method achieves over 90\\% accuracy and\nquantity for selecting true labels on CIFAR-type datasets under various\nsettings.\n","authors":["Shiyu Tian","Hongxin Wei","Yiqun Wang","Lei Feng"],"pdf_url":"https://arxiv.org/pdf/2303.10365v3.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18519v1","updated":"2024-03-27T12:50:27Z","published":"2024-03-27T12:50:27Z","title":"Improving Line Search Methods for Large Scale Neural Network Training","summary":"  In recent studies, line search methods have shown significant improvements in\nthe performance of traditional stochastic gradient descent techniques,\neliminating the need for a specific learning rate schedule. In this paper, we\nidentify existing issues in state-of-the-art line search methods, propose\nenhancements, and rigorously evaluate their effectiveness. We test these\nmethods on larger datasets and more complex data domains than before.\nSpecifically, we improve the Armijo line search by integrating the momentum\nterm from ADAM in its search direction, enabling efficient large-scale\ntraining, a task that was previously prone to failure using Armijo line search\nmethods. Our optimization approach outperforms both the previous Armijo\nimplementation and tuned learning rate schedules for Adam. Our evaluation\nfocuses on Transformers and CNNs in the domains of NLP and image data. Our work\nis publicly available as a Python package, which provides a hyperparameter free\nPytorch optimizer.\n","authors":["Philip Kenneweg","Tristan Kenneweg","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2403.18519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18517v1","updated":"2024-03-27T12:49:14Z","published":"2024-03-27T12:49:14Z","title":"Efficient Algorithms for Regularized Nonnegative Scale-invariant\n  Low-rank Approximation Models","summary":"  Regularized nonnegative low-rank approximations such as sparse Nonnegative\nMatrix Factorization or sparse Nonnegative Tucker Decomposition are an\nimportant branch of dimensionality reduction models with enhanced\ninterpretability. However, from a practical perspective, the choice of\nregularizers and regularization coefficients, as well as the design of\nefficient algorithms, is challenging because of the multifactor nature of these\nmodels and the lack of theory to back these choices. This paper aims at\nimproving upon these issues. By studying a more general model called the\nHomogeneous Regularized Scale-Invariant, we prove that the scale-invariance\ninherent to low-rank approximation models causes an implicit regularization\nwith both unexpected beneficial and detrimental effects. This observation\nallows to better understand the effect of regularization functions in low-rank\napproximation models, to guide the choice of the regularization\nhyperparameters, and to design balancing strategies to enhance the convergence\nspeed of dedicated optimization algorithms. Some of these results were already\nknown but restricted to specific instances of regularized low-rank\napproximations. We also derive a generic Majorization Minimization algorithm\nthat handles many regularized nonnegative low-rank approximations, with\nconvergence guarantees. We showcase our contributions on sparse Nonnegative\nMatrix Factorization, ridge-regularized Canonical Polyadic decomposition and\nsparse Nonnegative Tucker Decomposition.\n","authors":["Jeremy E. Cohen","Valentin Leplat"],"pdf_url":"https://arxiv.org/pdf/2403.18517v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18514v1","updated":"2024-03-27T12:44:57Z","published":"2024-03-27T12:44:57Z","title":"CT-3DFlow : Leveraging 3D Normalizing Flows for Unsupervised Detection\n  of Pathological Pulmonary CT scans","summary":"  Unsupervised pathology detection can be implemented by training a model on\nhealthy data only and measuring the deviation from the training set upon\ninference, for example with CNN-based feature extraction and one-class\nclassifiers, or reconstruction-score-based methods such as AEs, GANs and\nDiffusion models. Normalizing Flows (NF) have the ability to directly learn the\nprobability distribution of training examples through an invertible\narchitecture. We leverage this property in a novel 3D NF-based model named\nCT-3DFlow, specifically tailored for patient-level pulmonary pathology\ndetection in chest CT data. Our model is trained unsupervised on healthy 3D\npulmonary CT patches, and detects deviations from its log-likelihood\ndistribution as anomalies. We aggregate patches-level likelihood values from a\npatient's CT scan to provide a patient-level 'normal'/'abnormal' prediction.\nOut-of-distribution detection performance is evaluated using expert annotations\non a separate chest CT test dataset, outperforming other state-of-the-art\nmethods.\n","authors":["Aissam Djahnine","Alexandre Popoff","Emilien Jupin-Delevaux","Vincent Cottin","Olivier Nempont","Loic Boussel"],"pdf_url":"https://arxiv.org/pdf/2403.18514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18509v1","updated":"2024-03-27T12:39:16Z","published":"2024-03-27T12:39:16Z","title":"Distributed Maximum Consensus over Noisy Links","summary":"  We introduce a distributed algorithm, termed noise-robust distributed maximum\nconsensus (RD-MC), for estimating the maximum value within a multi-agent\nnetwork in the presence of noisy communication links. Our approach entails\nredefining the maximum consensus problem as a distributed optimization problem,\nallowing a solution using the alternating direction method of multipliers.\nUnlike existing algorithms that rely on multiple sets of noise-corrupted\nestimates, RD-MC employs a single set, enhancing both robustness and\nefficiency. To further mitigate the effects of link noise and improve\nrobustness, we apply moving averaging to the local estimates. Through extensive\nsimulations, we demonstrate that RD-MC is significantly more robust to\ncommunication link noise compared to existing maximum-consensus algorithms.\n","authors":["Ehsan Lari","Reza Arablouei","Naveen K. D. Venkategowda","Stefan Werner"],"pdf_url":"https://arxiv.org/pdf/2403.18509v1.pdf","comment":"5 pages, 7 figures, submitted to EUSIPCO 2024 conference"},{"id":"http://arxiv.org/abs/2403.18506v1","updated":"2024-03-27T12:35:23Z","published":"2024-03-27T12:35:23Z","title":"Faster Convergence for Transformer Fine-tuning with Line Search Methods","summary":"  Recent works have shown that line search methods greatly increase performance\nof traditional stochastic gradient descent methods on a variety of datasets and\narchitectures [1], [2]. In this work we succeed in extending line search\nmethods to the novel and highly popular Transformer architecture and dataset\ndomains in natural language processing. More specifically, we combine the\nArmijo line search with the Adam optimizer and extend it by subdividing the\nnetworks architecture into sensible units and perform the line search\nseparately on these local units. Our optimization method outperforms the\ntraditional Adam optimizer and achieves significant performance improvements\nfor small data sets or small training budgets, while performing equal or better\nfor other tested cases. Our work is publicly available as a python package,\nwhich provides a hyperparameter-free pytorch optimizer that is compatible with\narbitrary network architectures.\n","authors":["Philip Kenneweg","Leonardo Galli","Tristan Kenneweg","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2403.18506v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08579v2","updated":"2024-03-27T12:28:02Z","published":"2024-03-13T14:34:34Z","title":"Machine Learning Optimized Orthogonal Basis Piecewise Polynomial\n  Approximation","summary":"  Piecewise Polynomials (PPs) are utilized in several engineering disciplines,\nlike trajectory planning, to approximate position profiles given in the form of\na set of points. While the approximation target along with domain-specific\nrequirements, like Ck -continuity, can be formulated as a system of equations\nand a result can be computed directly, such closed-form solutions posses\nlimited flexibility with respect to polynomial degrees, polynomial bases or\nadding further domain-specific requirements. Sufficiently complex optimization\ngoals soon call for the use of numerical methods, like gradient descent. Since\ngradient descent lies at the heart of training Artificial Neural Networks\n(ANNs), modern Machine Learning (ML) frameworks like TensorFlow come with a set\nof gradient-based optimizers potentially suitable for a wide range of\noptimization problems beyond the training task for ANNs. Our approach is to\nutilize the versatility of PP models and combine it with the potential of\nmodern ML optimizers for the use in function approximation in 1D trajectory\nplanning in the context of electronic cam design. We utilize available\noptimizers of the ML framework TensorFlow directly, outside of the scope of\nANNs, to optimize model parameters of our PP model. In this paper, we show how\nan orthogonal polynomial basis contributes to improving approximation and\ncontinuity optimization performance. Utilizing Chebyshev polynomials of the\nfirst kind, we develop a novel regularization approach enabling clearly\nimproved convergence behavior. We show that, using this regularization\napproach, Chebyshev basis performs better than power basis for all relevant\noptimizers in the combined approximation and continuity optimization setting\nand demonstrate usability of the presented approach within the electronic cam\ndomain.\n","authors":["Hannes Waclawek","Stefan Huber"],"pdf_url":"https://arxiv.org/pdf/2403.08579v2.pdf","comment":"Submitted to LION18"},{"id":"http://arxiv.org/abs/2311.04698v3","updated":"2024-03-27T12:24:17Z","published":"2023-11-08T14:10:19Z","title":"Challenging Common Paradigms in Multi-Task Learning","summary":"  While multi-task learning (MTL) has gained significant attention in recent\nyears, its underlying mechanisms remain poorly understood. Recent methods did\nnot yield consistent performance improvements over single task learning (STL)\nbaselines, underscoring the importance of gaining more profound insights about\nchallenges specific to MTL. In our study, we challenge paradigms in MTL in the\ncontext of STL: First, the impact of the choice of optimizer has only been\nmildly investigated in MTL. We show the pivotal role of common STL tools such\nas the Adam optimizer in MTL empirically in various experiments. To further\ninvestigate Adam's effectiveness, we theoretical derive a partial loss-scale\ninvariance under mild assumptions. Second, the notion of gradient conflicts has\noften been phrased as a specific problem in MTL. We delve into the role of\ngradient conflicts in MTL and compare it to STL. For angular gradient alignment\nwe find no evidence that this is a unique problem in MTL. We emphasize\ndifferences in gradient magnitude as the main distinguishing factor. Lastly, we\ncompare the transferability of features learned through MTL and STL on common\nimage corruptions, and find light evidence that MTL can lead to superior\ntransferability. Overall, we find surprising similarities between STL and MTL\nsuggesting to consider methods from both fields in a broader context.\n","authors":["Cathrin Elich","Lukas Kirchdorfer","Jan M. Köhler","Lukas Schott"],"pdf_url":"https://arxiv.org/pdf/2311.04698v3.pdf","comment":"-"},{"id":"http://arxiv.org/abs/2403.18495v1","updated":"2024-03-27T12:15:22Z","published":"2024-03-27T12:15:22Z","title":"Direct mineral content prediction from drill core images via transfer\n  learning","summary":"  Deep subsurface exploration is important for mining, oil and gas industries,\nas well as in the assessment of geological units for the disposal of chemical\nor nuclear waste, or the viability of geothermal energy systems. Typically,\ndetailed examinations of subsurface formations or units are performed on\ncuttings or core materials extracted during drilling campaigns, as well as on\ngeophysical borehole data, which provide detailed information about the\npetrophysical properties of the rocks. Depending on the volume of rock samples\nand the analytical program, the laboratory analysis and diagnostics can be very\ntime-consuming. This study investigates the potential of utilizing machine\nlearning, specifically convolutional neural networks (CNN), to assess the\nlithology and mineral content solely from analysis of drill core images, aiming\nto support and expedite the subsurface geological exploration. The paper\noutlines a comprehensive methodology, encompassing data preprocessing, machine\nlearning methods, and transfer learning techniques. The outcome reveals a\nremarkable 96.7% accuracy in the classification of drill core segments into\ndistinct formation classes. Furthermore, a CNN model was trained for the\nevaluation of mineral content using a learning data set from multidimensional\nlog analysis data (silicate, total clay, carbonate). When benchmarked against\nlaboratory XRD measurements on samples from the cores, both the advanced\nmultidimensional log analysis model and the neural network approach developed\nhere provide equally good performance. This work demonstrates that deep\nlearning and particularly transfer learning can support extracting\npetrophysical properties, including mineral content and formation\nclassification, from drill core images, thus offering a road map for enhancing\nmodel performance and data set quality in image-based analysis of drill cores.\n","authors":["Romana Boiger","Sergey V. Churakov","Ignacio Ballester Llagaria","Georg Kosakowski","Raphael Wüst","Nikolaos I. Prasianakis"],"pdf_url":"https://arxiv.org/pdf/2403.18495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18494v1","updated":"2024-03-27T12:10:30Z","published":"2024-03-27T12:10:30Z","title":"Learning in PINNs: Phase transition, total diffusion, and generalization","summary":"  We investigate the learning dynamics of fully-connected neural networks\nthrough the lens of gradient signal-to-noise ratio (SNR), examining the\nbehavior of first-order optimizers like Adam in non-convex objectives. By\ninterpreting the drift/diffusion phases in the information bottleneck theory,\nfocusing on gradient homogeneity, we identify a third phase termed ``total\ndiffusion\", characterized by equilibrium in the learning rates and homogeneous\ngradients. This phase is marked by an abrupt SNR increase, uniform residuals\nacross the sample space and the most rapid training convergence. We propose a\nresidual-based re-weighting scheme to accelerate this diffusion in quadratic\nloss functions, enhancing generalization. We also explore the information\ncompression phenomenon, pinpointing a significant saturation-induced\ncompression of activations at the total diffusion phase, with deeper layers\nexperiencing negligible information loss. Supported by experimental data on\nphysics-informed neural networks (PINNs), which underscore the importance of\ngradient homogeneity due to their PDE-based sample inter-dependence, our\nfindings suggest that recognizing phase transitions could refine ML\noptimization strategies for improved generalization.\n","authors":["Sokratis J. Anagnostopoulos","Juan Diego Toscano","Nikolaos Stergiopulos","George Em Karniadakis"],"pdf_url":"https://arxiv.org/pdf/2403.18494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18489v1","updated":"2024-03-27T12:01:51Z","published":"2024-03-27T12:01:51Z","title":"Impact of Employing Weather Forecast Data as Input to the Estimation of\n  Evapotranspiration by Deep Neural Network Models","summary":"  Reference Evapotranspiration (ET0) is a key parameter for designing smart\nirrigation scheduling, since it is related by a coefficient to the water needs\nof a crop. The United Nations Food and Agriculture Organization, proposed a\nstandard method for ET0 computation (FAO56PM), based on the parameterization of\nthe Penman-Monteith equation, that is widely adopted in the literature. To\ncompute ET0 using the FAO56-PM method, four main weather parameters are needed:\ntemperature, humidity, wind, and solar radiation (SR). One way to make daily\nET0 estimations for future days is to use freely available weather forecast\nservices (WFSs), where many meteorological parameters are estimated up to the\nnext 15 days. A problem with this method is that currently, SR is not provided\nas a free forecast parameter on most of those online services or, normally,\nsuch forecasts present a financial cost penalty. For this reason, several ET0\nestimation models using machine and deep learning were developed and presented\nin the literature, that use as input features a reduced set of carefully\nselected weather parameters, that are compatible with common freely available\nWFSs. However, most studies on this topic have only evaluated model performance\nusing data from weather stations (WSs), without considering the effect of using\nweather forecast data. In this study, the performance of authors' previous\nmodels is evaluated when using weather forecast data from two online WFSs, in\nthe following scenarios: (i) direct ET0 estimation by an ANN model, and (ii)\nestimate SR by ANN model, and then use that estimation for ET0 computation,\nusing the FAO56-PM method. Employing data collected from two WFSs and a WS\nlocated in Vale do Lobo, Portugal, the latter approach achieved the best\nresult, with a coefficient of determination (R2) ranging between 0.893 and\n0.667, when considering forecasts up to 15 days.\n","authors":["Pedro J. Vaz","Gabriela Schütz","Carlos Guerrero","Pedro J. S. Cardoso"],"pdf_url":"https://arxiv.org/pdf/2403.18489v1.pdf","comment":"A partial version of the work submitted to ESRE/INTERNATIONAL\n  CONFERENCE ON ENVIRONMENTAL SCIENCES AND RENEWABLE ENERGY"},{"id":"http://arxiv.org/abs/2403.18486v1","updated":"2024-03-27T11:58:45Z","published":"2024-03-27T11:58:45Z","title":"Synthesizing EEG Signals from Event-Related Potential Paradigms with\n  Conditional Diffusion Models","summary":"  Data scarcity in the brain-computer interface field can be alleviated through\nthe use of generative models, specifically diffusion models. While diffusion\nmodels have previously been successfully applied to electroencephalogram (EEG)\ndata, existing models lack flexibility w.r.t.~sampling or require alternative\nrepresentations of the EEG data. To overcome these limitations, we introduce a\nnovel approach to conditional diffusion models that utilizes classifier-free\nguidance to directly generate subject-, session-, and class-specific EEG data.\nIn addition to commonly used metrics, domain-specific metrics are employed to\nevaluate the specificity of the generated samples. The results indicate that\nthe proposed model can generate EEG data that resembles real data for each\nsubject, session, and class.\n","authors":["Guido Klein","Pierre Guetschel","Gianluigi Silvestri","Michael Tangermann"],"pdf_url":"https://arxiv.org/pdf/2403.18486v1.pdf","comment":"submitted to 9th Graz BCI conference, 6 pages, 3 figures, first\n  figure is split into two subfigures, 1 table"},{"id":"http://arxiv.org/abs/2311.12028v2","updated":"2024-03-27T11:43:28Z","published":"2023-11-20T18:59:51Z","title":"Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose\n  Estimation","summary":"  Transformers have been successfully applied in the field of video-based 3D\nhuman pose estimation. However, the high computational costs of these video\npose transformers (VPTs) make them impractical on resource-constrained devices.\nIn this paper, we present a plug-and-play pruning-and-recovering framework,\ncalled Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose\nestimation from videos. Our HoT begins with pruning pose tokens of redundant\nframes and ends with recovering full-length tokens, resulting in a few pose\ntokens in the intermediate transformer blocks and thus improving the model\nefficiency. To effectively achieve this, we propose a token pruning cluster\n(TPC) that dynamically selects a few representative tokens with high semantic\ndiversity while eliminating the redundancy of video frames. In addition, we\ndevelop a token recovering attention (TRA) to restore the detailed\nspatio-temporal information based on the selected tokens, thereby expanding the\nnetwork output to the original full-length temporal resolution for fast\ninference. Extensive experiments on two benchmark datasets (i.e., Human3.6M and\nMPI-INF-3DHP) demonstrate that our method can achieve both high efficiency and\nestimation accuracy compared to the original VPT models. For instance, applying\nto MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPs\nwithout sacrificing accuracy and nearly 40% FLOPs with only 0.2% accuracy drop,\nrespectively. Code and models are available at\nhttps://github.com/NationalGAILab/HoT.\n","authors":["Wenhao Li","Mengyuan Liu","Hong Liu","Pichao Wang","Jialun Cai","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2311.12028v2.pdf","comment":"Accepted by CVPR 2024, Open Sourced"},{"id":"http://arxiv.org/abs/2403.18452v1","updated":"2024-03-27T11:11:08Z","published":"2024-03-27T11:11:08Z","title":"SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model","summary":"  There are five types of trajectory prediction tasks: deterministic,\nstochastic, domain adaptation, momentary observation, and few-shot. These\nassociated tasks are defined by various factors, such as the length of input\npaths, data split and pre-processing methods. Interestingly, even though they\ncommonly take sequential coordinates of observations as input and infer future\npaths in the same coordinates as output, designing specialized architectures\nfor each task is still necessary. For the other task, generality issues can\nlead to sub-optimal performances. In this paper, we propose SingularTrajectory,\na diffusion-based universal trajectory prediction framework to reduce the\nperformance gap across the five tasks. The core of SingularTrajectory is to\nunify a variety of human dynamics representations on the associated tasks. To\ndo this, we first build a Singular space to project all types of motion\npatterns from each task into one embedding space. We next propose an adaptive\nanchor working in the Singular space. Unlike traditional fixed anchor methods\nthat sometimes yield unacceptable paths, our adaptive anchor enables correct\nanchors, which are put into a wrong location, based on a traversability map.\nFinally, we adopt a diffusion-based predictor to further enhance the prototype\npaths using a cascaded denoising process. Our unified framework ensures the\ngenerality across various benchmark settings such as input modality, and\ntrajectory lengths. Extensive experiments on five public benchmarks demonstrate\nthat SingularTrajectory substantially outperforms existing models, highlighting\nits effectiveness in estimating general dynamics of human movements. Code is\npublicly available at https://github.com/inhwanbae/SingularTrajectory .\n","authors":["Inhwan Bae","Young-Jae Park","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2403.18452v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18451v1","updated":"2024-03-27T11:11:06Z","published":"2024-03-27T11:11:06Z","title":"CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in\n  Resource-Constrained CPS and IoT","summary":"  Foundation models (FMs) emerge as a promising solution to harness distributed\nand diverse environmental data by leveraging prior knowledge to understand the\ncomplicated temporal and spatial correlations within heterogeneous datasets.\nUnlike distributed learning frameworks such as federated learning, which often\nstruggle with multimodal data, FMs can transform diverse inputs into\nembeddings. This process facilitates the integration of information from\nvarious modalities and the application of prior learning to new domains.\nHowever, deploying FMs in resource-constrained edge systems poses significant\nchallenges. To this end, we introduce CoRAST, a novel learning framework that\nutilizes FMs for enhanced analysis of distributed, correlated heterogeneous\ndata. Utilizing a server-based FM, CoRAST can exploit existing environment\ninformation to extract temporal, spatial, and cross-modal correlations among\nsensor data. This enables CoRAST to offer context-aware insights for localized\nclient tasks through FM-powered global representation learning. Our evaluation\non real-world weather dataset demonstrates CoRAST's ability to exploit\ncorrelated heterogeneous data through environmental representation learning to\nreduce the forecast errors by up to 50.3% compared to the baselines.\n","authors":["Yi Hu","Jinhang Zuo","Alanis Zhao","Bob Iannucci","Carlee Joe-Wong"],"pdf_url":"https://arxiv.org/pdf/2403.18451v1.pdf","comment":"accepted and to be published in 2024 IEEE International Workshop on\n  Foundation Models for Cyber-Physical Systems & Internet of Things (FMSys)"},{"id":"http://arxiv.org/abs/2403.09267v3","updated":"2024-03-27T11:11:02Z","published":"2024-03-14T10:44:10Z","title":"Deep Limit Order Book Forecasting","summary":"  We exploit cutting-edge deep learning methodologies to explore the\npredictability of high-frequency Limit Order Book mid-price changes for a\nheterogeneous set of stocks traded on the NASDAQ exchange. In so doing, we\nrelease `LOBFrame', an open-source code base to efficiently process large-scale\nLimit Order Book data and quantitatively assess state-of-the-art deep learning\nmodels' forecasting capabilities. Our results are twofold. We demonstrate that\nthe stocks' microstructural characteristics influence the efficacy of deep\nlearning methods and that their high forecasting power does not necessarily\ncorrespond to actionable trading signals. We argue that traditional machine\nlearning metrics fail to adequately assess the quality of forecasts in the\nLimit Order Book context. As an alternative, we propose an innovative\noperational framework that evaluates predictions' practicality by focusing on\nthe probability of accurately forecasting complete transactions. This work\noffers academics and practitioners an avenue to make informed and robust\ndecisions on the application of deep learning techniques, their scope and\nlimitations, effectively exploiting emergent statistical properties of the\nLimit Order Book.\n","authors":["Antonio Briola","Silvia Bartolucci","Tomaso Aste"],"pdf_url":"https://arxiv.org/pdf/2403.09267v3.pdf","comment":"43 pages, 14 figures, 12 Tables"},{"id":"http://arxiv.org/abs/2403.18447v1","updated":"2024-03-27T11:06:44Z","published":"2024-03-27T11:06:44Z","title":"Can Language Beat Numerical Regression? Language-Based Multimodal\n  Trajectory Prediction","summary":"  Language models have demonstrated impressive ability in context understanding\nand generative performance. Inspired by the recent success of language\nfoundation models, in this paper, we propose LMTraj (Language-based Multimodal\nTrajectory predictor), which recasts the trajectory prediction task into a sort\nof question-answering problem. Departing from traditional numerical regression\nmodels, which treat the trajectory coordinate sequence as continuous signals,\nwe consider them as discrete signals like text prompts. Specially, we first\ntransform an input space for the trajectory coordinate into the natural\nlanguage space. Here, the entire time-series trajectories of pedestrians are\nconverted into a text prompt, and scene images are described as text\ninformation through image captioning. The transformed numerical and image data\nare then wrapped into the question-answering template for use in a language\nmodel. Next, to guide the language model in understanding and reasoning\nhigh-level knowledge, such as scene context and social relationships between\npedestrians, we introduce an auxiliary multi-task question and answering. We\nthen train a numerical tokenizer with the prompt data. We encourage the\ntokenizer to separate the integer and decimal parts well, and leverage it to\ncapture correlations between the consecutive numbers in the language model.\nLastly, we train the language model using the numerical tokenizer and all of\nthe question-answer prompts. Here, we propose a beam-search-based most-likely\nprediction and a temperature-based multimodal prediction to implement both\ndeterministic and stochastic inferences. Applying our LMTraj, we show that the\nlanguage-based model can be a powerful pedestrian trajectory predictor, and\noutperforms existing numerical-based predictor methods. Code is publicly\navailable at https://github.com/inhwanbae/LMTrajectory .\n","authors":["Inhwan Bae","Junoh Lee","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2403.18447v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18444v1","updated":"2024-03-27T11:00:53Z","published":"2024-03-27T11:00:53Z","title":"FRESCO: Federated Reinforcement Energy System for Cooperative\n  Optimization","summary":"  The rise in renewable energy is creating new dynamics in the energy grid that\npromise to create a cleaner and more participative energy grid, where\ntechnology plays a crucial part in making the required flexibility to achieve\nthe vision of the next-generation grid. This work presents FRESCO, a framework\nthat aims to ease the implementation of energy markets using a hierarchical\ncontrol architecture of reinforcement learning agents trained using federated\nlearning. The core concept we are proving is that having greedy agents subject\nto changing conditions from a higher level agent creates a cooperative setup\nthat will allow for fulfilling all the individual objectives. This paper\npresents a general overview of the framework, the current progress, and some\ninsights we obtained from the recent results.\n","authors":["Nicolas Mauricio Cuadrado","Roberto Alejandro Gutierrez","Martin Takáč"],"pdf_url":"https://arxiv.org/pdf/2403.18444v1.pdf","comment":"Tiny Paper at ICLR 2023"},{"id":"http://arxiv.org/abs/2403.18439v1","updated":"2024-03-27T10:47:06Z","published":"2024-03-27T10:47:06Z","title":"Generalized Policy Learning for Smart Grids: FL TRPO Approach","summary":"  The smart grid domain requires bolstering the capabilities of existing energy\nmanagement systems; Federated Learning (FL) aligns with this goal as it\ndemonstrates a remarkable ability to train models on heterogeneous datasets\nwhile maintaining data privacy, making it suitable for smart grid applications,\nwhich often involve disparate data distributions and interdependencies among\nfeatures that hinder the suitability of linear models. This paper introduces a\nframework that combines FL with a Trust Region Policy Optimization (FL TRPO)\naiming to reduce energy-associated emissions and costs. Our approach reveals\nlatent interconnections and employs personalized encoding methods to capture\nunique insights, understanding the relationships between features and optimal\nstrategies, allowing our model to generalize to previously unseen data.\nExperimental results validate the robustness of our approach, affirming its\nproficiency in effectively learning policy models for smart grid challenges.\n","authors":["Yunxiang Li","Nicolas Mauricio Cuadrado","Samuel Horváth","Martin Takáč"],"pdf_url":"https://arxiv.org/pdf/2403.18439v1.pdf","comment":"ICLR 2024 Workshop: Tackling Climate Change with Machine Learning"},{"id":"http://arxiv.org/abs/2403.18438v1","updated":"2024-03-27T10:45:16Z","published":"2024-03-27T10:45:16Z","title":"Global Vegetation Modeling with Pre-Trained Weather Transformers","summary":"  Accurate vegetation models can produce further insights into the complex\ninteraction between vegetation activity and ecosystem processes. Previous\nresearch has established that long-term trends and short-term variability of\ntemperature and precipitation affect vegetation activity. Motivated by the\nrecent success of Transformer-based Deep Learning models for medium-range\nweather forecasting, we adapt the publicly available pre-trained FourCastNet to\nmodel vegetation activity while accounting for the short-term dynamics of\nclimate variability. We investigate how the learned global representation of\nthe atmosphere's state can be transferred to model the normalized difference\nvegetation index (NDVI). Our model globally estimates vegetation activity at a\nresolution of \\SI{0.25}{\\degree} while relying only on meteorological data. We\ndemonstrate that leveraging pre-trained weather models improves the NDVI\nestimates compared to learning an NDVI model from scratch. Additionally, we\ncompare our results to other recent data-driven NDVI modeling approaches from\nmachine learning and ecology literature. We further provide experimental\nevidence on how much data and training time is necessary to turn FourCastNet\ninto an effective vegetation model. Code and models will be made available upon\npublication.\n","authors":["Pascal Janetzky","Florian Gallusser","Simon Hentschel","Andreas Hotho","Anna Krause"],"pdf_url":"https://arxiv.org/pdf/2403.18438v1.pdf","comment":"Tackling Climate Change with Machine Learning Workshop @ ICLR 2024"},{"id":"http://arxiv.org/abs/2403.18436v1","updated":"2024-03-27T10:40:27Z","published":"2024-03-27T10:40:27Z","title":"Collaborative Active Learning in Conditional Trust Environment","summary":"  In this paper, we investigate collaborative active learning, a paradigm in\nwhich multiple collaborators explore a new domain by leveraging their combined\nmachine learning capabilities without disclosing their existing data and\nmodels. Instead, the collaborators share prediction results from the new domain\nand newly acquired labels. This collaboration offers several advantages: (a) it\naddresses privacy and security concerns by eliminating the need for direct\nmodel and data disclosure; (b) it enables the use of different data sources and\ninsights without direct data exchange; and (c) it promotes cost-effectiveness\nand resource efficiency through shared labeling costs. To realize these\nbenefits, we introduce a collaborative active learning framework designed to\nfulfill the aforementioned objectives. We validate the effectiveness of the\nproposed framework through simulations. The results demonstrate that\ncollaboration leads to higher AUC scores compared to independent efforts,\nhighlighting the framework's ability to overcome the limitations of individual\nmodels. These findings support the use of collaborative approaches in active\nlearning, emphasizing their potential to enhance outcomes through collective\nexpertise and shared resources. Our work provides a foundation for further\nresearch on collaborative active learning and its practical applications in\nvarious domains where data privacy, cost efficiency, and model performance are\ncritical considerations.\n","authors":["Zan-Kai Chong","Hiroyuki Ohsaki","Bryan Ng"],"pdf_url":"https://arxiv.org/pdf/2403.18436v1.pdf","comment":"5 pages, 9 figures, conference"},{"id":"http://arxiv.org/abs/2403.18425v1","updated":"2024-03-27T10:26:42Z","published":"2024-03-27T10:26:42Z","title":"U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models","summary":"  Diffusion models have demonstrated remarkable performance in text-to-image\nsynthesis, producing realistic and high resolution images that faithfully\nadhere to the corresponding text-prompts. Despite their great success, they\nstill fall behind in sketch-to-image synthesis tasks, where in addition to\ntext-prompts, the spatial layout of the generated images has to closely follow\nthe outlines of certain reference sketches. Employing an MLP latent edge\npredictor to guide the spatial layout of the synthesized image by predicting\nedge maps at each denoising step has been recently proposed. Despite yielding\npromising results, the pixel-wise operation of the MLP does not take into\naccount the spatial layout as a whole, and demands numerous denoising\niterations to produce satisfactory images, leading to time inefficiency. To\nthis end, we introduce U-Sketch, a framework featuring a U-Net type latent edge\npredictor, which is capable of efficiently capturing both local and global\nfeatures, as well as spatial correlations between pixels. Moreover, we propose\nthe addition of a sketch simplification network that offers the user the choice\nof preprocessing and simplifying input sketches for enhanced outputs. The\nexperimental results, corroborated by user feedback, demonstrate that our\nproposed U-Net latent edge predictor leads to more realistic results, that are\nbetter aligned with the spatial outlines of the reference sketches, while\ndrastically reducing the number of required denoising steps and, consequently,\nthe overall execution time.\n","authors":["Ilias Mitsouras","Eleftherios Tsonis","Paraskevi Tzouveli","Athanasios Voulodimos"],"pdf_url":"https://arxiv.org/pdf/2403.18425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18423v1","updated":"2024-03-27T10:24:25Z","published":"2024-03-27T10:24:25Z","title":"SemRoDe: Macro Adversarial Training to Learn Representations That are\n  Robust to Word-Level Attacks","summary":"  Language models (LMs) are indispensable tools for natural language processing\ntasks, but their vulnerability to adversarial attacks remains a concern. While\ncurrent research has explored adversarial training techniques, their\nimprovements to defend against word-level attacks have been limited. In this\nwork, we propose a novel approach called Semantic Robust Defence (SemRoDe), a\nMacro Adversarial Training strategy to enhance the robustness of LMs. Drawing\ninspiration from recent studies in the image domain, we investigate and later\nconfirm that in a discrete data setting such as language, adversarial samples\ngenerated via word substitutions do indeed belong to an adversarial domain\nexhibiting a high Wasserstein distance from the base domain. Our method learns\na robust representation that bridges these two domains. We hypothesize that if\nsamples were not projected into an adversarial domain, but instead to a domain\nwith minimal shift, it would improve attack robustness. We align the domains by\nincorporating a new distance-based objective. With this, our model is able to\nlearn more generalized representations by aligning the model's high-level\noutput features and therefore better handling unseen adversarial samples. This\nmethod can be generalized across word embeddings, even when they share minimal\noverlap at both vocabulary and word-substitution levels. To evaluate the\neffectiveness of our approach, we conduct experiments on BERT and RoBERTa\nmodels on three datasets. The results demonstrate promising state-of-the-art\nrobustness.\n","authors":["Brian Formento","Wenjie Feng","Chuan Sheng Foo","Luu Anh Tuan","See-Kiong Ng"],"pdf_url":"https://arxiv.org/pdf/2403.18423v1.pdf","comment":"Published in NAACL 2024 (Main Track)"},{"id":"http://arxiv.org/abs/2402.01739v2","updated":"2024-03-27T10:21:24Z","published":"2024-01-29T12:05:02Z","title":"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models","summary":"  To help the open-source community have a better understanding of\nMixture-of-Experts (MoE) based large language models (LLMs), we train and\nrelease OpenMoE, a series of fully open-sourced and reproducible decoder-only\nMoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T\ntokens. Our investigation confirms that MoE-based LLMs can offer a more\nfavorable cost-effectiveness trade-off than dense LLMs, highlighting the\npotential effectiveness for future LLM development.\n  One more important contribution of this study is an in-depth analysis of the\nrouting mechanisms within our OpenMoE models, leading to three significant\nfindings: Context-Independent Specialization, Early Routing Learning, and\nDrop-towards-the-End. We discovered that routing decisions in MoE models are\npredominantly based on token IDs, with minimal context relevance. The\ntoken-to-expert assignments are determined early in the pre-training phase and\nremain largely unchanged. This imperfect routing can result in performance\ndegradation, particularly in sequential tasks like multi-turn conversations,\nwhere tokens appearing later in a sequence are more likely to be dropped.\nFinally, we rethink our design based on the above-mentioned observations and\nanalysis. To facilitate future MoE LLM development, we propose potential\nstrategies for mitigating the issues we found and further improving\noff-the-shelf MoE LLM designs.\n","authors":["Fuzhao Xue","Zian Zheng","Yao Fu","Jinjie Ni","Zangwei Zheng","Wangchunshu Zhou","Yang You"],"pdf_url":"https://arxiv.org/pdf/2402.01739v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01191v2","updated":"2024-03-27T10:12:31Z","published":"2023-11-02T12:36:19Z","title":"VIGraph: Generative Self-supervised Learning for Class-Imbalanced Node\n  Classification","summary":"  Class imbalance in graph data presents significant challenges for node\nclassification. While existing methods, such as SMOTE-based approaches,\npartially mitigate this issue, they still exhibit limitations in constructing\nimbalanced graphs. Generative self-supervised learning (SSL) methods,\nexemplified by graph autoencoders (GAEs), offer a promising solution by\ndirectly generating minority nodes from the data itself, yet their potential\nremains underexplored. In this paper, we delve into the shortcomings of\nSMOTE-based approaches in the construction of imbalanced graphs. Furthermore,\nwe introduce VIGraph, a simple yet effective generative SSL approach that\nrelies on the Variational GAE as the fundamental model. VIGraph strictly\nadheres to the concept of imbalance when constructing imbalanced graphs and\ninnovatively leverages the variational inference (VI) ability of Variational\nGAE to generate nodes for minority classes. VIGraph introduces comprehensive\ntraining strategies, including cross-view contrastive learning at the decoding\nphase to capture semantic knowledge, adjacency matrix reconstruction to\npreserve graph structure, and alignment strategy to ensure stable training.\nVIGraph can generate high-quality nodes directly usable for classification,\neliminating the need to integrate the generated nodes back to the graph as well\nas additional retraining found in SMOTE-based methods. We conduct extensive\nexperiments, results from which demonstrate the superiority and generality of\nour approach.\n","authors":["Yulan Hu","Sheng Ouyang","Zhirui Yang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2311.01191v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18415v1","updated":"2024-03-27T10:06:33Z","published":"2024-03-27T10:06:33Z","title":"The Topos of Transformer Networks","summary":"  The transformer neural network has significantly out-shined all other neural\nnetwork architectures as the engine behind large language models. We provide a\ntheoretical analysis of the expressivity of the transformer architecture\nthrough the lens of topos theory. From this viewpoint, we show that many common\nneural network architectures, such as the convolutional, recurrent and graph\nconvolutional networks, can be embedded in a pretopos of piecewise-linear\nfunctions, but that the transformer necessarily lives in its topos completion.\nIn particular, this suggests that the two network families instantiate\ndifferent fragments of logic: the former are first order, whereas transformers\nare higher-order reasoners. Furthermore, we draw parallels with architecture\nsearch and gradient descent, integrating our analysis in the framework of\ncybernetic agents.\n","authors":["Mattia Jacopo Villani","Peter McBurney"],"pdf_url":"https://arxiv.org/pdf/2403.18415v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06075v2","updated":"2024-03-27T09:51:15Z","published":"2023-09-12T09:12:37Z","title":"A2V: A Semi-Supervised Domain Adaptation Framework for Brain Vessel\n  Segmentation via Two-Phase Training Angiography-to-Venography Translation","summary":"  We present a semi-supervised domain adaptation framework for brain vessel\nsegmentation from different image modalities. Existing state-of-the-art methods\nfocus on a single modality, despite the wide range of available cerebrovascular\nimaging techniques. This can lead to significant distribution shifts that\nnegatively impact the generalization across modalities. By relying on annotated\nangiographies and a limited number of annotated venographies, our framework\naccomplishes image-to-image translation and semantic segmentation, leveraging a\ndisentangled and semantically rich latent space to represent heterogeneous data\nand perform image-level adaptation from source to target domains. Moreover, we\nreduce the typical complexity of cycle-based architectures and minimize the use\nof adversarial training, which allows us to build an efficient and intuitive\nmodel with stable training. We evaluate our method on magnetic resonance\nangiographies and venographies. While achieving state-of-the-art performance in\nthe source domain, our method attains a Dice score coefficient in the target\ndomain that is only 8.9% lower, highlighting its promising potential for robust\ncerebrovascular image segmentation across different modalities.\n","authors":["Francesco Galati","Daniele Falcetta","Rosa Cortese","Barbara Casolla","Ferran Prados","Ninon Burgos","Maria A. Zuluaga"],"pdf_url":"https://arxiv.org/pdf/2309.06075v2.pdf","comment":"Accepted at the 34th British Machine Vision Conference (BMVC)"},{"id":"http://arxiv.org/abs/2310.05723v2","updated":"2024-03-27T09:48:34Z","published":"2023-10-09T13:47:05Z","title":"Planning to Go Out-of-Distribution in Offline-to-Online Reinforcement\n  Learning","summary":"  Offline pretraining with a static dataset followed by online fine-tuning\n(offline-to-online, or OtO) is a paradigm well matched to a real-world RL\ndeployment process. In this scenario, we aim to find the best-performing policy\nwithin a limited budget of online interactions. Previous work in the OtO\nsetting has focused on correcting for bias introduced by the policy-constraint\nmechanisms of offline RL algorithms. Such constraints keep the learned policy\nclose to the behavior policy that collected the dataset, but we show this can\nunnecessarily limit policy performance if the behavior policy is far from\noptimal. Instead, we forgo constraints and frame OtO RL as an exploration\nproblem that aims to maximize the benefit of online data-collection. We first\nstudy the major online RL exploration methods based on intrinsic rewards and\nUCB in the OtO setting, showing that intrinsic rewards add training instability\nthrough reward-function modification, and UCB methods are myopic and it is\nunclear which learned-component's ensemble to use for action selection. We then\nintroduce an algorithm for planning to go out-of-distribution (PTGOOD) that\navoids these issues. PTGOOD uses a non-myopic planning procedure that targets\nexploration in relatively high-reward regions of the state-action space\nunlikely to be visited by the behavior policy. By leveraging concepts from the\nConditional Entropy Bottleneck, PTGOOD encourages data collected online to\nprovide new information relevant to improving the final deployment policy\nwithout altering rewards. We show empirically in several continuous control\ntasks that PTGOOD significantly improves agent returns during online\nfine-tuning and avoids the suboptimal policy convergence that many of our\nbaselines exhibit in several environments.\n","authors":["Trevor McInroe","Adam Jelley","Stefano V. Albrecht","Amos Storkey"],"pdf_url":"https://arxiv.org/pdf/2310.05723v2.pdf","comment":"10 pages, 17 figures, preprint"},{"id":"http://arxiv.org/abs/2403.18406v1","updated":"2024-03-27T09:48:23Z","published":"2024-03-27T09:48:23Z","title":"An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering\n  Using a VLM","summary":"  Stimulated by the sophisticated reasoning capabilities of recent Large\nLanguage Models (LLMs), a variety of strategies for bridging video modality\nhave been devised. A prominent strategy involves Video Language Models\n(VideoLMs), which train a learnable interface with video data to connect\nadvanced vision encoders with LLMs. Recently, an alternative strategy has\nsurfaced, employing readily available foundation models, such as VideoLMs and\nLLMs, across multiple stages for modality bridging. In this study, we introduce\na simple yet novel strategy where only a single Vision Language Model (VLM) is\nutilized. Our starting point is the plain insight that a video comprises a\nseries of images, or frames, interwoven with temporal information. The essence\nof video comprehension lies in adeptly managing the temporal aspects along with\nthe spatial details of each frame. Initially, we transform a video into a\nsingle composite image by arranging multiple frames in a grid layout. The\nresulting single image is termed as an image grid. This format, while\nmaintaining the appearance of a solitary image, effectively retains temporal\ninformation within the grid structure. Therefore, the image grid approach\nenables direct application of a single high-performance VLM without\nnecessitating any video-data training. Our extensive experimental analysis\nacross ten zero-shot video question answering benchmarks, including five\nopen-ended and five multiple-choice benchmarks, reveals that the proposed Image\nGrid Vision Language Model (IG-VLM) surpasses the existing methods in nine out\nof ten benchmarks.\n","authors":["Wonkyun Kim","Changin Choi","Wonseok Lee","Wonjong Rhee"],"pdf_url":"https://arxiv.org/pdf/2403.18406v1.pdf","comment":"Our code is available at https://github.com/imagegridworth/IG-VLM"},{"id":"http://arxiv.org/abs/2403.18402v1","updated":"2024-03-27T09:44:50Z","published":"2024-03-27T09:44:50Z","title":"On Spectrogram Analysis in a Multiple Classifier Fusion Framework for\n  Power Grid Classification Using Electric Network Frequency","summary":"  The Electric Network Frequency (ENF) serves as a unique signature inherent to\npower distribution systems. Here, a novel approach for power grid\nclassification is developed, leveraging ENF. Spectrograms are generated from\naudio and power recordings across different grids, revealing distinctive ENF\npatterns that aid in grid classification through a fusion of classifiers. Four\ntraditional machine learning classifiers plus a Convolutional Neural Network\n(CNN), optimized using Neural Architecture Search, are developed for One-vs-All\nclassification. This process generates numerous predictions per sample, which\nare then compiled and used to train a shallow multi-label neural network\nspecifically designed to model the fusion process, ultimately leading to the\nconclusive class prediction for each sample. Experimental findings reveal that\nboth validation and testing accuracy outperform those of current\nstate-of-the-art classifiers, underlining the effectiveness and robustness of\nthe proposed methodology.\n","authors":["Georgios Tzolopoulos","Christos Korgialas","Constantine Kotropoulos"],"pdf_url":"https://arxiv.org/pdf/2403.18402v1.pdf","comment":"13th International Conference on Pattern Recognition Applications and\n  Methods (ICPRAM)"},{"id":"http://arxiv.org/abs/2403.18397v1","updated":"2024-03-27T09:35:56Z","published":"2024-03-27T09:35:56Z","title":"Colour and Brush Stroke Pattern Recognition in Abstract Art using\n  Modified Deep Convolutional Generative Adversarial Networks","summary":"  Abstract Art is an immensely popular, discussed form of art that often has\nthe ability to depict the emotions of an artist. Many researchers have made\nattempts to study abstract art in the form of edge detection, brush stroke and\nemotion recognition algorithms using machine and deep learning. This papers\ndescribes the study of a wide distribution of abstract paintings using\nGenerative Adversarial Neural Networks(GAN). GANs have the ability to learn and\nreproduce a distribution enabling researchers and scientists to effectively\nexplore and study the generated image space. However, the challenge lies in\ndeveloping an efficient GAN architecture that overcomes common training\npitfalls. This paper addresses this challenge by introducing a modified-DCGAN\n(mDCGAN) specifically designed for high-quality artwork generation. The\napproach involves a thorough exploration of the modifications made, delving\ninto the intricate workings of DCGANs, optimisation techniques, and\nregularisation methods aimed at improving stability and realism in art\ngeneration enabling effective study of generated patterns. The proposed mDCGAN\nincorporates meticulous adjustments in layer configurations and architectural\nchoices, offering tailored solutions to the unique demands of art generation\nwhile effectively combating issues like mode collapse and gradient vanishing.\nFurther this paper explores the generated latent space by performing random\nwalks to understand vector relationships between brush strokes and colours in\nthe abstract art space and a statistical analysis of unstable outputs after a\ncertain period of GAN training and compare its significant difference. These\nfindings validate the effectiveness of the proposed approach, emphasising its\npotential to revolutionise the field of digital art generation and digital art\necosystem.\n","authors":["Srinitish Srinivasan","Varenya Pathak"],"pdf_url":"https://arxiv.org/pdf/2403.18397v1.pdf","comment":"28 pages, 5 tables, 7 figures"},{"id":"http://arxiv.org/abs/2403.18393v1","updated":"2024-03-27T09:30:50Z","published":"2024-03-27T09:30:50Z","title":"Tensor-based Graph Learning with Consistency and Specificity for\n  Multi-view Clustering","summary":"  Graph learning is widely recognized as a crucial technique in multi-view\nclustering. Existing graph learning methods typically involve constructing an\nadaptive neighbor graph based on probabilistic neighbors and then learning a\nconsensus graph to for clustering, however, they are confronted with two\nlimitations. Firstly, they often rely on Euclidean distance to measure\nsimilarity when constructing the adaptive neighbor graph, which proves\ninadequate in capturing the intrinsic structure among data points in many\nreal-world scenarios. Secondly, most of these methods focus solely on consensus\ngraph, ignoring view-specific graph information. In response to the\naforementioned drawbacks, we in this paper propose a novel tensor-based graph\nlearning framework that simultaneously considers consistency and specificity\nfor multi-view clustering. Specifically, we calculate the similarity distance\non the Stiefel manifold to preserve the intrinsic structure among data points.\nBy making an assumption that the learned neighbor graph of each view comprises\nboth a consistent graph and a view-specific graph, we formulate a new\ntensor-based target graph learning paradigm. Owing to the benefits of tensor\nsingular value decomposition (t-SVD) in uncovering high-order correlations,\nthis model is capable of achieving a complete understanding of the target\ngraph. Furthermore, we develop an iterative algorithm to solve the proposed\nobjective optimization problem. Experiments conducted on real-world datasets\nhave demonstrated the superior performance of the proposed method over some\nstate-of-the-art multi-view clustering methods. The source code has been\nreleased on https://github.com/lshi91/CSTGL-Code.\n","authors":["Long Shi","Lei Cao","Yunshan Ye","Yu Zhao","Badong Chen"],"pdf_url":"https://arxiv.org/pdf/2403.18393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18383v1","updated":"2024-03-27T09:21:07Z","published":"2024-03-27T09:21:07Z","title":"Generative Multi-modal Models are Good Class-Incremental Learners","summary":"  In class-incremental learning (CIL) scenarios, the phenomenon of catastrophic\nforgetting caused by the classifier's bias towards the current task has long\nposed a significant challenge. It is mainly caused by the characteristic of\ndiscriminative models. With the growing popularity of the generative\nmulti-modal models, we would explore replacing discriminative models with\ngenerative ones for CIL. However, transitioning from discriminative to\ngenerative models requires addressing two key challenges. The primary challenge\nlies in transferring the generated textual information into the classification\nof distinct categories. Additionally, it requires formulating the task of CIL\nwithin a generative framework. To this end, we propose a novel generative\nmulti-modal model (GMM) framework for class-incremental learning. Our approach\ndirectly generates labels for images using an adapted generative model. After\nobtaining the detailed text, we use a text encoder to extract text features and\nemploy feature matching to determine the most similar label as the\nclassification prediction. In the conventional CIL settings, we achieve\nsignificantly better results in long-sequence task scenarios. Under the\nFew-shot CIL setting, we have improved by at least 14\\% accuracy over all the\ncurrent state-of-the-art methods with significantly less forgetting. Our code\nis available at \\url{https://github.com/DoubleClass/GMM}.\n","authors":["Xusheng Cao","Haori Lu","Linlan Huang","Xialei Liu","Ming-Ming Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.18383v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18379v1","updated":"2024-03-27T09:17:50Z","published":"2024-03-27T09:17:50Z","title":"IIP-Mixer:Intra-Inter Patch Mixing Architecture for Battery Remaining\n  Useful Life Prediction","summary":"  Accurately estimating the Remaining Useful Life (RUL) of lithium-ion\nbatteries is crucial for maintaining the safe and stable operation of\nrechargeable battery management systems. However, this task is often\nchallenging due to the complex temporal dynamics involved. Recently,\nattention-based networks, such as Transformers and Informer, have been the\npopular architecture in time series forecasting. Despite their effectiveness,\nthese models with abundant parameters necessitate substantial training time to\nunravel temporal patterns. To tackle these challenges, we propose a simple\nMLP-Mixer-based architecture named 'Intra-Inter Patch Mixer' (IIP-Mixer), which\nis an architecture based exclusively on multi-layer perceptrons (MLPs),\nextracting information by mixing operations along both intra-patch and\ninter-patch dimensions for battery RUL prediction. The proposed IIP-Mixer\ncomprises parallel dual-head mixer layers: the intra-patch mixing MLP,\ncapturing local temporal patterns in the short-term period, and the inter-patch\nmixing MLP, capturing global temporal patterns in the long-term period.\nNotably, to address the varying importance of features in RUL prediction, we\nintroduce a weighted loss function in the MLP-Mixer-based architecture, marking\nthe first time such an approach has been employed. Our experiments demonstrate\nthat IIP-Mixer achieves competitive performance in battery RUL prediction,\noutperforming other popular time-series frameworks\n","authors":["Guangzai Ye","Li Feng","Jianlan Guo","Yuqiang Chen"],"pdf_url":"https://arxiv.org/pdf/2403.18379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18375v1","updated":"2024-03-27T09:14:36Z","published":"2024-03-27T09:14:36Z","title":"Stragglers-Aware Low-Latency Synchronous Federated Learning via\n  Layer-Wise Model Updates","summary":"  Synchronous federated learning (FL) is a popular paradigm for collaborative\nedge learning. It typically involves a set of heterogeneous devices locally\ntraining neural network (NN) models in parallel with periodic centralized\naggregations. As some of the devices may have limited computational resources\nand varying availability, FL latency is highly sensitive to stragglers.\nConventional approaches discard incomplete intra-model updates done by\nstragglers, alter the amount of local workload and architecture, or resort to\nasynchronous settings; which all affect the trained model performance under\ntight training latency constraints. In this work, we propose straggler-aware\nlayer-wise federated learning (SALF) that leverages the optimization procedure\nof NNs via backpropagation to update the global model in a layer-wise fashion.\nSALF allows stragglers to synchronously convey partial gradients, having each\nlayer of the global model be updated independently with a different\ncontributing set of users. We provide a theoretical analysis, establishing\nconvergence guarantees for the global model under mild assumptions on the\ndistribution of the participating devices, revealing that SALF converges at the\nsame asymptotic rate as FL with no timing limitations. This insight is matched\nwith empirical observations, demonstrating the performance gains of SALF\ncompared to alternative mechanisms mitigating the device heterogeneity gap in\nFL.\n","authors":["Natalie Lang","Alejandro Cohen","Nir Shlezinger"],"pdf_url":"https://arxiv.org/pdf/2403.18375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08533v4","updated":"2024-03-27T09:11:48Z","published":"2023-12-13T21:46:09Z","title":"World Models via Policy-Guided Trajectory Diffusion","summary":"  World models are a powerful tool for developing intelligent agents. By\npredicting the outcome of a sequence of actions, world models enable policies\nto be optimised via on-policy reinforcement learning (RL) using synthetic data,\ni.e. in \"in imagination\". Existing world models are autoregressive in that they\ninterleave predicting the next state with sampling the next action from the\npolicy. Prediction error inevitably compounds as the trajectory length grows.\nIn this work, we propose a novel world modelling approach that is not\nautoregressive and generates entire on-policy trajectories in a single pass\nthrough a diffusion model. Our approach, Policy-Guided Trajectory Diffusion\n(PolyGRAD), leverages a denoising model in addition to the gradient of the\naction distribution of the policy to diffuse a trajectory of initially random\nstates and actions into an on-policy synthetic trajectory. We analyse the\nconnections between PolyGRAD, score-based generative models, and\nclassifier-guided diffusion models. Our results demonstrate that PolyGRAD\noutperforms state-of-the-art baselines in terms of trajectory prediction error\nfor short trajectories, with the exception of autoregressive diffusion. For\nshort trajectories, PolyGRAD obtains similar errors to autoregressive\ndiffusion, but with lower computational requirements. For long trajectories,\nPolyGRAD obtains comparable performance to baselines. Our experiments\ndemonstrate that PolyGRAD enables performant policies to be trained via\non-policy RL in imagination for MuJoCo continuous control domains. Thus,\nPolyGRAD introduces a new paradigm for accurate on-policy world modelling\nwithout autoregressive sampling.\n","authors":["Marc Rigter","Jun Yamada","Ingmar Posner"],"pdf_url":"https://arxiv.org/pdf/2312.08533v4.pdf","comment":"Published in TMLR, March 2024"},{"id":"http://arxiv.org/abs/2102.12920v5","updated":"2024-03-27T09:07:29Z","published":"2021-02-25T15:18:13Z","title":"Emerging Trends in Federated Learning: From Model Fusion to Federated X\n  Learning","summary":"  Federated learning is a new learning paradigm that decouples data collection\nand model training via multi-party computation and model aggregation. As a\nflexible learning setting, federated learning has the potential to integrate\nwith other learning frameworks. We conduct a focused survey of federated\nlearning in conjunction with other learning algorithms. Specifically, we\nexplore various learning algorithms to improve the vanilla federated averaging\nalgorithm and review model fusion methods such as adaptive aggregation,\nregularization, clustered methods, and Bayesian methods. Following the emerging\ntrends, we also discuss federated learning in the intersection with other\nlearning paradigms, termed federated X learning, where X includes multitask\nlearning, meta-learning, transfer learning, unsupervised learning, and\nreinforcement learning. In addition to reviewing state-of-the-art studies, this\npaper also identifies key challenges and applications in this field, while also\nhighlighting promising future directions.\n","authors":["Shaoxiong Ji","Yue Tan","Teemu Saravirta","Zhiqin Yang","Yixin Liu","Lauri Vasankari","Shirui Pan","Guodong Long","Anwar Walid"],"pdf_url":"https://arxiv.org/pdf/2102.12920v5.pdf","comment":"To appear in the International Journal of Machine Learning and\n  Cybernetics"},{"id":"http://arxiv.org/abs/2403.17905v2","updated":"2024-03-27T09:07:02Z","published":"2024-03-26T17:45:06Z","title":"Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2","summary":"  We propose a new approach for non-Cartesian magnetic resonance image\nreconstruction. While unrolled architectures provide robustness via\ndata-consistency layers, embedding measurement operators in Deep Neural Network\n(DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP)\napproaches, where the denoising DNNs are blind to the measurement setting, are\nnot affected by this limitation and have also proven effective, but their\nhighly iterative nature also affects scalability. To address this scalability\nchallenge, we leverage the \"Residual-to-Residual DNN series for high-Dynamic\nrange imaging (R2D2)\" approach recently introduced in astronomical imaging.\nR2D2's reconstruction is formed as a series of residual images, iteratively\nestimated as outputs of DNNs taking the previous iteration's image estimate and\nassociated data residual as inputs. The method can be interpreted as a learned\nversion of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation,\nconsidering radial k-space sampling acquisition sequences. Our preliminary\nresults suggest that R2D2 achieves: (i) suboptimal performance compared to its\nunrolled incarnation R2D2-Net, which is however non-scalable due to the\nnecessary embedding of NUFFT-based data-consistency layers; (ii) superior\nreconstruction quality to a scalable version of R2D2-Net embedding an FFT-based\napproximation for data consistency; (iii) superior reconstruction quality to\nPnP, while only requiring few iterations.\n","authors":["Yiwei Chen","Chao Tang","Amir Aghabiglou","Chung San Chu","Yves Wiaux"],"pdf_url":"https://arxiv.org/pdf/2403.17905v2.pdf","comment":"submitted to IEEE EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2403.18370v1","updated":"2024-03-27T09:06:36Z","published":"2024-03-27T09:06:36Z","title":"Ship in Sight: Diffusion Models for Ship-Image Super Resolution","summary":"  In recent years, remarkable advancements have been achieved in the field of\nimage generation, primarily driven by the escalating demand for high-quality\noutcomes across various image generation subtasks, such as inpainting,\ndenoising, and super resolution. A major effort is devoted to exploring the\napplication of super-resolution techniques to enhance the quality of\nlow-resolution images. In this context, our method explores in depth the\nproblem of ship image super resolution, which is crucial for coastal and port\nsurveillance. We investigate the opportunity given by the growing interest in\ntext-to-image diffusion models, taking advantage of the prior knowledge that\nsuch foundation models have already learned. In particular, we present a\ndiffusion-model-based architecture that leverages text conditioning during\ntraining while being class-aware, to best preserve the crucial details of the\nships during the generation of the super-resoluted image. Since the specificity\nof this task and the scarcity availability of off-the-shelf data, we also\nintroduce a large labeled ship dataset scraped from online ship images, mostly\nfrom ShipSpotting\\footnote{\\url{www.shipspotting.com}} website. Our method\nachieves more robust results than other deep learning models previously\nemployed for super resolution, as proven by the multiple experiments performed.\nMoreover, we investigate how this model can benefit downstream tasks, such as\nclassification and object detection, thus emphasizing practical implementation\nin a real-world scenario. Experimental results show flexibility, reliability,\nand impressive performance of the proposed framework over state-of-the-art\nmethods for different tasks. The code is available at:\nhttps://github.com/LuigiSigillo/ShipinSight .\n","authors":["Luigi Sigillo","Riccardo Fosco Gramaccioni","Alessandro Nicolosi","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2403.18370v1.pdf","comment":"Accepted at 2024 International Joint Conference on Neural Networks\n  (IJCNN)"},{"id":"http://arxiv.org/abs/2307.13352v2","updated":"2024-03-27T09:04:04Z","published":"2023-07-25T09:14:45Z","title":"High Dimensional Distributed Gradient Descent with Arbitrary Number of\n  Byzantine Attackers","summary":"  Robust distributed learning with Byzantine failures has attracted extensive\nresearch interests in recent years. However, most of existing methods suffer\nfrom curse of dimensionality, which is increasingly serious with the growing\ncomplexity of modern machine learning models. In this paper, we design a new\nmethod that is suitable for high dimensional problems, under arbitrary number\nof Byzantine attackers. The core of our design is a direct high dimensional\nsemi-verified mean estimation method. Our idea is to identify a subspace first.\nThe components of mean value perpendicular to this subspace can be estimated\nvia gradient vectors uploaded from worker machines, while the components within\nthis subspace are estimated using auxiliary dataset. We then use our new method\nas the aggregator of distributed learning problems. Our theoretical analysis\nshows that the new method has minimax optimal statistical rates. In particular,\nthe dependence on dimensionality is significantly improved compared with\nprevious works.\n","authors":["Puning Zhao","Zhiguo Wan"],"pdf_url":"https://arxiv.org/pdf/2307.13352v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10158v2","updated":"2024-03-27T08:57:20Z","published":"2024-03-15T10:01:19Z","title":"Functional Graph Convolutional Networks: A unified multi-task and\n  multi-modal learning framework to facilitate health and social-care insights","summary":"  This paper introduces a novel Functional Graph Convolutional Network (funGCN)\nframework that combines Functional Data Analysis and Graph Convolutional\nNetworks to address the complexities of multi-task and multi-modal learning in\ndigital health and longitudinal studies. With the growing importance of health\nsolutions to improve health care and social support, ensure healthy lives, and\npromote well-being at all ages, funGCN offers a unified approach to handle\nmultivariate longitudinal data for multiple entities and ensures\ninterpretability even with small sample sizes. Key innovations include\ntask-specific embedding components that manage different data types, the\nability to perform classification, regression, and forecasting, and the\ncreation of a knowledge graph for insightful data interpretation. The efficacy\nof funGCN is validated through simulation experiments and a real-data\napplication.\n","authors":["Tobia Boschi","Francesca Bonin","Rodrigo Ordonez-Hurtado","Cécile Rousseau","Alessandra Pascale","John Dinsmore"],"pdf_url":"https://arxiv.org/pdf/2403.10158v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18364v1","updated":"2024-03-27T08:57:15Z","published":"2024-03-27T08:57:15Z","title":"Intent-Aware DRL-Based Uplink Dynamic Scheduler for 5G-NR","summary":"  We investigate the problem of supporting Industrial Internet of Things user\nequipment (IIoT UEs) with intent (i.e., requested quality of service (QoS)) and\nrandom traffic arrival. A deep reinforcement learning (DRL) based centralized\ndynamic scheduler for time-frequency resources is proposed to learn how to\nschedule the available communication resources among the IIoT UEs. The proposed\nscheduler leverages an RL framework to adapt to the dynamic changes in the\nwireless communication system and traffic arrivals. Moreover, a graph-based\nreduction scheme is proposed to reduce the state and action space of the RL\nframework to allow fast convergence and a better learning strategy. Simulation\nresults demonstrate the effectiveness of the proposed intelligent scheduler in\nguaranteeing the expressed intent of IIoT UEs compared to several traditional\nscheduling schemes, such as round-robin, semi-static, and heuristic approaches.\nThe proposed scheduler also outperforms the contention-free and\ncontention-based schemes in maximizing the number of successfully computed\ntasks.\n","authors":["Salwa Mostafa","Mateus P. Mota","Alvaro Valcarce","Mehdi Bennis"],"pdf_url":"https://arxiv.org/pdf/2403.18364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03325v2","updated":"2024-03-27T08:54:35Z","published":"2023-10-05T05:41:21Z","title":"Learning Concept-Based Causal Transition and Symbolic Reasoning for\n  Visual Planning","summary":"  Visual planning simulates how humans make decisions to achieve desired goals\nin the form of searching for visual causal transitions between an initial\nvisual state and a final visual goal state. It has become increasingly\nimportant in egocentric vision with its advantages in guiding agents to perform\ndaily tasks in complex environments. In this paper, we propose an interpretable\nand generalizable visual planning framework consisting of i) a novel\nSubstitution-based Concept Learner (SCL) that abstracts visual inputs into\ndisentangled concept representations, ii) symbol abstraction and reasoning that\nperforms task planning via the self-learned symbols, and iii) a Visual Causal\nTransition model (ViCT) that grounds visual causal transitions to semantically\nsimilar real-world actions. Given an initial state, we perform goal-conditioned\nvisual planning with a symbolic reasoning method fueled by the learned\nrepresentations and causal transitions to reach the goal state. To verify the\neffectiveness of the proposed model, we collect a large-scale visual planning\ndataset based on AI2-THOR, dubbed as CCTP. Extensive experiments on this\nchallenging dataset demonstrate the superior performance of our method in\nvisual task planning. Empirically, we show that our framework can generalize to\nunseen task trajectories, unseen object categories, and real-world data.\nFurther details of this work are provided at\nhttps://fqyqc.github.io/ConTranPlan/.\n","authors":["Yilue Qian","Peiyu Yu","Ying Nian Wu","Yao Su","Wei Wang","Lifeng Fan"],"pdf_url":"https://arxiv.org/pdf/2310.03325v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15837v2","updated":"2024-03-27T08:54:06Z","published":"2024-03-23T13:24:31Z","title":"Centered Masking for Language-Image Pre-Training","summary":"  We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel,\nstraightforward, and effective technique for masking image patches during\npre-training of a vision-language model. GLIP builds on Fast Language-Image\nPre-Training (FLIP), which randomly masks image patches while training a CLIP\nmodel. GLIP replaces random masking with centered masking, that uses a Gaussian\ndistribution and is inspired by the importance of image patches at the center\nof the image. GLIP retains the same computational savings as FLIP, while\nimproving performance across a range of downstream datasets and tasks, as\ndemonstrated by our experimental results. We show the benefits of GLIP to be\neasy to obtain, requiring no delicate tuning of the Gaussian, and also\napplicable to data sets containing images without an obvious center focus.\n","authors":["Mingliang Liang","Martha Larson"],"pdf_url":"https://arxiv.org/pdf/2403.15837v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17767v2","updated":"2024-03-27T08:49:19Z","published":"2024-03-26T14:54:35Z","title":"Asymptotic Bayes risk of semi-supervised learning with uncertain\n  labeling","summary":"  This article considers a semi-supervised classification setting on a Gaussian\nmixture model, where the data is not labeled strictly as usual, but instead\nwith uncertain labels. Our main aim is to compute the Bayes risk for this\nmodel. We compare the behavior of the Bayes risk and the best known algorithm\nfor this model. This comparison eventually gives new insights over the\nalgorithm.\n","authors":["Victor Leger","Romain Couillet"],"pdf_url":"https://arxiv.org/pdf/2403.17767v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18355v1","updated":"2024-03-27T08:48:16Z","published":"2024-03-27T08:48:16Z","title":"Supervised Multiple Kernel Learning approaches for multi-omics data\n  integration","summary":"  Advances in high-throughput technologies have originated an ever-increasing\navailability of omics datasets. The integration of multiple heterogeneous data\nsources is currently an issue for biology and bioinformatics. Multiple kernel\nlearning (MKL) has shown to be a flexible and valid approach to consider the\ndiverse nature of multi-omics inputs, despite being an underused tool in\ngenomic data mining.We provide novel MKL approaches based on different kernel\nfusion strategies.To learn from the meta-kernel of input kernels, we\nadaptedunsupervised integration algorithms for supervised tasks with support\nvector machines.We also tested deep learning architectures for kernel fusion\nand classification.The results show that MKL-based models can compete with more\ncomplex, state-of-the-art, supervised multi-omics integrative approaches.\nMultiple kernel learning offers a natural framework for predictive models in\nmulti-omics genomic data. Our results offer a direction for bio-data mining\nresearch and further development of methods for heterogeneous data integration.\n","authors":["Mitja Briscik","Gabriele Tazza","Marie-Agnes Dillies","László Vidács","Sébastien Dejean"],"pdf_url":"https://arxiv.org/pdf/2403.18355v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.02151v2","updated":"2024-03-27T08:43:28Z","published":"2023-05-03T14:33:23Z","title":"Identifying the Correlation Between Language Distance and Cross-Lingual\n  Transfer in a Multilingual Representation Space","summary":"  Prior research has investigated the impact of various linguistic features on\ncross-lingual transfer performance. In this study, we investigate the manner in\nwhich this effect can be mapped onto the representation space. While past\nstudies have focused on the impact on cross-lingual alignment in multilingual\nlanguage models during fine-tuning, this study examines the absolute evolution\nof the respective language representation spaces produced by MLLMs. We place a\nspecific emphasis on the role of linguistic characteristics and investigate\ntheir inter-correlation with the impact on representation spaces and\ncross-lingual transfer performance. Additionally, this paper provides\npreliminary evidence of how these findings can be leveraged to enhance transfer\nto linguistically distant languages.\n","authors":["Fred Philippy","Siwen Guo","Shohreh Haddadan"],"pdf_url":"https://arxiv.org/pdf/2305.02151v2.pdf","comment":"SIGTYP Workshop 2023 (co-located with EACL 2023)"},{"id":"http://arxiv.org/abs/2403.18351v1","updated":"2024-03-27T08:42:47Z","published":"2024-03-27T08:42:47Z","title":"Generating Diverse Agricultural Data for Vision-Based Farming\n  Applications","summary":"  We present a specialized procedural model for generating synthetic\nagricultural scenes, focusing on soybean crops, along with various weeds. This\nmodel is capable of simulating distinct growth stages of these plants, diverse\nsoil conditions, and randomized field arrangements under varying lighting\nconditions. The integration of real-world textures and environmental factors\ninto the procedural generation process enhances the photorealism and\napplicability of the synthetic data. Our dataset includes 12,000 images with\nsemantic labels, offering a comprehensive resource for computer vision tasks in\nprecision agriculture, such as semantic segmentation for autonomous weed\ncontrol. We validate our model's effectiveness by comparing the synthetic data\nagainst real agricultural images, demonstrating its potential to significantly\naugment training data for machine learning models in agriculture. This approach\nnot only provides a cost-effective solution for generating high-quality,\ndiverse data but also addresses specific needs in agricultural vision tasks\nthat are not fully covered by general-purpose models.\n","authors":["Mikolaj Cieslak","Umabharathi Govindarajan","Alejandro Garcia","Anuradha Chandrashekar","Torsten Hädrich","Aleksander Mendoza-Drosik","Dominik L. Michels","Sören Pirk","Chia-Chun Fu","Wojciech Pałubicki"],"pdf_url":"https://arxiv.org/pdf/2403.18351v1.pdf","comment":"10 pages, 8 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.18347v1","updated":"2024-03-27T08:38:56Z","published":"2024-03-27T08:38:56Z","title":"A Quantum Fuzzy-based Approach for Real-Time Detection of Solar Coronal\n  Holes","summary":"  The detection and analysis of the solar coronal holes (CHs) is an important\nfield of study in the domain of solar physics. Mainly, it is required for the\nproper prediction of the geomagnetic storms which directly or indirectly affect\nvarious space and ground-based systems. For the detection of CHs till date, the\nsolar scientist depends on manual hand-drawn approaches. However, with the\nadvancement of image processing technologies, some automated image segmentation\nmethods have been used for the detection of CHs. In-spite of this, fast and\naccurate detection of CHs are till a major issues. Here in this work, a novel\nquantum computing-based fast fuzzy c-mean technique has been developed for fast\ndetection of the CHs region. The task has been carried out in two stages, in\nfirst stage the solar image has been segmented using a quantum computing based\nfast fuzzy c-mean (QCFFCM) and in the later stage the CHs has been extracted\nout from the segmented image based on image morphological operation. In the\nwork, quantum computing has been used to optimize the cost function of the fast\nfuzzy c-mean (FFCM) algorithm, where quantum approximate optimization algorithm\n(QAOA) has been used to optimize the quadratic part of the cost function. The\nproposed method has been tested for 193 \\AA{} SDO/AIA full-disk solar image\ndatasets and has been compared with the existing techniques. The outcome shows\nthe comparable performance of the proposed method with the existing one within\na very lesser time.\n","authors":["Sanmoy Bandyopadhyay","Suman Kundu"],"pdf_url":"https://arxiv.org/pdf/2403.18347v1.pdf","comment":"14 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.18343v1","updated":"2024-03-27T08:34:39Z","published":"2024-03-27T08:34:39Z","title":"The Artificial Neural Twin -- Process Optimization and Continual\n  Learning in Distributed Process Chains","summary":"  Industrial process optimization and control is crucial to increase economic\nand ecologic efficiency. However, data sovereignty, differing goals, or the\nrequired expert knowledge for implementation impede holistic implementation.\nFurther, the increasing use of data-driven AI-methods in process models and\nindustrial sensory often requires regular fine-tuning to accommodate\ndistribution drifts. We propose the Artificial Neural Twin, which combines\nconcepts from model predictive control, deep learning, and sensor networks to\naddress these issues. Our approach introduces differentiable data fusion to\nestimate the state of distributed process steps and their dependence on input\ndata. By treating the interconnected process steps as a quasi neural-network,\nwe can backpropagate loss gradients for process optimization or model\nfine-tuning to process parameters or AI models respectively. The concept is\ndemonstrated on a virtual machine park simulated in Unity, consisting of bulk\nmaterial processes in plastic recycling.\n","authors":["Johannes Emmert","Ronald Mendez","Houman Mirzaalian Dastjerdi","Christopher Syben","Andreas Maier"],"pdf_url":"https://arxiv.org/pdf/2403.18343v1.pdf","comment":"20 pages, 11 figures"},{"id":"http://arxiv.org/abs/2403.18337v1","updated":"2024-03-27T08:21:41Z","published":"2024-03-27T08:21:41Z","title":"Macroscale fracture surface segmentation via semi-supervised learning\n  considering the structural similarity","summary":"  To this date the safety assessment of materials, used for example in the\nnuclear power sector, commonly relies on a fracture mechanical analysis\nutilizing macroscopic concepts, where a global load quantity K or J is compared\nto the materials fracture toughness curve. Part of the experimental effort\ninvolved in these concepts is dedicated to the quantitative analysis of\nfracture surfaces. Within the scope of this study a methodology for the\nsemi-supervised training of deep learning models for fracture surface\nsegmentation on a macroscopic level was established. Therefore, three distinct\nand unique datasets were created to analyze the influence of structural\nsimilarity on the segmentation capability. The structural similarity differs\ndue to the assessed materials and specimen, as well as imaging-induced variance\ndue to fluctuations in image acquisition in different laboratories. The\ndatasets correspond to typical isolated laboratory conditions, complex\nreal-world circumstances, and a curated subset of the two. We implemented a\nweak-to-strong consistency regularization for semi-supervised learning. On the\nheterogeneous dataset we were able to train robust and well-generalizing models\nthat learned feature representations from images across different domains\nwithout observing a significant drop in prediction quality. Furthermore, our\napproach reduced the number of labeled images required for training by a factor\nof 6. To demonstrate the success of our method and the benefit of our approach\nfor the fracture mechanics assessment, we utilized the models for initial crack\nsize measurements with the area average method. For the laboratory setting, the\ndeep learning assisted measurements proved to have the same quality as manual\nmeasurements. For models trained on the heterogeneous dataset, very good\nmeasurement accuracies with mean deviations smaller than 1 % could be\nachieved...\n","authors":["Johannes Rosenberger","Johannes Tlatlik","Sebastian Münstermann"],"pdf_url":"https://arxiv.org/pdf/2403.18337v1.pdf","comment":"During review title changed to: Deep learning based initial crack\n  size measurements utilizing macroscale fracture surface segmentation"},{"id":"http://arxiv.org/abs/2403.18336v1","updated":"2024-03-27T08:21:01Z","published":"2024-03-27T08:21:01Z","title":"A Dataset for Pharmacovigilance in German, French, and Japanese:\n  Annotating Adverse Drug Reactions across Languages","summary":"  User-generated data sources have gained significance in uncovering Adverse\nDrug Reactions (ADRs), with an increasing number of discussions occurring in\nthe digital world. However, the existing clinical corpora predominantly revolve\naround scientific articles in English. This work presents a multilingual corpus\nof texts concerning ADRs gathered from diverse sources, including patient fora,\nsocial media, and clinical reports in German, French, and Japanese. Our corpus\ncontains annotations covering 12 entity types, four attribute types, and 13\nrelation types. It contributes to the development of real-world multilingual\nlanguage models for healthcare. We provide statistics to highlight certain\nchallenges associated with the corpus and conduct preliminary experiments\nresulting in strong baselines for extracting entities and relations between\nthese entities, both within and across languages.\n","authors":["Lisa Raithel","Hui-Syuan Yeh","Shuntaro Yada","Cyril Grouin","Thomas Lavergne","Aurélie Névéol","Patrick Paroubek","Philippe Thomas","Tomohiro Nishiyama","Sebastian Möller","Eiji Aramaki","Yuji Matsumoto","Roland Roller","Pierre Zweigenbaum"],"pdf_url":"https://arxiv.org/pdf/2403.18336v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18330v1","updated":"2024-03-27T08:11:25Z","published":"2024-03-27T08:11:25Z","title":"Tracking-Assisted Object Detection with Event Cameras","summary":"  Event-based object detection has recently garnered attention in the computer\nvision community due to the exceptional properties of event cameras, such as\nhigh dynamic range and no motion blur. However, feature asynchronism and\nsparsity cause invisible objects due to no relative motion to the camera,\nposing a significant challenge in the task. Prior works have studied various\nmemory mechanisms to preserve as many features as possible at the current time,\nguided by temporal clues. While these implicit-learned memories retain some\nshort-term information, they still struggle to preserve long-term features\neffectively. In this paper, we consider those invisible objects as\npseudo-occluded objects and aim to reveal their features. Firstly, we introduce\nvisibility attribute of objects and contribute an auto-labeling algorithm to\nappend additional visibility labels on an existing event camera dataset.\nSecondly, we exploit tracking strategies for pseudo-occluded objects to\nmaintain their permanence and retain their bounding boxes, even when features\nhave not been available for a very long time. These strategies can be treated\nas an explicit-learned memory guided by the tracking objective to record the\ndisplacements of objects across frames. Lastly, we propose a spatio-temporal\nfeature aggregation module to enrich the latent features and a consistency loss\nto increase the robustness of the overall pipeline. We conduct comprehensive\nexperiments to verify our method's effectiveness where still objects are\nretained but real occluded objects are discarded. The results demonstrate that\n(1) the additional visibility labels can assist in supervised training, and (2)\nour method outperforms state-of-the-art approaches with a significant\nimprovement of 7.9% absolute mAP.\n","authors":["Ting-Kang Yen","Igor Morawski","Shusil Dangi","Kai He","Chung-Yi Lin","Jia-Fong Yeh","Hung-Ting Su","Winston Hsu"],"pdf_url":"https://arxiv.org/pdf/2403.18330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18326v1","updated":"2024-03-27T08:07:07Z","published":"2024-03-27T08:07:07Z","title":"Privacy-Preserving Distributed Nonnegative Matrix Factorization","summary":"  Nonnegative matrix factorization (NMF) is an effective data representation\ntool with numerous applications in signal processing and machine learning.\nHowever, deploying NMF in a decentralized manner over ad-hoc networks\nintroduces privacy concerns due to the conventional approach of sharing raw\ndata among network agents. To address this, we propose a privacy-preserving\nalgorithm for fully-distributed NMF that decomposes a distributed large data\nmatrix into left and right matrix factors while safeguarding each agent's local\ndata privacy. It facilitates collaborative estimation of the left matrix factor\namong agents and enables them to estimate their respective right factors\nwithout exposing raw data. To ensure data privacy, we secure information\nexchanges between neighboring agents utilizing the Paillier cryptosystem, a\nprobabilistic asymmetric algorithm for public-key cryptography that allows\ncomputations on encrypted data without decryption. Simulation results conducted\non synthetic and real-world datasets demonstrate the effectiveness of the\nproposed algorithm in achieving privacy-preserving distributed NMF over ad-hoc\nnetworks.\n","authors":["Ehsan Lari","Reza Arablouei","Stefan Werner"],"pdf_url":"https://arxiv.org/pdf/2403.18326v1.pdf","comment":"5 pages, 1 figure, submitted to EUSIPCO 2024 conference"},{"id":"http://arxiv.org/abs/2403.18322v1","updated":"2024-03-27T07:52:10Z","published":"2024-03-27T07:52:10Z","title":"Quantum Algorithms: A New Frontier in Financial Crime Prevention","summary":"  Financial crimes fast proliferation and sophistication require novel\napproaches that provide robust and effective solutions. This paper explores the\npotential of quantum algorithms in combating financial crimes. It highlights\nthe advantages of quantum computing by examining traditional and Machine\nLearning (ML) techniques alongside quantum approaches. The study showcases\nadvanced methodologies such as Quantum Machine Learning (QML) and Quantum\nArtificial Intelligence (QAI) as powerful solutions for detecting and\npreventing financial crimes, including money laundering, financial crime\ndetection, cryptocurrency attacks, and market manipulation. These quantum\napproaches leverage the inherent computational capabilities of quantum\ncomputers to overcome limitations faced by classical methods. Furthermore, the\npaper illustrates how quantum computing can support enhanced financial risk\nmanagement analysis. Financial institutions can improve their ability to\nidentify and mitigate risks, leading to more robust risk management strategies\nby exploiting the quantum advantage. This research underscores the\ntransformative impact of quantum algorithms on financial risk management. By\nembracing quantum technologies, organisations can enhance their capabilities to\ncombat evolving threats and ensure the integrity and stability of financial\nsystems.\n","authors":["Abraham Itzhak Weinberg","Alessio Faccia"],"pdf_url":"https://arxiv.org/pdf/2403.18322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18321v1","updated":"2024-03-27T07:50:45Z","published":"2024-03-27T07:50:45Z","title":"Implementation of the Principal Component Analysis onto High-Performance\n  Computer Facilities for Hyperspectral Dimensionality Reduction: Results and\n  Comparisons","summary":"  Dimensionality reduction represents a critical preprocessing step in order to\nincrease the efficiency and the performance of many hyperspectral imaging\nalgorithms. However, dimensionality reduction algorithms, such as the Principal\nComponent Analysis (PCA), suffer from their computationally demanding nature,\nbecoming advisable for their implementation onto high-performance computer\narchitectures for applications under strict latency constraints. This work\npresents the implementation of the PCA algorithm onto two different\nhigh-performance devices, namely, an NVIDIA Graphics Processing Unit (GPU) and\na Kalray manycore, uncovering a highly valuable set of tips and tricks in order\nto take full advantage of the inherent parallelism of these high-performance\ncomputing platforms, and hence, reducing the time that is required to process a\ngiven hyperspectral image. Moreover, the achieved results obtained with\ndifferent hyperspectral images have been compared with the ones that were\nobtained with a field programmable gate array (FPGA)-based implementation of\nthe PCA algorithm that has been recently published, providing, for the first\ntime in the literature, a comprehensive analysis in order to highlight the pros\nand cons of each option.\n","authors":["E. Martel","R. Lazcano","J. Lopez","D. Madroñal","R. Salvador","S. Lopez","E. Juarez","R. Guerra","C. Sanz","R. Sarmiento"],"pdf_url":"https://arxiv.org/pdf/2403.18321v1.pdf","comment":"30 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.18316v1","updated":"2024-03-27T07:38:36Z","published":"2024-03-27T07:38:36Z","title":"Multi-Modal Contrastive Learning for Online Clinical Time-Series\n  Applications","summary":"  Electronic Health Record (EHR) datasets from Intensive Care Units (ICU)\ncontain a diverse set of data modalities. While prior works have successfully\nleveraged multiple modalities in supervised settings, we apply advanced\nself-supervised multi-modal contrastive learning techniques to ICU data,\nspecifically focusing on clinical notes and time-series for clinically relevant\nonline prediction tasks. We introduce a loss function Multi-Modal Neighborhood\nContrastive Loss (MM-NCL), a soft neighborhood function, and showcase the\nexcellent linear probe and zero-shot performance of our approach.\n","authors":["Fabian Baldenweg","Manuel Burger","Gunnar Rätsch","Rita Kuznetsova"],"pdf_url":"https://arxiv.org/pdf/2403.18316v1.pdf","comment":"Accepted as a Workshop Paper at TS4H@ICLR2024"},{"id":"http://arxiv.org/abs/2403.12820v2","updated":"2024-03-27T07:35:47Z","published":"2024-03-19T15:21:00Z","title":"A Physics-embedded Deep Learning Framework for Cloth Simulation","summary":"  Delicate cloth simulations have long been desired in computer graphics.\nVarious methods were proposed to improve engaged force interactions, collision\nhandling, and numerical integrations. Deep learning has the potential to\nachieve fast and real-time simulation, but common neural network structures\noften demand many parameters to capture cloth dynamics. This paper proposes a\nphysics-embedded learning framework that directly encodes physical features of\ncloth simulation. The convolutional neural network is used to represent spatial\ncorrelations of the mass-spring system, after which three branches are designed\nto learn linear, nonlinear, and time derivate features of cloth physics. The\nframework can also integrate with other external forces and collision handling\nthrough either traditional simulators or sub neural networks. The model is\ntested across different cloth animation cases, without training with new data.\nAgreement with baselines and predictive realism successfully validate its\ngeneralization ability. Inference efficiency of the proposed model also defeats\ntraditional physics simulation. This framework is also designed to easily\nintegrate with other visual refinement techniques like wrinkle carving, which\nleaves significant chances to incorporate prevailing macing learning techniques\nin 3D cloth amination.\n","authors":["Zhiwei Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.12820v2.pdf","comment":"A derivation is incomplete, and updations are being processed"},{"id":"http://arxiv.org/abs/2403.18310v1","updated":"2024-03-27T07:22:32Z","published":"2024-03-27T07:22:32Z","title":"A thermodynamically consistent physics-informed deep learning material\n  model for short fiber/polymer nanocomposites","summary":"  This work proposes a physics-informed deep learning (PIDL)-based constitutive\nmodel for investigating the viscoelastic-viscoplastic behavior of short\nfiber-reinforced nanoparticle-filled epoxies under various ambient conditions.\nThe deep-learning model is trained to enforce thermodynamic principles, leading\nto a thermodynamically consistent constitutive model. To accomplish this, a\nlong short-term memory network is combined with a feed-forward neural network\nto predict internal variables required for characterizing the internal\ndissipation of the nanocomposite materials. In addition, another feed-forward\nneural network is used to indicate the free-energy function, which enables\ndefining the thermodynamic state of the entire system. The PIDL model is\ninitially developed for the three-dimensional case by generating synthetic data\nfrom a classical constitutive model. The model is then trained by extracting\nthe data directly from cyclic loading-unloading experimental tests. Numerical\nexamples show that the PIDL model can accurately predict the mechanical\nbehavior of epoxy-based nanocomposites for different volume fractions of fibers\nand nanoparticles under various hygrothermal conditions.\n","authors":["Betim Bahtiri","Behrouz Arash","Sven Scheffler","Maximilian Jux","Raimund Rolfes"],"pdf_url":"https://arxiv.org/pdf/2403.18310v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2305.08102"},{"id":"http://arxiv.org/abs/2310.17072v3","updated":"2024-03-27T07:04:58Z","published":"2023-10-26T00:28:37Z","title":"MMP++: Motion Manifold Primitives with Parametric Curve Models","summary":"  Motion Manifold Primitives (MMP), a manifold-based approach for encoding\nbasic motion skills, can produce diverse trajectories, enabling the system to\nadapt to unseen constraints. Nonetheless, we argue that current MMP models lack\ncrucial functionalities of movement primitives, such as temporal and via-points\nmodulation, found in traditional approaches. This shortfall primarily stems\nfrom MMP's reliance on discrete-time trajectories. To overcome these\nlimitations, we introduce Motion Manifold Primitives++ (MMP++), a new model\nthat integrates the strengths of both MMP and traditional methods by\nincorporating parametric curve representations into the MMP framework.\nFurthermore, we identify a significant challenge with MMP++: performance\ndegradation due to geometric distortions in the latent space, meaning that\nsimilar motions are not closely positioned. To address this, Isometric Motion\nManifold Primitives++ (IMMP++) is proposed to ensure the latent space\naccurately preserves the manifold's geometry. Our experimental results across\nvarious applications, including 2-DoF planar motions, 7-DoF robot arm motions,\nand SE(3) trajectory planning, show that MMP++ and IMMP++ outperform existing\nmethods in trajectory generation tasks, achieving substantial improvements in\nsome cases. Moreover, they enable the modulation of latent coordinates and\nvia-points, thereby allowing efficient online adaptation to dynamic\nenvironments.\n","authors":["Yonghyeon Lee"],"pdf_url":"https://arxiv.org/pdf/2310.17072v3.pdf","comment":"12 pages. This work has been submitted to the IEEE for possible\n  publication"},{"id":"http://arxiv.org/abs/2403.18302v1","updated":"2024-03-27T06:58:01Z","published":"2024-03-27T06:58:01Z","title":"Super-Resolution of SOHO/MDI Magnetograms of Solar Active Regions Using\n  SDO/HMI Data and an Attention-Aided Convolutional Neural Network","summary":"  Image super-resolution has been an important subject in image processing and\nrecognition. Here, we present an attention-aided convolutional neural network\n(CNN) for solar image super-resolution. Our method, named SolarCNN, aims to\nenhance the quality of line-of-sight (LOS) magnetograms of solar active regions\n(ARs) collected by the Michelson Doppler Imager (MDI) on board the Solar and\nHeliospheric Observatory (SOHO). The ground-truth labels used for training\nSolarCNN are the LOS magnetograms collected by the Helioseismic and Magnetic\nImager (HMI) on board the Solar Dynamics Observatory (SDO). Solar ARs consist\nof strong magnetic fields in which magnetic energy can suddenly be released to\nproduce extreme space weather events, such as solar flares, coronal mass\nejections, and solar energetic particles. SOHO/MDI covers Solar Cycle 23, which\nis stronger with more eruptive events than Cycle 24. Enhanced SOHO/MDI\nmagnetograms allow for better understanding and forecasting of violent events\nof space weather. Experimental results show that SolarCNN improves the quality\nof SOHO/MDI magnetograms in terms of the structural similarity index measure\n(SSIM), Pearson's correlation coefficient (PCC), and the peak signal-to-noise\nratio (PSNR).\n","authors":["Chunhui Xu","Jason T. L. Wang","Haimin Wang","Haodi Jiang","Qin Li","Yasser Abduallah","Yan Xu"],"pdf_url":"https://arxiv.org/pdf/2403.18302v1.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2302.06912v4","updated":"2024-03-27T06:57:30Z","published":"2023-02-14T08:56:50Z","title":"Regret-Based Defense in Adversarial Reinforcement Learning","summary":"  Deep Reinforcement Learning (DRL) policies have been shown to be vulnerable\nto small adversarial noise in observations. Such adversarial noise can have\ndisastrous consequences in safety-critical environments. For instance, a\nself-driving car receiving adversarially perturbed sensory observations about\nnearby signs (e.g., a stop sign physically altered to be perceived as a speed\nlimit sign) or objects (e.g., cars altered to be recognized as trees) can be\nfatal. Existing approaches for making RL algorithms robust to an\nobservation-perturbing adversary have focused on reactive approaches that\niteratively improve against adversarial examples generated at each iteration.\nWhile such approaches have been shown to provide improvements over regular RL\nmethods, they are reactive and can fare significantly worse if certain\ncategories of adversarial examples are not generated during training. To that\nend, we pursue a more proactive approach that relies on directly optimizing a\nwell-studied robustness measure, regret instead of expected value. We provide a\nprincipled approach that minimizes maximum regret over a \"neighborhood\" of\nobservations to the received \"observation\". Our regret criterion can be used to\nmodify existing value- and policy-based Deep RL methods. We demonstrate that\nour approaches provide a significant improvement in performance across a wide\nvariety of benchmarks against leading approaches for robust Deep RL.\n","authors":["Roman Belaire","Pradeep Varakantham","Thanh Nguyen","David Lo"],"pdf_url":"https://arxiv.org/pdf/2302.06912v4.pdf","comment":"Accepted at AAMAS 2024"},{"id":"http://arxiv.org/abs/2403.18301v1","updated":"2024-03-27T06:55:23Z","published":"2024-03-27T06:55:23Z","title":"Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives","summary":"  The rise in internet usage has led to the generation of massive amounts of\ndata, resulting in the adoption of various supervised and semi-supervised\nmachine learning algorithms, which can effectively utilize the colossal amount\nof data to train models. However, before deploying these models in the real\nworld, these must be strictly evaluated on performance measures like worst-case\nrecall and satisfy constraints such as fairness. We find that current\nstate-of-the-art empirical techniques offer sub-optimal performance on these\npractical, non-decomposable performance objectives. On the other hand, the\ntheoretical techniques necessitate training a new model from scratch for each\nperformance objective. To bridge the gap, we propose SelMix, a selective\nmixup-based inexpensive fine-tuning technique for pre-trained models, to\noptimize for the desired objective. The core idea of our framework is to\ndetermine a sampling distribution to perform a mixup of features between\nsamples from particular classes such that it optimizes the given objective. We\ncomprehensively evaluate our technique against the existing empirical and\ntheoretically principled methods on standard benchmark datasets for imbalanced\nclassification. We find that proposed SelMix fine-tuning significantly improves\nthe performance for various practical non-decomposable objectives across\nbenchmarks.\n","authors":["Shrinivas Ramasubramanian","Harsh Rangwani","Sho Takemori","Kunal Samanta","Yuhei Umeda","Venkatesh Babu Radhakrishnan"],"pdf_url":"https://arxiv.org/pdf/2403.18301v1.pdf","comment":"ICLR 2024 SpotLight"},{"id":"http://arxiv.org/abs/2403.18296v1","updated":"2024-03-27T06:46:59Z","published":"2024-03-27T06:46:59Z","title":"GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic\n  Communication Paradigm","summary":"  Traditional approaches to semantic communication tasks rely on the knowledge\nof the signal-to-noise ratio (SNR) to mitigate channel noise. However, these\nmethods necessitate training under specific SNR conditions, entailing\nconsiderable time and computational resources. In this paper, we propose GeNet,\na Graph Neural Network (GNN)-based paradigm for semantic communication aimed at\ncombating noise, thereby facilitating Task-Oriented Communication (TOC). We\npropose a novel approach where we first transform the input data image into\ngraph structures. Then we leverage a GNN-based encoder to extract semantic\ninformation from the source data. This extracted semantic information is then\ntransmitted through the channel. At the receiver's end, a GNN-based decoder is\nutilized to reconstruct the relevant semantic information from the source data\nfor TOC. Through experimental evaluation, we show GeNet's effectiveness in\nanti-noise TOC while decoupling the SNR dependency. We further evaluate GeNet's\nperformance by varying the number of nodes, revealing its versatility as a new\nparadigm for semantic communication. Additionally, we show GeNet's robustness\nto geometric transformations by testing it with different rotation angles,\nwithout resorting to data augmentation.\n","authors":["Chunhang Zheng","Kechao Cai"],"pdf_url":"https://arxiv.org/pdf/2403.18296v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18286v1","updated":"2024-03-27T06:25:40Z","published":"2024-03-27T06:25:40Z","title":"Few-Shot Recalibration of Language Models","summary":"  Recent work has uncovered promising ways to extract well-calibrated\nconfidence estimates from language models (LMs), where the model's confidence\nscore reflects how likely it is to be correct. However, while LMs may appear\nwell-calibrated over broad distributions, this often hides significant\nmiscalibration within narrower slices (e.g., systemic over-confidence in math\ncan balance out systemic under-confidence in history, yielding perfect\ncalibration in aggregate). To attain well-calibrated confidence estimates for\nany slice of a distribution, we propose a new framework for few-shot\nslice-specific recalibration. Specifically, we train a recalibration model that\ntakes in a few unlabeled examples from any given slice and predicts a curve\nthat remaps confidence scores to be more accurate for that slice. Our trained\nmodel can recalibrate for arbitrary new slices, without using any labeled data\nfrom that slice. This enables us to identify domain-specific confidence\nthresholds above which the LM's predictions can be trusted, and below which it\nshould abstain. Experiments show that our few-shot recalibrator consistently\noutperforms existing calibration methods, for instance improving calibration\nerror for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.\n","authors":["Xiang Lisa Li","Urvashi Khandelwal","Kelvin Guu"],"pdf_url":"https://arxiv.org/pdf/2403.18286v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2403.18269v1","updated":"2024-03-27T05:50:23Z","published":"2024-03-27T05:50:23Z","title":"Clustering Change Sign Detection by Fusing Mixture Complexity","summary":"  This paper proposes an early detection method for cluster structural changes.\nCluster structure refers to discrete structural characteristics, such as the\nnumber of clusters, when data are represented using finite mixture models, such\nas Gaussian mixture models. We focused on scenarios in which the cluster\nstructure gradually changed over time. For finite mixture models, the concept\nof mixture complexity (MC) measures the continuous cluster size by considering\nthe cluster proportion bias and overlap between clusters. In this paper, we\npropose MC fusion as an extension of MC to handle situations in which multiple\nmixture numbers are possible in a finite mixture model. By incorporating the\nfusion of multiple models, our approach accurately captured the cluster\nstructure during transitional periods of gradual change. Moreover, we introduce\na method for detecting changes in the cluster structure by examining the\ntransition of MC fusion. We demonstrate the effectiveness of our method through\nempirical analysis using both artificial and real-world datasets.\n","authors":["Kento Urano","Ryo Yuki","Kenji Yamanishi"],"pdf_url":"https://arxiv.org/pdf/2403.18269v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2312.12558v2","updated":"2024-03-27T05:48:21Z","published":"2023-12-19T19:53:58Z","title":"Sample Efficient Reinforcement Learning with Partial Dynamics Knowledge","summary":"  The problem of sample complexity of online reinforcement learning is often\nstudied in the literature without taking into account any partial knowledge\nabout the system dynamics that could potentially accelerate the learning\nprocess. In this paper, we study the sample complexity of online Q-learning\nmethods when some prior knowledge about the dynamics is available or can be\nlearned efficiently. We focus on systems that evolve according to an additive\ndisturbance model of the form $S_{h+1} = f(S_h, A_h) + W_h$, where $f$\nrepresents the underlying system dynamics, and $W_h$ are unknown disturbances\nindependent of states and actions. In the setting of finite episodic Markov\ndecision processes with $S$ states, $A$ actions, and episode length $H$, we\npresent an optimistic Q-learning algorithm that achieves\n$\\tilde{\\mathcal{O}}(\\text{Poly}(H)\\sqrt{T})$ regret under perfect knowledge of\n$f$, where $T$ is the total number of interactions with the system. This is in\ncontrast to the typical $\\tilde{\\mathcal{O}}(\\text{Poly}(H)\\sqrt{SAT})$ regret\nfor existing Q-learning methods. Further, if only a noisy estimate $\\hat{f}$ of\n$f$ is available, our method can learn an approximately optimal policy in a\nnumber of samples that is independent of the cardinalities of state and action\nspaces. The sub-optimality gap depends on the approximation error $\\hat{f}-f$,\nas well as the Lipschitz constant of the corresponding optimal value function.\nOur approach does not require modeling of the transition probabilities and\nenjoys the same memory complexity as model-free methods.\n","authors":["Meshal Alharbi","Mardavij Roozbehani","Munther Dahleh"],"pdf_url":"https://arxiv.org/pdf/2312.12558v2.pdf","comment":"Published in the 38th Annual AAAI Conference on Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2305.14258v2","updated":"2024-03-27T05:45:37Z","published":"2023-05-23T17:11:33Z","title":"Weakly Supervised AUC Optimization: A Unified Partial AUC Approach","summary":"  Since acquiring perfect supervision is usually difficult, real-world machine\nlearning tasks often confront inaccurate, incomplete, or inexact supervision,\ncollectively referred to as weak supervision. In this work, we present WSAUC, a\nunified framework for weakly supervised AUC optimization problems, which covers\nnoisy label learning, positive-unlabeled learning, multi-instance learning, and\nsemi-supervised learning scenarios. Within the WSAUC framework, we first frame\nthe AUC optimization problems in various weakly supervised scenarios as a\ncommon formulation of minimizing the AUC risk on contaminated sets, and\ndemonstrate that the empirical risk minimization problems are consistent with\nthe true AUC. Then, we introduce a new type of partial AUC, specifically, the\nreversed partial AUC (rpAUC), which serves as a robust training objective for\nAUC maximization in the presence of contaminated labels. WSAUC offers a\nuniversal solution for AUC optimization in various weakly supervised scenarios\nby maximizing the empirical rpAUC. Theoretical and experimental results under\nmultiple settings support the effectiveness of WSAUC on a range of weakly\nsupervised AUC optimization tasks.\n","authors":["Zheng Xie","Yu Liu","Hao-Yuan He","Ming Li","Zhi-Hua Zhou"],"pdf_url":"https://arxiv.org/pdf/2305.14258v2.pdf","comment":"Accepted by IEEE TPAMI"},{"id":"http://arxiv.org/abs/2403.18267v1","updated":"2024-03-27T05:41:50Z","published":"2024-03-27T05:41:50Z","title":"DSF-GAN: DownStream Feedback Generative Adversarial Network","summary":"  Utility and privacy are two crucial measurements of the quality of synthetic\ntabular data. While significant advancements have been made in privacy\nmeasures, generating synthetic samples with high utility remains challenging.\nTo enhance the utility of synthetic samples, we propose a novel architecture\ncalled the DownStream Feedback Generative Adversarial Network (DSF-GAN). This\napproach incorporates feedback from a downstream prediction model during\ntraining to augment the generator's loss function with valuable information.\nThus, DSF-GAN utilizes a downstream prediction task to enhance the utility of\nsynthetic samples. To evaluate our method, we tested it using two popular\ndatasets. Our experiments demonstrate improved model performance when training\non synthetic samples generated by DSF-GAN, compared to those generated by the\nsame GAN architecture without feedback. The evaluation was conducted on the\nsame validation set comprising real samples. All code and datasets used in this\nresearch will be made openly available for ease of reproduction.\n","authors":["Oriel Perets","Nadav Rappoport"],"pdf_url":"https://arxiv.org/pdf/2403.18267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18266v1","updated":"2024-03-27T05:38:48Z","published":"2024-03-27T05:38:48Z","title":"Branch-Tuning: Balancing Stability and Plasticity for Continual\n  Self-Supervised Learning","summary":"  Self-supervised learning (SSL) has emerged as an effective paradigm for\nderiving general representations from vast amounts of unlabeled data. However,\nas real-world applications continually integrate new content, the high\ncomputational and resource demands of SSL necessitate continual learning rather\nthan complete retraining. This poses a challenge in striking a balance between\nstability and plasticity when adapting to new information. In this paper, we\nemploy Centered Kernel Alignment for quantitatively analyzing model stability\nand plasticity, revealing the critical roles of batch normalization layers for\nstability and convolutional layers for plasticity. Motivated by this, we\npropose Branch-tuning, an efficient and straightforward method that achieves a\nbalance between stability and plasticity in continual SSL. Branch-tuning\nconsists of branch expansion and compression, and can be easily applied to\nvarious SSL methods without the need of modifying the original methods,\nretaining old data or models. We validate our method through incremental\nexperiments on various benchmark datasets, demonstrating its effectiveness and\npractical value in real-world scenarios. We hope our work offers new insights\nfor future continual self-supervised learning research. The code will be made\npublicly available.\n","authors":["Wenzhuo Liu","Fei Zhu","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2403.18266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02561v2","updated":"2024-03-27T05:23:40Z","published":"2024-02-04T16:27:37Z","title":"Foundation Model Makes Clustering A Better Initialization For Cold-Start\n  Active Learning","summary":"  Active learning selects the most informative samples from the unlabelled\ndataset to annotate in the context of a limited annotation budget. While\nnumerous methods have been proposed for subsequent sample selection based on an\ninitialized model, scant attention has been paid to the indispensable phase of\nactive learning: selecting samples for model cold-start initialization. Most of\nthe previous studies resort to random sampling or naive clustering. However,\nrandom sampling is prone to fluctuation, and naive clustering suffers from\nconvergence speed, particularly when dealing with high-dimensional data such as\nimaging data. In this work, we propose to integrate foundation models with\nclustering methods to select samples for cold-start active learning\ninitialization. Foundation models refer to those trained on massive datasets by\nthe self-supervised paradigm and capable of generating informative and\ncompacted embeddings for various downstream tasks. Leveraging these embeddings\nto replace raw features such as pixel values, clustering quickly converges and\nidentifies better initial samples. For a comprehensive comparison, we included\na classic ImageNet-supervised model to acquire embeddings. Experiments on two\nclinical tasks of image classification and segmentation demonstrated that\nfoundation model-based clustering efficiently pinpointed informative initial\nsamples, leading to models showcasing enhanced performance than the baseline\nmethods. We envisage that this study provides an effective paradigm for future\ncold-start active learning.\n","authors":["Han Yuan","Chuan Hong"],"pdf_url":"https://arxiv.org/pdf/2402.02561v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17458v2","updated":"2024-03-27T04:54:59Z","published":"2024-03-26T07:46:27Z","title":"Expectations Versus Reality: Evaluating Intrusion Detection Systems in\n  Practice","summary":"  Our paper provides empirical comparisons between recent IDSs to provide an\nobjective comparison between them to help users choose the most appropriate\nsolution based on their requirements. Our results show that no one solution is\nthe best, but is dependent on external variables such as the types of attacks,\ncomplexity, and network environment in the dataset. For example, BoT_IoT and\nStratosphere IoT datasets both capture IoT-related attacks, but the deep neural\nnetwork performed the best when tested using the BoT_IoT dataset while HELAD\nperformed the best when tested using the Stratosphere IoT dataset. So although\nwe found that a deep neural network solution had the highest average F1 scores\non tested datasets, it is not always the best-performing one. We further\ndiscuss difficulties in using IDS from literature and project repositories,\nwhich complicated drawing definitive conclusions regarding IDS selection.\n","authors":["Jake Hesford","Daniel Cheng","Alan Wan","Larry Huynh","Seungho Kim","Hyoungshick Kim","Jin B. Hong"],"pdf_url":"https://arxiv.org/pdf/2403.17458v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2312.07950v3","updated":"2024-03-27T04:51:51Z","published":"2023-12-13T07:56:27Z","title":"CBQ: Cross-Block Quantization for Large Language Models","summary":"  Post-training quantization (PTQ) has played a key role in compressing large\nlanguage models (LLMs) with ultra-low costs. However, existing PTQ methods only\nfocus on handling the outliers within one layer or one block, which ignores the\ndependency of blocks and leads to severe performance degradation in low-bit\nsettings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ\nmethod for LLMs. CBQ employs a cross-block dependency using a homologous\nreconstruction scheme, establishing long-range dependencies across multiple\nblocks to minimize error accumulation. Furthermore, CBQ incorporates a\ncoarse-to-fine preprocessing (CFP) strategy for suppressing weight and\nactivation outliers, coupled with an adaptive LoRA-Rounding technique for\nprecise weight quantization. These innovations enable CBQ to not only handle\nextreme outliers effectively but also improve overall quantization accuracy.\nExtensive experiments show that CBQ achieves superior low-bit quantization\n(W4A4, W4A8, W2A16) and outperforms existing state-of-the-art methods across\nvarious LLMs and datasets. Notably, CBQ quantizes the 4-bit LLAMA1-65B model\nwithin only 4.3 hours on a single GPU, achieving a commendable tradeoff between\nperformance and quantization efficiency.\n","authors":["Xin Ding","Xiaoyu Liu","Zhijun Tu","Yun Zhang","Wei Li","Jie Hu","Hanting Chen","Yehui Tang","Zhiwei Xiong","Baoqun Yin","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2312.07950v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18252v1","updated":"2024-03-27T04:49:23Z","published":"2024-03-27T04:49:23Z","title":"Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models","summary":"  Visual representation learning has been a cornerstone in computer vision,\nevolving from supervised learning with human-annotated labels to aligning\nimage-text pairs from the Internet. Despite recent advancements in multi-modal\nlarge language models (MLLMs), the visual representations they rely on, such as\nCLIP embeddings, often lack access to external world knowledge critical for\nreal-world visual reasoning. In this work, we propose Visual Table, a novel\nvisual representation tailored for MLLMs. It provides hierarchical text\ndescriptions of holistic visual scenes, consisting of a scene description and\nmultiple object-centric descriptions that encompass categories, attributes, and\nknowledge at instance level. We further develop a scalable generator for visual\ntable generation and train it on small-scale annotations from GPT4V. Extensive\nevaluations demonstrate that, with generated visual tables as additional visual\nrepresentations, our model can consistently outperform the state-of-the-art\n(SOTA) MLLMs across diverse benchmarks. When visual tables serve as standalone\nvisual representations, our model can closely match or even beat the SOTA MLLMs\nthat are built on CLIP visual embeddings. Our code is available at\nhttps://github.com/LaVi-Lab/Visual-Table.\n","authors":["Yiwu Zhong","Zi-Yuan Hu","Michael R. Lyu","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18252v1.pdf","comment":"Project page: https://github.com/LaVi-Lab/Visual-Table"},{"id":"http://arxiv.org/abs/2401.16025v2","updated":"2024-03-27T04:36:17Z","published":"2024-01-29T10:17:54Z","title":"Simple Policy Optimization","summary":"  PPO (Proximal Policy Optimization) algorithm has demonstrated excellent\nperformance in many fields, and it is considered as a simple version of TRPO\n(Trust Region Policy Optimization) algorithm. However, the ratio clipping\noperation in PPO may not always effectively enforce the trust region\nconstraints, this can be a potential factor affecting the stability of the\nalgorithm. In this paper, we propose Simple Policy Optimization (SPO)\nalgorithm, which introduces a novel clipping method for KL divergence between\nthe old and current policies. Extensive experimental results in Atari 2600\nenvironments indicate that, compared to the mainstream variants of PPO, SPO\nachieves better sample efficiency, extremely low KL divergence, and higher\npolicy entropy, and is robust to the increase in network depth or complexity.\nMore importantly, SPO maintains the simplicity of an unconstrained first-order\nalgorithm. Code is available at\nhttps://github.com/MyRepositories-hub/Simple-Policy-Optimization.\n","authors":["Zhengpeng Xie"],"pdf_url":"https://arxiv.org/pdf/2401.16025v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18241v1","updated":"2024-03-27T04:09:34Z","published":"2024-03-27T04:09:34Z","title":"NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion,\n  Reconstruction, and Generation","summary":"  3D shape generation aims to produce innovative 3D content adhering to\nspecific conditions and constraints. Existing methods often decompose 3D shapes\ninto a sequence of localized components, treating each element in isolation\nwithout considering spatial consistency. As a result, these approaches exhibit\nlimited versatility in 3D data representation and shape generation, hindering\ntheir ability to generate highly diverse 3D shapes that comply with the\nspecified constraints. In this paper, we introduce a novel spatial-aware 3D\nshape generation framework that leverages 2D plane representations for enhanced\n3D shape modeling. To ensure spatial coherence and reduce memory usage, we\nincorporate a hybrid shape representation technique that directly learns a\ncontinuous signed distance field representation of the 3D shape using\northogonal 2D planes. Additionally, we meticulously enforce spatial\ncorrespondences across distinct planes using a transformer-based autoencoder\nstructure, promoting the preservation of spatial relationships in the generated\n3D shapes. This yields an algorithm that consistently outperforms\nstate-of-the-art 3D shape generation methods on various tasks, including\nunconditional shape generation, multi-modal shape completion, single-view\nreconstruction, and text-to-shape synthesis.\n","authors":["Ruikai Cui","Weizhe Liu","Weixuan Sun","Senbo Wang","Taizhang Shang","Yang Li","Xibin Song","Han Yan","Zhennan Wu","Shenzhou Chen","Hongdong Li","Pan Ji"],"pdf_url":"https://arxiv.org/pdf/2403.18241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.08657v6","updated":"2024-03-27T03:53:23Z","published":"2022-06-17T09:42:35Z","title":"BridgeTower: Building Bridges Between Encoders in Vision-Language\n  Representation Learning","summary":"  Vision-Language (VL) models with the Two-Tower architecture have dominated\nvisual-language representation learning in recent years. Current VL models\neither use lightweight uni-modal encoders and learn to extract, align and fuse\nboth modalities simultaneously in a deep cross-modal encoder, or feed the\nlast-layer uni-modal representations from the deep pre-trained uni-modal\nencoders into the top cross-modal encoder. Both approaches potentially restrict\nvision-language representation learning and limit model performance. In this\npaper, we propose BridgeTower, which introduces multiple bridge layers that\nbuild a connection between the top layers of uni-modal encoders and each layer\nof the cross-modal encoder. This enables effective bottom-up cross-modal\nalignment and fusion between visual and textual representations of different\nsemantic levels of pre-trained uni-modal encoders in the cross-modal encoder.\nPre-trained with only 4M images, BridgeTower achieves state-of-the-art\nperformance on various downstream vision-language tasks. In particular, on the\nVQAv2 test-std set, BridgeTower achieves an accuracy of 78.73%, outperforming\nthe previous state-of-the-art model METER by 1.09% with the same pre-training\ndata and almost negligible additional parameters and computational costs.\nNotably, when further scaling the model, BridgeTower achieves an accuracy of\n81.15%, surpassing models that are pre-trained on orders-of-magnitude larger\ndatasets. Code and checkpoints are available at\nhttps://github.com/microsoft/BridgeTower.\n","authors":["Xiao Xu","Chenfei Wu","Shachar Rosenman","Vasudev Lal","Wanxiang Che","Nan Duan"],"pdf_url":"https://arxiv.org/pdf/2206.08657v6.pdf","comment":"Accepted by AAAI 2023, Oral"},{"id":"http://arxiv.org/abs/2301.11104v4","updated":"2024-03-27T03:47:20Z","published":"2023-01-26T13:58:46Z","title":"Discovering and Mitigating Visual Biases through Keyword Explanation","summary":"  Addressing biases in computer vision models is crucial for real-world AI\ndeployments. However, mitigating visual biases is challenging due to their\nunexplainable nature, often identified indirectly through visualization or\nsample statistics, which necessitates additional human supervision for\ninterpretation. To tackle this issue, we propose the Bias-to-Text (B2T)\nframework, which interprets visual biases as keywords. Specifically, we extract\ncommon keywords from the captions of mispredicted images to identify potential\nbiases in the model. We then validate these keywords by measuring their\nsimilarity to the mispredicted images using a vision-language scoring model.\nThe keyword explanation form of visual bias offers several advantages, such as\na clear group naming for bias discovery and a natural extension for debiasing\nusing these group names. Our experiments demonstrate that B2T can identify\nknown biases, such as gender bias in CelebA, background bias in Waterbirds, and\ndistribution shifts in ImageNet-R/C. Additionally, B2T uncovers novel biases in\nlarger datasets, such as Dollar Street and ImageNet. For example, we discovered\na contextual bias between \"bee\" and \"flower\" in ImageNet. We also highlight\nvarious applications of B2T keywords, including debiased training, CLIP\nprompting, and model comparison.\n","authors":["Younghyun Kim","Sangwoo Mo","Minkyu Kim","Kyungmin Lee","Jaeho Lee","Jinwoo Shin"],"pdf_url":"https://arxiv.org/pdf/2301.11104v4.pdf","comment":"CVPR 2024. First two authors contributed equally"},{"id":"http://arxiv.org/abs/2403.18233v1","updated":"2024-03-27T03:39:57Z","published":"2024-03-27T03:39:57Z","title":"Benchmarking Image Transformers for Prostate Cancer Detection from\n  Ultrasound Data","summary":"  PURPOSE: Deep learning methods for classifying prostate cancer (PCa) in\nultrasound images typically employ convolutional networks (CNNs) to detect\ncancer in small regions of interest (ROI) along a needle trace region. However,\nthis approach suffers from weak labelling, since the ground-truth\nhistopathology labels do not describe the properties of individual ROIs.\nRecently, multi-scale approaches have sought to mitigate this issue by\ncombining the context awareness of transformers with a CNN feature extractor to\ndetect cancer from multiple ROIs using multiple-instance learning (MIL). In\nthis work, we present a detailed study of several image transformer\narchitectures for both ROI-scale and multi-scale classification, and a\ncomparison of the performance of CNNs and transformers for ultrasound-based\nprostate cancer classification. We also design a novel multi-objective learning\nstrategy that combines both ROI and core predictions to further mitigate label\nnoise. METHODS: We evaluate 3 image transformers on ROI-scale cancer\nclassification, then use the strongest model to tune a multi-scale classifier\nwith MIL. We train our MIL models using our novel multi-objective learning\nstrategy and compare our results to existing baselines. RESULTS: We find that\nfor both ROI-scale and multi-scale PCa detection, image transformer backbones\nlag behind their CNN counterparts. This deficit in performance is even more\nnoticeable for larger models. When using multi-objective learning, we can\nimprove performance of MIL, with a 77.9% AUROC, a sensitivity of 75.9%, and a\nspecificity of 66.3%. CONCLUSION: Convolutional networks are better suited for\nmodelling sparse datasets of prostate ultrasounds, producing more robust\nfeatures than transformers in PCa detection. Multi-scale methods remain the\nbest architecture for this task, with multi-objective learning presenting an\neffective way to improve performance.\n","authors":["Mohamed Harmanani","Paul F. R. Wilson","Fahimeh Fooladgar","Amoon Jamzad","Mahdi Gilany","Minh Nguyen Nhat To","Brian Wodlinger","Purang Abolmaesumi","Parvin Mousavi"],"pdf_url":"https://arxiv.org/pdf/2403.18233v1.pdf","comment":"early draft, 7 pages; Accepted to SPIE Medical Imaging 2024"},{"id":"http://arxiv.org/abs/2403.18228v1","updated":"2024-03-27T03:31:16Z","published":"2024-03-27T03:31:16Z","title":"Fourier or Wavelet bases as counterpart self-attention in spikformer for\n  efficient visual classification","summary":"  Energy-efficient spikformer has been proposed by integrating the biologically\nplausible spiking neural network (SNN) and artificial Transformer, whereby the\nSpiking Self-Attention (SSA) is used to achieve both higher accuracy and lower\ncomputational cost. However, it seems that self-attention is not always\nnecessary, especially in sparse spike-form calculation manners. In this paper,\nwe innovatively replace vanilla SSA (using dynamic bases calculating from Query\nand Key) with spike-form Fourier Transform, Wavelet Transform, and their\ncombinations (using fixed triangular or wavelets bases), based on a key\nhypothesis that both of them use a set of basis functions for information\ntransformation. Hence, the Fourier-or-Wavelet-based spikformer (FWformer) is\nproposed and verified in visual classification tasks, including both static\nimage and event-based video datasets. The FWformer can achieve comparable or\neven higher accuracies ($0.4\\%$-$1.5\\%$), higher running speed ($9\\%$-$51\\%$\nfor training and $19\\%$-$70\\%$ for inference), reduced theoretical energy\nconsumption ($20\\%$-$25\\%$), and reduced GPU memory usage ($4\\%$-$26\\%$),\ncompared to the standard spikformer. Our result indicates the continuous\nrefinement of new Transformers, that are inspired either by biological\ndiscovery (spike-form), or information theory (Fourier or Wavelet Transform),\nis promising.\n","authors":["Qingyu Wang","Duzhen Zhang","Tilelin Zhang","Bo Xu"],"pdf_url":"https://arxiv.org/pdf/2403.18228v1.pdf","comment":"18 pages, 2 figures. arXiv admin note: substantial text overlap with\n  arXiv:2308.02557"},{"id":"http://arxiv.org/abs/2403.18223v1","updated":"2024-03-27T03:25:45Z","published":"2024-03-27T03:25:45Z","title":"A Transformer-Based Framework for Payload Malware Detection and\n  Classification","summary":"  As malicious cyber threats become more sophisticated in breaching computer\nnetworks, the need for effective intrusion detection systems (IDSs) becomes\ncrucial. Techniques such as Deep Packet Inspection (DPI) have been introduced\nto allow IDSs analyze the content of network packets, providing more context\nfor identifying potential threats. IDSs traditionally rely on using\nanomaly-based and signature-based detection techniques to detect unrecognized\nand suspicious activity. Deep learning techniques have shown great potential in\nDPI for IDSs due to their efficiency in learning intricate patterns from the\npacket content being transmitted through the network. In this paper, we propose\na revolutionary DPI algorithm based on transformers adapted for the purpose of\ndetecting malicious traffic with a classifier head. Transformers learn the\ncomplex content of sequence data and generalize them well to similar scenarios\nthanks to their self-attention mechanism. Our proposed method uses the raw\npayload bytes that represent the packet contents and is deployed as\nman-in-the-middle. The payload bytes are used to detect malicious packets and\nclassify their types. Experimental results on the UNSW-NB15 and CIC-IOT23\ndatasets demonstrate that our transformer-based model is effective in\ndistinguishing malicious from benign traffic in the test dataset, attaining an\naverage accuracy of 79\\% using binary classification and 72\\% on the\nmulti-classification experiment, both using solely payload bytes.\n","authors":["Kyle Stein","Arash Mahyari","Guillermo Francia III","Eman El-Sheikh"],"pdf_url":"https://arxiv.org/pdf/2403.18223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18222v1","updated":"2024-03-27T03:19:36Z","published":"2024-03-27T03:19:36Z","title":"Uncertainty-Aware Deployment of Pre-trained Language-Conditioned\n  Imitation Learning Policies","summary":"  Large-scale robotic policies trained on data from diverse tasks and robotic\nplatforms hold great promise for enabling general-purpose robots; however,\nreliable generalization to new environment conditions remains a major\nchallenge. Toward addressing this challenge, we propose a novel approach for\nuncertainty-aware deployment of pre-trained language-conditioned imitation\nlearning agents. Specifically, we use temperature scaling to calibrate these\nmodels and exploit the calibrated model to make uncertainty-aware decisions by\naggregating the local information of candidate actions. We implement our\napproach in simulation using three such pre-trained models, and showcase its\npotential to significantly enhance task completion rates. The accompanying code\nis accessible at the link:\nhttps://github.com/BobWu1998/uncertainty_quant_all.git\n","authors":["Bo Wu","Bruce D. Lee","Kostas Daniilidis","Bernadette Bucher","Nikolai Matni"],"pdf_url":"https://arxiv.org/pdf/2403.18222v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2312.03256v2","updated":"2024-03-27T03:14:14Z","published":"2023-12-06T03:09:19Z","title":"CAFE: Towards Compact, Adaptive, and Fast Embedding for Large-scale\n  Recommendation Models","summary":"  Recently, the growing memory demands of embedding tables in Deep Learning\nRecommendation Models (DLRMs) pose great challenges for model training and\ndeployment. Existing embedding compression solutions cannot simultaneously meet\nthree key design requirements: memory efficiency, low latency, and adaptability\nto dynamic data distribution. This paper presents CAFE, a Compact, Adaptive,\nand Fast Embedding compression framework that addresses the above requirements.\nThe design philosophy of CAFE is to dynamically allocate more memory resources\nto important features (called hot features), and allocate less memory to\nunimportant ones. In CAFE, we propose a fast and lightweight sketch data\nstructure, named HotSketch, to capture feature importance and report hot\nfeatures in real time. For each reported hot feature, we assign it a unique\nembedding. For the non-hot features, we allow multiple features to share one\nembedding by using hash embedding technique. Guided by our design philosophy,\nwe further propose a multi-level hash embedding framework to optimize the\nembedding tables of non-hot features. We theoretically analyze the accuracy of\nHotSketch, and analyze the model convergence against deviation. Extensive\nexperiments show that CAFE significantly outperforms existing embedding\ncompression methods, yielding 3.92% and 3.68% superior testing AUC on Criteo\nKaggle dataset and CriteoTB dataset at a compression ratio of 10000x. The\nsource codes of CAFE are available at GitHub.\n","authors":["Hailin Zhang","Zirui Liu","Boxuan Chen","Yikai Zhao","Tong Zhao","Tong Yang","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2312.03256v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18219v1","updated":"2024-03-27T03:07:18Z","published":"2024-03-27T03:07:18Z","title":"From Two-Dimensional to Three-Dimensional Environment with Q-Learning:\n  Modeling Autonomous Navigation with Reinforcement Learning and no Libraries","summary":"  Reinforcement learning (RL) algorithms have become indispensable tools in\nartificial intelligence, empowering agents to acquire optimal decision-making\npolicies through interactions with their environment and feedback mechanisms.\nThis study explores the performance of RL agents in both two-dimensional (2D)\nand three-dimensional (3D) environments, aiming to research the dynamics of\nlearning across different spatial dimensions. A key aspect of this\ninvestigation is the absence of pre-made libraries for learning, with the\nalgorithm developed exclusively through computational mathematics. The\nmethodological framework centers on RL principles, employing a Q-learning agent\nclass and distinct environment classes tailored to each spatial dimension. The\nresearch aims to address the question: How do reinforcement learning agents\nadapt and perform in environments of varying spatial dimensions, particularly\nin 2D and 3D settings? Through empirical analysis, the study evaluates agents'\nlearning trajectories and adaptation processes, revealing insights into the\nefficacy of RL algorithms in navigating complex, multi-dimensional spaces.\nReflections on the findings prompt considerations for future research,\nparticularly in understanding the dynamics of learning in higher-dimensional\nenvironments.\n","authors":["Ergon Cugler de Moraes Silva"],"pdf_url":"https://arxiv.org/pdf/2403.18219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18216v1","updated":"2024-03-27T02:59:04Z","published":"2024-03-27T02:59:04Z","title":"Minimax Optimal Fair Classification with Bounded Demographic Disparity","summary":"  Mitigating the disparate impact of statistical machine learning methods is\ncrucial for ensuring fairness. While extensive research aims to reduce\ndisparity, the effect of using a \\emph{finite dataset} -- as opposed to the\nentire population -- remains unclear. This paper explores the statistical\nfoundations of fair binary classification with two protected groups, focusing\non controlling demographic disparity, defined as the difference in acceptance\nrates between the groups. Although fairness may come at the cost of accuracy\neven with infinite data, we show that using a finite sample incurs additional\ncosts due to the need to estimate group-specific acceptance thresholds. We\nstudy the minimax optimal classification error while constraining demographic\ndisparity to a user-specified threshold. To quantify the impact of fairness\nconstraints, we introduce a novel measure called \\emph{fairness-aware excess\nrisk} and derive a minimax lower bound on this measure that all classifiers\nmust satisfy. Furthermore, we propose FairBayes-DDP+, a group-wise thresholding\nmethod with an offset that we show attains the minimax lower bound. Our lower\nbound proofs involve several innovations. Experiments support that\nFairBayes-DDP+ controls disparity at the user-specified level, while being\nfaster and having a more favorable fairness-accuracy tradeoff than several\nbaselines.\n","authors":["Xianli Zeng","Guang Cheng","Edgar Dobriban"],"pdf_url":"https://arxiv.org/pdf/2403.18216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19060v1","updated":"2024-03-27T23:55:02Z","published":"2024-03-27T23:55:02Z","title":"Towards Human-Centered Construction Robotics: An RL-Driven Companion\n  Robot For Contextually Assisting Carpentry Workers","summary":"  In the dynamic construction industry, traditional robotic integration has\nprimarily focused on automating specific tasks, often overlooking the\ncomplexity and variability of human aspects in construction workflows. This\npaper introduces a human-centered approach with a ``work companion rover\"\ndesigned to assist construction workers within their existing practices, aiming\nto enhance safety and workflow fluency while respecting construction labor's\nskilled nature. We conduct an in-depth study on deploying a robotic system in\ncarpentry formwork, showcasing a prototype that emphasizes mobility, safety,\nand comfortable worker-robot collaboration in dynamic environments through a\ncontextual Reinforcement Learning (RL)-driven modular framework. Our research\nadvances robotic applications in construction, advocating for collaborative\nmodels where adaptive robots support rather than replace humans, underscoring\nthe potential for an interactive and collaborative human-robot workforce.\n","authors":["Yuning Wu","Jiaying Wei","Jean Oh","Daniel Cardoso Llach"],"pdf_url":"https://arxiv.org/pdf/2403.19060v1.pdf","comment":"8 pages, 9 figures. This work has been submitted to the IEEE for\n  possible publication. Copyright may be transferred without notice, after\n  which this version may no longer be accessible"},{"id":"http://arxiv.org/abs/2306.12627v2","updated":"2024-03-27T23:54:26Z","published":"2023-06-22T01:33:47Z","title":"Targeted collapse regularized autoencoder for anomaly detection: black\n  hole at the center","summary":"  Autoencoders have been extensively used in the development of recent anomaly\ndetection techniques. The premise of their application is based on the notion\nthat after training the autoencoder on normal training data, anomalous inputs\nwill exhibit a significant reconstruction error. Consequently, this enables a\nclear differentiation between normal and anomalous samples. In practice,\nhowever, it is observed that autoencoders can generalize beyond the normal\nclass and achieve a small reconstruction error on some of the anomalous\nsamples. To improve the performance, various techniques propose additional\ncomponents and more sophisticated training procedures. In this work, we propose\na remarkably straightforward alternative: instead of adding neural network\ncomponents, involved computations, and cumbersome training, we complement the\nreconstruction loss with a computationally light term that regulates the norm\nof representations in the latent space. The simplicity of our approach\nminimizes the requirement for hyperparameter tuning and customization for new\napplications which, paired with its permissive data modality constraint,\nenhances the potential for successful adoption across a broad range of\napplications. We test the method on various visual and tabular benchmarks and\ndemonstrate that the technique matches and frequently outperforms more complex\nalternatives. We further demonstrate that implementing this idea in the context\nof state-of-the-art methods can further improve their performance. We also\nprovide a theoretical analysis and numerical simulations that help demonstrate\nthe underlying process that unfolds during training and how it helps with\nanomaly detection. This mitigates the black-box nature of autoencoder-based\nanomaly detection algorithms and offers an avenue for further investigation of\nadvantages, fail cases, and potential new directions.\n","authors":["Amin Ghafourian","Huanyi Shui","Devesh Upadhyay","Rajesh Gupta","Dimitar Filev","Iman Soltani Bozchalooi"],"pdf_url":"https://arxiv.org/pdf/2306.12627v2.pdf","comment":"18 pages, 4 figures, 8 tables"},{"id":"http://arxiv.org/abs/2403.19057v1","updated":"2024-03-27T23:49:22Z","published":"2024-03-27T23:49:22Z","title":"Equity in Healthcare: Analyzing Disparities in Machine Learning\n  Predictions of Diabetic Patient Readmissions","summary":"  This study investigates how machine learning (ML) models can predict hospital\nreadmissions for diabetic patients fairly and accurately across different\ndemographics (age, gender, race). We compared models like Deep Learning,\nGeneralized Linear Models, Gradient Boosting Machines (GBM), and Naive Bayes.\nGBM stood out with an F1-score of 84.3% and accuracy of 82.2%, accurately\npredicting readmissions across demographics. A fairness analysis was conducted\nacross all the models. GBM minimized disparities in predictions, achieving\nbalanced results across genders and races. It showed low False Discovery Rates\n(FDR) (6-7%) and False Positive Rates (FPR) (5%) for both genders.\nAdditionally, FDRs remained low for racial groups, such as African Americans\n(8%) and Asians (7%). Similarly, FPRs were consistent across age groups (4%)\nfor both patients under 40 and those above 40, indicating its precision and\nability to reduce bias. These findings emphasize the importance of choosing ML\nmodels carefully to ensure both accuracy and fairness for all patients. By\nshowcasing effectiveness of various models with fairness metrics, this study\npromotes personalized medicine and the need for fair ML algorithms in\nhealthcare. This can ultimately reduce disparities and improve outcomes for\ndiabetic patients of all backgrounds.\n","authors":["Zainab Al-Zanbouri","Gauri Sharma","Shaina Raza"],"pdf_url":"https://arxiv.org/pdf/2403.19057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13099v2","updated":"2024-03-27T23:49:07Z","published":"2023-11-22T01:58:26Z","title":"PIE-NeRF: Physics-based Interactive Elastodynamics with NeRF","summary":"  We show that physics-based simulations can be seamlessly integrated with NeRF\nto generate high-quality elastodynamics of real-world objects. Unlike existing\nmethods, we discretize nonlinear hyperelasticity in a meshless way, obviating\nthe necessity for intermediate auxiliary shape proxies like a tetrahedral mesh\nor voxel grid. A quadratic generalized moving least square (Q-GMLS) is employed\nto capture nonlinear dynamics and large deformation on the implicit model. Such\nmeshless integration enables versatile simulations of complex and codimensional\nshapes. We adaptively place the least-square kernels according to the NeRF\ndensity field to significantly reduce the complexity of the nonlinear\nsimulation. As a result, physically realistic animations can be conveniently\nsynthesized using our method for a wide range of hyperelastic materials at an\ninteractive rate. For more information, please visit our project page at\nhttps://fytalon.github.io/pienerf/.\n","authors":["Yutao Feng","Yintong Shang","Xuan Li","Tianjia Shao","Chenfanfu Jiang","Yin Yang"],"pdf_url":"https://arxiv.org/pdf/2311.13099v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.13883v6","updated":"2024-03-27T23:27:58Z","published":"2022-03-25T19:45:33Z","title":"Multi-modal Misinformation Detection: Approaches, Challenges and\n  Opportunities","summary":"  As social media platforms are evolving from text-based forums into\nmulti-modal environments, the nature of misinformation in social media is also\ntransforming accordingly. Taking advantage of the fact that visual modalities\nsuch as images and videos are more favorable and attractive to the users and\ntextual contents are sometimes skimmed carelessly, misinformation spreaders\nhave recently targeted contextual connections between the modalities e.g., text\nand image. Hence many researchers have developed automatic techniques for\ndetecting possible cross-modal discordance in web-based content. We analyze,\ncategorize and identify existing approaches in addition to challenges and\nshortcomings they face in order to unearth new research opportunities in the\nfield of multi-modal misinformation detection.\n","authors":["Sara Abdali","Sina shaham","Bhaskar Krishnamachari"],"pdf_url":"https://arxiv.org/pdf/2203.13883v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.03683v2","updated":"2024-03-27T23:15:33Z","published":"2023-07-07T15:51:55Z","title":"Differentiable Turbulence: Closure as a partial differential equation\n  constrained optimization","summary":"  Deep learning is increasingly becoming a promising pathway to improving the\naccuracy of sub-grid scale (SGS) turbulence closure models for large eddy\nsimulations (LES). We leverage the concept of differentiable turbulence,\nwhereby an end-to-end differentiable solver is used in combination with\nphysics-inspired choices of deep learning architectures to learn highly\neffective and versatile SGS models for two-dimensional turbulent flow. We\nperform an in-depth analysis of the inductive biases in the chosen\narchitectures, finding that the inclusion of small-scale non-local features is\nmost critical to effective SGS modeling, while large-scale features can improve\npointwise accuracy of the \\textit{a-posteriori} solution field. The velocity\ngradient tensor on the LES grid can be mapped directly to the SGS stress via\ndecomposition of the inputs and outputs into isotropic, deviatoric, and\nanti-symmetric components. We see that the model can generalize to a variety of\nflow configurations, including higher and lower Reynolds numbers and different\nforcing conditions. We show that the differentiable physics paradigm is more\nsuccessful than offline, \\textit{a-priori} learning, and that hybrid\nsolver-in-the-loop approaches to deep learning offer an ideal balance between\ncomputational efficiency, accuracy, and generalization. Our experiments provide\nphysics-based recommendations for deep-learning based SGS modeling for\ngeneralizable closure modeling of turbulence.\n","authors":["Varun Shankar","Dibyajyoti Chakraborty","Venkatasubramanian Viswanathan","Romit Maulik"],"pdf_url":"https://arxiv.org/pdf/2307.03683v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19050v1","updated":"2024-03-27T23:10:33Z","published":"2024-03-27T23:10:33Z","title":"Detecting Generative Parroting through Overfitting Masked Autoencoders","summary":"  The advent of generative AI models has revolutionized digital content\ncreation, yet it introduces challenges in maintaining copyright integrity due\nto generative parroting, where models mimic their training data too closely.\nOur research presents a novel approach to tackle this issue by employing an\noverfitted Masked Autoencoder (MAE) to detect such parroted samples\neffectively. We establish a detection threshold based on the mean loss across\nthe training dataset, allowing for the precise identification of parroted\ncontent in modified datasets. Preliminary evaluations demonstrate promising\nresults, suggesting our method's potential to ensure ethical use and enhance\nthe legal compliance of generative models.\n","authors":["Saeid Asgari Taghanaki","Joseph Lambourne"],"pdf_url":"https://arxiv.org/pdf/2403.19050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03160v2","updated":"2024-03-27T22:58:34Z","published":"2023-12-05T22:04:49Z","title":"HybridNeRF: Efficient Neural Rendering via Adaptive Volumetric Surfaces","summary":"  Neural radiance fields provide state-of-the-art view synthesis quality but\ntend to be slow to render. One reason is that they make use of volume\nrendering, thus requiring many samples (and model queries) per ray at render\ntime. Although this representation is flexible and easy to optimize, most\nreal-world objects can be modeled more efficiently with surfaces instead of\nvolumes, requiring far fewer samples per ray. This observation has spurred\nconsiderable progress in surface representations such as signed distance\nfunctions, but these may struggle to model semi-opaque and thin structures. We\npropose a method, HybridNeRF, that leverages the strengths of both\nrepresentations by rendering most objects as surfaces while modeling the\n(typically) small fraction of challenging regions volumetrically. We evaluate\nHybridNeRF against the challenging Eyeful Tower dataset along with other\ncommonly used view synthesis datasets. When comparing to state-of-the-art\nbaselines, including recent rasterization-based approaches, we improve error\nrates by 15-30% while achieving real-time framerates (at least 36 FPS) for\nvirtual-reality resolutions (2Kx2K).\n","authors":["Haithem Turki","Vasu Agrawal","Samuel Rota Bulò","Lorenzo Porzi","Peter Kontschieder","Deva Ramanan","Michael Zollhöfer","Christian Richardt"],"pdf_url":"https://arxiv.org/pdf/2312.03160v2.pdf","comment":"CVPR 2024 Project page: https://haithemturki.com/hybrid-nerf/"},{"id":"http://arxiv.org/abs/2403.19040v1","updated":"2024-03-27T22:26:50Z","published":"2024-03-27T22:26:50Z","title":"Visualizing High-Dimensional Temporal Data Using Direction-Aware t-SNE","summary":"  Many real-world data sets contain a temporal component or involve transitions\nfrom state to state. For exploratory data analysis, we can represent these\nhigh-dimensional data sets in two-dimensional maps, using embeddings of the\ndata objects under exploration and representing their temporal relationships\nwith directed edges. Most existing dimensionality reduction techniques, such as\nt-SNE and UMAP, do not take into account the temporal or relational nature of\nthe data when constructing the embeddings, resulting in temporally cluttered\nvisualizations that obscure potentially interesting patterns. To address this\nproblem, we propose two complementary, direction-aware loss terms in the\noptimization function of t-SNE that emphasize the temporal aspects of the data,\nguiding the optimization and the resulting embedding to reveal temporal\npatterns that might otherwise go unnoticed. The Directional Coherence Loss\n(DCL) encourages nearby arrows connecting two adjacent time series points to\npoint in the same direction, while the Edge Length Loss (ELL) penalizes arrows\n- which effectively represent time gaps in the visualized embedding - based on\ntheir length. Both loss terms are differentiable and can be easily incorporated\ninto existing dimensionality reduction techniques. By promoting local\ndirectionality of the directed edges, our procedure produces more temporally\nmeaningful and less cluttered visualizations. We demonstrate the effectiveness\nof our approach on a toy dataset and two real-world datasets.\n","authors":["Pavlin G. Poličar","Blaž Zupan"],"pdf_url":"https://arxiv.org/pdf/2403.19040v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19031v1","updated":"2024-03-27T22:05:10Z","published":"2024-03-27T22:05:10Z","title":"Evaluating Large Language Models for Health-Related Text Classification\n  Tasks with Public Social Media Data","summary":"  Large language models (LLMs) have demonstrated remarkable success in NLP\ntasks. However, there is a paucity of studies that attempt to evaluate their\nperformances on social media-based health-related natural language processing\ntasks, which have traditionally been difficult to achieve high scores in. We\nbenchmarked one supervised classic machine learning model based on Support\nVector Machines (SVMs), three supervised pretrained language models (PLMs)\nbased on RoBERTa, BERTweet, and SocBERT, and two LLM based classifiers (GPT3.5\nand GPT4), across 6 text classification tasks. We developed three approaches\nfor leveraging LLMs for text classification: employing LLMs as zero-shot\nclassifiers, us-ing LLMs as annotators to annotate training data for supervised\nclassifiers, and utilizing LLMs with few-shot examples for augmentation of\nmanually annotated data. Our comprehensive experiments demonstrate that\nemploy-ing data augmentation using LLMs (GPT-4) with relatively small\nhuman-annotated data to train lightweight supervised classification models\nachieves superior results compared to training with human-annotated data alone.\nSupervised learners also outperform GPT-4 and GPT-3.5 in zero-shot settings. By\nleveraging this data augmentation strategy, we can harness the power of LLMs to\ndevelop smaller, more effective domain-specific NLP models. LLM-annotated data\nwithout human guidance for training light-weight supervised classification\nmodels is an ineffective strategy. However, LLM, as a zero-shot classifier,\nshows promise in excluding false negatives and potentially reducing the human\neffort required for data annotation. Future investigations are imperative to\nexplore optimal training data sizes and the optimal amounts of augmented data.\n","authors":["Yuting Guo","Anthony Ovadje","Mohammed Ali Al-Garadi","Abeed Sarker"],"pdf_url":"https://arxiv.org/pdf/2403.19031v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01990v2","updated":"2024-03-27T22:03:46Z","published":"2023-11-03T15:42:12Z","title":"Conditions on Preference Relations that Guarantee the Existence of\n  Optimal Policies","summary":"  Learning from Preferential Feedback (LfPF) plays an essential role in\ntraining Large Language Models, as well as certain types of interactive\nlearning agents. However, a substantial gap exists between the theory and\napplication of LfPF algorithms. Current results guaranteeing the existence of\noptimal policies in LfPF problems assume that both the preferences and\ntransition dynamics are determined by a Markov Decision Process. We introduce\nthe Direct Preference Process, a new framework for analyzing LfPF problems in\npartially-observable, non-Markovian environments. Within this framework, we\nestablish conditions that guarantee the existence of optimal policies by\nconsidering the ordinal structure of the preferences. We show that a\ndecision-making problem can have optimal policies -- that are characterized by\nrecursive optimality equations -- even when no reward function can express the\nlearning goal. These findings underline the need to explore preference-based\nlearning strategies which do not assume that preferences are generated by\nreward.\n","authors":["Jonathan Colaço Carr","Prakash Panangaden","Doina Precup"],"pdf_url":"https://arxiv.org/pdf/2311.01990v2.pdf","comment":"v2: replaced with accepted AISTATS 2024 version, containing a new\n  summary figure and one extra example. Results and conclusions are unchanged"},{"id":"http://arxiv.org/abs/2204.11970v3","updated":"2024-03-27T22:02:30Z","published":"2022-04-25T21:20:27Z","title":"Visual Acuity Prediction on Real-Life Patient Data Using a Machine\n  Learning Based Multistage System","summary":"  In ophthalmology, intravitreal operative medication therapy (IVOM) is a\nwidespread treatment for diseases related to the age-related macular\ndegeneration (AMD), the diabetic macular edema (DME), as well as the retinal\nvein occlusion (RVO). However, in real-world settings, patients often suffer\nfrom loss of vision on time scales of years despite therapy, whereas the\nprediction of the visual acuity (VA) and the earliest possible detection of\ndeterioration under real-life conditions is challenging due to heterogeneous\nand incomplete data. In this contribution, we present a workflow for the\ndevelopment of a research-compatible data corpus fusing different IT systems of\nthe department of ophthalmology of a German maximum care hospital. The\nextensive data corpus allows predictive statements of the expected progression\nof a patient and his or her VA in each of the three diseases. For the disease\nAMD, we found out a significant deterioration of the visual acuity over time.\nWithin our proposed multistage system, we subsequently classify the VA\nprogression into the three groups of therapy \"winners\", \"stabilizers\", and\n\"losers\" (WSL classification scheme). Our OCT biomarker classification using an\nensemble of deep neural networks results in a classification accuracy\n(F1-score) of over 98 %, enabling us to complete incomplete OCT documentations\nwhile allowing us to exploit them for a more precise VA modelling process. Our\nVA prediction requires at least four VA examinations and optionally OCT\nbiomarkers from the same time period to predict the VA progression within a\nforecasted time frame, whereas our prediction is currently restricted to IVOM /\nno therapy. We achieve a final prediction accuracy of 69 % in macro average\nF1-score, while being in the same range as the ophthalmologists with 57.8 and\n50 +- 10.7 % F1-score.\n","authors":["Tobias Schlosser","Frederik Beuth","Trixy Meyer","Arunodhayan Sampath Kumar","Gabriel Stolze","Olga Furashova","Katrin Engelmann","Danny Kowerko"],"pdf_url":"https://arxiv.org/pdf/2204.11970v3.pdf","comment":"Preprint for journal Scientific Reports (Springer)"},{"id":"http://arxiv.org/abs/2310.15301v2","updated":"2024-03-27T21:56:59Z","published":"2023-10-23T19:07:33Z","title":"ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital\n  Biomarkers of Alzheimer's Disease","summary":"  Alzheimer's Disease (AD) and related dementia are a growing global health\nchallenge due to the aging population. In this paper, we present ADMarker, the\nfirst end-to-end system that integrates multi-modal sensors and new federated\nlearning algorithms for detecting multidimensional AD digital biomarkers in\nnatural living environments. ADMarker features a novel three-stage multi-modal\nfederated learning architecture that can accurately detect digital biomarkers\nin a privacy-preserving manner. Our approach collectively addresses several\nmajor real-world challenges, such as limited data labels, data heterogeneity,\nand limited computing resources. We built a compact multi-modality hardware\nsystem and deployed it in a four-week clinical trial involving 91 elderly\nparticipants. The results indicate that ADMarker can accurately detect a\ncomprehensive set of digital biomarkers with up to 93.8% accuracy and identify\nearly AD with an average of 88.9% accuracy. ADMarker offers a new platform that\ncan allow AD clinicians to characterize and track the complex correlation\nbetween multidimensional interpretable digital biomarkers, demographic factors\nof patients, and AD diagnosis in a longitudinal manner.\n","authors":["Xiaomin Ouyang","Xian Shuai","Yang Li","Li Pan","Xifan Zhang","Heming Fu","Xinyan Wang","Shihua Cao","Jiang Xin","Hazel Mok","Zhenyu Yan","Doris Sau Fung Yu","Timothy Kwok","Guoliang Xing"],"pdf_url":"https://arxiv.org/pdf/2310.15301v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12984v2","updated":"2024-03-27T21:51:03Z","published":"2024-03-03T11:09:32Z","title":"When SMILES have Language: Drug Classification using Text Classification\n  Methods on Drug SMILES Strings","summary":"  Complex chemical structures, like drugs, are usually defined by SMILES\nstrings as a sequence of molecules and bonds. These SMILES strings are used in\ndifferent complex machine learning-based drug-related research and\nrepresentation works. Escaping from complex representation, in this work, we\npose a single question: What if we treat drug SMILES as conventional sentences\nand engage in text classification for drug classification? Our experiments\naffirm the possibility with very competitive scores. The study explores the\nnotion of viewing each atom and bond as sentence components, employing basic\nNLP methods to categorize drug types, proving that complex problems can also be\nsolved with simpler perspectives. The data and code are available here:\nhttps://github.com/azminewasi/Drug-Classification-NLP.\n","authors":["Azmine Toushik Wasi","Šerbetar Karlo","Raima Islam","Taki Hasan Rafi","Dong-Kyu Chae"],"pdf_url":"https://arxiv.org/pdf/2403.12984v2.pdf","comment":"7 pages, 2 figures, 5 tables, Accepted (invited to present) to the\n  The Second Tiny Papers Track at ICLR 2024\n  (https://openreview.net/forum?id=VUYCyH8fCw)"},{"id":"http://arxiv.org/abs/2403.17210v2","updated":"2024-03-27T21:47:49Z","published":"2024-03-25T21:37:31Z","title":"CADGL: Context-Aware Deep Graph Learning for Predicting Drug-Drug\n  Interactions","summary":"  Examining Drug-Drug Interactions (DDIs) is a pivotal element in the process\nof drug development. DDIs occur when one drug's properties are affected by the\ninclusion of other drugs. Detecting favorable DDIs has the potential to pave\nthe way for creating and advancing innovative medications applicable in\npractical settings. However, existing DDI prediction models continue to face\nchallenges related to generalization in extreme cases, robust feature\nextraction, and real-life application possibilities. We aim to address these\nchallenges by leveraging the effectiveness of context-aware deep graph learning\nby introducing a novel framework named CADGL. Based on a customized variational\ngraph autoencoder (VGAE), we capture critical structural and physio-chemical\ninformation using two context preprocessors for feature extraction from two\ndifferent perspectives: local neighborhood and molecular context, in a\nheterogeneous graphical structure. Our customized VGAE consists of a graph\nencoder, a latent information encoder, and an MLP decoder. CADGL surpasses\nother state-of-the-art DDI prediction models, excelling in predicting\nclinically valuable novel DDIs, supported by rigorous case studies.\n","authors":["Azmine Toushik Wasi","Taki Hasan Rafi","Raima Islam","Serbetar Karlo","Dong-Kyu Chae"],"pdf_url":"https://arxiv.org/pdf/2403.17210v2.pdf","comment":"8 Pages, 4 Figures; In review"},{"id":"http://arxiv.org/abs/2402.08714v2","updated":"2024-03-27T21:37:39Z","published":"2024-02-13T18:58:16Z","title":"PRDP: Proximal Reward Difference Prediction for Large-Scale Reward\n  Finetuning of Diffusion Models","summary":"  Reward finetuning has emerged as a promising approach to aligning foundation\nmodels with downstream objectives. Remarkable success has been achieved in the\nlanguage domain by using reinforcement learning (RL) to maximize rewards that\nreflect human preference. However, in the vision domain, existing RL-based\nreward finetuning methods are limited by their instability in large-scale\ntraining, rendering them incapable of generalizing to complex, unseen prompts.\nIn this paper, we propose Proximal Reward Difference Prediction (PRDP),\nenabling stable black-box reward finetuning for diffusion models for the first\ntime on large-scale prompt datasets with over 100K prompts. Our key innovation\nis the Reward Difference Prediction (RDP) objective that has the same optimal\nsolution as the RL objective while enjoying better training stability.\nSpecifically, the RDP objective is a supervised regression objective that tasks\nthe diffusion model with predicting the reward difference of generated image\npairs from their denoising trajectories. We theoretically prove that the\ndiffusion model that obtains perfect reward difference prediction is exactly\nthe maximizer of the RL objective. We further develop an online algorithm with\nproximal updates to stably optimize the RDP objective. In experiments, we\ndemonstrate that PRDP can match the reward maximization ability of\nwell-established RL-based methods in small-scale training. Furthermore, through\nlarge-scale training on text prompts from the Human Preference Dataset v2 and\nthe Pick-a-Pic v1 dataset, PRDP achieves superior generation quality on a\ndiverse set of complex, unseen prompts whereas RL-based methods completely\nfail.\n","authors":["Fei Deng","Qifei Wang","Wei Wei","Matthias Grundmann","Tingbo Hou"],"pdf_url":"https://arxiv.org/pdf/2402.08714v2.pdf","comment":"CVPR 2024. Project page: https://fdeng18.github.io/prdp"},{"id":"http://arxiv.org/abs/2403.19024v1","updated":"2024-03-27T21:31:46Z","published":"2024-03-27T21:31:46Z","title":"Exploiting Symmetry in Dynamics for Model-Based Reinforcement Learning\n  with Asymmetric Rewards","summary":"  Recent work in reinforcement learning has leveraged symmetries in the model\nto improve sample efficiency in training a policy. A commonly used simplifying\nassumption is that the dynamics and reward both exhibit the same symmetry.\nHowever, in many real-world environments, the dynamical model exhibits symmetry\nindependent of the reward model: the reward may not satisfy the same symmetries\nas the dynamics. In this paper, we investigate scenarios where only the\ndynamics are assumed to exhibit symmetry, extending the scope of problems in\nreinforcement learning and learning in control theory where symmetry techniques\ncan be applied. We use Cartan's moving frame method to introduce a technique\nfor learning dynamics which, by construction, exhibit specified symmetries. We\ndemonstrate through numerical experiments that the proposed method learns a\nmore accurate dynamical model.\n","authors":["Yasin Sonmez","Neelay Junnarkar","Murat Arcak"],"pdf_url":"https://arxiv.org/pdf/2403.19024v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2403.18821v1","updated":"2024-03-27T17:59:56Z","published":"2024-03-27T17:59:56Z","title":"Real Acoustic Fields: An Audio-Visual Room Acoustics Dataset and\n  Benchmark","summary":"  We present a new dataset called Real Acoustic Fields (RAF) that captures real\nacoustic room data from multiple modalities. The dataset includes high-quality\nand densely captured room impulse response data paired with multi-view images,\nand precise 6DoF pose tracking data for sound emitters and listeners in the\nrooms. We used this dataset to evaluate existing methods for novel-view\nacoustic synthesis and impulse response generation which previously relied on\nsynthetic data. In our evaluation, we thoroughly assessed existing audio and\naudio-visual models against multiple criteria and proposed settings to enhance\ntheir performance on real-world data. We also conducted experiments to\ninvestigate the impact of incorporating visual data (i.e., images and depth)\ninto neural acoustic field models. Additionally, we demonstrated the\neffectiveness of a simple sim2real approach, where a model is pre-trained with\nsimulated data and fine-tuned with sparse real-world data, resulting in\nsignificant improvements in the few-shot learning approach. RAF is the first\ndataset to provide densely captured room acoustic data, making it an ideal\nresource for researchers working on audio and audio-visual neural acoustic\nfield modeling techniques. Demos and datasets are available on our project\npage: https://facebookresearch.github.io/real-acoustic-fields/\n","authors":["Ziyang Chen","Israel D. Gebru","Christian Richardt","Anurag Kumar","William Laney","Andrew Owens","Alexander Richard"],"pdf_url":"https://arxiv.org/pdf/2403.18821v1.pdf","comment":"Accepted to CVPR 2024. Project site:\n  https://facebookresearch.github.io/real-acoustic-fields/"},{"id":"http://arxiv.org/abs/2403.18715v1","updated":"2024-03-27T16:04:47Z","published":"2024-03-27T16:04:47Z","title":"Mitigating Hallucinations in Large Vision-Language Models with\n  Instruction Contrastive Decoding","summary":"  Large Vision-Language Models (LVLMs) are increasingly adept at generating\ncontextually detailed and coherent responses from visual inputs. However, their\napplication in multimodal decision-making and open-ended generation is hindered\nby a notable rate of hallucinations, where generated text inaccurately\nrepresents the visual contents. To address this issue, this paper introduces\nthe Instruction Contrastive Decoding (ICD) method, a novel approach designed to\nreduce hallucinations during LVLM inference. Our method is inspired by our\nobservation that what we call disturbance instructions significantly exacerbate\nhallucinations in multimodal fusion modules. ICD contrasts distributions from\nstandard and instruction disturbance, thereby increasing alignment uncertainty\nand effectively subtracting hallucinated concepts from the original\ndistribution. Through comprehensive experiments on discriminative benchmarks\n(POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that\nICD significantly mitigates both object-level and attribute-level\nhallucinations. Moreover, our method not only addresses hallucinations but also\nsignificantly enhances the general perception and recognition capabilities of\nLVLMs.\n","authors":["Xintong Wang","Jingheng Pan","Liang Ding","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2403.18715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18714v1","updated":"2024-03-27T16:02:00Z","published":"2024-03-27T16:02:00Z","title":"Bringing Textual Prompt to AI-Generated Image Quality Assessment","summary":"  AI-Generated Images (AGIs) have inherent multimodal nature. Unlike\ntraditional image quality assessment (IQA) on natural scenarios, AGIs quality\nassessment (AGIQA) takes the correspondence of image and its textual prompt\ninto consideration. This is coupled in the ground truth score, which confuses\nthe unimodal IQA methods. To solve this problem, we introduce IP-IQA (AGIs\nQuality Assessment via Image and Prompt), a multimodal framework for AGIQA via\ncorresponding image and prompt incorporation. Specifically, we propose a novel\nincremental pretraining task named Image2Prompt for better understanding of\nAGIs and their corresponding textual prompts. An effective and efficient\nimage-prompt fusion module, along with a novel special [QA] token, are also\napplied. Both are plug-and-play and beneficial for the cooperation of image and\nits corresponding prompt. Experiments demonstrate that our IP-IQA achieves the\nstate-of-the-art on AGIQA-1k and AGIQA-3k datasets. Code will be available.\n","authors":["Bowen Qu","Haohui Li","Wei Gao"],"pdf_url":"https://arxiv.org/pdf/2403.18714v1.pdf","comment":"6 pages, 3 figures, accepted by ICME2024"},{"id":"http://arxiv.org/abs/2402.14326v2","updated":"2024-03-27T13:25:17Z","published":"2024-02-22T06:38:25Z","title":"Think before You Leap: Content-Aware Low-Cost Edge-Assisted Video\n  Semantic Segmentation","summary":"  Offloading computing to edge servers is a promising solution to support\ngrowing video understanding applications at resource-constrained IoT devices.\nRecent efforts have been made to enhance the scalability of such systems by\nreducing inference costs on edge servers. However, existing research is not\ndirectly applicable to pixel-level vision tasks such as video semantic\nsegmentation (VSS), partly due to the fluctuating VSS accuracy and segment\nbitrate caused by the dynamic video content. In response, we present Penance, a\nnew edge inference cost reduction framework. By exploiting softmax outputs of\nVSS models and the prediction mechanism of H.264/AVC codecs, Penance optimizes\nmodel selection and compression settings to minimize the inference cost while\nmeeting the required accuracy within the available bandwidth constraints. We\nimplement Penance in a commercial IoT device with only CPUs. Experimental\nresults show that Penance consumes a negligible 6.8% more computation resources\nthan the optimal strategy while satisfying accuracy and bandwidth constraints\nwith a low failure rate.\n","authors":["Mingxuan Yan","Yi Wang","Xuedou Xiao","Zhiqing Luo","Jianhua He","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2402.14326v2.pdf","comment":"Accepted by ACM Multimedia 2023"},{"id":"http://arxiv.org/abs/2403.18323v1","updated":"2024-03-27T07:52:51Z","published":"2024-03-27T07:52:51Z","title":"How to Cache Important Contents for Multi-modal Service in Dynamic\n  Networks: A DRL-based Caching Scheme","summary":"  With the continuous evolution of networking technologies, multi-modal\nservices that involve video, audio, and haptic contents are expected to become\nthe dominant multimedia service in the near future. Edge caching is a key\ntechnology that can significantly reduce network load and content transmission\nlatency, which is critical for the delivery of multi-modal contents. However,\nexisting caching approaches only rely on a limited number of factors, e.g.,\npopularity, to evaluate their importance for caching, which is inefficient for\ncaching multi-modal contents, especially in dynamic network environments. To\novercome this issue, we propose a content importance-based caching scheme which\nconsists of a content importance evaluation model and a caching model. By\nleveraging dueling double deep Q networks (D3QN) model, the content importance\nevaluation model can adaptively evaluate contents' importance in dynamic\nnetworks. Based on the evaluated contents' importance, the caching model can\neasily cache and evict proper contents to improve caching efficiency. The\nsimulation results show that the proposed content importance-based caching\nscheme outperforms existing caching schemes in terms of caching hit ratio (at\nleast 15% higher), reduced network load (up to 22% reduction), average number\nof hops (up to 27% lower), and unsatisfied requests ratio (more than 47%\nreduction).\n","authors":["Zhe Zhang","Marc St-Hilaire","Xin Wei","Haiwei Dong","Abdulmotaleb El Saddik"],"pdf_url":"https://arxiv.org/pdf/2403.18323v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18252v1","updated":"2024-03-27T04:49:23Z","published":"2024-03-27T04:49:23Z","title":"Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models","summary":"  Visual representation learning has been a cornerstone in computer vision,\nevolving from supervised learning with human-annotated labels to aligning\nimage-text pairs from the Internet. Despite recent advancements in multi-modal\nlarge language models (MLLMs), the visual representations they rely on, such as\nCLIP embeddings, often lack access to external world knowledge critical for\nreal-world visual reasoning. In this work, we propose Visual Table, a novel\nvisual representation tailored for MLLMs. It provides hierarchical text\ndescriptions of holistic visual scenes, consisting of a scene description and\nmultiple object-centric descriptions that encompass categories, attributes, and\nknowledge at instance level. We further develop a scalable generator for visual\ntable generation and train it on small-scale annotations from GPT4V. Extensive\nevaluations demonstrate that, with generated visual tables as additional visual\nrepresentations, our model can consistently outperform the state-of-the-art\n(SOTA) MLLMs across diverse benchmarks. When visual tables serve as standalone\nvisual representations, our model can closely match or even beat the SOTA MLLMs\nthat are built on CLIP visual embeddings. Our code is available at\nhttps://github.com/LaVi-Lab/Visual-Table.\n","authors":["Yiwu Zhong","Zi-Yuan Hu","Michael R. Lyu","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18252v1.pdf","comment":"Project page: https://github.com/LaVi-Lab/Visual-Table"},{"id":"http://arxiv.org/abs/2403.10066v3","updated":"2024-03-27T02:25:51Z","published":"2024-03-15T07:16:07Z","title":"Contrastive Pre-Training with Multi-View Fusion for No-Reference Point\n  Cloud Quality Assessment","summary":"  No-reference point cloud quality assessment (NR-PCQA) aims to automatically\nevaluate the perceptual quality of distorted point clouds without available\nreference, which have achieved tremendous improvements due to the utilization\nof deep neural networks. However, learning-based NR-PCQA methods suffer from\nthe scarcity of labeled data and usually perform suboptimally in terms of\ngeneralization. To solve the problem, we propose a novel contrastive\npre-training framework tailored for PCQA (CoPA), which enables the pre-trained\nmodel to learn quality-aware representations from unlabeled data. To obtain\nanchors in the representation space, we project point clouds with different\ndistortions into images and randomly mix their local patches to form mixed\nimages with multiple distortions. Utilizing the generated anchors, we constrain\nthe pre-training process via a quality-aware contrastive loss following the\nphilosophy that perceptual quality is closely related to both content and\ndistortion. Furthermore, in the model fine-tuning stage, we propose a\nsemantic-guided multi-view fusion module to effectively integrate the features\nof projected images from multiple perspectives. Extensive experiments show that\nour method outperforms the state-of-the-art PCQA methods on popular benchmarks.\nFurther investigations demonstrate that CoPA can also benefit existing\nlearning-based PCQA models.\n","authors":["Ziyu Shan","Yujie Zhang","Qi Yang","Haichen Yang","Yiling Xu","Jenq-Neng Hwang","Xiaozhong Xu","Shan Liu"],"pdf_url":"https://arxiv.org/pdf/2403.10066v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.13883v6","updated":"2024-03-27T23:27:58Z","published":"2022-03-25T19:45:33Z","title":"Multi-modal Misinformation Detection: Approaches, Challenges and\n  Opportunities","summary":"  As social media platforms are evolving from text-based forums into\nmulti-modal environments, the nature of misinformation in social media is also\ntransforming accordingly. Taking advantage of the fact that visual modalities\nsuch as images and videos are more favorable and attractive to the users and\ntextual contents are sometimes skimmed carelessly, misinformation spreaders\nhave recently targeted contextual connections between the modalities e.g., text\nand image. Hence many researchers have developed automatic techniques for\ndetecting possible cross-modal discordance in web-based content. We analyze,\ncategorize and identify existing approaches in addition to challenges and\nshortcomings they face in order to unearth new research opportunities in the\nfield of multi-modal misinformation detection.\n","authors":["Sara Abdali","Sina shaham","Bhaskar Krishnamachari"],"pdf_url":"https://arxiv.org/pdf/2203.13883v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10667v2","updated":"2024-03-27T21:11:19Z","published":"2024-03-15T20:21:31Z","title":"Towards Unified Multi-Modal Personalization: Large Vision-Language\n  Models for Generative Recommendation and Beyond","summary":"  Developing a universal model that can effectively harness heterogeneous\nresources and respond to a wide range of personalized needs has been a\nlongstanding community aspiration. Our daily choices, especially in domains\nlike fashion and retail, are substantially shaped by multi-modal data, such as\npictures and textual descriptions. These modalities not only offer intuitive\nguidance but also cater to personalized user preferences. However, the\npredominant personalization approaches mainly focus on the ID or text-based\nrecommendation problem, failing to comprehend the information spanning various\ntasks or modalities. In this paper, our goal is to establish a Unified paradigm\nfor Multi-modal Personalization systems (UniMP), which effectively leverages\nmulti-modal data while eliminating the complexities associated with task- and\nmodality-specific customization. We argue that the advancements in foundational\ngenerative modeling have provided the flexibility and effectiveness necessary\nto achieve the objective. In light of this, we develop a generic and extensible\npersonalization generative framework, that can handle a wide range of\npersonalized needs including item recommendation, product search, preference\nprediction, explanation generation, and further user-guided image generation.\nOur methodology enhances the capabilities of foundational language models for\npersonalized tasks by seamlessly ingesting interleaved cross-modal user history\ninformation, ensuring a more precise and customized experience for users. To\ntrain and evaluate the proposed multi-modal personalized tasks, we also\nintroduce a novel and comprehensive benchmark covering a variety of user\nrequirements. Our experiments on the real-world benchmark showcase the model's\npotential, outperforming competitive methods specialized for each task.\n","authors":["Tianxin Wei","Bowen Jin","Ruirui Li","Hansi Zeng","Zhengyang Wang","Jianhui Sun","Qingyu Yin","Hanqing Lu","Suhang Wang","Jingrui He","Xianfeng Tang"],"pdf_url":"https://arxiv.org/pdf/2403.10667v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.19002v1","updated":"2024-03-27T20:52:30Z","published":"2024-03-27T20:52:30Z","title":"Robust Active Speaker Detection in Noisy Environments","summary":"  This paper addresses the issue of active speaker detection (ASD) in noisy\nenvironments and formulates a robust active speaker detection (rASD) problem.\nExisting ASD approaches leverage both audio and visual modalities, but\nnon-speech sounds in the surrounding environment can negatively impact\nperformance. To overcome this, we propose a novel framework that utilizes\naudio-visual speech separation as guidance to learn noise-free audio features.\nThese features are then utilized in an ASD model, and both tasks are jointly\noptimized in an end-to-end framework. Our proposed framework mitigates residual\nnoise and audio quality reduction issues that can occur in a naive cascaded\ntwo-stage framework that directly uses separated speech for ASD, and enables\nthe two tasks to be optimized simultaneously. To further enhance the robustness\nof the audio features and handle inherent speech noises, we propose a dynamic\nweighted loss approach to train the speech separator. We also collected a\nreal-world noise audio dataset to facilitate investigations. Experiments\ndemonstrate that non-speech audio noises significantly impact ASD models, and\nour proposed approach improves ASD performance in noisy environments. The\nframework is general and can be applied to different ASD approaches to improve\ntheir robustness. Our code, models, and data will be released.\n","authors":["Siva Sai Nagender Vasireddy","Chenxu Zhang","Xiaohu Guo","Yapeng Tian"],"pdf_url":"https://arxiv.org/pdf/2403.19002v1.pdf","comment":"15 pages, 5 figures"}]},"2024-03-28T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2403.19651v1","updated":"2024-03-28T17:59:20Z","published":"2024-03-28T17:59:20Z","title":"MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions","summary":"  Image retrieval, i.e., finding desired images given a reference image,\ninherently encompasses rich, multi-faceted search intents that are difficult to\ncapture solely using image-based measures. Recent work leverages text\ninstructions to allow users to more freely express their search intents.\nHowever, existing work primarily focuses on image pairs that are visually\nsimilar and/or can be characterized by a small set of pre-defined relations.\nThe core thesis of this paper is that text instructions can enable retrieving\nimages with richer relations beyond visual similarity. To show this, we\nintroduce MagicLens, a series of self-supervised image retrieval models that\nsupport open-ended instructions. MagicLens is built on a key novel insight:\nimage pairs that naturally occur on the same web pages contain a wide range of\nimplicit relations (e.g., inside view of), and we can bring those implicit\nrelations explicit by synthesizing instructions via large multimodal models\n(LMMs) and large language models (LLMs). Trained on 36.7M (query image,\ninstruction, target image) triplets with rich semantic relations mined from the\nweb, MagicLens achieves comparable or better results on eight benchmarks of\nvarious image retrieval tasks than prior state-of-the-art (SOTA) methods.\nRemarkably, it outperforms previous SOTA but with a 50X smaller model size on\nmultiple benchmarks. Additional human analyses on a 1.4M-image unseen corpus\nfurther demonstrate the diversity of search intents supported by MagicLens.\n","authors":["Kai Zhang","Yi Luan","Hexiang Hu","Kenton Lee","Siyuan Qiao","Wenhu Chen","Yu Su","Ming-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2403.19651v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2403.19647v1","updated":"2024-03-28T17:56:07Z","published":"2024-03-28T17:56:07Z","title":"Sparse Feature Circuits: Discovering and Editing Interpretable Causal\n  Graphs in Language Models","summary":"  We introduce methods for discovering and applying sparse feature circuits.\nThese are causally implicated subnetworks of human-interpretable features for\nexplaining language model behaviors. Circuits identified in prior work consist\nof polysemantic and difficult-to-interpret units like attention heads or\nneurons, rendering them unsuitable for many downstream applications. In\ncontrast, sparse feature circuits enable detailed understanding of\nunanticipated mechanisms. Because they are based on fine-grained units, sparse\nfeature circuits are useful for downstream tasks: We introduce SHIFT, where we\nimprove the generalization of a classifier by ablating features that a human\njudges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised\nand scalable interpretability pipeline by discovering thousands of sparse\nfeature circuits for automatically discovered model behaviors.\n","authors":["Samuel Marks","Can Rager","Eric J. Michaud","Yonatan Belinkov","David Bau","Aaron Mueller"],"pdf_url":"https://arxiv.org/pdf/2403.19647v1.pdf","comment":"Code and data at https://github.com/saprmarks/feature-circuits.\n  Demonstration at https://feature-circuits.xyz"},{"id":"http://arxiv.org/abs/2403.19634v1","updated":"2024-03-28T17:49:31Z","published":"2024-03-28T17:49:31Z","title":"Asymmetric and trial-dependent modeling: the contribution of LIA to SdSV\n  Challenge Task 2","summary":"  The SdSv challenge Task 2 provided an opportunity to assess efficiency and\nrobustness of modern text-independent speaker verification systems. But it also\nmade it possible to test new approaches, capable of taking into account the\nmain issues of this challenge (duration, language, ...). This paper describes\nthe contributions of our laboratory to the speaker recognition field. These\ncontributions highlight two other challenges in addition to short-duration and\nlanguage: the mismatch between enrollment and test data and the one between\nsubsets of the evaluation trial dataset. The proposed approaches experimentally\nshow their relevance and efficiency on the SdSv evaluation, and could be of\ninterest in many real-life applications.\n","authors":["Pierre-Michel Bousquet","Mickael Rouvier"],"pdf_url":"https://arxiv.org/pdf/2403.19634v1.pdf","comment":"LIA system description for the Short Duration Speaker Verification\n  (SdSv) challenge 2020 Task 2"},{"id":"http://arxiv.org/abs/2403.19631v1","updated":"2024-03-28T17:47:19Z","published":"2024-03-28T17:47:19Z","title":"Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in\n  Language Models","summary":"  Large Language Models (LLMs) have shown proficiency in question-answering\ntasks but often struggle to integrate real-time knowledge updates, leading to\npotentially outdated or inaccurate responses. This problem becomes even more\nchallenging when dealing with multi-hop questions since they require LLMs to\nupdate and integrate multiple knowledge pieces relevant to the questions. To\ntackle the problem, we propose the Retrieval-Augmented model Editing (RAE)\nframework tailored for multi-hop question answering. RAE first retrieves edited\nfacts and then refines the language model through in-context learning.\nSpecifically, our retrieval approach, based on mutual information maximization,\nleverages the reasoning abilities of LLMs to identify chain facts that na\\\"ive\nsimilarity-based searches might miss. Additionally, our framework incorporates\na pruning strategy to eliminate redundant information from the retrieved facts,\nwhich enhances the editing accuracy and mitigates the hallucination problem.\nOur framework is supported by theoretical justification for its fact retrieval\nefficacy. Finally, comprehensive evaluation across various LLMs validates RAE's\nability in providing accurate answers with updated knowledge.\n","authors":["Yucheng Shi","Qiaoyu Tan","Xuansheng Wu","Shaochen Zhong","Kaixiong Zhou","Ninghao Liu"],"pdf_url":"https://arxiv.org/pdf/2403.19631v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2403.19603v1","updated":"2024-03-28T17:27:44Z","published":"2024-03-28T17:27:44Z","title":"Semantic Map-based Generation of Navigation Instructions","summary":"  We are interested in the generation of navigation instructions, either in\ntheir own right or as training material for robotic navigation task. In this\npaper, we propose a new approach to navigation instruction generation by\nframing the problem as an image captioning task using semantic maps as visual\ninput. Conventional approaches employ a sequence of panorama images to generate\nnavigation instructions. Semantic maps abstract away from visual details and\nfuse the information in multiple panorama images into a single top-down\nrepresentation, thereby reducing computational complexity to process the input.\nWe present a benchmark dataset for instruction generation using semantic maps,\npropose an initial model and ask human subjects to manually assess the quality\nof generated instructions. Our initial investigations show promise in using\nsemantic maps for instruction generation instead of a sequence of panorama\nimages, but there is vast scope for improvement. We release the code for data\npreparation and model training at https://github.com/chengzu-li/VLGen.\n","authors":["Chengzu Li","Chao Zhang","Simone Teufel","Rama Sanand Doddipatla","Svetlana Stoyanchev"],"pdf_url":"https://arxiv.org/pdf/2403.19603v1.pdf","comment":"5 pages, 2 figures, 3 tables (13 pages, 3 figures, 5 tables including\n  references and appendices), accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2401.06877v2","updated":"2024-03-28T17:17:17Z","published":"2024-01-12T20:08:39Z","title":"Promptly Predicting Structures: The Return of Inference","summary":"  Prompt-based methods have been used extensively across NLP to build zero- and\nfew-shot label predictors. Many NLP tasks are naturally structured: that is,\ntheir outputs consist of multiple labels which constrain each other. Annotating\ndata for such tasks can be cumbersome. Can the promise of the prompt-based\nparadigm be extended to such structured outputs? In this paper, we present a\nframework for constructing zero- and few-shot linguistic structure predictors.\nOur key insight is that we can use structural constraints -- and combinatorial\ninference derived from them -- to filter out inconsistent structures predicted\nby large language models. We instantiated this framework on two structured\nprediction tasks, and five datasets. Across all cases, our results show that\nenforcing consistency not only constructs structurally valid outputs, but also\nimproves performance over the unconstrained variants.\n","authors":["Maitrey Mehta","Valentina Pyatkin","Vivek Srikumar"],"pdf_url":"https://arxiv.org/pdf/2401.06877v2.pdf","comment":"18 pages, 13 figures Accepted to NAACL'2024 (Main)"},{"id":"http://arxiv.org/abs/2403.18346v2","updated":"2024-03-28T17:09:36Z","published":"2024-03-27T08:38:49Z","title":"Quantifying and Mitigating Unimodal Biases in Multimodal Large Language\n  Models: A Causal Perspective","summary":"  Recent advancements in Large Language Models (LLMs) have facilitated the\ndevelopment of Multimodal LLMs (MLLMs). Despite their impressive capabilities,\nMLLMs often suffer from an over-reliance on unimodal biases (e.g., language\nbias and vision bias), leading to incorrect answers in complex multimodal\ntasks. To investigate this issue, we propose a causal framework to interpret\nthe biases in Visual Question Answering (VQA) problems. Within our framework,\nwe devise a causal graph to elucidate the predictions of MLLMs on VQA problems,\nand assess the causal effect of biases through an in-depth causal analysis.\nMotivated by the causal graph, we introduce a novel MORE dataset, consisting of\n12,000 VQA instances. This dataset is designed to challenge MLLMs' abilities,\nnecessitating multi-hop reasoning and the surmounting of unimodal biases.\nFurthermore, we propose two strategies to mitigate unimodal biases and enhance\nMLLMs' reasoning capabilities, including a Decompose-Verify-Answer (DeVA)\nframework for limited-access MLLMs and the refinement of open-source MLLMs\nthrough fine-tuning. Extensive quantitative and qualitative experiments offer\nvaluable insights for future research. Our project page is at\nhttps://opencausalab.github.io/MORE.\n","authors":["Meiqi Chen","Yixin Cao","Yan Zhang","Chaochao Lu"],"pdf_url":"https://arxiv.org/pdf/2403.18346v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16385v2","updated":"2024-03-28T16:45:44Z","published":"2024-03-25T03:02:27Z","title":"Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators\n  for Reasoning-Based Chart VQA","summary":"  Understanding data visualizations like charts and plots requires reasoning\nabout both visual elements and numerics. Although strong in extractive\nquestions, current chart visual question answering (chart VQA) models suffer on\ncomplex reasoning questions. In this work, we address the lack of reasoning\nability by data augmentation. We leverage Large Language Models (LLMs), which\nhave shown to have strong reasoning ability, as an automatic data annotator\nthat generates question-answer annotations for chart images. The key innovation\nin our method lies in the Synthesize Step-by-Step strategy: our LLM-based data\ngenerator learns to decompose the complex question into step-by-step\nsub-questions (rationales), which are then used to derive the final answer\nusing external tools, i.e. Python. This step-wise generation procedure is\ntrained on synthetic data generated using a template-based QA generation\npipeline. Experimental results highlight the significance of the proposed\nstep-by-step generation. By training with the LLM-augmented data (LAMENDA), we\nsignificantly enhance the chart VQA models, achieving the state-of-the-art\naccuracy on the ChartQA and PlotQA datasets. In particular, our approach\nimproves the accuracy of the previous state-of-the-art approach from 38% to 54%\non the human-written questions in the ChartQA dataset, which needs strong\nreasoning. We hope our work underscores the potential of synthetic data and\nencourages further exploration of data augmentation using LLMs for\nreasoning-heavy tasks.\n","authors":["Zhuowan Li","Bhavan Jasani","Peng Tang","Shabnam Ghadar"],"pdf_url":"https://arxiv.org/pdf/2403.16385v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.19559v1","updated":"2024-03-28T16:44:14Z","published":"2024-03-28T16:44:14Z","title":"Improving Adversarial Data Collection by Supporting Annotators: Lessons\n  from GAHD, a German Hate Speech Dataset","summary":"  Hate speech detection models are only as good as the data they are trained\non. Datasets sourced from social media suffer from systematic gaps and biases,\nleading to unreliable models with simplistic decision boundaries. Adversarial\ndatasets, collected by exploiting model weaknesses, promise to fix this\nproblem. However, adversarial data collection can be slow and costly, and\nindividual annotators have limited creativity. In this paper, we introduce\nGAHD, a new German Adversarial Hate speech Dataset comprising ca.\\ 11k\nexamples. During data collection, we explore new strategies for supporting\nannotators, to create more diverse adversarial examples more efficiently and\nprovide a manual analysis of annotator disagreements for each strategy. Our\nexperiments show that the resulting dataset is challenging even for\nstate-of-the-art hate speech detection models, and that training on GAHD\nclearly improves model robustness. Further, we find that mixing multiple\nsupport strategies is most advantageous. We make GAHD publicly available at\nhttps://github.com/jagol/gahd.\n","authors":["Janis Goldzycher","Paul Röttger","Gerold Schneider"],"pdf_url":"https://arxiv.org/pdf/2403.19559v1.pdf","comment":"Accepted at NAACL 2024 (main conference)"},{"id":"http://arxiv.org/abs/2403.15456v2","updated":"2024-03-28T16:40:05Z","published":"2024-03-19T06:39:23Z","title":"WoLF: Large Language Model Framework for CXR Understanding","summary":"  Significant methodological strides have been made toward Chest X-ray (CXR)\nunderstanding via modern vision-language models (VLMs), demonstrating\nimpressive Visual Question Answering (VQA) and CXR report generation abilities.\nHowever, existing CXR understanding frameworks still possess several procedural\ncaveats. (1) Previous methods solely use CXR reports, which are insufficient\nfor comprehensive Visual Question Answering (VQA), especially when additional\nhealth-related data like medication history and prior diagnoses are needed. (2)\nPrevious methods use raw CXR reports, which are often arbitrarily structured.\nWhile modern language models can understand various text formats, restructuring\nreports for clearer, organized anatomy-based information could enhance their\nusefulness. (3) Current evaluation methods for CXR-VQA primarily emphasize\nlinguistic correctness, lacking the capability to offer nuanced assessments of\nthe generated answers. In this work, to address the aforementioned caveats, we\nintroduce WoLF, a Wide-scope Large Language Model Framework for CXR\nunderstanding. To resolve (1), we capture multi-faceted records of patients,\nwhich are utilized for accurate diagnoses in real-world clinical scenarios.\nSpecifically, we adopt the Electronic Health Records (EHR) to generate\ninstruction-following data suited for CXR understanding. Regarding (2), we\nenhance report generation performance by decoupling knowledge in CXR reports\nbased on anatomical structure even within the attention step via masked\nattention. To address (3), we introduce an AI-evaluation protocol optimized for\nassessing the capabilities of LLM. Through extensive experimental validations,\nWoLF demonstrates superior performance over other models on MIMIC-CXR in the\nAI-evaluation arena about VQA (up to +9.47%p mean score) and by metrics about\nreport generation (+7.3%p BLEU-1).\n","authors":["Seil Kang","Donghyun Kim","Junhyeok Kim","Hyo Kyung Lee","Seong Jae Hwang"],"pdf_url":"https://arxiv.org/pdf/2403.15456v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09363v2","updated":"2024-03-28T16:31:26Z","published":"2023-11-15T20:52:56Z","title":"Investigating the Emergent Audio Classification Ability of ASR\n  Foundation Models","summary":"  Text and vision foundation models can perform many tasks in a zero-shot\nsetting, a desirable property that enables these systems to be applied in\ngeneral and low-resource settings. There has been far less work, however, on\nthe zero-shot abilities of ASR foundation models, with these systems typically\nfine-tuned to specific tasks or constrained to applications that match their\ntraining criterion and data annotation. In this work we investigate the ability\nof Whisper and MMS, ASR foundation models trained primarily for speech\nrecognition, to perform zero-shot audio classification. We use simple\ntemplate-based text prompts at the decoder and use the resulting decoding\nprobabilities to generate zero-shot predictions. Without training the model on\nextra data or adding any new parameters, we demonstrate that Whisper shows\npromising zero-shot classification performance on a range of 8\naudio-classification datasets, outperforming the accuracy of existing\nstate-of-the-art zero-shot baselines by an average of 9%. One important step to\nunlock the emergent ability is debiasing, where a simple unsupervised\nreweighting method of the class probabilities yields consistent significant\nperformance gains. We further show that performance increases with model size,\nimplying that as ASR foundation models scale up, they may exhibit improved\nzero-shot performance.\n","authors":["Rao Ma","Adian Liusie","Mark J. F. Gales","Kate M. Knill"],"pdf_url":"https://arxiv.org/pdf/2311.09363v2.pdf","comment":"NAACL 2024 (main conference)"},{"id":"http://arxiv.org/abs/2403.19548v1","updated":"2024-03-28T16:28:38Z","published":"2024-03-28T16:28:38Z","title":"WaterJudge: Quality-Detection Trade-off when Watermarking Large Language\n  Models","summary":"  Watermarking generative-AI systems, such as LLMs, has gained considerable\ninterest, driven by their enhanced capabilities across a wide range of tasks.\nAlthough current approaches have demonstrated that small, context-dependent\nshifts in the word distributions can be used to apply and detect watermarks,\nthere has been little work in analyzing the impact that these perturbations\nhave on the quality of generated texts. Balancing high detectability with\nminimal performance degradation is crucial in terms of selecting the\nappropriate watermarking setting; therefore this paper proposes a simple\nanalysis framework where comparative assessment, a flexible NLG evaluation\nframework, is used to assess the quality degradation caused by a particular\nwatermark setting. We demonstrate that our framework provides easy\nvisualization of the quality-detection trade-off of watermark settings,\nenabling a simple solution to find an LLM watermark operating point that\nprovides a well-balanced performance. This approach is applied to two different\nsummarization systems and a translation system, enabling cross-model analysis\nfor a task, and cross-task analysis.\n","authors":["Piotr Molenda","Adian Liusie","Mark J. F. Gales"],"pdf_url":"https://arxiv.org/pdf/2403.19548v1.pdf","comment":"NAACL 2024 (Findings)"},{"id":"http://arxiv.org/abs/2403.15268v2","updated":"2024-03-28T16:28:24Z","published":"2024-03-22T15:06:45Z","title":"Imagination Augmented Generation: Learning to Imagine Richer Context for\n  Question Answering over Large Language Models","summary":"  Retrieval-Augmented-Generation and Gener-ation-Augmented-Generation have been\nproposed to enhance the knowledge required for question answering over Large\nLanguage Models (LLMs). However, the former depends on external resources, and\nboth require incorporating the explicit documents into the context, which\nresults in longer contexts that lead to more resource consumption. Recent works\nindicate that LLMs have modeled rich knowledge, albeit not effectively\ntriggered or activated. Inspired by this, we propose a novel\nknowledge-augmented framework, Imagination-Augmented-Generation (IAG), which\nsimulates the human capacity to compensate for knowledge deficits while\nanswering questions solely through imagination, without relying on external\nresources. Guided by IAG, we propose an imagine richer context method for\nquestion answering (IMcQA), which obtains richer context through the following\ntwo modules: explicit imagination by generating a short dummy document with\nlong context compress and implicit imagination with HyperNetwork for generating\nadapter weights. Experimental results on three datasets demonstrate that IMcQA\nexhibits significant advantages in both open-domain and closed-book settings,\nas well as in both in-distribution performance and out-of-distribution\ngeneralizations. Our code will be available at\nhttps://github.com/Xnhyacinth/IAG.\n","authors":["Huanxuan Liao","Shizhu He","Yao Xu","Yuanzhe Zhang","Kang Liu","Shengping Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.15268v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18034v2","updated":"2024-03-28T16:27:01Z","published":"2023-05-29T11:54:50Z","title":"A Corpus for Sentence-level Subjectivity Detection on English News\n  Articles","summary":"  We develop novel annotation guidelines for sentence-level subjectivity\ndetection, which are not limited to language-specific cues. We use our\nguidelines to collect NewsSD-ENG, a corpus of 638 objective and 411 subjective\nsentences extracted from English news articles on controversial topics. Our\ncorpus paves the way for subjectivity detection in English and across other\nlanguages without relying on language-specific tools, such as lexicons or\nmachine translation. We evaluate state-of-the-art multilingual\ntransformer-based models on the task in mono-, multi-, and cross-language\nsettings. For this purpose, we re-annotate an existing Italian corpus. We\nobserve that models trained in the multilingual setting achieve the best\nperformance on the task.\n","authors":["Francesco Antici","Andrea Galassi","Federico Ruggeri","Katerina Korre","Arianna Muti","Alessandra Bardi","Alice Fedotova","Alberto Barrón-Cedeño"],"pdf_url":"https://arxiv.org/pdf/2305.18034v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17645v2","updated":"2024-03-28T15:59:09Z","published":"2024-03-26T12:27:32Z","title":"DANCER: Entity Description Augmented Named Entity Corrector for\n  Automatic Speech Recognition","summary":"  End-to-end automatic speech recognition (E2E ASR) systems often suffer from\nmistranscription of domain-specific phrases, such as named entities, sometimes\nleading to catastrophic failures in downstream tasks. A family of fast and\nlightweight named entity correction (NEC) models for ASR have recently been\nproposed, which normally build on phonetic-level edit distance algorithms and\nhave shown impressive NEC performance. However, as the named entity (NE) list\ngrows, the problems of phonetic confusion in the NE list are exacerbated; for\nexample, homophone ambiguities increase substantially. In view of this, we\nproposed a novel Description Augmented Named entity CorrEctoR (dubbed DANCER),\nwhich leverages entity descriptions to provide additional information to\nfacilitate mitigation of phonetic confusion for NEC on ASR transcription. To\nthis end, an efficient entity description augmented masked language model\n(EDA-MLM) comprised of a dense retrieval model is introduced, enabling MLM to\nadapt swiftly to domain-specific entities for the NEC task. A series of\nexperiments conducted on the AISHELL-1 and Homophone datasets confirm the\neffectiveness of our modeling approach. DANCER outperforms a strong baseline,\nthe phonetic edit-distance-based NEC model (PED-NEC), by a character error rate\n(CER) reduction of about 7% relatively on AISHELL-1 for named entities. More\nnotably, when tested on Homophone that contain named entities of high phonetic\nconfusion, DANCER offers a more pronounced CER reduction of 46% relatively over\nPED-NEC for named entities.\n","authors":["Yi-Cheng Wang","Hsin-Wei Wang","Bi-Cheng Yan","Chi-Han Lin","Berlin Chen"],"pdf_url":"https://arxiv.org/pdf/2403.17645v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01286v4","updated":"2024-03-28T15:56:55Z","published":"2024-01-02T16:54:58Z","title":"A Comprehensive Study of Knowledge Editing for Large Language Models","summary":"  Large Language Models (LLMs) have shown extraordinary capabilities in\nunderstanding and generating text that closely mirrors human communication.\nHowever, a primary limitation lies in the significant computational demands\nduring training, arising from their extensive parameterization. This challenge\nis further intensified by the dynamic nature of the world, necessitating\nfrequent updates to LLMs to correct outdated information or integrate new\nknowledge, thereby ensuring their continued relevance. Note that many\napplications demand continual model adjustments post-training to address\ndeficiencies or undesirable behaviors. There is an increasing interest in\nefficient, lightweight methods for on-the-fly model modifications. To this end,\nrecent years have seen a burgeoning in the techniques of knowledge editing for\nLLMs, which aim to efficiently modify LLMs' behaviors within specific domains\nwhile preserving overall performance across various inputs. In this paper, we\nfirst define the knowledge editing problem and then provide a comprehensive\nreview of cutting-edge approaches. Drawing inspiration from educational and\ncognitive research theories, we propose a unified categorization criterion that\nclassifies knowledge editing methods into three groups: resorting to external\nknowledge, merging knowledge into the model, and editing intrinsic knowledge.\nFurthermore, we introduce a new benchmark, KnowEdit, for a comprehensive\nempirical evaluation of representative knowledge editing approaches.\nAdditionally, we provide an in-depth analysis of knowledge location, which can\ngive a deeper understanding of the knowledge structures inherent within LLMs.\nFinally, we discuss several potential applications of knowledge editing,\noutlining its broad and impactful implications.\n","authors":["Ningyu Zhang","Yunzhi Yao","Bozhong Tian","Peng Wang","Shumin Deng","Mengru Wang","Zekun Xi","Shengyu Mao","Jintian Zhang","Yuansheng Ni","Siyuan Cheng","Ziwen Xu","Xin Xu","Jia-Chen Gu","Yong Jiang","Pengjun Xie","Fei Huang","Lei Liang","Zhiqiang Zhang","Xiaowei Zhu","Jun Zhou","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2401.01286v4.pdf","comment":"Ongoing work; 52 pages, 282 citations; benchmark is available at\n  https://huggingface.co/datasets/zjunlp/KnowEdit code is available at\n  https://github.com/zjunlp/EasyEdit paper list is available at\n  https://github.com/zjunlp/KnowledgeEditingPapers"},{"id":"http://arxiv.org/abs/2403.19521v1","updated":"2024-03-28T15:54:59Z","published":"2024-03-28T15:54:59Z","title":"Interpreting Key Mechanisms of Factual Recall in Transformer-Based\n  Language Models","summary":"  In this paper, we deeply explore the mechanisms employed by Transformer-based\nlanguage models in factual recall tasks. In zero-shot scenarios, given a prompt\nlike \"The capital of France is,\" task-specific attention heads extract the\ntopic entity, such as \"France,\" from the context and pass it to subsequent MLPs\nto recall the required answer such as \"Paris.\" We introduce a novel analysis\nmethod aimed at decomposing the outputs of the MLP into components\nunderstandable by humans. Through this method, we quantify the function of the\nMLP layer following these task-specific heads. In the residual stream, it\neither erases or amplifies the information originating from individual heads.\nMoreover, it generates a component that redirects the residual stream towards\nthe direction of its expected answer. These zero-shot mechanisms are also\nemployed in few-shot scenarios. Additionally, we observed a widely existent\nanti-overconfidence mechanism in the final layer of models, which suppresses\ncorrect predictions. We mitigate this suppression by leveraging our\ninterpretation to improve factual recall performance. Our interpretations have\nbeen evaluated across various language models, from the GPT-2 families to 1.3B\nOPT, and across tasks covering different domains of factual knowledge.\n","authors":["Ang Lv","Kaiyi Zhang","Yuhan Chen","Yulong Wang","Lifeng Liu","Ji-Rong Wen","Jian Xie","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2403.19521v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17919v2","updated":"2024-03-28T15:44:39Z","published":"2024-03-26T17:55:02Z","title":"LISA: Layerwise Importance Sampling for Memory-Efficient Large Language\n  Model Fine-Tuning","summary":"  The machine learning community has witnessed impressive advancements since\nthe first appearance of large language models (LLMs), yet their huge memory\nconsumption has become a major roadblock to large-scale training. Parameter\nEfficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been\nproposed to alleviate this problem, but their performance still fails to match\nfull parameter training in most large-scale fine-tuning settings. Attempting to\ncomplement this deficiency, we investigate layerwise properties of LoRA on\nfine-tuning tasks and observe an uncommon skewness of weight norms across\ndifferent layers. Utilizing this key observation, a surprisingly simple\ntraining strategy is discovered, which outperforms both LoRA and full parameter\ntraining in a wide range of settings with memory costs as low as LoRA. We name\nit Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA,\nwhich applies the idea of importance sampling to different layers in LLMs and\nrandomly freeze most middle layers during optimization. Experimental results\nshow that with similar or less GPU memory consumption, LISA surpasses LoRA or\neven full parameter tuning in downstream fine-tuning tasks, where LISA\nconsistently outperforms LoRA by over $11\\%$-$37\\%$ in terms of MT-Bench\nscores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or\nbetter performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating\nits effectiveness across different domains.\n","authors":["Rui Pan","Xiang Liu","Shizhe Diao","Renjie Pi","Jipeng Zhang","Chi Han","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.17919v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19511v1","updated":"2024-03-28T15:44:18Z","published":"2024-03-28T15:44:18Z","title":"Improving Clinical NLP Performance through Language Model-Generated\n  Synthetic Clinical Data","summary":"  Generative models have been showing potential for producing data in mass.\nThis study explores the enhancement of clinical natural language processing\nperformance by utilizing synthetic data generated from advanced language\nmodels. Promising results show feasible applications in such a high-stakes\ndomain.\n","authors":["Shan Chen","Jack Gallifant","Marco Guevara","Yanjun Gao","Majid Afshar","Timothy Miller","Dmitriy Dligach","Danielle S. Bitterman"],"pdf_url":"https://arxiv.org/pdf/2403.19511v1.pdf","comment":"submitted to review"},{"id":"http://arxiv.org/abs/2403.19509v1","updated":"2024-03-28T15:42:07Z","published":"2024-03-28T15:42:07Z","title":"Phonetic Segmentation of the UCLA Phonetics Lab Archive","summary":"  Research in speech technologies and comparative linguistics depends on access\nto diverse and accessible speech data. The UCLA Phonetics Lab Archive is one of\nthe earliest multilingual speech corpora, with long-form audio recordings and\nphonetic transcriptions for 314 languages (Ladefoged et al., 2009). Recently,\n95 of these languages were time-aligned with word-level phonetic transcriptions\n(Li et al., 2021). Here we present VoxAngeles, a corpus of audited phonetic\ntranscriptions and phone-level alignments of the UCLA Phonetics Lab Archive,\nwhich uses the 95-language CMU re-release as our starting point. VoxAngeles\nalso includes word- and phone-level segmentations from the original UCLA\ncorpus, as well as phonetic measurements of word and phone durations, vowel\nformants, and vowel f0. This corpus enhances the usability of the original\ndata, particularly for quantitative phonetic typology, as demonstrated through\na case study of vowel intrinsic f0. We also discuss the utility of the\nVoxAngeles corpus for general research and pedagogy in crosslinguistic\nphonetics, as well as for low-resource and multilingual speech technologies.\nVoxAngeles is free to download and use under a CC-BY-NC 4.0 license.\n","authors":["Eleanor Chodroff","Blaž Pažon","Annie Baker","Steven Moran"],"pdf_url":"https://arxiv.org/pdf/2403.19509v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.14472v2","updated":"2024-03-28T15:24:17Z","published":"2024-03-21T15:18:30Z","title":"Detoxifying Large Language Models via Knowledge Editing","summary":"  This paper investigates using knowledge editing techniques to detoxify Large\nLanguage Models (LLMs). We construct a benchmark, SafeEdit, which covers nine\nunsafe categories with various powerful attack prompts and equips comprehensive\nmetrics for systematic evaluation. We conduct experiments with several\nknowledge editing approaches, indicating that knowledge editing has the\npotential to efficiently detoxify LLMs with limited impact on general\nperformance. Then, we propose a simple yet effective baseline, dubbed\nDetoxifying with Intraoperative Neural Monitoring (DINM), to diminish the\ntoxicity of LLMs within a few tuning steps via only one instance. We further\nprovide an in-depth analysis of the internal mechanism for various detoxify\napproaches, demonstrating that previous methods like SFT and DPO may merely\nsuppress the activations of toxic parameters, while DINM mitigates the toxicity\nof the toxic parameters to a certain extent, making permanent adjustments. We\nhope that these insights could shed light on future work of developing\ndetoxifying approaches and the underlying knowledge mechanisms of LLMs. Code\nand benchmark are available at https://github.com/zjunlp/EasyEdit.\n","authors":["Mengru Wang","Ningyu Zhang","Ziwen Xu","Zekun Xi","Shumin Deng","Yunzhi Yao","Qishen Zhang","Linyi Yang","Jindong Wang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14472v2.pdf","comment":"Ongoing work. Project website:\n  https://zjunlp.github.io/project/SafeEdit Due to the specificity of the\n  knowledge editing setting, we revise Tables 1 and 3 to present a fair\n  comparison of experimental results. More experimental results will be updated\n  soon"},{"id":"http://arxiv.org/abs/2402.01786v2","updated":"2024-03-28T15:22:42Z","published":"2024-02-01T21:51:09Z","title":"COA-GPT: Generative Pre-trained Transformers for Accelerated Course of\n  Action Development in Military Operations","summary":"  The development of Courses of Action (COAs) in military operations is\ntraditionally a time-consuming and intricate process. Addressing this\nchallenge, this study introduces COA-GPT, a novel algorithm employing Large\nLanguage Models (LLMs) for rapid and efficient generation of valid COAs.\nCOA-GPT incorporates military doctrine and domain expertise to LLMs through\nin-context learning, allowing commanders to input mission information - in both\ntext and image formats - and receive strategically aligned COAs for review and\napproval. Uniquely, COA-GPT not only accelerates COA development, producing\ninitial COAs within seconds, but also facilitates real-time refinement based on\ncommander feedback. This work evaluates COA-GPT in a military-relevant scenario\nwithin a militarized version of the StarCraft II game, comparing its\nperformance against state-of-the-art reinforcement learning algorithms. Our\nresults demonstrate COA-GPT's superiority in generating strategically sound\nCOAs more swiftly, with added benefits of enhanced adaptability and alignment\nwith commander intentions. COA-GPT's capability to rapidly adapt and update\nCOAs during missions presents a transformative potential for military planning,\nparticularly in addressing planning discrepancies and capitalizing on emergent\nwindows of opportunities.\n","authors":["Vinicius G. Goecks","Nicholas Waytowich"],"pdf_url":"https://arxiv.org/pdf/2402.01786v2.pdf","comment":"Accepted at the NATO Science and Technology Organization Symposium\n  (ICMCIS) organized by the Information Systems Technology (IST) Panel,\n  IST-205-RSY - the ICMCIS, held in Koblenz, Germany, 23-24 April 2024"},{"id":"http://arxiv.org/abs/2402.07946v2","updated":"2024-03-28T15:17:30Z","published":"2024-02-09T16:10:29Z","title":"Re-Envisioning Command and Control","summary":"  Future warfare will require Command and Control (C2) decision-making to occur\nin more complex, fast-paced, ill-structured, and demanding conditions. C2 will\nbe further complicated by operational challenges such as Denied, Degraded,\nIntermittent, and Limited (DDIL) communications and the need to account for\nmany data streams, potentially across multiple domains of operation. Yet,\ncurrent C2 practices -- which stem from the industrial era rather than the\nemerging intelligence era -- are linear and time-consuming. Critically, these\napproaches may fail to maintain overmatch against adversaries on the future\nbattlefield. To address these challenges, we propose a vision for future C2\nbased on robust partnerships between humans and artificial intelligence (AI)\nsystems. This future vision is encapsulated in three operational impacts:\nstreamlining the C2 operations process, maintaining unity of effort, and\ndeveloping adaptive collective knowledge systems. This paper illustrates the\nenvisaged future C2 capabilities, discusses the assumptions that shaped them,\nand describes how the proposed developments could transform C2 in future\nwarfare.\n","authors":["Kaleb McDowell","Ellen Novoseller","Anna Madison","Vinicius G. Goecks","Christopher Kelshaw"],"pdf_url":"https://arxiv.org/pdf/2402.07946v2.pdf","comment":"Accepted at the NATO Science and Technology Organization Symposium\n  (ICMCIS) organized by the Information Systems Technology (IST) Panel,\n  IST-205-RSY - the ICMCIS, held in Koblenz, Germany, 23-24 April 2024"},{"id":"http://arxiv.org/abs/2402.06501v2","updated":"2024-03-28T15:17:01Z","published":"2024-02-09T16:11:04Z","title":"Scalable Interactive Machine Learning for Future Command and Control","summary":"  Future warfare will require Command and Control (C2) personnel to make\ndecisions at shrinking timescales in complex and potentially ill-defined\nsituations. Given the need for robust decision-making processes and\ndecision-support tools, integration of artificial and human intelligence holds\nthe potential to revolutionize the C2 operations process to ensure adaptability\nand efficiency in rapidly changing operational environments. We propose to\nleverage recent promising breakthroughs in interactive machine learning, in\nwhich humans can cooperate with machine learning algorithms to guide machine\nlearning algorithm behavior. This paper identifies several gaps in\nstate-of-the-art science and technology that future work should address to\nextend these approaches to function in complex C2 contexts. In particular, we\ndescribe three research focus areas that together, aim to enable scalable\ninteractive machine learning (SIML): 1) developing human-AI interaction\nalgorithms to enable planning in complex, dynamic situations; 2) fostering\nresilient human-AI teams through optimizing roles, configurations, and trust;\nand 3) scaling algorithms and human-AI teams for flexibility across a range of\npotential contexts and situations.\n","authors":["Anna Madison","Ellen Novoseller","Vinicius G. Goecks","Benjamin T. Files","Nicholas Waytowich","Alfred Yu","Vernon J. Lawhern","Steven Thurman","Christopher Kelshaw","Kaleb McDowell"],"pdf_url":"https://arxiv.org/pdf/2402.06501v2.pdf","comment":"Accepted at the NATO Science and Technology Organization Symposium\n  (ICMCIS) organized by the Information Systems Technology (IST) Panel,\n  IST-205-RSY - the ICMCIS, held in Koblenz, Germany, 23-24 April 2024"},{"id":"http://arxiv.org/abs/2403.02472v5","updated":"2024-03-28T14:44:48Z","published":"2024-03-04T20:34:58Z","title":"OffLanDat: A Community Based Implicit Offensive Language Dataset\n  Generated by Large Language Model Through Prompt Engineering","summary":"  The widespread presence of hateful languages on social media has resulted in\nadverse effects on societal well-being. As a result, it has become very\nimportant to address this issue with high priority. Hate speech or offensive\nlanguages exist in both explicit and implicit forms, with the latter being more\nchallenging to detect. Current research in this domain encounters several\nchallenges. Firstly, the existing datasets primarily rely on the collection of\ntexts containing explicit offensive keywords, making it challenging to capture\nimplicitly offensive contents that are devoid of these keywords. Secondly,\nusual methodologies tend to focus solely on textual analysis, neglecting the\nvaluable insights that community information can provide. In this research\npaper, we introduce a novel dataset OffLanDat, a community based implicit\noffensive language dataset generated by ChatGPT containing data for 38\ndifferent target groups. Despite limitations in generating offensive texts\nusing ChatGPT due to ethical constraints, we present a prompt-based approach\nthat effectively generates implicit offensive languages. To ensure data\nquality, we evaluate our data with human. Additionally, we employ a\nprompt-based Zero-Shot method with ChatGPT and compare the detection results\nbetween human annotation and ChatGPT annotation. We utilize existing\nstate-of-the-art models to see how effective they are in detecting such\nlanguages. We will make our code and dataset public for other researchers.\n","authors":["Amit Das","Mostafa Rahgouy","Dongji Feng","Zheng Zhang","Tathagata Bhattacharya","Nilanjana Raychawdhary","Mary Sandage","Lauramarie Pope","Gerry Dozier","Cheryl Seals"],"pdf_url":"https://arxiv.org/pdf/2403.02472v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.12612v2","updated":"2024-03-28T14:23:08Z","published":"2023-05-22T00:33:52Z","title":"PrOnto: Language Model Evaluations for 859 Languages","summary":"  Evaluation datasets are critical resources for measuring the quality of\npretrained language models. However, due to the high cost of dataset\nannotation, these resources are scarce for most languages other than English,\nmaking it difficult to assess the quality of language models. In this work, we\npresent a new method for evaluation dataset construction which enables any\nlanguage with a New Testament translation to receive a suite of evaluation\ndatasets suitable for pretrained language model evaluation. The method\ncritically involves aligning verses with those in the New Testament portion of\nEnglish OntoNotes, and then projecting annotations from English to the target\nlanguage, with no manual annotation required. We apply this method to 1051 New\nTestament translations in 859 and make them publicly available. Additionally,\nwe conduct experiments which demonstrate the efficacy of our method for\ncreating evaluation tasks which can assess language model quality.\n","authors":["Luke Gessler"],"pdf_url":"https://arxiv.org/pdf/2305.12612v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.19454v1","updated":"2024-03-28T14:22:54Z","published":"2024-03-28T14:22:54Z","title":"JDocQA: Japanese Document Question Answering Dataset for Generative\n  Language Models","summary":"  Document question answering is a task of question answering on given\ndocuments such as reports, slides, pamphlets, and websites, and it is a truly\ndemanding task as paper and electronic forms of documents are so common in our\nsociety. This is known as a quite challenging task because it requires not only\ntext understanding but also understanding of figures and tables, and hence\nvisual question answering (VQA) methods are often examined in addition to\ntextual approaches. We introduce Japanese Document Question Answering (JDocQA),\na large-scale document-based QA dataset, essentially requiring both visual and\ntextual information to answer questions, which comprises 5,504 documents in PDF\nformat and annotated 11,600 question-and-answer instances in Japanese. Each QA\ninstance includes references to the document pages and bounding boxes for the\nanswer clues. We incorporate multiple categories of questions and unanswerable\nquestions from the document for realistic question-answering applications. We\nempirically evaluate the effectiveness of our dataset with text-based large\nlanguage models (LLMs) and multimodal models. Incorporating unanswerable\nquestions in finetuning may contribute to harnessing the so-called\nhallucination generation.\n","authors":["Eri Onami","Shuhei Kurita","Taiki Miyanishi","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2403.19454v1.pdf","comment":"LREC-COLING2024"},{"id":"http://arxiv.org/abs/2403.19443v1","updated":"2024-03-28T14:15:10Z","published":"2024-03-28T14:15:10Z","title":"Mixed Preference Optimization: Reinforcement Learning with Data\n  Selection and Better Reference Model","summary":"  Large Language Models (LLMs) have become increasingly popular due to their\nability to process and generate natural language. However, as they are trained\non massive datasets of text, LLMs can inherit harmful biases and produce\noutputs that are not aligned with human values. This paper studies two main\napproaches to LLM alignment: Reinforcement Learning with Human Feedback (RLHF)\nand contrastive learning-based methods like Direct Preference Optimization\n(DPO). By analyzing the stability and robustness of RLHF and DPO, we propose\nMPO (Mixed Preference Optimization), a novel method that mitigates the\nweaknesses of both approaches. Specifically, we propose a two-stage training\nprocedure: first train DPO on an easy dataset, and then perform RLHF on a\ndifficult set with DPO model being the reference model. Here, the easy and\ndifficult sets are constructed by a well-trained reward model that splits\nresponse pairs into those with large gaps of reward (easy), and those with\nsmall gaps (difficult). The first stage allows us to obtain a relatively\noptimal policy (LLM) model quickly, whereas the second stage refines LLM with\nonline RLHF, thus mitigating the distribution shift issue associated with DPO.\nExperiments are conducted on two public alignment datasets, namely HH-RLHF and\nTLDR, demonstrating the effectiveness of MPO, both in terms of GPT4 and human\nevaluation.\n","authors":["Qi Gou","Cam-Tu Nguyen"],"pdf_url":"https://arxiv.org/pdf/2403.19443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19432v1","updated":"2024-03-28T14:03:12Z","published":"2024-03-28T14:03:12Z","title":"Uncovering Misattributed Suicide Causes through Annotation Inconsistency\n  Detection in Death Investigation Notes","summary":"  Data accuracy is essential for scientific research and policy development.\nThe National Violent Death Reporting System (NVDRS) data is widely used for\ndiscovering the patterns and causes of death. Recent studies suggested the\nannotation inconsistencies within the NVDRS and the potential impact on\nerroneous suicide-cause attributions. We present an empirical Natural Language\nProcessing (NLP) approach to detect annotation inconsistencies and adopt a\ncross-validation-like paradigm to identify problematic instances. We analyzed\n267,804 suicide death incidents between 2003 and 2020 from the NVDRS. Our\nresults showed that incorporating the target state's data into training the\nsuicide-crisis classifier brought an increase of 5.4% to the F-1 score on the\ntarget state's test set and a decrease of 1.1% on other states' test set. To\nconclude, we demonstrated the annotation inconsistencies in NVDRS's death\ninvestigation notes, identified problematic instances, evaluated the\neffectiveness of correcting problematic instances, and eventually proposed an\nNLP improvement solution.\n","authors":["Song Wang","Yiliang Zhou","Ziqiang Han","Cui Tao","Yunyu Xiao","Ying Ding","Joydeep Ghosh","Yifan Peng"],"pdf_url":"https://arxiv.org/pdf/2403.19432v1.pdf","comment":"19 pages, 6 figures"},{"id":"http://arxiv.org/abs/2306.17563v2","updated":"2024-03-28T13:59:09Z","published":"2023-06-30T11:32:25Z","title":"Large Language Models are Effective Text Rankers with Pairwise Ranking\n  Prompting","summary":"  Ranking documents using Large Language Models (LLMs) by directly feeding the\nquery and candidate documents into the prompt is an interesting and practical\nproblem. However, researchers have found it difficult to outperform fine-tuned\nbaseline rankers on benchmark datasets. We analyze pointwise and listwise\nranking prompts used by existing methods and argue that off-the-shelf LLMs do\nnot fully understand these challenging ranking formulations. In this paper, we\npropose to significantly reduce the burden on LLMs by using a new technique\ncalled Pairwise Ranking Prompting (PRP). Our results are the first in the\nliterature to achieve state-of-the-art ranking performance on standard\nbenchmarks using moderate-sized open-sourced LLMs. On TREC-DL 2019&2020, PRP\nbased on the Flan-UL2 model with 20B parameters performs favorably with the\nprevious best approach in the literature, which is based on the blackbox\ncommercial GPT-4 that has 50x (estimated) model size, while outperforming other\nLLM-based solutions, such as InstructGPT which has 175B parameters, by over 10%\nfor all ranking metrics. By using the same prompt template on seven BEIR tasks,\nPRP outperforms supervised baselines and outperforms the blackbox commercial\nChatGPT solution by 4.2% and pointwise LLM-based solutions by more than 10% on\naverage NDCG@10. Furthermore, we propose several variants of PRP to improve\nefficiency and show that it is possible to achieve competitive results even\nwith linear complexity.\n","authors":["Zhen Qin","Rolf Jagerman","Kai Hui","Honglei Zhuang","Junru Wu","Le Yan","Jiaming Shen","Tianqi Liu","Jialu Liu","Donald Metzler","Xuanhui Wang","Michael Bendersky"],"pdf_url":"https://arxiv.org/pdf/2306.17563v2.pdf","comment":"Accepted to NAACL 2024. Corrected results of RankT5 on TREC-DL19"},{"id":"http://arxiv.org/abs/2403.19424v1","updated":"2024-03-28T13:56:23Z","published":"2024-03-28T13:56:23Z","title":"The Role of Syntactic Span Preferences in Post-Hoc Explanation\n  Disagreement","summary":"  Post-hoc explanation methods are an important tool for increasing model\ntransparency for users. Unfortunately, the currently used methods for\nattributing token importance often yield diverging patterns. In this work, we\nstudy potential sources of disagreement across methods from a linguistic\nperspective. We find that different methods systematically select different\nclasses of words and that methods that agree most with other methods and with\nhumans display similar linguistic preferences. Token-level differences between\nmethods are smoothed out if we compare them on the syntactic span level. We\nalso find higher agreement across methods by estimating the most important\nspans dynamically instead of relying on a fixed subset of size $k$. We\nsystematically investigate the interaction between $k$ and spans and propose an\nimproved configuration for selecting important tokens.\n","authors":["Jonathan Kamp","Lisa Beinborn","Antske Fokkens"],"pdf_url":"https://arxiv.org/pdf/2403.19424v1.pdf","comment":"Long paper accepted to LREC-Coling 2024 main conference. Please cite\n  the conference proceedings version when available"},{"id":"http://arxiv.org/abs/2403.19423v1","updated":"2024-03-28T13:55:51Z","published":"2024-03-28T13:55:51Z","title":"Echo-chambers and Idea Labs: Communication Styles on Twitter","summary":"  This paper investigates the communication styles and structures of Twitter\n(X) communities within the vaccination context. While mainstream research\nprimarily focuses on the echo-chamber phenomenon, wherein certain ideas are\nreinforced and participants are isolated from opposing opinions, this study\nreveals the presence of diverse communication styles across various\ncommunities. In addition to the communities exhibiting echo-chamber behavior,\nthis research uncovers communities with distinct communication patterns. By\nshedding light on the nuanced nature of communication within social networks,\nthis study emphasizes the significance of understanding the diversity of\nperspectives within online communities.\n","authors":["Aleksandra Sorokovikova","Michael Becker","Ivan P. Yamshchikov"],"pdf_url":"https://arxiv.org/pdf/2403.19423v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19414v1","updated":"2024-03-28T13:38:13Z","published":"2024-03-28T13:38:13Z","title":"BP4ER: Bootstrap Prompting for Explicit Reasoning in Medical Dialogue\n  Generation","summary":"  Medical dialogue generation (MDG) has gained increasing attention due to its\nsubstantial practical value. Previous works typically employ a\nsequence-to-sequence framework to generate medical responses by modeling\ndialogue context as sequential text with annotated medical entities. While\nthese methods have been successful in generating fluent responses, they fail to\nprovide process explanations of reasoning and require extensive entity\nannotation. To address these limitations, we propose the method Bootstrap\nPrompting for Explicit Reasoning in MDG (BP4ER), which explicitly model MDG's\nmulti-step reasoning process and iteratively enhance this reasoning process. We\nemploy a least-to-most prompting strategy to guide a large language model (LLM)\nin explicit reasoning, breaking down MDG into simpler sub-questions. These\nsub-questions build on answers from previous ones. Additionally, we also\nintroduce two distinct bootstrapping techniques for prompting, which\nautonomously correct errors and facilitate the LLM's explicit reasoning. This\napproach eliminates the need for entity annotation and increases the\ntransparency of the MDG process by explicitly generating the intermediate\nreasoning chain. The experimental findings on the two public datasets indicate\nthat BP4ER outperforms state-of-the-art methods in terms of both objective and\nsubjective evaluation metrics.\n","authors":["Yuhong He","Yongqi Zhang","Shizhu He","Jun Wan"],"pdf_url":"https://arxiv.org/pdf/2403.19414v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.19399v1","updated":"2024-03-28T13:19:16Z","published":"2024-03-28T13:19:16Z","title":"KazParC: Kazakh Parallel Corpus for Machine Translation","summary":"  We introduce KazParC, a parallel corpus designed for machine translation\nacross Kazakh, English, Russian, and Turkish. The first and largest publicly\navailable corpus of its kind, KazParC contains a collection of 371,902 parallel\nsentences covering different domains and developed with the assistance of human\ntranslators. Our research efforts also extend to the development of a neural\nmachine translation model nicknamed Tilmash. Remarkably, the performance of\nTilmash is on par with, and in certain instances, surpasses that of industry\ngiants, such as Google Translate and Yandex Translate, as measured by standard\nevaluation metrics, such as BLEU and chrF. Both KazParC and Tilmash are openly\navailable for download under the Creative Commons Attribution 4.0 International\nLicense (CC BY 4.0) through our GitHub repository.\n","authors":["Rustem Yeshpanov","Alina Polonskaya","Huseyin Atakan Varol"],"pdf_url":"https://arxiv.org/pdf/2403.19399v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19390v1","updated":"2024-03-28T13:01:18Z","published":"2024-03-28T13:01:18Z","title":"Checkpoint Merging via Bayesian Optimization in LLM Pretraining","summary":"  The rapid proliferation of large language models (LLMs) such as GPT-4 and\nGemini underscores the intense demand for resources during their training\nprocesses, posing significant challenges due to substantial computational and\nenvironmental costs. To alleviate this issue, we propose checkpoint merging in\npretraining LLM. This method utilizes LLM checkpoints with shared training\ntrajectories, and is rooted in an extensive search space exploration for the\nbest merging weight via Bayesian optimization. Through various experiments, we\ndemonstrate that: (1) Our proposed methodology exhibits the capacity to augment\npretraining, presenting an opportunity akin to obtaining substantial benefits\nat minimal cost; (2) Our proposed methodology, despite requiring a given\nheld-out dataset, still demonstrates robust generalization capabilities across\ndiverse domains, a pivotal aspect in pretraining.\n","authors":["Deyuan Liu","Zecheng Wang","Bingning Wang","Weipeng Chen","Chunshan Li","Zhiying Tu","Dianhui Chu","Bo Li","Dianbo Sui"],"pdf_url":"https://arxiv.org/pdf/2403.19390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08403v3","updated":"2024-03-28T12:51:44Z","published":"2024-02-13T12:04:43Z","title":"LLMs and the Human Condition","summary":"  Theory based AI research has had a hard time recently and the aim here is to\npropose a model of what LLMs are actually doing when they impress us with their\nlanguage skills. The model integrates three established theories of human\ndecision-making from philosophy, sociology, and computer science. The paper\nstarts with the collective understanding of reasoning from the early days of AI\nresearch - primarily because that model is how we humans think we think, and is\nthe most accessible. It then describes what is commonly thought of as \"reactive\nsystems\" which is the position taken by many philosophers and indeed many\ncontemporary AI researchers. The third component to the proposed model is from\nsociology and, although not flattering to our modern ego, provides an\nexplanation to a puzzle that for many years has occupied those of us working on\nconversational user interfaces.\n","authors":["Peter Wallis"],"pdf_url":"https://arxiv.org/pdf/2402.08403v3.pdf","comment":"3rd draft includes Roger's comments. Added images of Sagrada Familia\n  and termite mounds. target is IVA in 2024"},{"id":"http://arxiv.org/abs/2312.02051v2","updated":"2024-03-28T12:41:14Z","published":"2023-12-04T17:09:52Z","title":"TimeChat: A Time-sensitive Multimodal Large Language Model for Long\n  Video Understanding","summary":"  This work proposes TimeChat, a time-sensitive multimodal large language model\nspecifically designed for long video understanding. Our model incorporates two\nkey architectural contributions: (1) a timestamp-aware frame encoder that binds\nvisual content with the timestamp of each frame, and (2) a sliding video\nQ-Former that produces a video token sequence of varying lengths to accommodate\nvideos of various durations. Additionally, we construct an instruction-tuning\ndataset, encompassing 6 tasks and a total of 125K instances, to further enhance\nTimeChat's instruction-following performance. Experiment results across various\nvideo understanding tasks, such as dense captioning, temporal grounding, and\nhighlight detection, demonstrate TimeChat's strong zero-shot temporal\nlocalization and reasoning capabilities. For example, it achieves +9.2 F1 score\nand +2.8 CIDEr on YouCook2, +5.8 HIT@1 on QVHighlights, and +27.5 R@1 (IoU=0.5)\non Charades-STA, compared to state-of-the-art video large language models,\nholding the potential to serve as a versatile video assistant for long-form\nvideo comprehension tasks and satisfy realistic user requirements.\n","authors":["Shuhuai Ren","Linli Yao","Shicheng Li","Xu Sun","Lu Hou"],"pdf_url":"https://arxiv.org/pdf/2312.02051v2.pdf","comment":"CVPR 2024 camera-ready version, code is available at\n  https://github.com/RenShuhuai-Andy/TimeChat"},{"id":"http://arxiv.org/abs/2403.19365v1","updated":"2024-03-28T12:26:45Z","published":"2024-03-28T12:26:45Z","title":"EthioMT: Parallel Corpus for Low-resource Ethiopian Languages","summary":"  Recent research in natural language processing (NLP) has achieved impressive\nperformance in tasks such as machine translation (MT), news classification, and\nquestion-answering in high-resource languages. However, the performance of MT\nleaves much to be desired for low-resource languages. This is due to the\nsmaller size of available parallel corpora in these languages, if such corpora\nare available at all. NLP in Ethiopian languages suffers from the same issues\ndue to the unavailability of publicly accessible datasets for NLP tasks,\nincluding MT. To help the research community and foster research for Ethiopian\nlanguages, we introduce EthioMT -- a new parallel corpus for 15 languages. We\nalso create a new benchmark by collecting a dataset for better-researched\nlanguages in Ethiopia. We evaluate the newly collected corpus and the benchmark\ndataset for 23 Ethiopian languages using transformer and fine-tuning\napproaches.\n","authors":["Atnafu Lambebo Tonja","Olga Kolesnikova","Alexander Gelbukh","Jugal Kalita"],"pdf_url":"https://arxiv.org/pdf/2403.19365v1.pdf","comment":"Accepted at The Fifth workshop on Resources for African Indigenous\n  Languages (RAIL) 2024 ( LREC-COLING 2024)"},{"id":"http://arxiv.org/abs/2403.19358v1","updated":"2024-03-28T12:17:36Z","published":"2024-03-28T12:17:36Z","title":"Risk prediction of pathological gambling on social media","summary":"  This paper addresses the problem of risk prediction on social media data,\nspecifically focusing on the classification of Reddit users as having a\npathological gambling disorder. To tackle this problem, this paper focuses on\nincorporating temporal and emotional features into the model. The preprocessing\nphase involves dealing with the time irregularity of posts by padding\nsequences. Two baseline architectures are used for preliminary evaluation: BERT\nclassifier on concatenated posts per user and GRU with LSTM on sequential data.\nExperimental results demonstrate that the sequential models outperform the\nconcatenation-based model. The results of the experiments conclude that the\nincorporation of a time decay layer (TD) and passing the emotion classification\nlayer (EmoBERTa) through LSTM improves the performance significantly.\nExperiments concluded that the addition of a self-attention layer didn't\nsignificantly improve the performance of the model, however provided easily\ninterpretable attention scores. The developed architecture with the inclusion\nof EmoBERTa and TD layers achieved a high F1 score, beating existing benchmarks\non pathological gambling dataset. Future work may involve the early prediction\nof risk factors associated with pathological gambling disorder and testing\nmodels on other datasets. Overall, this research highlights the significance of\nthe sequential processing of posts including temporal and emotional features to\nboost the predictive power, as well as adding an attention layer for\ninterpretability.\n","authors":["Angelina Parfenova","Marianne Clausel"],"pdf_url":"https://arxiv.org/pdf/2403.19358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19354v1","updated":"2024-03-28T12:10:30Z","published":"2024-03-28T12:10:30Z","title":"AIpom at SemEval-2024 Task 8: Detecting AI-produced Outputs in M4","summary":"  This paper describes AIpom, a system designed to detect a boundary between\nhuman-written and machine-generated text (SemEval-2024 Task 8, Subtask C:\nHuman-Machine Mixed Text Detection). We propose a two-stage pipeline combining\npredictions from an instruction-tuned decoder-only model and encoder-only\nsequence taggers. AIpom is ranked second on the leaderboard while achieving a\nMean Absolute Error of 15.94. Ablation studies confirm the benefits of\npipelining encoder and decoder models, particularly in terms of improved\nperformance.\n","authors":["Alexander Shirnin","Nikita Andreev","Vladislav Mikhailov","Ekaterina Artemova"],"pdf_url":"https://arxiv.org/pdf/2403.19354v1.pdf","comment":"2nd place at SemEval-2024 Task 8, Subtask C, to appear in\n  SemEval-2024 proceedings"},{"id":"http://arxiv.org/abs/2403.19352v1","updated":"2024-03-28T12:08:39Z","published":"2024-03-28T12:08:39Z","title":"A diverse Multilingual News Headlines Dataset from around the World","summary":"  Babel Briefings is a novel dataset featuring 4.7 million news headlines from\nAugust 2020 to November 2021, across 30 languages and 54 locations worldwide\nwith English translations of all articles included. Designed for natural\nlanguage processing and media studies, it serves as a high-quality dataset for\ntraining or evaluating language models as well as offering a simple, accessible\ncollection of articles, for example, to analyze global news coverage and\ncultural narratives. As a simple demonstration of the analyses facilitated by\nthis dataset, we use a basic procedure using a TF-IDF weighted similarity\nmetric to group articles into clusters about the same event. We then visualize\nthe \\emph{event signatures} of the event showing articles of which languages\nappear over time, revealing intuitive features based on the proximity of the\nevent and unexpectedness of the event. The dataset is available on\n\\href{https://www.kaggle.com/datasets/felixludos/babel-briefings}{Kaggle} and\n\\href{https://huggingface.co/datasets/felixludos/babel-briefings}{HuggingFace}\nwith accompanying \\href{https://github.com/felixludos/babel-briefings}{GitHub}\ncode.\n","authors":["Felix Leeb","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2403.19352v1.pdf","comment":"Published in NAACL 2024 Proceedings (Short Paper track)"},{"id":"http://arxiv.org/abs/2403.19346v1","updated":"2024-03-28T12:04:28Z","published":"2024-03-28T12:04:28Z","title":"Large Language Models Are Unconscious of Unreasonability in Math\n  Problems","summary":"  Large language models (LLMs) demonstrate substantial capabilities in solving\nmath problems. However, they tend to produce hallucinations when given\nquestions containing unreasonable errors. In this paper, we study the behavior\nof LLMs when faced with unreasonable math problems and further explore their\npotential to address these problems. First, we construct the Unreasonable Math\nProblem (UMP) benchmark to examine the error detection ability of LLMs.\nExperiments show that LLMs are able to detect unreasonable errors, but still\nfail in generating non-hallucinatory content. In order to improve their ability\nof error detection and correction, we further design a strategic prompt\ntemplate called Critical Calculation and Conclusion(CCC). With CCC, LLMs can\nbetter self-evaluate and detect unreasonable errors in math questions, making\nthem more reliable and safe in practical application scenarios.\n","authors":["Jingyuan Ma","Damai Dai","Zhifang Sui"],"pdf_url":"https://arxiv.org/pdf/2403.19346v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.19340v1","updated":"2024-03-28T11:57:08Z","published":"2024-03-28T11:57:08Z","title":"Dataverse: Open-Source ETL (Extract, Transform, Load) Pipeline for Large\n  Language Models","summary":"  To address the challenges associated with data processing at scale, we\npropose Dataverse, a unified open-source Extract-Transform-Load (ETL) pipeline\nfor large language models (LLMs) with a user-friendly design at its core. Easy\naddition of custom processors with block-based interface in Dataverse allows\nusers to readily and efficiently use Dataverse to build their own ETL pipeline.\nWe hope that Dataverse will serve as a vital tool for LLM development and open\nsource the entire library to welcome community contribution. Additionally, we\nprovide a concise, two-minute video demonstration of our system, illustrating\nits capabilities and implementation.\n","authors":["Hyunbyung Park","Sukyung Lee","Gyoungjin Gim","Yungi Kim","Dahyun Kim","Chanjun Park"],"pdf_url":"https://arxiv.org/pdf/2403.19340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04260v2","updated":"2024-03-28T11:55:32Z","published":"2024-03-07T06:49:37Z","title":"Can Small Language Models be Good Reasoners for Sequential\n  Recommendation?","summary":"  Large language models (LLMs) open up new horizons for sequential\nrecommendations, owing to their remarkable language comprehension and\ngeneration capabilities. However, there are still numerous challenges that\nshould be addressed to successfully implement sequential recommendations\nempowered by LLMs. Firstly, user behavior patterns are often complex, and\nrelying solely on one-step reasoning from LLMs may lead to incorrect or\ntask-irrelevant responses. Secondly, the prohibitively resource requirements of\nLLM (e.g., ChatGPT-175B) are overwhelmingly high and impractical for real\nsequential recommender systems. In this paper, we propose a novel Step-by-step\nknowLedge dIstillation fraMework for recommendation (SLIM), paving a promising\npath for sequential recommenders to enjoy the exceptional reasoning\ncapabilities of LLMs in a \"slim\" (i.e., resource-efficient) manner. We\nintroduce CoT prompting based on user behavior sequences for the larger teacher\nmodel. The rationales generated by the teacher model are then utilized as\nlabels to distill the downstream smaller student model (e.g., LLaMA2-7B). In\nthis way, the student model acquires the step-by-step reasoning capabilities in\nrecommendation tasks. We encode the generated rationales from the student model\ninto a dense vector, which empowers recommendation in both ID-based and\nID-agnostic scenarios. Extensive experiments demonstrate the effectiveness of\nSLIM over state-of-the-art baselines, and further analysis showcasing its\nability to generate meaningful recommendation reasoning at affordable costs.\n","authors":["Yuling Wang","Changxin Tian","Binbin Hu","Yanhua Yu","Ziqi Liu","Zhiqiang Zhang","Jun Zhou","Liang Pang","Xiao Wang"],"pdf_url":"https://arxiv.org/pdf/2403.04260v2.pdf","comment":"Accepted by TheWebConf (WWW) 2024"},{"id":"http://arxiv.org/abs/2403.19335v1","updated":"2024-03-28T11:51:11Z","published":"2024-03-28T11:51:11Z","title":"KazSAnDRA: Kazakh Sentiment Analysis Dataset of Reviews and Attitudes","summary":"  This paper presents KazSAnDRA, a dataset developed for Kazakh sentiment\nanalysis that is the first and largest publicly available dataset of its kind.\nKazSAnDRA comprises an extensive collection of 180,064 reviews obtained from\nvarious sources and includes numerical ratings ranging from 1 to 5, providing a\nquantitative representation of customer attitudes. The study also pursued the\nautomation of Kazakh sentiment classification through the development and\nevaluation of four machine learning models trained for both polarity\nclassification and score classification. Experimental analysis included\nevaluation of the results considering both balanced and imbalanced scenarios.\nThe most successful model attained an F1-score of 0.81 for polarity\nclassification and 0.39 for score classification on the test sets. The dataset\nand fine-tuned models are open access and available for download under the\nCreative Commons Attribution 4.0 International License (CC BY 4.0) through our\nGitHub repository.\n","authors":["Rustem Yeshpanov","Huseyin Atakan Varol"],"pdf_url":"https://arxiv.org/pdf/2403.19335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15596v2","updated":"2024-03-28T11:35:55Z","published":"2023-11-27T07:44:25Z","title":"EgoThink: Evaluating First-Person Perspective Thinking Capability of\n  Vision-Language Models","summary":"  Vision-language models (VLMs) have recently shown promising results in\ntraditional downstream tasks. Evaluation studies have emerged to assess their\nabilities, with the majority focusing on the third-person perspective, and only\na few addressing specific tasks from the first-person perspective. However, the\ncapability of VLMs to \"think\" from a first-person perspective, a crucial\nattribute for advancing autonomous agents and robotics, remains largely\nunexplored. To bridge this research gap, we introduce EgoThink, a novel visual\nquestion-answering benchmark that encompasses six core capabilities with twelve\ndetailed dimensions. The benchmark is constructed using selected clips from\negocentric videos, with manually annotated question-answer pairs containing\nfirst-person information. To comprehensively assess VLMs, we evaluate eighteen\npopular VLMs on EgoThink. Moreover, given the open-ended format of the answers,\nwe use GPT-4 as the automatic judge to compute single-answer grading.\nExperimental results indicate that although GPT-4V leads in numerous\ndimensions, all evaluated VLMs still possess considerable potential for\nimprovement in first-person perspective tasks. Meanwhile, enlarging the number\nof trainable parameters has the most significant impact on model performance on\nEgoThink. In conclusion, EgoThink serves as a valuable addition to existing\nevaluation benchmarks for VLMs, providing an indispensable resource for future\nresearch in the realm of embodied artificial intelligence and robotics.\n","authors":["Sijie Cheng","Zhicheng Guo","Jingwen Wu","Kechen Fang","Peng Li","Huaping Liu","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2311.15596v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2010.05330v2","updated":"2024-03-28T11:26:58Z","published":"2020-10-11T19:51:21Z","title":"Incremental Processing in the Age of Non-Incremental Encoders: An\n  Empirical Assessment of Bidirectional Models for Incremental NLU","summary":"  While humans process language incrementally, the best language encoders\ncurrently used in NLP do not. Both bidirectional LSTMs and Transformers assume\nthat the sequence that is to be encoded is available in full, to be processed\neither forwards and backwards (BiLSTMs) or as a whole (Transformers). We\ninvestigate how they behave under incremental interfaces, when partial output\nmust be provided based on partial input seen up to a certain time step, which\nmay happen in interactive systems. We test five models on various NLU datasets\nand compare their performance using three incremental evaluation metrics. The\nresults support the possibility of using bidirectional encoders in incremental\nmode while retaining most of their non-incremental quality. The\n\"omni-directional\" BERT model, which achieves better non-incremental\nperformance, is impacted more by the incremental access. This can be alleviated\nby adapting the training regime (truncated training), or the testing procedure,\nby delaying the output until some right context is available or by\nincorporating hypothetical right contexts generated by a language model like\nGPT-2.\n","authors":["Brielen Madureira","David Schlangen"],"pdf_url":"https://arxiv.org/pdf/2010.05330v2.pdf","comment":"Accepted to the EMNLP 2020 conference (long paper). V2 has minor\n  updates, see note in last page"},{"id":"http://arxiv.org/abs/2403.19322v1","updated":"2024-03-28T11:26:30Z","published":"2024-03-28T11:26:30Z","title":"Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models","summary":"  The surge of Multimodal Large Language Models (MLLMs), given their prominent\nemergent capabilities in instruction following and reasoning, has greatly\nadvanced the field of visual reasoning. However, constrained by their\nnon-lossless image tokenization, most MLLMs fall short of comprehensively\ncapturing details of text and objects, especially in high-resolution images. To\naddress this, we propose P2G, a novel framework for plug-and-play grounding of\nreasoning in MLLMs. Specifically, P2G exploits the tool-usage potential of\nMLLMs to employ expert agents to achieve on-the-fly grounding to critical\nvisual and textual objects of image, thus achieving deliberate reasoning via\nmultimodal prompting. We further create P2GB, a benchmark aimed at assessing\nMLLMs' ability to understand inter-object relationships and text in challenging\nhigh-resolution images. Comprehensive experiments on visual reasoning tasks\ndemonstrate the superiority of P2G. Noteworthy, P2G achieved comparable\nperformance with GPT-4V on P2GB, with a 7B backbone. Our work highlights the\npotential of plug-and-play grounding of reasoning and opens up a promising\nalternative beyond model scaling.\n","authors":["Jiaxing Chen","Yuxuan Liu","Dehu Li","Xiang An","Ziyong Feng","Yongle Zhao","Yin Xie"],"pdf_url":"https://arxiv.org/pdf/2403.19322v1.pdf","comment":"14 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.19318v1","updated":"2024-03-28T11:21:12Z","published":"2024-03-28T11:21:12Z","title":"TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office\n  Usage Scenarios","summary":"  We introduce TableLLM, a robust large language model (LLM) with 13 billion\nparameters, purpose-built for proficiently handling tabular data manipulation\ntasks, whether they are embedded within documents or spreadsheets, catering to\nreal-world office scenarios. We propose a distant supervision method for\ntraining, which comprises a reasoning process extension strategy, aiding in\ntraining LLMs to understand reasoning patterns more effectively as well as a\ncross-way validation strategy, ensuring the quality of the automatically\ngenerated data. To evaluate the performance of TableLLM, we have crafted a\nbenchmark tailored to address both document and spreadsheet formats as well as\nconstructed a well-organized evaluation pipeline capable of handling both\nscenarios. Thorough evaluations underscore the advantages of TableLLM when\ncompared to various existing general-purpose and tabular data-focused LLMs. We\nhave publicly released the model checkpoint, source code, benchmarks, and a web\napplication for user interaction.\n","authors":["Xiaokang Zhang","Jing Zhang","Zeyao Ma","Yang Li","Bohan Zhang","Guanlin Li","Zijun Yao","Kangli Xu","Jinchang Zhou","Daniel Zhang-Li","Jifan Yu","Shu Zhao","Juanzi Li","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2403.19318v1.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2403.19317v1","updated":"2024-03-28T11:18:31Z","published":"2024-03-28T11:18:31Z","title":"Beyond Borders: Investigating Cross-Jurisdiction Transfer in Legal Case\n  Summarization","summary":"  Legal professionals face the challenge of managing an overwhelming volume of\nlengthy judgments, making automated legal case summarization crucial. However,\nprior approaches mainly focused on training and evaluating these models within\nthe same jurisdiction. In this study, we explore the cross-jurisdictional\ngeneralizability of legal case summarization models.Specifically, we explore\nhow to effectively summarize legal cases of a target jurisdiction where\nreference summaries are not available. In particular, we investigate whether\nsupplementing models with unlabeled target jurisdiction corpus and extractive\nsilver summaries obtained from unsupervised algorithms on target data enhances\ntransfer performance. Our comprehensive study on three datasets from different\njurisdictions highlights the role of pre-training in improving transfer\nperformance. We shed light on the pivotal influence of jurisdictional\nsimilarity in selecting optimal source datasets for effective transfer.\nFurthermore, our findings underscore that incorporating unlabeled target data\nyields improvements in general pre-trained models, with additional gains when\nsilver summaries are introduced. This augmentation is especially valuable when\ndealing with extractive datasets and scenarios featuring limited alignment\nbetween source and target jurisdictions. Our study provides key insights for\ndeveloping adaptable legal case summarization systems, transcending\njurisdictional boundaries.\n","authors":["T. Y. S. S Santosh","Vatsal Venkatkrishna","Saptarshi Ghosh","Matthias Grabmair"],"pdf_url":"https://arxiv.org/pdf/2403.19317v1.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2402.11549v2","updated":"2024-03-28T11:16:28Z","published":"2024-02-18T11:46:16Z","title":"Syntactic Language Change in English and German: Metrics, Parsers, and\n  Convergences","summary":"  Many studies have shown that human languages tend to optimize for lower\ncomplexity and increased communication efficiency. Syntactic dependency\ndistance, which measures the linear distance between dependent words, is often\nconsidered a key indicator of language processing difficulty and working memory\nload. The current paper looks at diachronic trends in syntactic language change\nin both English and German, using corpora of parliamentary debates from the\nlast c. 160 years. We base our observations on five dependency parsers,\nincluding the widely used Stanford CoreNLP as well as 4 newer alternatives. Our\nanalysis of syntactic language change goes beyond linear dependency distance\nand explores 15 metrics relevant to dependency distance minimization (DDM)\nand/or based on tree graph properties, such as the tree height and degree\nvariance. Even though we have evidence that recent parsers trained on modern\ntreebanks are not heavily affected by data 'noise' such as spelling changes and\nOCR errors in our historic data, we find that results of syntactic language\nchange are sensitive to the parsers involved, which is a caution against using\na single parser for evaluating syntactic language change as done in previous\nwork. We also show that syntactic language change over the time period\ninvestigated is largely similar between English and German for the different\nmetrics explored: only 4% of cases we examine yield opposite conclusions\nregarding upwards and downtrends of syntactic metrics across German and\nEnglish. We also show that changes in syntactic measures seem to be more\nfrequent at the tails of sentence length distributions. To our best knowledge,\nours is the most comprehensive analysis of syntactic language change using\nmodern NLP technology in recent corpora of English and German.\n","authors":["Yanran Chen","Wei Zhao","Anne Breitbarth","Manuel Stoeckel","Alexander Mehler","Steffen Eger"],"pdf_url":"https://arxiv.org/pdf/2402.11549v2.pdf","comment":"Updated to the current version"},{"id":"http://arxiv.org/abs/2403.18025v2","updated":"2024-03-28T11:01:21Z","published":"2024-03-26T18:23:16Z","title":"Improving Pre-trained Language Model Sensitivity via Mask Specific\n  losses: A case study on Biomedical NER","summary":"  Adapting language models (LMs) to novel domains is often achieved through\nfine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning\nintroduces new knowledge into an LM, enabling it to comprehend and efficiently\nperform a target domain task. Fine-tuning can however be inadvertently\ninsensitive if it ignores the wide array of disparities (e.g in word meaning)\nbetween source and target domains. For instance, words such as chronic and\npressure may be treated lightly in social conversations, however, clinically,\nthese words are usually an expression of concern. To address insensitive\nfine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach\nthat efficiently acquires target domain knowledge by appropriately weighting\nthe importance of domain-specific terms (DS-terms) during fine-tuning. MSLM\njointly masks DS-terms and generic words, then learns mask-specific losses by\nensuring LMs incur larger penalties for inaccurately predicting DS-terms\ncompared to generic words. Results of our analysis show that MSLM improves LMs\nsensitivity and detection of DS-terms. We empirically show that an optimal\nmasking rate not only depends on the LM, but also on the dataset and the length\nof sequences. Our proposed masking strategy outperforms advanced masking\nstrategies such as span- and PMI-based masking.\n","authors":["Micheal Abaho","Danushka Bollegala","Gary Leeming","Dan Joyce","Iain E Buchan"],"pdf_url":"https://arxiv.org/pdf/2403.18025v2.pdf","comment":"Paper alrerady accepted for publishing by the NAACL 2024 conference\n  (main conference paper)"},{"id":"http://arxiv.org/abs/2403.19305v1","updated":"2024-03-28T10:41:47Z","published":"2024-03-28T10:41:47Z","title":"MATEval: A Multi-Agent Discussion Framework for Advancing Open-Ended\n  Text Evaluation","summary":"  Recent advancements in generative Large Language Models(LLMs) have been\nremarkable, however, the quality of the text generated by these models often\nreveals persistent issues. Evaluating the quality of text generated by these\nmodels, especially in open-ended text, has consistently presented a significant\nchallenge. Addressing this, recent work has explored the possibility of using\nLLMs as evaluators. While using a single LLM as an evaluation agent shows\npotential, it is filled with significant uncertainty and instability. To\naddress these issues, we propose the MATEval: A \"Multi-Agent Text Evaluation\nframework\" where all agents are played by LLMs like GPT-4. The MATEval\nframework emulates human collaborative discussion methods, integrating multiple\nagents' interactions to evaluate open-ended text. Our framework incorporates\nself-reflection and Chain-of-Thought (CoT) strategies, along with feedback\nmechanisms, enhancing the depth and breadth of the evaluation process and\nguiding discussions towards consensus, while the framework generates\ncomprehensive evaluation reports, including error localization, error types and\nscoring. Experimental results show that our framework outperforms existing\nopen-ended text evaluation methods and achieves the highest correlation with\nhuman evaluation, which confirms the effectiveness and advancement of our\nframework in addressing the uncertainties and instabilities in evaluating\nLLMs-generated text. Furthermore, our framework significantly improves the\nefficiency of text evaluation and model iteration in industrial scenarios.\n","authors":["Yu Li","Shenyu Zhang","Rui Wu","Xiutian Huang","Yongrui Chen","Wenhao Xu","Guilin Qi","Dehai Min"],"pdf_url":"https://arxiv.org/pdf/2403.19305v1.pdf","comment":"This paper has been ACCEPTED as a LONG PAPER presentation by DASFAA\n  2024 Industrial Track"},{"id":"http://arxiv.org/abs/2403.18018v2","updated":"2024-03-28T10:19:46Z","published":"2024-03-26T18:07:10Z","title":"DORE: A Dataset For Portuguese Definition Generation","summary":"  Definition modelling (DM) is the task of automatically generating a\ndictionary definition for a specific word. Computational systems that are\ncapable of DM can have numerous applications benefiting a wide range of\naudiences. As DM is considered a supervised natural language generation\nproblem, these systems require large annotated datasets to train the machine\nlearning (ML) models. Several DM datasets have been released for English and\nother high-resource languages. While Portuguese is considered a\nmid/high-resource language in most natural language processing tasks and is\nspoken by more than 200 million native speakers, there is no DM dataset\navailable for Portuguese. In this research, we fill this gap by introducing\nDORE; the first dataset for Definition MOdelling for PoRtuguEse containing more\nthan 100,000 definitions. We also evaluate several deep learning based DM\nmodels on DORE and report the results. The dataset and the findings of this\npaper will facilitate research and study of Portuguese in wider contexts.\n","authors":["Anna Beatriz Dimas Furtado","Tharindu Ranasinghe","Frédéric Blain","Ruslan Mitkov"],"pdf_url":"https://arxiv.org/pdf/2403.18018v2.pdf","comment":"Accepted to LREC-COLING 2024 (The 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation)"},{"id":"http://arxiv.org/abs/2403.19285v1","updated":"2024-03-28T10:13:34Z","published":"2024-03-28T10:13:34Z","title":"Going Beyond Word Matching: Syntax Improves In-context Example Selection\n  for Machine Translation","summary":"  In-context learning (ICL) is the trending prompting strategy in the era of\nlarge language models (LLMs), where a few examples are demonstrated to evoke\nLLMs' power for a given task. How to select informative examples remains an\nopen issue. Previous works on in-context example selection for machine\ntranslation (MT) focus on superficial word-level features while ignoring deep\nsyntax-level knowledge. In this paper, we propose a syntax-based in-context\nexample selection method for MT, by computing the syntactic similarity between\ndependency trees using Polynomial Distance. In addition, we propose an ensemble\nstrategy combining examples selected by both word-level and syntax-level\ncriteria. Experimental results between English and 6 common languages indicate\nthat syntax can effectively enhancing ICL for MT, obtaining the highest COMET\nscores on 11 out of 12 translation directions.\n","authors":["Chenming Tang","Zhixiang Wang","Yunfang Wu"],"pdf_url":"https://arxiv.org/pdf/2403.19285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19283v1","updated":"2024-03-28T10:05:57Z","published":"2024-03-28T10:05:57Z","title":"Ungrammatical-syntax-based In-context Example Selection for Grammatical\n  Error Correction","summary":"  In the era of large language models (LLMs), in-context learning (ICL) stands\nout as an effective prompting strategy that explores LLMs' potency across\nvarious tasks. However, applying LLMs to grammatical error correction (GEC) is\nstill a challenging task. In this paper, we propose a novel\nungrammatical-syntax-based in-context example selection strategy for GEC.\nSpecifically, we measure similarity of sentences based on their syntactic\nstructures with diverse algorithms, and identify optimal ICL examples sharing\nthe most similar ill-formed syntax to the test input. Additionally, we carry\nout a two-stage process to further improve the quality of selection results. On\nbenchmark English GEC datasets, empirical results show that our proposed\nungrammatical-syntax-based strategies outperform commonly-used word-matching or\nsemantics-based methods with multiple LLMs. This indicates that for a\nsyntax-oriented task like GEC, paying more attention to syntactic information\ncan effectively boost LLMs' performance. Our code will be publicly available\nafter the publication of this paper.\n","authors":["Chenming Tang","Fanyi Qu","Yunfang Wu"],"pdf_url":"https://arxiv.org/pdf/2403.19283v1.pdf","comment":"Accepted to NAACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2403.19279v1","updated":"2024-03-28T10:02:10Z","published":"2024-03-28T10:02:10Z","title":"Fine-Tuning Language Models with Reward Learning on Policy","summary":"  Reinforcement learning from human feedback (RLHF) has emerged as an effective\napproach to aligning large language models (LLMs) to human preferences. RLHF\ncontains three steps, i.e., human preference collecting, reward learning, and\npolicy optimization, which are usually performed serially. Despite its\npopularity, however, (fixed) reward models may suffer from inaccurate\noff-distribution, since policy optimization continuously shifts LLMs' data\ndistribution. Repeatedly collecting new preference data from the latest LLMs\nmay alleviate this issue, which unfortunately makes the resulting system more\ncomplicated and difficult to optimize. In this paper, we propose reward\nlearning on policy (RLP), an unsupervised framework that refines a reward model\nusing policy samples to keep it on-distribution. Specifically, an unsupervised\nmulti-view learning method is introduced to learn robust representations of\npolicy samples. Meanwhile, a synthetic preference generation approach is\ndeveloped to simulate high-quality preference data with policy outputs.\nExtensive experiments on three benchmark datasets show that RLP consistently\noutperforms the state-of-the-art. Our code is available at\n\\url{https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/rlp}.\n","authors":["Hao Lang","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2403.19279v1.pdf","comment":"NAACL2024 Main Track Long Paper"},{"id":"http://arxiv.org/abs/2403.19275v1","updated":"2024-03-28T10:01:23Z","published":"2024-03-28T10:01:23Z","title":"Knowledge Boundary and Persona Dynamic Shape A Better Social Media Agent","summary":"  Constructing personalized and anthropomorphic agents holds significant\nimportance in the simulation of social networks. However, there are still two\nkey problems in existing works: the agent possesses world knowledge that does\nnot belong to its personas, and it cannot eliminate the interference of diverse\npersona information on current actions, which reduces the personalization and\nanthropomorphism of the agent. To solve the above problems, we construct the\nsocial media agent based on personalized knowledge and dynamic persona\ninformation. For personalized knowledge, we add external knowledge sources and\nmatch them with the persona information of agents, thereby giving the agent\npersonalized world knowledge. For dynamic persona information, we use current\naction information to internally retrieve the persona information of the agent,\nthereby reducing the interference of diverse persona information on the current\naction. To make the agent suitable for social media, we design five basic\nmodules for it: persona, planning, action, memory and reflection. To provide an\ninteraction and verification environment for the agent, we build a social media\nsimulation sandbox. In the experimental verification, automatic and human\nevaluations demonstrated the effectiveness of the agent we constructed.\n","authors":["Junkai Zhou","Liang Pang","Ya Jing","Jia Gu","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.19275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17752v2","updated":"2024-03-28T09:57:05Z","published":"2024-03-26T14:43:48Z","title":"Can multiple-choice questions really be useful in detecting the\n  abilities of LLMs?","summary":"  Multiple-choice questions (MCQs) are widely used in the evaluation of large\nlanguage models (LLMs) due to their simplicity and efficiency. However, there\nare concerns about whether MCQs can truly measure LLM's capabilities,\nparticularly in knowledge-intensive scenarios where long-form generation (LFG)\nanswers are required. The misalignment between the task and the evaluation\nmethod demands a thoughtful analysis of MCQ's efficacy, which we undertake in\nthis paper by evaluating nine LLMs on four question-answering (QA) datasets in\ntwo languages: Chinese and English. We identify a significant issue: LLMs\nexhibit an order sensitivity in bilingual MCQs, favoring answers located at\nspecific positions, i.e., the first position. We further quantify the gap\nbetween MCQs and long-form generation questions (LFGQs) by comparing their\ndirect outputs, token logits, and embeddings. Our results reveal a relatively\nlow correlation between answers from MCQs and LFGQs for identical questions.\nAdditionally, we propose two methods to quantify the consistency and confidence\nof LLMs' output, which can be generalized to other QA evaluation benchmarks.\nNotably, our analysis challenges the idea that the higher the consistency, the\ngreater the accuracy. We also find MCQs to be less reliable than LFGQs in terms\nof expected calibration error. Finally, the misalignment between MCQs and LFGQs\nis not only reflected in the evaluation performance but also in the embedding\nspace. Our code and models can be accessed at\nhttps://github.com/Meetyou-AI-Lab/Can-MC-Evaluate-LLMs.\n","authors":["Wangyue Li","Liangzhi Li","Tong Xiang","Xiao Liu","Wei Deng","Noa Garcia"],"pdf_url":"https://arxiv.org/pdf/2403.17752v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19270v1","updated":"2024-03-28T09:56:04Z","published":"2024-03-28T09:56:04Z","title":"sDPO: Don't Use Your Data All at Once","summary":"  As development of large language models (LLM) progresses, aligning them with\nhuman preferences has become increasingly important. We propose stepwise DPO\n(sDPO), an extension of the recently popularized direct preference optimization\n(DPO) for alignment tuning. This approach involves dividing the available\npreference datasets and utilizing them in a stepwise manner, rather than\nemploying it all at once. We demonstrate that this method facilitates the use\nof more precisely aligned reference models within the DPO training framework.\nFurthermore, sDPO trains the final model to be more performant, even\noutperforming other popular LLMs with more parameters.\n","authors":["Dahyun Kim","Yungi Kim","Wonho Song","Hyeonwoo Kim","Yunsu Kim","Sanghoon Kim","Chanjun Park"],"pdf_url":"https://arxiv.org/pdf/2403.19270v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19267v1","updated":"2024-03-28T09:53:41Z","published":"2024-03-28T09:53:41Z","title":"MineLand: Simulating Large-Scale Multi-Agent Interactions with Limited\n  Multimodal Senses and Physical Needs","summary":"  Conventional multi-agent simulators often assume perfect information and\nlimitless capabilities, hindering the ecological validity of social\ninteractions. We propose a multi-agent Minecraft simulator, MineLand, that\nbridges this gap by introducing limited multimodal senses and physical needs.\nOur simulator supports up to 48 agents with limited visual, auditory, and\nenvironmental awareness, forcing them to actively communicate and collaborate\nto fulfill physical needs like food and resources. This fosters dynamic and\nvalid multi-agent interactions. We further introduce an AI agent framework,\nAlex, inspired by multitasking theory, enabling agents to handle intricate\ncoordination and scheduling. Our experiments demonstrate that the simulator,\nthe corresponding benchmark, and the AI agent framework contribute to more\necological and nuanced collective behavior. The source code of MineLand and\nAlex is openly available at https://github.com/cocacola-lab/MineLand.\n","authors":["Xianhao Yu","Jiaqi Fu","Renjia Deng","Wenjuan Han"],"pdf_url":"https://arxiv.org/pdf/2403.19267v1.pdf","comment":"Project website: https://github.com/cocacola-lab/MineLand"},{"id":"http://arxiv.org/abs/2403.19260v1","updated":"2024-03-28T09:34:31Z","published":"2024-03-28T09:34:31Z","title":"NaijaHate: Evaluating Hate Speech Detection on Nigerian Twitter Using\n  Representative Data","summary":"  To address the global issue of hateful content proliferating in online\nplatforms, hate speech detection (HSD) models are typically developed on\ndatasets collected in the United States, thereby failing to generalize to\nEnglish dialects from the Majority World. Furthermore, HSD models are often\nevaluated on curated samples, raising concerns about overestimating model\nperformance in real-world settings. In this work, we introduce NaijaHate, the\nfirst dataset annotated for HSD which contains a representative sample of\nNigerian tweets. We demonstrate that HSD evaluated on biased datasets\ntraditionally used in the literature largely overestimates real-world\nperformance on representative data. We also propose NaijaXLM-T, a pretrained\nmodel tailored to the Nigerian Twitter context, and establish the key role\nplayed by domain-adaptive pretraining and finetuning in maximizing HSD\nperformance. Finally, we show that in this context, a human-in-the-loop\napproach to content moderation where humans review 1% of Nigerian tweets\nflagged as hateful would enable to moderate 60% of all hateful content. Taken\ntogether, these results pave the way towards robust HSD systems and a better\nprotection of social media users from hateful content in low-resource settings.\n","authors":["Manuel Tonneau","Pedro Vitor Quinta de Castro","Karim Lasri","Ibrahim Farouq","Lakshminarayanan Subramanian","Victor Orozco-Olvera","Samuel Fraiberger"],"pdf_url":"https://arxiv.org/pdf/2403.19260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19259v1","updated":"2024-03-28T09:32:43Z","published":"2024-03-28T09:32:43Z","title":"J-CRe3: A Japanese Conversation Dataset for Real-world Reference\n  Resolution","summary":"  Understanding expressions that refer to the physical world is crucial for\nsuch human-assisting systems in the real world, as robots that must perform\nactions that are expected by users. In real-world reference resolution, a\nsystem must ground the verbal information that appears in user interactions to\nthe visual information observed in egocentric views. To this end, we propose a\nmultimodal reference resolution task and construct a Japanese Conversation\ndataset for Real-world Reference Resolution (J-CRe3). Our dataset contains\negocentric video and dialogue audio of real-world conversations between two\npeople acting as a master and an assistant robot at home. The dataset is\nannotated with crossmodal tags between phrases in the utterances and the object\nbounding boxes in the video frames. These tags include indirect reference\nrelations, such as predicate-argument structures and bridging references as\nwell as direct reference relations. We also constructed an experimental model\nand clarified the challenges in multimodal reference resolution tasks.\n","authors":["Nobuhiro Ueda","Hideko Habe","Yoko Matsui","Akishige Yuguchi","Seiya Kawano","Yasutomo Kawanishi","Sadao Kurohashi","Koichiro Yoshino"],"pdf_url":"https://arxiv.org/pdf/2403.19259v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2212.00851v2","updated":"2024-03-28T09:25:21Z","published":"2022-12-01T20:18:21Z","title":"SOLD: Sinhala Offensive Language Dataset","summary":"  The widespread of offensive content online, such as hate speech and\ncyber-bullying, is a global phenomenon. This has sparked interest in the\nartificial intelligence (AI) and natural language processing (NLP) communities,\nmotivating the development of various systems trained to detect potentially\nharmful content automatically. These systems require annotated datasets to\ntrain the machine learning (ML) models. However, with a few notable exceptions,\nmost datasets on this topic have dealt with English and a few other\nhigh-resource languages. As a result, the research in offensive language\nidentification has been limited to these languages. This paper addresses this\ngap by tackling offensive language identification in Sinhala, a low-resource\nIndo-Aryan language spoken by over 17 million people in Sri Lanka. We introduce\nthe Sinhala Offensive Language Dataset (SOLD) and present multiple experiments\non this dataset. SOLD is a manually annotated dataset containing 10,000 posts\nfrom Twitter annotated as offensive and not offensive at both sentence-level\nand token-level, improving the explainability of the ML models. SOLD is the\nfirst large publicly available offensive language dataset compiled for Sinhala.\nWe also introduce SemiSOLD, a larger dataset containing more than 145,000\nSinhala tweets, annotated following a semi-supervised approach.\n","authors":["Tharindu Ranasinghe","Isuri Anuradha","Damith Premasiri","Kanishka Silva","Hansi Hettiarachchi","Lasitha Uyangodage","Marcos Zampieri"],"pdf_url":"https://arxiv.org/pdf/2212.00851v2.pdf","comment":"Accepted to Language Resources and Evaluation, Springer"},{"id":"http://arxiv.org/abs/2403.19219v1","updated":"2024-03-28T08:32:14Z","published":"2024-03-28T08:32:14Z","title":"Collaborative Knowledge Infusion for Low-resource Stance Detection","summary":"  Stance detection is the view towards a specific target by a given context\n(\\textit{e.g.} tweets, commercial reviews). Target-related knowledge is often\nneeded to assist stance detection models in understanding the target well and\nmaking detection correctly. However, prevailing works for knowledge-infused\nstance detection predominantly incorporate target knowledge from a singular\nsource that lacks knowledge verification in limited domain knowledge. The\nlow-resource training data further increases the challenge for the data-driven\nlarge models in this task. To address those challenges, we propose a\ncollaborative knowledge infusion approach for low-resource stance detection\ntasks, employing a combination of aligned knowledge enhancement and efficient\nparameter learning techniques. Specifically, our stance detection approach\nleverages target background knowledge collaboratively from different knowledge\nsources with the help of knowledge alignment. Additionally, we also introduce\nthe parameter-efficient collaborative adaptor with a staged optimization\nalgorithm, which collaboratively addresses the challenges associated with\nlow-resource stance detection tasks from both network structure and learning\nperspectives. To assess the effectiveness of our method, we conduct extensive\nexperiments on three public stance detection datasets, including low-resource\nand cross-target settings. The results demonstrate significant performance\nimprovements compared to the existing stance detection approaches.\n","authors":["Ming Yan","Joey Tianyi Zhou","Ivor W. Tsang"],"pdf_url":"https://arxiv.org/pdf/2403.19219v1.pdf","comment":"13 pages, 3 figures, Big Data Mining and Analysis"},{"id":"http://arxiv.org/abs/2403.18159v2","updated":"2024-03-28T08:22:31Z","published":"2024-03-26T23:51:44Z","title":"Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal\n  Propagation Analysis for Large Language Models","summary":"  Large generative models such as large language models (LLMs) and diffusion\nmodels have revolutionized the fields of NLP and computer vision respectively.\nHowever, their slow inference, high computation and memory requirement makes it\nchallenging to deploy them on edge devices. In this study, we propose a\nlight-weight quantization aware fine tuning technique using knowledge\ndistillation (KD-QAT) to improve the performance of 4-bit weight quantized LLMs\nusing commonly available datasets to realize a popular language use case, on\ndevice chat applications. To improve this paradigm of finetuning, as main\ncontributions, we provide insights into stability of KD-QAT by empirically\nstudying the gradient propagation during training to better understand the\nvulnerabilities of KD-QAT based approaches to low-bit quantization errors.\nBased on our insights, we propose ov-freeze, a simple technique to stabilize\nthe KD-QAT process. Finally, we experiment with the popular 7B LLaMAv2-Chat\nmodel at 4-bit quantization level and demonstrate that ov-freeze results in\nnear floating point precision performance, i.e., less than 0.7% loss of\naccuracy on Commonsense Reasoning benchmarks.\n","authors":["Kartikeya Bhardwaj","Nilesh Prasad Pandey","Sweta Priyadarshi","Kyunggeun Lee","Jun Ma","Harris Teague"],"pdf_url":"https://arxiv.org/pdf/2403.18159v2.pdf","comment":"Accepted at Practical ML for Low Resource Settings Workshop at ICLR\n  2024"},{"id":"http://arxiv.org/abs/2212.08686v2","updated":"2024-03-28T08:20:12Z","published":"2022-12-16T19:30:01Z","title":"Evaluating Step-by-Step Reasoning through Symbolic Verification","summary":"  Pre-trained language models (LMs) have shown remarkable reasoning performance\nusing explanations or chain-of-thoughts (CoT)) for in-context learning. On the\nother hand, these reasoning tasks are usually presumed to be more approachable\nfor symbolic programming. To understand the mechanism of reasoning of LMs, we\ncurate synthetic datasets containing equivalent (natural, symbolic) data pairs,\nwhere symbolic examples contain first-order logic rules and predicates from\nnon-parametric knowledge bases (KBs), supporting automated verification of\nintermediate reasoning results. Then we revisit neuro-symbolic approaches and\npropose to learn from demonstrations containing logic rules and corresponding\nexamples to iteratively reason over KBs, recovering Prolog's backward chaining\nalgorithm and supporting automated verification of LMs' outputs. Comprehensive\nexperiments are included to systematically compare LMLP with CoT in deductive\nreasoning settings, showing that LMLP enjoys more than $25\\%$ higher accuracy\nthan CoT on length generalization benchmarks even with smaller model sizes.\n","authors":["Yi-Fan Zhang","Hanlin Zhang","Li Erran Li","Eric Xing"],"pdf_url":"https://arxiv.org/pdf/2212.08686v2.pdf","comment":"NAACL-Findings, 2024"},{"id":"http://arxiv.org/abs/2403.19211v1","updated":"2024-03-28T08:19:33Z","published":"2024-03-28T08:19:33Z","title":"Dual-Personalizing Adapter for Federated Foundation Models","summary":"  Recently, foundation models, particularly large language models (LLMs), have\ndemonstrated an impressive ability to adapt to various tasks by fine-tuning\nlarge amounts of instruction data. Notably, federated foundation models emerge\nas a privacy preservation method to fine-tune models collaboratively under\nfederated learning (FL) settings by leveraging many distributed datasets with\nnon-IID data. To alleviate communication and computation overhead,\nparameter-efficient methods are introduced for efficiency, and some research\nadapted personalization methods to federated foundation models for better user\npreferences alignment. However, a critical gap in existing research is the\nneglect of test-time distribution shifts in real-world applications. Therefore,\nto bridge this gap, we propose a new setting, termed test-time personalization,\nwhich not only concentrates on the targeted local task but also extends to\nother tasks that exhibit test-time distribution shifts. To address challenges\nin this new setting, we explore a simple yet effective solution to learn a\ncomprehensive foundation model. Specifically, a dual-personalizing adapter\narchitecture (FedDPA) is proposed, comprising a global adapter and a local\nadapter for addressing test-time distribution shifts and personalization,\nrespectively. Additionally, we introduce an instance-wise dynamic weighting\nmechanism to optimize the balance between the global and local adapters,\nenhancing overall performance. The effectiveness of the proposed method has\nbeen evaluated on benchmark datasets across different NLP tasks.\n","authors":["Yiyuan Yang","Guodong Long","Tao Shen","Jing Jiang","Michael Blumenstein"],"pdf_url":"https://arxiv.org/pdf/2403.19211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19201v1","updated":"2024-03-28T07:55:29Z","published":"2024-03-28T07:55:29Z","title":"Understanding Archives: Towards New Research Interfaces Relying on the\n  Semantic Annotation of Documents","summary":"  The digitisation campaigns carried out by libraries and archives in recent\nyears have facilitated access to documents in their collections. However,\nexploring and exploiting these documents remain difficult tasks due to the\nsheer quantity of documents available for consultation. In this article, we\nshow how the semantic annotation of the textual content of study corpora of\narchival documents allow to facilitate their exploitation and valorisation.\nFirst, we present a methodological framework for the construction of new\ninterfaces based on textual semantics, then address the current technological\nobstacles and their potential solutions. We conclude by presenting a practical\ncase of the application of this framework.\n","authors":["Nicolas Gutehrlé","Iana Atanassova"],"pdf_url":"https://arxiv.org/pdf/2403.19201v1.pdf","comment":"in French language. CiDE.23: Document et archivage: pratiques\n  formelles et informelles, Oct 2023, Grenoble, France"},{"id":"http://arxiv.org/abs/2403.19183v1","updated":"2024-03-28T07:27:10Z","published":"2024-03-28T07:27:10Z","title":"Empirical Analysis for Unsupervised Universal Dependency Parse Tree\n  Aggregation","summary":"  Dependency parsing is an essential task in NLP, and the quality of dependency\nparsers is crucial for many downstream tasks. Parsers' quality often varies\ndepending on the domain and the language involved. Therefore, it is essential\nto combat the issue of varying quality to achieve stable performance. In\nvarious NLP tasks, aggregation methods are used for post-processing aggregation\nand have been shown to combat the issue of varying quality. However,\naggregation methods for post-processing aggregation have not been sufficiently\nstudied in dependency parsing tasks. In an extensive empirical study, we\ncompare different unsupervised post-processing aggregation methods to identify\nthe most suitable dependency tree structure aggregation method.\n","authors":["Adithya Kulkarni","Oliver Eulenstein","Qi Li"],"pdf_url":"https://arxiv.org/pdf/2403.19183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19181v1","updated":"2024-03-28T07:22:16Z","published":"2024-03-28T07:22:16Z","title":"Make Large Language Model a Better Ranker","summary":"  The evolution of Large Language Models (LLMs) has significantly enhanced\ncapabilities across various fields, leading to a paradigm shift in how\nRecommender Systems (RSs) are conceptualized and developed. However, existing\nresearch primarily focuses on point-wise and pair-wise recommendation\nparadigms. These approaches prove inefficient in LLM-based recommenders due to\nthe high computational cost of utilizing Large Language Models. While some\nstudies have delved into list-wise approaches, they fall short in ranking\ntasks. This shortfall is attributed to the misalignment between the objectives\nof ranking and language generation. To this end, this paper introduces the\nLanguage Model Framework with Aligned Listwise Ranking Objectives (ALRO). ALRO\nis designed to bridge the gap between the capabilities of LLMs and the nuanced\nrequirements of ranking tasks within recommender systems. A key feature of ALRO\nis the introduction of soft lambda loss, an adaptation of lambda loss tailored\nto suit language generation tasks. Additionally, ALRO incorporates a\npermutation-sensitive learning mechanism that addresses position bias, a\nprevalent issue in generative models, without imposing additional computational\nburdens during inference. Our evaluative studies reveal that ALRO outperforms\nexisting embedding-based recommendation methods and the existing LLM-based\nrecommendation baselines, highlighting its efficacy.\n","authors":["Wenshuo Chao","Zhi Zheng","Hengshu Zhu","Hao Liu"],"pdf_url":"https://arxiv.org/pdf/2403.19181v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.19056v3","updated":"2024-03-28T06:49:56Z","published":"2023-10-29T16:04:10Z","title":"MILL: Mutual Verification with Large Language Models for Zero-Shot Query\n  Expansion","summary":"  Query expansion, pivotal in search engines, enhances the representation of\nuser information needs with additional terms. While existing methods expand\nqueries using retrieved or generated contextual documents, each approach has\nnotable limitations. Retrieval-based methods often fail to accurately capture\nsearch intent, particularly with brief or ambiguous queries. Generation-based\nmethods, utilizing large language models (LLMs), generally lack corpus-specific\nknowledge and entail high fine-tuning costs. To address these gaps, we propose\na novel zero-shot query expansion framework utilizing LLMs for mutual\nverification. Specifically, we first design a query-query-document generation\nmethod, leveraging LLMs' zero-shot reasoning ability to produce diverse\nsub-queries and corresponding documents. Then, a mutual verification process\nsynergizes generated and retrieved documents for optimal expansion. Our\nproposed method is fully zero-shot, and extensive experiments on three public\nbenchmark datasets are conducted to demonstrate its effectiveness over existing\nmethods. Our code is available online at\nhttps://github.com/Applied-Machine-Learning-Lab/MILL to ease reproduction.\n","authors":["Pengyue Jia","Yiding Liu","Xiangyu Zhao","Xiaopeng Li","Changying Hao","Shuaiqiang Wang","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2310.19056v3.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2403.14403v2","updated":"2024-03-28T06:45:11Z","published":"2024-03-21T13:52:30Z","title":"Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language\n  Models through Question Complexity","summary":"  Retrieval-Augmented Large Language Models (LLMs), which incorporate the\nnon-parametric knowledge from external knowledge bases into LLMs, have emerged\nas a promising approach to enhancing response accuracy in several tasks, such\nas Question-Answering (QA). However, even though there are various approaches\ndealing with queries of different complexities, they either handle simple\nqueries with unnecessary computational overhead or fail to adequately address\ncomplex multi-step queries; yet, not all user requests fall into only one of\nthe simple or complex categories. In this work, we propose a novel adaptive QA\nframework, that can dynamically select the most suitable strategy for\n(retrieval-augmented) LLMs from the simplest to the most sophisticated ones\nbased on the query complexity. Also, this selection process is operationalized\nwith a classifier, which is a smaller LM trained to predict the complexity\nlevel of incoming queries with automatically collected labels, obtained from\nactual predicted outcomes of models and inherent inductive biases in datasets.\nThis approach offers a balanced strategy, seamlessly adapting between the\niterative and single-step retrieval-augmented LLMs, as well as the no-retrieval\nmethods, in response to a range of query complexities. We validate our model on\na set of open-domain QA datasets, covering multiple query complexities, and\nshow that ours enhances the overall efficiency and accuracy of QA systems,\ncompared to relevant baselines including the adaptive retrieval approaches.\nCode is available at: https://github.com/starsuzi/Adaptive-RAG.\n","authors":["Soyeong Jeong","Jinheon Baek","Sukmin Cho","Sung Ju Hwang","Jong C. Park"],"pdf_url":"https://arxiv.org/pdf/2403.14403v2.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.19167v1","updated":"2024-03-28T06:28:35Z","published":"2024-03-28T06:28:35Z","title":"Mitigating Misleading Chain-of-Thought Reasoning with Selective\n  Filtering","summary":"  Large language models have manifested remarkable capabilities by leveraging\nchain-of-thought (CoT) reasoning techniques to solve intricate questions\nthrough step-by-step reasoning chains. Despite its success, the efficacy of\nsuch reasoning is inherently contingent upon the quality of CoT. However,\nflawless CoT reasoning cannot be guaranteed due to the presence of\nindecomposable questions and the potential for erroneous reasoning chains,\nparticularly in the case of small-scale language models. To tackle this\nchallenge, we propose a novel approach called the selective filtering reasoner\n(SelF-Reasoner) that assesses the entailment relationship between the question\nand the candidate reasoning chain. Then, we proceed with CoT reasoning when the\nreasoning chain demonstrates confidence; otherwise, we opt to predict the\nanswer directly. SelF-Reasoner improves the fine-tuned T5 baseline consistently\nover the ScienceQA, ECQA, and LastLetter tasks. Code is available at\n\\texttt{https://github.com/LibroWu/SelF-Reasoner}.\n","authors":["Yexin Wu","Zhuosheng Zhang","Hai Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.19167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19161v1","updated":"2024-03-28T06:07:15Z","published":"2024-03-28T06:07:15Z","title":"Improving Vietnamese-English Medical Machine Translation","summary":"  Machine translation for Vietnamese-English in the medical domain is still an\nunder-explored research area. In this paper, we introduce MedEV -- a\nhigh-quality Vietnamese-English parallel dataset constructed specifically for\nthe medical domain, comprising approximately 360K sentence pairs. We conduct\nextensive experiments comparing Google Translate, ChatGPT (gpt-3.5-turbo),\nstate-of-the-art Vietnamese-English neural machine translation models and\npre-trained bilingual/multilingual sequence-to-sequence models on our new MedEV\ndataset. Experimental results show that the best performance is achieved by\nfine-tuning \"vinai-translate\" for each translation direction. We publicly\nrelease our dataset to promote further research.\n","authors":["Nhu Vo","Dat Quoc Nguyen","Dung D. Le","Massimo Piccardi","Wray Buntine"],"pdf_url":"https://arxiv.org/pdf/2403.19161v1.pdf","comment":"To appear in Proceedings of LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2212.08635v3","updated":"2024-03-28T06:06:59Z","published":"2022-12-16T18:23:43Z","title":"Self-Prompting Large Language Models for Zero-Shot Open-Domain QA","summary":"  Open-Domain Question Answering (ODQA) aims to answer questions without\nexplicitly providing specific background documents. This task becomes notably\nchallenging in a zero-shot setting where no data is available to train tailored\nretrieval-reader models. While recent Large Language Models (LLMs) like GPT-3\nhave demonstrated their effectiveness in zero-shot ODQA using direct prompting\nmethods, these methods still fall short of fully harnessing the potential of\nLLMs when implicitly invoked. In this paper, we propose a Self-Prompting\nframework to explicitly utilize the massive knowledge encoded in the parameters\nof LLMs and their strong instruction understanding abilities. Concretely, we\nprompt LLMs step by step to generate multiple pseudo QA pairs with background\npassages and explanations entirely from scratch. These generated elements are\nthen utilized for in-context learning. Experimental results show that our\nmethod significantly surpasses previous state-of-the-art zero-shot methods on\nthree widely-used ODQA datasets and even achieves comparable performance with\nvarious customized fine-tuned models on full training data. Our code is\navailable at https://github.com/lockon-n/self-prompting.\n","authors":["Junlong Li","Jinyuan Wang","Zhuosheng Zhang","Hai Zhao"],"pdf_url":"https://arxiv.org/pdf/2212.08635v3.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.19159v1","updated":"2024-03-28T06:03:47Z","published":"2024-03-28T06:03:47Z","title":"Disentangling Length from Quality in Direct Preference Optimization","summary":"  Reinforcement Learning from Human Feedback (RLHF) has been a crucial\ncomponent in the recent success of Large Language Models. However, RLHF is know\nto exploit biases in human preferences, such as verbosity. A well-formatted and\neloquent answer is often more highly rated by users, even when it is less\nhelpful and objective. A number of approaches have been developed to control\nthose biases in the classical RLHF literature, but the problem remains\nrelatively under-explored for Direct Alignment Algorithms such as Direct\nPreference Optimization (DPO). Unlike classical RLHF, DPO does not train a\nseparate reward model or use reinforcement learning directly, so previous\napproaches developed to control verbosity cannot be directly applied to this\nsetting. Our work makes several contributions. For the first time, we study the\nlength problem in the DPO setting, showing significant exploitation in DPO and\nlinking it to out-of-distribution bootstrapping. We then develop a principled\nbut simple regularization strategy that prevents length exploitation, while\nstill maintaining improvements in model quality. We demonstrate these effects\nacross datasets on summarization and dialogue, where we achieve up to 20\\%\nimprovement in win rates when controlling for length, despite the GPT4 judge's\nwell-known verbosity bias.\n","authors":["Ryan Park","Rafael Rafailov","Stefano Ermon","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2403.19159v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19154v1","updated":"2024-03-28T05:35:22Z","published":"2024-03-28T05:35:22Z","title":"STaR-GATE: Teaching Language Models to Ask Clarifying Questions","summary":"  When prompting language models to complete a task, users often leave\nimportant aspects unsaid. While asking questions could resolve this ambiguity\n\\citep[GATE;][]{li2023eliciting}, models often struggle to ask good questions.\nWe explore a language model's ability to self-improve\n\\citep[STaR;][]{zelikman2022star} by rewarding the model for generating useful\nquestions -- a simple method we dub STaR-GATE. We generate a synthetic dataset\nof 25,500 unique persona-task prompts to simulate conversations between a\npretrained language model -- the \\texttt{Questioner} -- and a\n\\texttt{Roleplayer} whose preferences are unknown to the \\texttt{Questioner}.\nBy asking questions, the \\texttt{Questioner} elicits preferences from the\n\\texttt{Roleplayer}. The \\texttt{Questioner} is iteratively finetuned on\nquestions that increase the probability of high-quality responses to the task,\nwhich are generated by an \\texttt{Oracle} with access to the\n\\texttt{Roleplayer}'s latent preferences. After two iterations of\nself-improvement, the \\texttt{Questioner} asks better questions, allowing it to\ngenerate responses that are preferred over responses from the initial model on\n\\highlightpink{\\textbf{72\\%}} of tasks. Our results indicate that teaching a\nlanguage model to ask better questions leads to better personalized responses.\n","authors":["Chinmaya Andukuri","Jan-Philipp Fränken","Tobias Gerstenberg","Noah D. Goodman"],"pdf_url":"https://arxiv.org/pdf/2403.19154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18314v2","updated":"2024-03-28T05:27:43Z","published":"2024-03-27T07:34:44Z","title":"Chinese Offensive Language Detection:Current Status and Future\n  Directions","summary":"  Despite the considerable efforts being made to monitor and regulate\nuser-generated content on social media platforms, the pervasiveness of\noffensive language, such as hate speech or cyberbullying, in the digital space\nremains a significant challenge. Given the importance of maintaining a\ncivilized and respectful online environment, there is an urgent and growing\nneed for automatic systems capable of detecting offensive speech in real time.\nHowever, developing effective systems for processing languages such as Chinese\npresents a significant challenge, owing to the language's complex and nuanced\nnature, which makes it difficult to process automatically. This paper provides\na comprehensive overview of offensive language detection in Chinese, examining\ncurrent benchmarks and approaches and highlighting specific models and tools\nfor addressing the unique challenges of detecting offensive language in this\ncomplex language. The primary objective of this survey is to explore the\nexisting techniques and identify potential avenues for further research that\ncan address the cultural and linguistic complexities of Chinese.\n","authors":["Yunze Xiao","Houda Bouamor","Wajdi Zaghouani"],"pdf_url":"https://arxiv.org/pdf/2403.18314v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19142v1","updated":"2024-03-28T04:30:07Z","published":"2024-03-28T04:30:07Z","title":"A Tulu Resource for Machine Translation","summary":"  We present the first parallel dataset for English-Tulu translation. Tulu,\nclassified within the South Dravidian linguistic family branch, is\npredominantly spoken by approximately 2.5 million individuals in southwestern\nIndia. Our dataset is constructed by integrating human translations into the\nmultilingual machine translation resource FLORES-200. Furthermore, we use this\ndataset for evaluation purposes in developing our English-Tulu machine\ntranslation model. For the model's training, we leverage resources available\nfor related South Dravidian languages. We adopt a transfer learning approach\nthat exploits similarities between high-resource and low-resource languages.\nThis method enables the training of a machine translation system even in the\nabsence of parallel data between the source and target language, thereby\novercoming a significant obstacle in machine translation development for\nlow-resource languages. Our English-Tulu system, trained without using parallel\nEnglish-Tulu data, outperforms Google Translate by 19 BLEU points (in September\n2023). The dataset and code are available here:\nhttps://github.com/manunarayanan/Tulu-NMT.\n","authors":["Manu Narayanan","Noëmi Aepli"],"pdf_url":"https://arxiv.org/pdf/2403.19142v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.19135v1","updated":"2024-03-28T04:12:13Z","published":"2024-03-28T04:12:13Z","title":"Compressing Large Language Models by Streamlining the Unimportant Layer","summary":"  Large language models (LLM) have been extensively applied in various natural\nlanguage tasks and domains, but their applicability is constrained by the large\nnumber of parameters of the models. Consequently, there is an increasing\nemphasis on compact models that exhibit high performance. In this study, we\nobserve that different layers in LLM have varying degrees of perturbation on\nthe hidden states, which allows us to identify less important layers. Based on\nthis phenomenon, we propose LLM-Streamline, which consists of two parts: layer\npruning, where we remove a set of consecutive layers with the lowest importance\nin the model according to the target sparsity; and layer replacement, where we\ntrain a lightweight model to substitute the pruned layers, thereby mitigating\nthe performance degradation caused by pruning. In our experiments, we utilize\nstructures such as a multi-layer perceptron (MLP) and a transformer layer as\nlightweight models and ultimately demonstrate that a single MLP can effectively\nfit the pruned layers. Comprehensive experiments show that our proposed method,\nLLM-Streamline, outperforms previous state-of-the-art (SOTA) model pruning\nmethods.\n","authors":["Xiaodong Chen","Yuxuan Hu","Jing Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.19135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19121v1","updated":"2024-03-28T03:25:23Z","published":"2024-03-28T03:25:23Z","title":"Code Comparison Tuning for Code Large Language Models","summary":"  We present Code Comparison Tuning (CCT), a simple and effective tuning method\nfor code large language models (Code LLMs) to better handle subtle code errors.\nSpecifically, we integrate the concept of comparison into instruction tuning,\nboth at the token and sequence levels, enabling the model to discern even the\nslightest deviations in code. To compare the original code with an erroneous\nversion containing manually added code errors, we use token-level preference\nloss for detailed token-level comparisons. Additionally, we combine code\nsegments to create a new instruction tuning sample for sequence-level\ncomparisons, enhancing the model's bug-fixing capability. Experimental results\non the HumanEvalFix benchmark show that CCT surpasses instruction tuning in\npass@1 scores by up to 4 points across diverse code LLMs, and extensive\nanalysis demonstrates the effectiveness of our method.\n","authors":["Yufan Jiang","Qiaozhi He","Xiaomin Zhuang","Zhihua Wu"],"pdf_url":"https://arxiv.org/pdf/2403.19121v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2306.03799v2","updated":"2024-03-28T03:23:59Z","published":"2023-06-06T15:43:16Z","title":"Prompt Space Optimizing Few-shot Reasoning Success with Large Language\n  Models","summary":"  Prompt engineering is an essential technique for enhancing the abilities of\nlarge language models (LLMs) by providing explicit and specific instructions.\nIt enables LLMs to excel in various tasks, such as arithmetic reasoning,\nquestion answering, summarization, relation extraction, machine translation,\nand sentiment analysis. Researchers have been actively exploring different\nprompt engineering strategies, such as Chain of Thought (CoT), Zero-CoT, and\nIn-context learning. However, an unresolved problem arises from the fact that\ncurrent approaches lack a solid mathematical solution for determining optimal\nprompts. To address this issue in prompt engineering, we propose a new and\neffective approach called Prompt Space. Our methodology utilizes text\nembeddings to obtain basis vectors by matrix decomposition, and then constructs\na space for representing all prompts. Prompt Space significantly outperforms\nstate-of-the-art prompt paradigms on ten public reasoning benchmarks. Notably,\nwithout the help of the CoT method and the prompt \"Let's think step by step\",\nPrompt Space shows superior performance over the few-shot method. Overall, our\napproach provides a robust and effective mathematical framework for selecting\nsimple and effective prompts. This advancement marks a significant step towards\nimproving prompt engineering for a wide variety of applications in LLMs. Our\ncode is publicly available at\n\\textcolor{blue}{\\url{https://github.com/YouBLEI/Prompt-Space}}\n","authors":["Fobo Shi","Peijun Qing","Dong Yang","Nan Wang","Youbo Lei","Haonan Lu","Xiaodong Lin","Duantengchuan Li"],"pdf_url":"https://arxiv.org/pdf/2306.03799v2.pdf","comment":"Natural language processing (NLP)"},{"id":"http://arxiv.org/abs/2403.19116v1","updated":"2024-03-28T03:14:18Z","published":"2024-03-28T03:14:18Z","title":"MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering","summary":"  In today's fast-paced industry, professionals face the challenge of\nsummarizing a large number of documents and extracting vital information from\nthem on a daily basis. These metrics are frequently hidden away in tables\nand/or their nested hyperlinks. To address this challenge, the approach of\nTable Question Answering (QA) has been developed to extract the relevant\ninformation. However, traditional Table QA training tasks that provide a table\nand an answer(s) from a gold cell coordinate(s) for a question may not always\nensure extracting the accurate answer(s). Recent advancements in Large Language\nModels (LLMs) have opened up new possibilities for extracting information from\ntabular data using prompts. In this paper, we introduce the Multi-hop Few-shot\nOpen Rich Table QA (MFORT-QA) approach, which consists of two major steps. The\nfirst step involves Few-Shot Learning (FSL), where relevant tables and\nassociated contexts of hyperlinks are retrieved based on a given question. The\nretrieved content is then used to construct few-shot prompts as inputs to an\nLLM, such as ChatGPT. To tackle the challenge of answering complex questions,\nthe second step leverages Chain-of-thought (CoT) prompting to decompose the\ncomplex question into a sequential chain of questions and reasoning thoughts in\na multi-hop manner. Retrieval-Augmented Generation (RAG) enhances this process\nby retrieving relevant tables and contexts of hyperlinks that are relevant to\nthe resulting reasoning thoughts and questions. These additional contexts are\nthen used to supplement the prompt used in the first step, resulting in more\naccurate answers from an LLM. Empirical results from OTT-QA demonstrate that\nour abstractive QA approach significantly improves the accuracy of extractive\nTable QA methods.\n","authors":["Che Guan","Mengyu Huang","Peng Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.19116v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.19114v1","updated":"2024-03-28T03:10:39Z","published":"2024-03-28T03:10:39Z","title":"Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval:\n  Evolving Coding Benchmarks via LLM","summary":"  LLMs have become the go-to choice for code generation tasks, with an\nexponential increase in the training, development, and usage of LLMs\nspecifically for code generation. To evaluate the ability of LLMs on code, both\nacademic and industry practitioners rely on popular handcrafted benchmarks.\nHowever, prior benchmarks contain only a very limited set of problems, both in\nquantity and variety. Further, due to popularity and age, many benchmarks are\nprone to data leakage where example solutions can be readily found on the web\nand thus potentially in training data. Such limitations inevitably lead us to\ninquire: Is the leaderboard performance on existing benchmarks reliable and\ncomprehensive enough to measure the program synthesis ability of LLMs? To\naddress this, we introduce EvoEval -- a program synthesis benchmark suite\ncreated by evolving existing benchmarks into different targeted domains for a\ncomprehensive evaluation of LLM coding abilities. Our study on 51 LLMs shows\nthat compared to the high performance obtained on standard benchmarks like\nHumanEval, there is a significant drop in performance (on average 39.4%) when\nusing EvoEval. Additionally, the decrease in performance can range from 19.6%\nto 47.7%, leading to drastic ranking changes amongst LLMs and showing potential\noverfitting of existing benchmarks. Furthermore, we showcase various insights,\nincluding the brittleness of instruction-following models when encountering\nrewording or subtle changes as well as the importance of learning problem\ncomposition and decomposition. EvoEval not only provides comprehensive\nbenchmarks, but can be used to further evolve arbitrary problems to keep up\nwith advances and the ever-changing landscape of LLMs for code. We have\nopen-sourced our benchmarks, tools, and complete LLM generations at\nhttps://github.com/evo-eval/evoeval\n","authors":["Chunqiu Steven Xia","Yinlin Deng","Lingming Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.19114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19113v1","updated":"2024-03-28T03:09:42Z","published":"2024-03-28T03:09:42Z","title":"FACTOID: FACtual enTailment fOr hallucInation Detection","summary":"  The widespread adoption of Large Language Models (LLMs) has facilitated\nnumerous benefits. However, hallucination is a significant concern. In\nresponse, Retrieval Augmented Generation (RAG) has emerged as a highly\npromising paradigm to improve LLM outputs by grounding them in factual\ninformation. RAG relies on textual entailment (TE) or similar methods to check\nif the text produced by LLMs is supported or contradicted, compared to\nretrieved documents. This paper argues that conventional TE methods are\ninadequate for spotting hallucinations in content generated by LLMs. For\ninstance, consider a prompt about the 'USA's stance on the Ukraine war''. The\nAI-generated text states, ...U.S. President Barack Obama says the U.S. will not\nput troops in Ukraine...'' However, during the war the U.S. president is Joe\nBiden which contradicts factual reality. Moreover, current TE systems are\nunable to accurately annotate the given text and identify the exact portion\nthat is contradicted. To address this, we introduces a new type of TE called\n``Factual Entailment (FE).'', aims to detect factual inaccuracies in content\ngenerated by LLMs while also highlighting the specific text segment that\ncontradicts reality. We present FACTOID (FACTual enTAILment for hallucInation\nDetection), a benchmark dataset for FE. We propose a multi-task learning (MTL)\nframework for FE, incorporating state-of-the-art (SoTA) long text embeddings\nsuch as e5-mistral-7b-instruct, along with GPT-3, SpanBERT, and RoFormer. The\nproposed MTL architecture for FE achieves an avg. 40\\% improvement in accuracy\non the FACTOID benchmark compared to SoTA TE methods. As FE automatically\ndetects hallucinations, we assessed 15 modern LLMs and ranked them using our\nproposed Auto Hallucination Vulnerability Index (HVI_auto). This index\nquantifies and offers a comparative scale to evaluate and rank LLMs according\nto their hallucinations.\n","authors":["Vipula Rawte","S. M Towhidul Islam Tonmoy","Krishnav Rajbangshi","Shravani Nag","Aman Chadha","Amit P. Sheth","Amitava Das"],"pdf_url":"https://arxiv.org/pdf/2403.19113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04021v4","updated":"2024-03-28T03:01:45Z","published":"2023-12-07T03:37:39Z","title":"A Study on the Calibration of In-context Learning","summary":"  Accurate uncertainty quantification is crucial for the safe deployment of\nmachine learning models, and prior research has demonstrated improvements in\nthe calibration of modern language models (LMs). We study in-context learning\n(ICL), a prevalent method for adapting static LMs through tailored prompts, and\nexamine the balance between performance and calibration across a broad spectrum\nof natural language understanding and reasoning tasks. Through comprehensive\nexperiments, we observe that, with an increasing number of ICL examples, models\ninitially exhibit increased miscalibration before achieving better calibration\nand miscalibration tends to arise in low-shot settings. Moreover, we find that\nmethods aimed at improving usability, such as fine-tuning and chain-of-thought\n(CoT) prompting, can lead to miscalibration and unreliable natural language\nexplanations. Furthermore, we explore recalibration techniques and find that a\nscaling-binning calibrator can reduce calibration errors consistently.\n","authors":["Hanlin Zhang","Yi-Fan Zhang","Yaodong Yu","Dhruv Madeka","Dean Foster","Eric Xing","Himabindu Lakkaraju","Sham Kakade"],"pdf_url":"https://arxiv.org/pdf/2312.04021v4.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.19103v1","updated":"2024-03-28T02:35:53Z","published":"2024-03-28T02:35:53Z","title":"Automated Black-box Prompt Engineering for Personalized Text-to-Image\n  Generation","summary":"  Prompt engineering is effective for controlling the output of text-to-image\n(T2I) generative models, but it is also laborious due to the need for manually\ncrafted prompts. This challenge has spurred the development of algorithms for\nautomated prompt generation. However, these methods often struggle with\ntransferability across T2I models, require white-box access to the underlying\nmodel, and produce non-intuitive prompts. In this work, we introduce PRISM, an\nalgorithm that automatically identifies human-interpretable and transferable\nprompts that can effectively generate desired concepts given only black-box\naccess to T2I models. Inspired by large language model (LLM) jailbreaking,\nPRISM leverages the in-context learning ability of LLMs to iteratively refine\nthe candidate prompts distribution for given reference images. Our experiments\ndemonstrate the versatility and effectiveness of PRISM in generating accurate\nprompts for objects, styles and images across multiple T2I models, including\nStable Diffusion, DALL-E, and Midjourney.\n","authors":["Yutong He","Alexander Robey","Naoki Murata","Yiding Jiang","Joshua Williams","George J. Pappas","Hamed Hassani","Yuki Mitsufuji","Ruslan Salakhutdinov","J. Zico Kolter"],"pdf_url":"https://arxiv.org/pdf/2403.19103v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19094v1","updated":"2024-03-28T02:12:49Z","published":"2024-03-28T02:12:49Z","title":"Learning From Correctness Without Prompting Makes LLM Efficient Reasoner","summary":"  Large language models (LLMs) have demonstrated outstanding performance across\nvarious tasks, yet they still exhibit limitations such as hallucination,\nunfaithful reasoning, and toxic content. One potential approach to mitigate\nthese issues is learning from human or external feedback (e.g. tools). In this\npaper, we introduce an intrinsic self-correct reasoning framework for LLMs that\neliminates the need for human feedback, external tools, and handcraft prompts.\nThe proposed framework, based on a multi-step reasoning paradigm\n\\textbf{Le}arning from \\textbf{Co}rrectness (\\textsc{LeCo}), improves reasoning\nperformance without needing to learn from errors. This paradigm prioritizes\nlearning from correct reasoning steps, and a unique method to measure\nconfidence for each reasoning step based on generation logits. Experimental\nresults across various multi-step reasoning tasks demonstrate the effectiveness\nof the framework in improving reasoning performance with reduced token\nconsumption.\n","authors":["Yuxuan Yao","Han Wu","Zhijiang Guo","Biyan Zhou","Jiahui Gao","Sichun Luo","Hanxu Hou","Xiaojin Fu","Linqi Song"],"pdf_url":"https://arxiv.org/pdf/2403.19094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08059v2","updated":"2024-03-28T00:59:37Z","published":"2024-03-12T20:11:38Z","title":"FluoroSAM: A Language-aligned Foundation Model for X-ray Image\n  Segmentation","summary":"  Automated X-ray image segmentation would accelerate research and development\nin diagnostic and interventional precision medicine. Prior efforts have\ncontributed task-specific models capable of solving specific image analysis\nproblems, but the utility of these models is restricted to their particular\ntask domain, and expanding to broader use requires additional data, labels, and\nretraining efforts. Recently, foundation models (FMs) -- machine learning\nmodels trained on large amounts of highly variable data thus enabling broad\napplicability -- have emerged as promising tools for automated image analysis.\nExisting FMs for medical image analysis focus on scenarios and modalities where\nobjects are clearly defined by visually apparent boundaries, such as surgical\ntool segmentation in endoscopy. X-ray imaging, by contrast, does not generally\noffer such clearly delineated boundaries or structure priors. During X-ray\nimage formation, complex 3D structures are projected in transmission onto the\nimaging plane, resulting in overlapping features of varying opacity and shape.\nTo pave the way toward an FM for comprehensive and automated analysis of\narbitrary medical X-ray images, we develop FluoroSAM, a language-aligned\nvariant of the Segment-Anything Model, trained from scratch on 1.6M synthetic\nX-ray images. FluoroSAM is trained on data including masks for 128 organ types\nand 464 non-anatomical objects, such as tools and implants. In real X-ray\nimages of cadaveric specimens, FluoroSAM is able to segment bony anatomical\nstructures based on text-only prompting with 0.51 and 0.79 DICE with\npoint-based refinement, outperforming competing SAM variants for all\nstructures. FluoroSAM is also capable of zero-shot generalization to segmenting\nclasses beyond the training set thanks to its language alignment, which we\ndemonstrate for full lung segmentation on real chest X-rays.\n","authors":["Benjamin D. Killeen","Liam J. Wang","Han Zhang","Mehran Armand","Russell H. Taylor","Dave Dreizin","Greg Osgood","Mathias Unberath"],"pdf_url":"https://arxiv.org/pdf/2403.08059v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09336v2","updated":"2024-03-28T00:50:55Z","published":"2023-11-15T19:52:11Z","title":"LLMRefine: Pinpointing and Refining Large Language Models via\n  Fine-Grained Actionable Feedback","summary":"  Recent large language models (LLM) are leveraging human feedback to improve\ntheir generation quality. However, human feedback is costly to obtain,\nespecially during inference. In this work, we propose LLMRefine, an inference\ntime optimization method to refine LLM's output. The core idea is to use a\nlearned fine-grained feedback model to pinpoint defects and guide LLM to refine\nthem iteratively. Using original LLM as a proposal of edits, LLMRefine searches\nfor defect-less text via simulated annealing, trading off the exploration and\nexploitation. We conduct experiments on three text generation tasks, including\nmachine translation, long-form question answering (QA), and topical\nsummarization. LLMRefine consistently outperforms all baseline approaches,\nachieving improvements up to 1.7 MetricX points on translation tasks, 8.1\nROUGE-L on ASQA, 2.2 ROUGE-L on topical summarization.\n","authors":["Wenda Xu","Daniel Deutsch","Mara Finkelstein","Juraj Juraska","Biao Zhang","Zhongtao Liu","William Yang Wang","Lei Li","Markus Freitag"],"pdf_url":"https://arxiv.org/pdf/2311.09336v2.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2403.19887v1","updated":"2024-03-28T23:55:06Z","published":"2024-03-28T23:55:06Z","title":"Jamba: A Hybrid Transformer-Mamba Language Model","summary":"  We present Jamba, a new base large language model based on a novel hybrid\nTransformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba\ninterleaves blocks of Transformer and Mamba layers, enjoying the benefits of\nboth model families. MoE is added in some of these layers to increase model\ncapacity while keeping active parameter usage manageable. This flexible\narchitecture allows resource- and objective-specific configurations. In the\nparticular configuration we have implemented, we end up with a powerful model\nthat fits in a single 80GB GPU. Built at large scale, Jamba provides high\nthroughput and small memory footprint compared to vanilla Transformers, and at\nthe same time state-of-the-art performance on standard language model\nbenchmarks and long-context evaluations. Remarkably, the model presents strong\nresults for up to 256K tokens context length. We study various architectural\ndecisions, such as how to combine Transformer and Mamba layers, and how to mix\nexperts, and show that some of them are crucial in large scale modeling. We\nalso describe several interesting properties of these architectures which the\ntraining and evaluation of Jamba have revealed, and plan to release checkpoints\nfrom various ablation runs, to encourage further exploration of this novel\narchitecture. We make the weights of our implementation of Jamba publicly\navailable under a permissive license.\n","authors":["Opher Lieber","Barak Lenz","Hofit Bata","Gal Cohen","Jhonathan Osin","Itay Dalmedigos","Erez Safahi","Shaked Meirom","Yonatan Belinkov","Shai Shalev-Shwartz","Omri Abend","Raz Alon","Tomer Asida","Amir Bergman","Roman Glozman","Michael Gokhman","Avashalom Manevich","Nir Ratner","Noam Rozen","Erez Shwartz","Mor Zusman","Yoav Shoham"],"pdf_url":"https://arxiv.org/pdf/2403.19887v1.pdf","comment":"Webpage: https://www.ai21.com/jamba"},{"id":"http://arxiv.org/abs/2307.02477v3","updated":"2024-03-28T23:37:24Z","published":"2023-07-05T17:50:42Z","title":"Reasoning or Reciting? Exploring the Capabilities and Limitations of\n  Language Models Through Counterfactual Tasks","summary":"  The impressive performance of recent language models across a wide range of\ntasks suggests that they possess a degree of abstract reasoning skills. Are\nthese skills general and transferable, or specialized to specific tasks seen\nduring pretraining? To disentangle these effects, we propose an evaluation\nframework based on \"counterfactual\" task variants that deviate from the default\nassumptions underlying standard tasks. Across a suite of 11 tasks, we observe\nnontrivial performance on the counterfactual variants, but nevertheless find\nthat performance substantially and consistently degrades compared to the\ndefault conditions. This suggests that while current LMs may possess abstract\ntask-solving skills to an extent, they often also rely on narrow,\nnon-transferable procedures for task-solving. These results motivate a more\ncareful interpretation of language model performance that teases apart these\naspects of behavior.\n","authors":["Zhaofeng Wu","Linlu Qiu","Alexis Ross","Ekin Akyürek","Boyuan Chen","Bailin Wang","Najoung Kim","Jacob Andreas","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2307.02477v3.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2311.17076v2","updated":"2024-03-28T23:02:27Z","published":"2023-11-27T22:23:27Z","title":"Compositional Chain-of-Thought Prompting for Large Multimodal Models","summary":"  The combination of strong visual backbones and Large Language Model (LLM)\nreasoning has led to Large Multimodal Models (LMMs) becoming the current\nstandard for a wide range of vision and language (VL) tasks. However, recent\nresearch has shown that even the most advanced LMMs still struggle to capture\naspects of compositional visual reasoning, such as attributes and relationships\nbetween objects. One solution is to utilize scene graphs (SGs)--a formalization\nof objects and their relations and attributes that has been extensively used as\na bridge between the visual and textual domains. Yet, scene graph data requires\nscene graph annotations, which are expensive to collect and thus not easily\nscalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic\nforgetting of the pretraining objective. To overcome this, inspired by\nchain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a\nnovel zero-shot Chain-of-Thought prompting method that utilizes SG\nrepresentations in order to extract compositional knowledge from an LMM.\nSpecifically, we first generate an SG using the LMM, and then use that SG in\nthe prompt to produce a response. Through extensive experiments, we find that\nthe proposed CCoT approach not only improves LMM performance on several vision\nand language VL compositional benchmarks but also improves the performance of\nseveral popular LMMs on general multimodal benchmarks, without the need for\nfine-tuning or annotated ground-truth SGs. Code:\nhttps://github.com/chancharikmitra/CCoT\n","authors":["Chancharik Mitra","Brandon Huang","Trevor Darrell","Roei Herzig"],"pdf_url":"https://arxiv.org/pdf/2311.17076v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19851v1","updated":"2024-03-28T21:53:24Z","published":"2024-03-28T21:53:24Z","title":"Localizing Paragraph Memorization in Language Models","summary":"  Can we localize the weights and mechanisms used by a language model to\nmemorize and recite entire paragraphs of its training data? In this paper, we\nshow that while memorization is spread across multiple layers and model\ncomponents, gradients of memorized paragraphs have a distinguishable spatial\npattern, being larger in lower model layers than gradients of non-memorized\nexamples. Moreover, the memorized examples can be unlearned by fine-tuning only\nthe high-gradient weights. We localize a low-layer attention head that appears\nto be especially involved in paragraph memorization. This head is predominantly\nfocusing its attention on distinctive, rare tokens that are least frequent in a\ncorpus-level unigram distribution. Next, we study how localized memorization is\nacross the tokens in the prefix by perturbing tokens and measuring the caused\nchange in the decoding. A few distinctive tokens early in a prefix can often\ncorrupt the entire continuation. Overall, memorized continuations are not only\nharder to unlearn, but also to corrupt than non-memorized ones.\n","authors":["Niklas Stoehr","Mitchell Gordon","Chiyuan Zhang","Owen Lewis"],"pdf_url":"https://arxiv.org/pdf/2403.19851v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.05632v2","updated":"2024-03-28T21:47:46Z","published":"2024-01-11T03:04:38Z","title":"Natural Language Processing for Dialects of a Language: A Survey","summary":"  State-of-the-art natural language processing (NLP) models are trained on\nmassive training corpora, and report a superlative performance on evaluation\ndatasets. This survey delves into an important attribute of these datasets: the\ndialect of a language. Motivated by the performance degradation of NLP models\nfor dialectic datasets and its implications for the equity of language\ntechnologies, we survey past research in NLP for dialects in terms of datasets,\nand approaches. We describe a wide range of NLP tasks in terms of two\ncategories: natural language understanding (NLU) (for tasks such as dialect\nclassification, sentiment analysis, parsing, and NLU benchmarks) and natural\nlanguage generation (NLG) (for summarisation, machine translation, and dialogue\nsystems). The survey is also broad in its coverage of languages which include\nEnglish, Arabic, German among others. We observe that past work in NLP\nconcerning dialects goes deeper than mere dialect classification, and . This\nincludes early approaches that used sentence transduction that lead to the\nrecent approaches that integrate hypernetworks into LoRA. We expect that this\nsurvey will be useful to NLP researchers interested in building equitable\nlanguage technologies by rethinking LLM benchmarks and model architectures.\n","authors":["Aditya Joshi","Raj Dabre","Diptesh Kanojia","Zhuang Li","Haolan Zhan","Gholamreza Haffari","Doris Dippold"],"pdf_url":"https://arxiv.org/pdf/2401.05632v2.pdf","comment":"The paper is under review at ACM Computing Surveys. Please reach out\n  to the authors in the case of feedback"},{"id":"http://arxiv.org/abs/2403.17343v3","updated":"2024-03-28T21:28:00Z","published":"2024-03-26T03:05:20Z","title":"Residual-based Language Models are Free Boosters for Biomedical Imaging","summary":"  In this study, we uncover the unexpected efficacy of residual-based large\nlanguage models (LLMs) as part of encoders for biomedical imaging tasks, a\ndomain traditionally devoid of language or textual data. The approach diverges\nfrom established methodologies by utilizing a frozen transformer block,\nextracted from pre-trained LLMs, as an innovative encoder layer for the direct\nprocessing of visual tokens. This strategy represents a significant departure\nfrom the standard multi-modal vision-language frameworks, which typically hinge\non language-driven prompts and inputs. We found that these LLMs could boost\nperformance across a spectrum of biomedical imaging applications, including\nboth 2D and 3D visual classification tasks, serving as plug-and-play boosters.\nMore interestingly, as a byproduct, we found that the proposed framework\nachieved superior performance, setting new state-of-the-art results on\nextensive, standardized datasets in MedMNIST-2D and 3D. Through this work, we\naim to open new avenues for employing LLMs in biomedical imaging and enriching\nthe understanding of their potential in this specialized domain.\n","authors":["Zhixin Lai","Jing Wu","Suiyao Chen","Yucheng Zhou","Naira Hovakimyan"],"pdf_url":"https://arxiv.org/pdf/2403.17343v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15454v2","updated":"2024-03-28T21:26:39Z","published":"2024-03-18T23:22:50Z","title":"Emotion Detection with Transformers: A Comparative Study","summary":"  In this study, we explore the application of transformer-based models for\nemotion classification on text data. We train and evaluate several pre-trained\ntransformer models, on the Emotion dataset using different variants of\ntransformers. The paper also analyzes some factors that in-fluence the\nperformance of the model, such as the fine-tuning of the transformer layer, the\ntrainability of the layer, and the preprocessing of the text data. Our analysis\nreveals that commonly applied techniques like removing punctuation and stop\nwords can hinder model performance. This might be because transformers strength\nlies in understanding contextual relationships within text. Elements like\npunctuation and stop words can still convey sentiment or emphasis and removing\nthem might disrupt this context.\n","authors":["Mahdi Rezapour"],"pdf_url":"https://arxiv.org/pdf/2403.15454v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19839v1","updated":"2024-03-28T21:20:27Z","published":"2024-03-28T21:20:27Z","title":"The New Agronomists: Language Models are Experts in Crop Management","summary":"  Crop management plays a crucial role in determining crop yield, economic\nprofitability, and environmental sustainability. Despite the availability of\nmanagement guidelines, optimizing these practices remains a complex and\nmultifaceted challenge. In response, previous studies have explored using\nreinforcement learning with crop simulators, typically employing simple\nneural-network-based reinforcement learning (RL) agents. Building on this\nfoundation, this paper introduces a more advanced intelligent crop management\nsystem. This system uniquely combines RL, a language model (LM), and crop\nsimulations facilitated by the Decision Support System for Agrotechnology\nTransfer (DSSAT). We utilize deep RL, specifically a deep Q-network, to train\nmanagement policies that process numerous state variables from the simulator as\nobservations. A novel aspect of our approach is the conversion of these state\nvariables into more informative language, facilitating the language model's\ncapacity to understand states and explore optimal management practices. The\nempirical results reveal that the LM exhibits superior learning capabilities.\nThrough simulation experiments with maize crops in Florida (US) and Zaragoza\n(Spain), the LM not only achieves state-of-the-art performance under various\nevaluation metrics but also demonstrates a remarkable improvement of over 49\\%\nin economic profit, coupled with reduced environmental impact when compared to\nbaseline methods. Our code is available at\n\\url{https://github.com/jingwu6/LM_AG}.\n","authors":["Jing Wu","Zhixin Lai","Suiyao Chen","Ran Tao","Pan Zhao","Naira Hovakimyan"],"pdf_url":"https://arxiv.org/pdf/2403.19839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19837v1","updated":"2024-03-28T21:15:38Z","published":"2024-03-28T21:15:38Z","title":"Concept-based Analysis of Neural Networks via Vision-Language Models","summary":"  Formal analysis of vision-based deep neural networks (DNNs) is highly\ndesirable but it is very challenging due to the difficulty of expressing formal\nspecifications for vision tasks and the lack of efficient verification\nprocedures. In this paper, we propose to leverage emerging multimodal,\nvision-language, foundation models (VLMs) as a lens through which we can reason\nabout vision models. VLMs have been trained on a large body of images\naccompanied by their textual description, and are thus implicitly aware of\nhigh-level, human-understandable concepts describing the images. We describe a\nlogical specification language $\\texttt{Con}_{\\texttt{spec}}$ designed to\nfacilitate writing specifications in terms of these concepts. To define and\nformally check $\\texttt{Con}_{\\texttt{spec}}$ specifications, we leverage a\nVLM, which provides a means to encode and efficiently check natural-language\nproperties of vision models. We demonstrate our techniques on a ResNet-based\nclassifier trained on the RIVAL-10 dataset leveraging CLIP as the multimodal\nmodel.\n","authors":["Ravi Mangal","Nina Narodytska","Divya Gopinath","Boyue Caroline Hu","Anirban Roy","Susmit Jha","Corina Pasareanu"],"pdf_url":"https://arxiv.org/pdf/2403.19837v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19836v1","updated":"2024-03-28T21:15:15Z","published":"2024-03-28T21:15:15Z","title":"Target Span Detection for Implicit Harmful Content","summary":"  Identifying the targets of hate speech is a crucial step in grasping the\nnature of such speech and, ultimately, in improving the detection of offensive\nposts on online forums. Much harmful content on online platforms uses implicit\nlanguage especially when targeting vulnerable and protected groups such as\nusing stereotypical characteristics instead of explicit target names, making it\nharder to detect and mitigate the language. In this study, we focus on\nidentifying implied targets of hate speech, essential for recognizing subtler\nhate speech and enhancing the detection of harmful content on digital\nplatforms. We define a new task aimed at identifying the targets even when they\nare not explicitly stated. To address that task, we collect and annotate target\nspans in three prominent implicit hate speech datasets: SBIC, DynaHate, and\nIHC. We call the resulting merged collection Implicit-Target-Span. The\ncollection is achieved using an innovative pooling method with matching scores\nbased on human annotations and Large Language Models (LLMs). Our experiments\nindicate that Implicit-Target-Span provides a challenging test bed for target\nspan detection methods.\n","authors":["Nazanin Jafari","James Allan"],"pdf_url":"https://arxiv.org/pdf/2403.19836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.01818v3","updated":"2024-03-28T20:38:01Z","published":"2022-06-03T21:01:48Z","title":"QAGCN: Answering Multi-Relation Questions via Single-Step Implicit\n  Reasoning over Knowledge Graphs","summary":"  Multi-relation question answering (QA) is a challenging task, where given\nquestions usually require long reasoning chains in KGs that consist of multiple\nrelations. Recently, methods with explicit multi-step reasoning over KGs have\nbeen prominently used in this task and have demonstrated promising performance.\nExamples include methods that perform stepwise label propagation through KG\ntriples and methods that navigate over KG triples based on reinforcement\nlearning. A main weakness of these methods is that their reasoning mechanisms\nare usually complex and difficult to implement or train. In this paper, we\nargue that multi-relation QA can be achieved via end-to-end single-step\nimplicit reasoning, which is simpler, more efficient, and easier to adopt. We\npropose QAGCN -- a Question-Aware Graph Convolutional Network (GCN)-based\nmethod that includes a novel GCN architecture with controlled\nquestion-dependent message propagation for the implicit reasoning. Extensive\nexperiments have been conducted, where QAGCN achieved competitive and even\nsuperior performance compared to state-of-the-art explicit-reasoning methods.\nOur code and pre-trained models are available in the repository:\nhttps://github.com/ruijie-wang-uzh/QAGCN\n","authors":["Ruijie Wang","Luca Rossetto","Michael Cochez","Abraham Bernstein"],"pdf_url":"https://arxiv.org/pdf/2206.01818v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19827v1","updated":"2024-03-28T20:35:10Z","published":"2024-03-28T20:35:10Z","title":"Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case\n  of the Missing AANNs","summary":"  Language models learn rare syntactic phenomena, but it has been argued that\nthey rely on rote memorization, as opposed to grammatical generalization.\nTraining on a corpus of human-scale in size (100M words), we iteratively\ntrained transformer language models on systematically manipulated corpora and\nthen evaluated their learning of a particular rare grammatical phenomenon: the\nEnglish Article+Adjective+Numeral+Noun (AANN) construction (``a beautiful five\ndays''). We first compared how well this construction was learned on the\ndefault corpus relative to a counterfactual corpus in which the AANN sentences\nwere removed. AANNs were still learned better than systematically perturbed\nvariants of the construction. Using additional counterfactual corpora, we\nsuggest that this learning occurs through generalization from related\nconstructions (e.g., ``a few days''). An additional experiment showed that this\nlearning is enhanced when there is more variability in the input. Taken\ntogether, our results provide an existence proof that models learn rare\ngrammatical phenomena by generalization from less rare phenomena. Code\navailable at https://github.com/kanishkamisra/aannalysis\n","authors":["Kanishka Misra","Kyle Mahowald"],"pdf_url":"https://arxiv.org/pdf/2403.19827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19822v1","updated":"2024-03-28T20:23:39Z","published":"2024-03-28T20:23:39Z","title":"Multi-Stage Multi-Modal Pre-Training for Automatic Speech Recognition","summary":"  Recent advances in machine learning have demonstrated that multi-modal\npre-training can improve automatic speech recognition (ASR) performance\ncompared to randomly initialized models, even when models are fine-tuned on\nuni-modal tasks. Existing multi-modal pre-training methods for the ASR task\nhave primarily focused on single-stage pre-training where a single unsupervised\ntask is used for pre-training followed by fine-tuning on the downstream task.\nIn this work, we introduce a novel method combining multi-modal and multi-task\nunsupervised pre-training with a translation-based supervised mid-training\napproach. We empirically demonstrate that such a multi-stage approach leads to\nrelative word error rate (WER) improvements of up to 38.45% over baselines on\nboth Librispeech and SUPERB. Additionally, we share several important findings\nfor choosing pre-training methods and datasets.\n","authors":["Yash Jain","David Chan","Pranav Dheram","Aparna Khare","Olabanji Shonibare","Venkatesh Ravichandran","Shalini Ghosh"],"pdf_url":"https://arxiv.org/pdf/2403.19822v1.pdf","comment":"Accepted in LREC-COLING 2024 - The 2024 Joint International\n  Conference on Computational Linguistics, Language Resources and Evaluation"},{"id":"http://arxiv.org/abs/2402.10189v2","updated":"2024-03-28T19:41:34Z","published":"2024-02-15T18:46:24Z","title":"Uncertainty Quantification for In-Context Learning of Large Language\n  Models","summary":"  In-context learning has emerged as a groundbreaking ability of Large Language\nModels (LLMs) and revolutionized various fields by providing a few\ntask-relevant demonstrations in the prompt. However, trustworthy issues with\nLLM's response, such as hallucination, have also been actively discussed.\nExisting works have been devoted to quantifying the uncertainty in LLM's\nresponse, but they often overlook the complex nature of LLMs and the uniqueness\nof in-context learning. In this work, we delve into the predictive uncertainty\nof LLMs associated with in-context learning, highlighting that such\nuncertainties may stem from both the provided demonstrations (aleatoric\nuncertainty) and ambiguities tied to the model's configurations (epistemic\nuncertainty). We propose a novel formulation and corresponding estimation\nmethod to quantify both types of uncertainties. The proposed method offers an\nunsupervised way to understand the prediction of in-context learning in a\nplug-and-play fashion. Extensive experiments are conducted to demonstrate the\neffectiveness of the decomposition. The code and data are available at:\nhttps://github.com/lingchen0331/UQ_ICL.\n","authors":["Chen Ling","Xujiang Zhao","Xuchao Zhang","Wei Cheng","Yanchi Liu","Yiyou Sun","Mika Oishi","Takao Osaki","Katsushi Matsuda","Jie Ji","Guangji Bai","Liang Zhao","Haifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2402.10189v2.pdf","comment":"Accepted to the main conference of NAACL 2024"},{"id":"http://arxiv.org/abs/2403.19802v1","updated":"2024-03-28T19:31:32Z","published":"2024-03-28T19:31:32Z","title":"Developing Healthcare Language Model Embedding Spaces","summary":"  Pre-trained Large Language Models (LLMs) often struggle on out-of-domain\ndatasets like healthcare focused text. We explore specialized pre-training to\nadapt smaller LLMs to different healthcare datasets. Three methods are\nassessed: traditional masked language modeling, Deep Contrastive Learning for\nUnsupervised Textual Representations (DeCLUTR), and a novel pre-training\nobjective utilizing metadata categories from the healthcare settings. These\nschemes are evaluated on downstream document classification tasks for each\ndataset, with additional analysis of the resultant embedding spaces.\nContrastively trained models outperform other approaches on the classification\ntasks, delivering strong performance from limited labeled data and with fewer\nmodel parameter updates required. While metadata-based pre-training does not\nfurther improve classifications across the datasets, it yields interesting\nembedding cluster separability. All domain adapted LLMs outperform their\npublicly available general base LLM, validating the importance of\ndomain-specialization. This research illustrates efficient approaches to\ninstill healthcare competency in compact LLMs even under tight computational\nbudgets, an essential capability for responsible and sustainable deployment in\nlocal healthcare settings. We provide pre-training guidelines for specialized\nhealthcare LLMs, motivate continued inquiry into contrastive objectives, and\ndemonstrates adaptation techniques to align small LLMs with privacy-sensitive\nmedical tasks.\n","authors":["Niall Taylor","Dan Schofield","Andrey Kormilitzin","Dan W Joyce","Alejo Nevado-Holgado"],"pdf_url":"https://arxiv.org/pdf/2403.19802v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19758v1","updated":"2024-03-28T18:15:07Z","published":"2024-03-28T18:15:07Z","title":"Natural Language, AI, and Quantum Computing in 2024: Research\n  Ingredients and Directions in QNLP","summary":"  Language processing is at the heart of current developments in artificial\nintelligence, and quantum computers are becoming available at the same time.\nThis has led to great interest in quantum natural language processing, and\nseveral early proposals and experiments. This paper surveys the state of this\narea, showing how NLP-related techniques including word embeddings, sequential\nmodels, attention, and grammatical parsing have been used in quantum language\nprocessing. We introduce a new quantum design for the basic task of text\nencoding (representing a string of characters in memory), which has not been\naddressed in detail before.\n  As well as motivating new technologies, quantum theory has made key\ncontributions to the challenging questions of 'What is uncertainty?' and 'What\nis intelligence?' As these questions are taking on fresh urgency with\nartificial systems, the paper also considers some of the ways facts are\nconceptualized and presented in language. In particular, we argue that the\nproblem of 'hallucinations' arises through a basic misunderstanding: language\nexpresses any number of plausible hypotheses, only a few of which become\nactual, a distinction that is ignored in classical mechanics, but present\n(albeit confusing) in quantum mechanics.\n","authors":["Dominic Widdows","Willie Aboumrad","Dohun Kim","Sayonee Ray","Jonathan Mei"],"pdf_url":"https://arxiv.org/pdf/2403.19758v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19754v1","updated":"2024-03-28T18:08:22Z","published":"2024-03-28T18:08:22Z","title":"GOLD: Generalized Knowledge Distillation via Out-of-Distribution-Guided\n  Language Data Generation","summary":"  Knowledge distillation from LLMs is essential for the efficient deployment of\nlanguage models. Prior works have proposed data generation using LLMs for\npreparing distilled models. We argue that generating data with LLMs is prone to\nsampling mainly from the center of original content distribution. This\nlimitation hinders the distilled model from learning the true underlying data\ndistribution and to forget the tails of the distributions (samples with lower\nprobability). To this end, we propose GOLD, a task-agnostic data generation and\nknowledge distillation framework, which employs an iterative\nout-of-distribution-guided feedback mechanism for the LLM. As a result, the\ngenerated data improves the generalizability of distilled models. An\nenergy-based OOD evaluation approach is also introduced to deal with noisy\ngenerated data. Our extensive experiments on 10 different classification and\nsequence-to-sequence tasks in NLP show that GOLD respectively outperforms prior\narts and the LLM with an average improvement of 5% and 14%. We will also show\nthat the proposed method is applicable to less explored and novel tasks. The\ncode is available.\n","authors":["Mohsen Gholami","Mohammad Akbari","Cindy Hu","Vaden Masrani","Z. Jane Wang","Yong Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.19754v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19728v1","updated":"2024-03-28T10:31:09Z","published":"2024-03-28T10:31:09Z","title":"EmoScan: Automatic Screening of Depression Symptoms in Romanized Sinhala\n  Tweets","summary":"  This work explores the utilization of Romanized Sinhala social media data to\nidentify individuals at risk of depression. A machine learning-based framework\nis presented for the automatic screening of depression symptoms by analyzing\nlanguage patterns, sentiment, and behavioural cues within a comprehensive\ndataset of social media posts. The research has been carried out to compare the\nsuitability of Neural Networks over the classical machine learning techniques.\nThe proposed Neural Network with an attention layer which is capable of\nhandling long sequence data, attains a remarkable accuracy of 93.25% in\ndetecting depression symptoms, surpassing current state-of-the-art methods.\nThese findings underscore the efficacy of this approach in pinpointing\nindividuals in need of proactive interventions and support. Mental health\nprofessionals, policymakers, and social media companies can gain valuable\ninsights through the proposed model. Leveraging natural language processing\ntechniques and machine learning algorithms, this work offers a promising\npathway for mental health screening in the digital era. By harnessing the\npotential of social media data, the framework introduces a proactive method for\nrecognizing and assisting individuals at risk of depression. In conclusion,\nthis research contributes to the advancement of proactive interventions and\nsupport systems for mental health, thereby influencing both research and\npractical applications in the field.\n","authors":["Jayathi Hewapathirana","Deshan Sumanathilaka"],"pdf_url":"https://arxiv.org/pdf/2403.19728v1.pdf","comment":"4 pages, 2 tables, 1 Figure , Preprint"},{"id":"http://arxiv.org/abs/2403.19727v1","updated":"2024-03-28T08:40:02Z","published":"2024-03-28T08:40:02Z","title":"New Semantic Task for the French Spoken Language Understanding MEDIA\n  Benchmark","summary":"  Intent classification and slot-filling are essential tasks of Spoken Language\nUnderstanding (SLU). In most SLUsystems, those tasks are realized by\nindependent modules. For about fifteen years, models achieving both of\nthemjointly and exploiting their mutual enhancement have been proposed. A\nmultilingual module using a joint modelwas envisioned to create a touristic\ndialogue system for a European project, HumanE-AI-Net. A combination ofmultiple\ndatasets, including the MEDIA dataset, was suggested for training this joint\nmodel. The MEDIA SLU datasetis a French dataset distributed since 2005 by ELRA,\nmainly used by the French research community and free foracademic research\nsince 2020. Unfortunately, it is annotated only in slots but not intents. An\nenhanced version ofMEDIA annotated with intents has been built to extend its\nuse to more tasks and use cases. This paper presents thesemi-automatic\nmethodology used to obtain this enhanced version. In addition, we present the\nfirst results of SLUexperiments on this enhanced dataset using joint models for\nintent classification and slot-filling.\n","authors":["Nadège Alavoine","Gaëlle Laperriere","Christophe Servan","Sahar Ghannay","Sophie Rosset"],"pdf_url":"https://arxiv.org/pdf/2403.19727v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2403.19655v1","updated":"2024-03-28T17:59:50Z","published":"2024-03-28T17:59:50Z","title":"GaussianCube: Structuring Gaussian Splatting using Optimal Transport for\n  3D Generative Modeling","summary":"  3D Gaussian Splatting (GS) have achieved considerable improvement over Neural\nRadiance Fields in terms of 3D fitting fidelity and rendering speed. However,\nthis unstructured representation with scattered Gaussians poses a significant\nchallenge for generative modeling. To address the problem, we introduce\nGaussianCube, a structured GS representation that is both powerful and\nefficient for generative modeling. We achieve this by first proposing a\nmodified densification-constrained GS fitting algorithm which can yield\nhigh-quality fitting results using a fixed number of free Gaussians, and then\nre-arranging the Gaussians into a predefined voxel grid via Optimal Transport.\nThe structured grid representation allows us to use standard 3D U-Net as our\nbackbone in diffusion generative modeling without elaborate designs. Extensive\nexperiments conducted on ShapeNet and OmniObject3D show that our model achieves\nstate-of-the-art generation results both qualitatively and quantitatively,\nunderscoring the potential of GaussianCube as a powerful and versatile 3D\nrepresentation.\n","authors":["Bowen Zhang","Yiji Cheng","Jiaolong Yang","Chunyu Wang","Feng Zhao","Yansong Tang","Dong Chen","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2403.19655v1.pdf","comment":"Project Page: https://gaussiancube.github.io/"},{"id":"http://arxiv.org/abs/2403.19654v1","updated":"2024-03-28T17:59:49Z","published":"2024-03-28T17:59:49Z","title":"RSMamba: Remote Sensing Image Classification with State Space Model","summary":"  Remote sensing image classification forms the foundation of various\nunderstanding tasks, serving a crucial function in remote sensing image\ninterpretation. The recent advancements of Convolutional Neural Networks (CNNs)\nand Transformers have markedly enhanced classification accuracy. Nonetheless,\nremote sensing scene classification remains a significant challenge, especially\ngiven the complexity and diversity of remote sensing scenarios and the\nvariability of spatiotemporal resolutions. The capacity for whole-image\nunderstanding can provide more precise semantic cues for scene discrimination.\nIn this paper, we introduce RSMamba, a novel architecture for remote sensing\nimage classification. RSMamba is based on the State Space Model (SSM) and\nincorporates an efficient, hardware-aware design known as the Mamba. It\nintegrates the advantages of both a global receptive field and linear modeling\ncomplexity. To overcome the limitation of the vanilla Mamba, which can only\nmodel causal sequences and is not adaptable to two-dimensional image data, we\npropose a dynamic multi-path activation mechanism to augment Mamba's capacity\nto model non-causal data. Notably, RSMamba maintains the inherent modeling\nmechanism of the vanilla Mamba, yet exhibits superior performance across\nmultiple remote sensing image classification datasets. This indicates that\nRSMamba holds significant potential to function as the backbone of future\nvisual foundation models. The code will be available at\n\\url{https://github.com/KyanChen/RSMamba}.\n","authors":["Keyan Chen","Bowen Chen","Chenyang Liu","Wenyuan Li","Zhengxia Zou","Zhenwei Shi"],"pdf_url":"https://arxiv.org/pdf/2403.19654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19653v1","updated":"2024-03-28T17:59:42Z","published":"2024-03-28T17:59:42Z","title":"Detecting Image Attribution for Text-to-Image Diffusion Models in RGB\n  and Beyond","summary":"  Modern text-to-image (T2I) diffusion models can generate images with\nremarkable realism and creativity. These advancements have sparked research in\nfake image detection and attribution, yet prior studies have not fully explored\nthe practical and scientific dimensions of this task. In addition to\nattributing images to 12 state-of-the-art T2I generators, we provide extensive\nanalyses on what inference stage hyperparameters and image modifications are\ndiscernible. Our experiments reveal that initialization seeds are highly\ndetectable, along with other subtle variations in the image generation process\nto some extent. We further investigate what visual traces are leveraged in\nimage attribution by perturbing high-frequency details and employing mid-level\nrepresentations of image style and structure. Notably, altering high-frequency\ninformation causes only slight reductions in accuracy, and training an\nattributor on style representations outperforms training on RGB images. Our\nanalyses underscore that fake images are detectable and attributable at various\nlevels of visual granularity than previously explored.\n","authors":["Katherine Xu","Lingzhi Zhang","Jianbo Shi"],"pdf_url":"https://arxiv.org/pdf/2403.19653v1.pdf","comment":"Code available at https://github.com/k8xu/ImageAttribution"},{"id":"http://arxiv.org/abs/2403.19652v1","updated":"2024-03-28T17:59:30Z","published":"2024-03-28T17:59:30Z","title":"InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction","summary":"  Text-conditioned human motion generation has experienced significant\nadvancements with diffusion models trained on extensive motion capture data and\ncorresponding textual annotations. However, extending such success to 3D\ndynamic human-object interaction (HOI) generation faces notable challenges,\nprimarily due to the lack of large-scale interaction data and comprehensive\ndescriptions that align with these interactions. This paper takes the\ninitiative and showcases the potential of generating human-object interactions\nwithout direct training on text-interaction pair data. Our key insight in\nachieving this is that interaction semantics and dynamics can be decoupled.\nBeing unable to learn interaction semantics through supervised training, we\ninstead leverage pre-trained large models, synergizing knowledge from a large\nlanguage model and a text-to-motion model. While such knowledge offers\nhigh-level control over interaction semantics, it cannot grasp the intricacies\nof low-level interaction dynamics. To overcome this issue, we further introduce\na world model designed to comprehend simple physics, modeling how human actions\ninfluence object motion. By integrating these components, our novel framework,\nInterDreamer, is able to generate text-aligned 3D HOI sequences in a zero-shot\nmanner. We apply InterDreamer to the BEHAVE and CHAIRS datasets, and our\ncomprehensive experimental analysis demonstrates its capability to generate\nrealistic and coherent interaction sequences that seamlessly align with the\ntext directives.\n","authors":["Sirui Xu","Ziyin Wang","Yu-Xiong Wang","Liang-Yan Gui"],"pdf_url":"https://arxiv.org/pdf/2403.19652v1.pdf","comment":"Project Page: https://sirui-xu.github.io/InterDreamer/"},{"id":"http://arxiv.org/abs/2403.19651v1","updated":"2024-03-28T17:59:20Z","published":"2024-03-28T17:59:20Z","title":"MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions","summary":"  Image retrieval, i.e., finding desired images given a reference image,\ninherently encompasses rich, multi-faceted search intents that are difficult to\ncapture solely using image-based measures. Recent work leverages text\ninstructions to allow users to more freely express their search intents.\nHowever, existing work primarily focuses on image pairs that are visually\nsimilar and/or can be characterized by a small set of pre-defined relations.\nThe core thesis of this paper is that text instructions can enable retrieving\nimages with richer relations beyond visual similarity. To show this, we\nintroduce MagicLens, a series of self-supervised image retrieval models that\nsupport open-ended instructions. MagicLens is built on a key novel insight:\nimage pairs that naturally occur on the same web pages contain a wide range of\nimplicit relations (e.g., inside view of), and we can bring those implicit\nrelations explicit by synthesizing instructions via large multimodal models\n(LMMs) and large language models (LLMs). Trained on 36.7M (query image,\ninstruction, target image) triplets with rich semantic relations mined from the\nweb, MagicLens achieves comparable or better results on eight benchmarks of\nvarious image retrieval tasks than prior state-of-the-art (SOTA) methods.\nRemarkably, it outperforms previous SOTA but with a 50X smaller model size on\nmultiple benchmarks. Additional human analyses on a 1.4M-image unseen corpus\nfurther demonstrate the diversity of search intents supported by MagicLens.\n","authors":["Kai Zhang","Yi Luan","Hexiang Hu","Kenton Lee","Siyuan Qiao","Wenhu Chen","Yu Su","Ming-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2403.19651v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2311.14097v3","updated":"2024-03-28T17:59:06Z","published":"2023-11-23T16:49:06Z","title":"ACT-Diffusion: Efficient Adversarial Consistency Training for One-step\n  Diffusion Models","summary":"  Though diffusion models excel in image generation, their step-by-step\ndenoising leads to slow generation speeds. Consistency training addresses this\nissue with single-step sampling but often produces lower-quality generations\nand requires high training costs. In this paper, we show that optimizing\nconsistency training loss minimizes the Wasserstein distance between target and\ngenerated distributions. As timestep increases, the upper bound accumulates\nprevious consistency training losses. Therefore, larger batch sizes are needed\nto reduce both current and accumulated losses. We propose Adversarial\nConsistency Training (ACT), which directly minimizes the Jensen-Shannon (JS)\ndivergence between distributions at each timestep using a discriminator.\nTheoretically, ACT enhances generation quality, and convergence. By\nincorporating a discriminator into the consistency training framework, our\nmethod achieves improved FID scores on CIFAR10 and ImageNet 64$\\times$64 and\nLSUN Cat 256$\\times$256 datasets, retains zero-shot image inpainting\ncapabilities, and uses less than $1/6$ of the original batch size and fewer\nthan $1/2$ of the model parameters and training steps compared to the baseline\nmethod, this leads to a substantial reduction in resource consumption. Our code\nis available:https://github.com/kong13661/ACT\n","authors":["Fei Kong","Jinhao Duan","Lichao Sun","Hao Cheng","Renjing Xu","Hengtao Shen","Xiaofeng Zhu","Xiaoshuang Shi","Kaidi Xu"],"pdf_url":"https://arxiv.org/pdf/2311.14097v3.pdf","comment":"To appear in CVPR 2024"},{"id":"http://arxiv.org/abs/2403.19649v1","updated":"2024-03-28T17:57:27Z","published":"2024-03-28T17:57:27Z","title":"GraspXL: Generating Grasping Motions for Diverse Objects at Scale","summary":"  Human hands possess the dexterity to interact with diverse objects such as\ngrasping specific parts of the objects and/or approaching them from desired\ndirections. More importantly, humans can grasp objects of any shape without\nobject-specific skills. Recent works synthesize grasping motions following\nsingle objectives such as a desired approach heading direction or a grasping\narea. Moreover, they usually rely on expensive 3D hand-object data during\ntraining and inference, which limits their capability to synthesize grasping\nmotions for unseen objects at scale. In this paper, we unify the generation of\nhand-object grasping motions across multiple motion objectives, diverse object\nshapes and dexterous hand morphologies in a policy learning framework GraspXL.\nThe objectives are composed of the graspable area, heading direction during\napproach, wrist rotation, and hand position. Without requiring any 3D\nhand-object interaction data, our policy trained with 58 objects can robustly\nsynthesize diverse grasping motions for more than 500k unseen objects with a\nsuccess rate of 82.2%. At the same time, the policy adheres to objectives,\nwhich enables the generation of diverse grasps per object. Moreover, we show\nthat our framework can be deployed to different dexterous hands and work with\nreconstructed or generated objects. We quantitatively and qualitatively\nevaluate our method to show the efficacy of our approach. Our model and code\nwill be available.\n","authors":["Hui Zhang","Sammy Christen","Zicong Fan","Otmar Hilliges","Jie Song"],"pdf_url":"https://arxiv.org/pdf/2403.19649v1.pdf","comment":"Project Page: https://eth-ait.github.io/graspxl/"},{"id":"http://arxiv.org/abs/2403.19646v1","updated":"2024-03-28T17:55:42Z","published":"2024-03-28T17:55:42Z","title":"Change-Agent: Towards Interactive Comprehensive Change Interpretation\n  and Analysis from Change Detection and Change Captioning","summary":"  Monitoring changes in the Earth's surface is crucial for understanding\nnatural processes and human impacts, necessitating precise and comprehensive\ninterpretation methodologies. Remote sensing satellite imagery offers a unique\nperspective for monitoring these changes, leading to the emergence of remote\nsensing image change interpretation (RSICI) as a significant research focus.\nCurrent RSICI technology encompasses change detection and change captioning,\neach with its limitations in providing comprehensive interpretation. To address\nthis, we propose an interactive Change-Agent which integrates a multi-level\nchange interpretation (MCI) model as eyes and a large language model (LLM) as\nthe brain. Our Change-Agent can follow user instructions to achieve\ncomprehensive change interpretation and insightful analysis according to user\ninstructions, such as change detection and change captioning, change object\ncounting, change cause analysis, etc. Our proposed MCI model contains two\nbranches of pixel-level change detection and semantic-level change captioning,\nin which multiple BI-temporal Iterative Interaction (BI3) layers utilize Local\nPerception Enhancement (LPE) and the Global Difference Fusion Attention (GDFA)\nmodules to enhance the model's discriminative feature representation\ncapabilities. To train the MCI model, we build the LEVIR-MCI dataset with\nchange masks and captions of bi-temporal images. Extensive experiments\ndemonstrate the effectiveness of the proposed change interpretation model and\nhighlight the promising potential of our Change-Agent in facilitating\ncomprehensive and intelligent interpretation of surface changes. We will make\nour dataset and codebase of the change interpretation model and Change-Agent\npublicly available to facilitate future research at\nhttps://github.com/Chen-Yang-Liu/Change-Agent\n","authors":["Chenyang Liu","Keyan Chen","Haotian Zhang","Zipeng Qi","Zhengxia Zou","Zhenwei Shi"],"pdf_url":"https://arxiv.org/pdf/2403.19646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.01362v3","updated":"2024-03-28T17:55:39Z","published":"2023-07-03T21:33:40Z","title":"Direct Superpoints Matching for Robust Point Cloud Registration","summary":"  Deep neural networks endow the downsampled superpoints with highly\ndiscriminative feature representations. Previous dominant point cloud\nregistration approaches match these feature representations as the first step,\ne.g., using the Sinkhorn algorithm. A RANSAC-like method is then usually\nadopted as a post-processing refinement to filter the outliers. Other dominant\nmethod is to directly predict the superpoint matchings using learned MLP\nlayers. Both of them have drawbacks: RANSAC-based methods are computationally\nintensive and prediction-based methods suffer from outputing non-existing\npoints in the point cloud. In this paper, we propose a straightforward and\neffective baseline to find correspondences of superpoints in a global matching\nmanner. We employ the normalized matching scores as weights for each\ncorrespondence, allowing us to reject the outliers and further weigh the rest\ninliers when fitting the transformation matrix without relying on the\ncumbersome RANSAC. Moreover, the entire model can be trained in an end-to-end\nfashion, leading to better accuracy. Our simple yet effective baseline shows\ncomparable or even better results than state-of-the-art methods on three\ndatasets including ModelNet, 3DMatch, and KITTI. We do not advocate our\napproach to be \\emph{the} solution for point cloud registration but use the\nresults to emphasize the role of matching strategy for point cloud\nregistration. The code and models are available at\nhttps://github.com/neu-vi/Superpoints_Registration.\n","authors":["Aniket Gupta","Yiming Xie","Hanumant Singh","Huaizu Jiang"],"pdf_url":"https://arxiv.org/pdf/2307.01362v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19645v1","updated":"2024-03-28T17:55:16Z","published":"2024-03-28T17:55:16Z","title":"GANTASTIC: GAN-based Transfer of Interpretable Directions for\n  Disentangled Image Editing in Text-to-Image Diffusion Models","summary":"  The rapid advancement in image generation models has predominantly been\ndriven by diffusion models, which have demonstrated unparalleled success in\ngenerating high-fidelity, diverse images from textual prompts. Despite their\nsuccess, diffusion models encounter substantial challenges in the domain of\nimage editing, particularly in executing disentangled edits-changes that target\nspecific attributes of an image while leaving irrelevant parts untouched. In\ncontrast, Generative Adversarial Networks (GANs) have been recognized for their\nsuccess in disentangled edits through their interpretable latent spaces. We\nintroduce GANTASTIC, a novel framework that takes existing directions from\npre-trained GAN models-representative of specific, controllable attributes-and\ntransfers these directions into diffusion-based models. This novel approach not\nonly maintains the generative quality and diversity that diffusion models are\nknown for but also significantly enhances their capability to perform precise,\ntargeted image edits, thereby leveraging the best of both worlds.\n","authors":["Yusuf Dalva","Hidir Yesiltepe","Pinar Yanardag"],"pdf_url":"https://arxiv.org/pdf/2403.19645v1.pdf","comment":"Project page: https://gantastic.github.io"},{"id":"http://arxiv.org/abs/2304.09704v2","updated":"2024-03-28T17:53:08Z","published":"2023-04-19T14:49:31Z","title":"Learnable Earth Parser: Discovering 3D Prototypes in Aerial Scans","summary":"  We propose an unsupervised method for parsing large 3D scans of real-world\nscenes with easily-interpretable shapes. This work aims to provide a practical\ntool for analyzing 3D scenes in the context of aerial surveying and mapping,\nwithout the need for user annotations. Our approach is based on a probabilistic\nreconstruction model that decomposes an input 3D point cloud into a small set\nof learned prototypical 3D shapes. The resulting reconstruction is visually\ninterpretable and can be used to perform unsupervised instance and low-shot\nsemantic segmentation of complex scenes. We demonstrate the usefulness of our\nmodel on a novel dataset of seven large aerial LiDAR scans from diverse\nreal-world scenarios. Our approach outperforms state-of-the-art unsupervised\nmethods in terms of decomposition accuracy while remaining visually\ninterpretable. Our code and dataset are available at\nhttps://romainloiseau.fr/learnable-earth-parser/\n","authors":["Romain Loiseau","Elliot Vincent","Mathieu Aubry","Loic Landrieu"],"pdf_url":"https://arxiv.org/pdf/2304.09704v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19638v1","updated":"2024-03-28T17:52:24Z","published":"2024-03-28T17:52:24Z","title":"Siamese Vision Transformers are Scalable Audio-visual Learners","summary":"  Traditional audio-visual methods rely on independent audio and visual\nbackbones, which is costly and not scalable. In this work, we investigate using\nan audio-visual siamese network (AVSiam) for efficient and scalable\naudio-visual pretraining. Our framework uses a single shared vision transformer\nbackbone to process audio and visual inputs, improving its parameter\nefficiency, reducing the GPU memory footprint, and allowing us to scale our\nmethod to larger datasets and model sizes. We pretrain our model using a\ncontrastive audio-visual matching objective with a multi-ratio random masking\nscheme, which enables our model to process larger audio-visual instance\nbatches, helpful for contrastive learning. Unlike prior audio-visual methods,\nour method can robustly handle audio, visual, and audio-visual inputs with a\nsingle shared ViT backbone. Furthermore, despite using the shared backbone for\nboth modalities, AVSiam achieves competitive or even better results than prior\nmethods on AudioSet and VGGSound for audio-visual classification and retrieval.\nOur code is available at https://github.com/GenjiB/AVSiam\n","authors":["Yan-Bo Lin","Gedas Bertasius"],"pdf_url":"https://arxiv.org/pdf/2403.19638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19632v1","updated":"2024-03-28T17:47:31Z","published":"2024-03-28T17:47:31Z","title":"GauStudio: A Modular Framework for 3D Gaussian Splatting and Beyond","summary":"  We present GauStudio, a novel modular framework for modeling 3D Gaussian\nSplatting (3DGS) to provide standardized, plug-and-play components for users to\neasily customize and implement a 3DGS pipeline. Supported by our framework, we\npropose a hybrid Gaussian representation with foreground and skyball background\nmodels. Experiments demonstrate this representation reduces artifacts in\nunbounded outdoor scenes and improves novel view synthesis. Finally, we propose\nGaussian Splatting Surface Reconstruction (GauS), a novel render-then-fuse\napproach for high-fidelity mesh reconstruction from 3DGS inputs without\nfine-tuning. Overall, our GauStudio framework, hybrid representation, and GauS\napproach enhance 3DGS modeling and rendering capabilities, enabling\nhigher-quality novel view synthesis and surface reconstruction.\n","authors":["Chongjie Ye","Yinyu Nie","Jiahao Chang","Yuantao Chen","Yihao Zhi","Xiaoguang Han"],"pdf_url":"https://arxiv.org/pdf/2403.19632v1.pdf","comment":"Code: https://github.com/GAP-LAB-CUHK-SZ/gaustudio"},{"id":"http://arxiv.org/abs/2403.19622v1","updated":"2024-03-28T17:42:54Z","published":"2024-03-28T17:42:54Z","title":"RH20T-P: A Primitive-Level Robotic Dataset Towards Composable\n  Generalization Agents","summary":"  The ultimate goals of robotic learning is to acquire a comprehensive and\ngeneralizable robotic system capable of performing both seen skills within the\ntraining distribution and unseen skills in novel environments. Recent progress\nin utilizing language models as high-level planners has demonstrated that the\ncomplexity of tasks can be reduced through decomposing them into\nprimitive-level plans, making it possible to generalize on novel robotic tasks\nin a composable manner. Despite the promising future, the community is not yet\nadequately prepared for composable generalization agents, particularly due to\nthe lack of primitive-level real-world robotic datasets. In this paper, we\npropose a primitive-level robotic dataset, namely RH20T-P, which contains about\n33000 video clips covering 44 diverse and complicated robotic tasks. Each clip\nis manually annotated according to a set of meticulously designed primitive\nskills, facilitating the future development of composable generalization\nagents. To validate the effectiveness of RH20T-P, we also construct a potential\nand scalable agent based on RH20T-P, called RA-P. Equipped with two planners\nspecialized in task decomposition and motion planning, RA-P can adapt to novel\nphysical skills through composable generalization. Our website and videos can\nbe found at https://sites.google.com/view/rh20t-primitive/main. Dataset and\ncode will be made available soon.\n","authors":["Zeren Chen","Zhelun Shi","Xiaoya Lu","Lehan He","Sucheng Qian","Hao Shu Fang","Zhenfei Yin","Wanli Ouyang","Jing Shao","Yu Qiao","Cewu Lu","Lu Sheng"],"pdf_url":"https://arxiv.org/pdf/2403.19622v1.pdf","comment":"24 pages, 12 figures, 6 tables"},{"id":"http://arxiv.org/abs/2403.19620v1","updated":"2024-03-28T17:40:15Z","published":"2024-03-28T17:40:15Z","title":"Collaborative Interactive Evolution of Art in the Latent Space of Deep\n  Generative Models","summary":"  Generative Adversarial Networks (GANs) have shown great success in generating\nhigh quality images and are thus used as one of the main approaches to generate\nart images. However, usually the image generation process involves sampling\nfrom the latent space of the learned art representations, allowing little\ncontrol over the output. In this work, we first employ GANs that are trained to\nproduce creative images using an architecture known as Creative Adversarial\nNetworks (CANs), then, we employ an evolutionary approach to navigate within\nthe latent space of the models to discover images. We use automatic aesthetic\nand collaborative interactive human evaluation metrics to assess the generated\nimages. In the human interactive evaluation case, we propose a collaborative\nevaluation based on the assessments of several participants. Furthermore, we\nalso experiment with an intelligent mutation operator that aims to improve the\nquality of the images through local search based on an aesthetic measure. We\nevaluate the effectiveness of this approach by comparing the results produced\nby the automatic and collaborative interactive evolution. The results show that\nthe proposed approach can generate highly attractive art images when the\nevolution is guided by collaborative human feedback.\n","authors":["Ole Hall","Anil Yaman"],"pdf_url":"https://arxiv.org/pdf/2403.19620v1.pdf","comment":"Preprint. The Version of Record of this contribution is to be\n  published in the proceedings of the 13th International Conference on\n  Artificial Intelligence in Music, Sound, Art and Design (EvoMUSART) 2024"},{"id":"http://arxiv.org/abs/2304.09224v2","updated":"2024-03-28T17:36:50Z","published":"2023-04-18T18:23:20Z","title":"Quantum machine learning for image classification","summary":"  Image classification, a pivotal task in multiple industries, faces\ncomputational challenges due to the burgeoning volume of visual data. This\nresearch addresses these challenges by introducing two quantum machine learning\nmodels that leverage the principles of quantum mechanics for effective\ncomputations. Our first model, a hybrid quantum neural network with parallel\nquantum circuits, enables the execution of computations even in the noisy\nintermediate-scale quantum era, where circuits with a large number of qubits\nare currently infeasible. This model demonstrated a record-breaking\nclassification accuracy of 99.21% on the full MNIST dataset, surpassing the\nperformance of known quantum-classical models, while having eight times fewer\nparameters than its classical counterpart. Also, the results of testing this\nhybrid model on a Medical MNIST (classification accuracy over 99%), and on\nCIFAR-10 (classification accuracy over 82%), can serve as evidence of the\ngeneralizability of the model and highlights the efficiency of quantum layers\nin distinguishing common features of input data. Our second model introduces a\nhybrid quantum neural network with a Quanvolutional layer, reducing image\nresolution via a convolution process. The model matches the performance of its\nclassical counterpart, having four times fewer trainable parameters, and\noutperforms a classical model with equal weight parameters. These models\nrepresent advancements in quantum machine learning research and illuminate the\npath towards more accurate image classification systems.\n","authors":["Arsenii Senokosov","Alexandr Sedykh","Asel Sagingalieva","Basil Kyriacou","Alexey Melnikov"],"pdf_url":"https://arxiv.org/pdf/2304.09224v2.pdf","comment":"13 pages, 10 figures, 1 table"},{"id":"http://arxiv.org/abs/2312.07360v2","updated":"2024-03-28T17:35:29Z","published":"2023-12-12T15:30:24Z","title":"Boosting Latent Diffusion with Flow Matching","summary":"  Recently, there has been tremendous progress in visual synthesis and the\nunderlying generative models. Here, diffusion models (DMs) stand out\nparticularly, but lately, flow matching (FM) has also garnered considerable\ninterest. While DMs excel in providing diverse images, they suffer from long\ntraining and slow generation. With latent diffusion, these issues are only\npartially alleviated. Conversely, FM offers faster training and inference but\nexhibits less diversity in synthesis. We demonstrate that introducing FM\nbetween the Diffusion model and the convolutional decoder offers\nhigh-resolution image synthesis with reduced computational cost and model size.\nDiffusion can then efficiently provide the necessary generation diversity. FM\ncompensates for the lower resolution, mapping the small latent space to a\nhigh-dimensional one. Subsequently, the convolutional decoder of the LDM maps\nthese latents to high-resolution images. By combining the diversity of DMs, the\nefficiency of FMs, and the effectiveness of convolutional decoders, we achieve\nstate-of-the-art high-resolution image synthesis at $1024^2$ with minimal\ncomputational cost. Importantly, our approach is orthogonal to recent\napproximation and speed-up strategies for the underlying DMs, making it easily\nintegrable into various DM frameworks.\n","authors":["Johannes S. Fischer","Ming Gui","Pingchuan Ma","Nick Stracke","Stefan A. Baumann","Björn Ommer"],"pdf_url":"https://arxiv.org/pdf/2312.07360v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19615v1","updated":"2024-03-28T17:32:58Z","published":"2024-03-28T17:32:58Z","title":"SA-GS: Scale-Adaptive Gaussian Splatting for Training-Free Anti-Aliasing","summary":"  In this paper, we present a Scale-adaptive method for Anti-aliasing Gaussian\nSplatting (SA-GS). While the state-of-the-art method Mip-Splatting needs\nmodifying the training procedure of Gaussian splatting, our method functions at\ntest-time and is training-free. Specifically, SA-GS can be applied to any\npretrained Gaussian splatting field as a plugin to significantly improve the\nfield's anti-alising performance. The core technique is to apply 2D\nscale-adaptive filters to each Gaussian during test time. As pointed out by\nMip-Splatting, observing Gaussians at different frequencies leads to mismatches\nbetween the Gaussian scales during training and testing. Mip-Splatting resolves\nthis issue using 3D smoothing and 2D Mip filters, which are unfortunately not\naware of testing frequency. In this work, we show that a 2D scale-adaptive\nfilter that is informed of testing frequency can effectively match the Gaussian\nscale, thus making the Gaussian primitive distribution remain consistent across\ndifferent testing frequencies. When scale inconsistency is eliminated, sampling\nrates smaller than the scene frequency result in conventional jaggedness, and\nwe propose to integrate the projected 2D Gaussian within each pixel during\ntesting. This integration is actually a limiting case of super-sampling, which\nsignificantly improves anti-aliasing performance over vanilla Gaussian\nSplatting. Through extensive experiments using various settings and both\nbounded and unbounded scenes, we show SA-GS performs comparably with or better\nthan Mip-Splatting. Note that super-sampling and integration are only effective\nwhen our scale-adaptive filtering is activated. Our codes, data and models are\navailable at https://github.com/zsy1987/SA-GS.\n","authors":["Xiaowei Song","Jv Zheng","Shiran Yuan","Huan-ang Gao","Jingwei Zhao","Xiang He","Weihao Gu","Hao Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.19615v1.pdf","comment":"Project page: https://kevinsong729.github.io/project-pages/SA-GS/\n  Code: https://github.com/zsy1987/SA-GS"},{"id":"http://arxiv.org/abs/2403.19612v1","updated":"2024-03-28T17:32:01Z","published":"2024-03-28T17:32:01Z","title":"ILPO-NET: Network for the invariant recognition of arbitrary volumetric\n  patterns in 3D","summary":"  Effective recognition of spatial patterns and learning their hierarchy is\ncrucial in modern spatial data analysis. Volumetric data applications seek\ntechniques ensuring invariance not only to shifts but also to pattern\nrotations. While traditional methods can readily achieve translational\ninvariance, rotational invariance possesses multiple challenges and remains an\nactive area of research. Here, we present ILPO-Net (Invariant to Local Patterns\nOrientation Network), a novel approach that handles arbitrarily shaped patterns\nwith the convolutional operation inherently invariant to local spatial pattern\norientations using the Wigner matrix expansions. Our architecture seamlessly\nintegrates the new convolution operator and, when benchmarked on diverse\nvolumetric datasets such as MedMNIST and CATH, demonstrates superior\nperformance over the baselines with significantly reduced parameter counts - up\nto 1000 times fewer in the case of MedMNIST. Beyond these demonstrations,\nILPO-Net's rotational invariance paves the way for other applications across\nmultiple disciplines. Our code is publicly available at\nhttps://gricad-gitlab.univ-grenoble-alpes.fr/GruLab/ILPONet.\n","authors":["Dmitrii Zhemchuzhnikov","Sergei Grudinin"],"pdf_url":"https://arxiv.org/pdf/2403.19612v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19611v1","updated":"2024-03-28T17:31:23Z","published":"2024-03-28T17:31:23Z","title":"Nearest Neighbor Classication for Classical Image Upsampling","summary":"  Given a set of ordered pixel data in the form of an image, our goal is to\nperform upsampling on the data such that: the resulting resolution is improved\nby some factor, the final result passes the human test, having added new,\nbelievable, and realistic information and detail to the image, the time\ncomplexity for upscaling is relatively close to that of lossy upscaling\nimplementations.\n","authors":["Evan Matthews","Nicolas Prate"],"pdf_url":"https://arxiv.org/pdf/2403.19611v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2403.19607v1","updated":"2024-03-28T17:28:32Z","published":"2024-03-28T17:28:32Z","title":"SAID-NeRF: Segmentation-AIDed NeRF for Depth Completion of Transparent\n  Objects","summary":"  Acquiring accurate depth information of transparent objects using\noff-the-shelf RGB-D cameras is a well-known challenge in Computer Vision and\nRobotics. Depth estimation/completion methods are typically employed and\ntrained on datasets with quality depth labels acquired from either simulation,\nadditional sensors or specialized data collection setups and known 3d models.\nHowever, acquiring reliable depth information for datasets at scale is not\nstraightforward, limiting training scalability and generalization. Neural\nRadiance Fields (NeRFs) are learning-free approaches and have demonstrated wide\nsuccess in novel view synthesis and shape recovery. However, heuristics and\ncontrolled environments (lights, backgrounds, etc) are often required to\naccurately capture specular surfaces. In this paper, we propose using Visual\nFoundation Models (VFMs) for segmentation in a zero-shot, label-free way to\nguide the NeRF reconstruction process for these objects via the simultaneous\nreconstruction of semantic fields and extensions to increase robustness. Our\nproposed method Segmentation-AIDed NeRF (SAID-NeRF) shows significant\nperformance on depth completion datasets for transparent objects and robotic\ngrasping.\n","authors":["Avinash Ummadisingu","Jongkeum Choi","Koki Yamane","Shimpei Masuda","Naoki Fukaya","Kuniyuki Takahashi"],"pdf_url":"https://arxiv.org/pdf/2403.19607v1.pdf","comment":"8 pages. An accompanying video is available at\n  https://www.youtube.com/watch?v=S4NCoUq4bmE"},{"id":"http://arxiv.org/abs/2403.19603v1","updated":"2024-03-28T17:27:44Z","published":"2024-03-28T17:27:44Z","title":"Semantic Map-based Generation of Navigation Instructions","summary":"  We are interested in the generation of navigation instructions, either in\ntheir own right or as training material for robotic navigation task. In this\npaper, we propose a new approach to navigation instruction generation by\nframing the problem as an image captioning task using semantic maps as visual\ninput. Conventional approaches employ a sequence of panorama images to generate\nnavigation instructions. Semantic maps abstract away from visual details and\nfuse the information in multiple panorama images into a single top-down\nrepresentation, thereby reducing computational complexity to process the input.\nWe present a benchmark dataset for instruction generation using semantic maps,\npropose an initial model and ask human subjects to manually assess the quality\nof generated instructions. Our initial investigations show promise in using\nsemantic maps for instruction generation instead of a sequence of panorama\nimages, but there is vast scope for improvement. We release the code for data\npreparation and model training at https://github.com/chengzu-li/VLGen.\n","authors":["Chengzu Li","Chao Zhang","Simone Teufel","Rama Sanand Doddipatla","Svetlana Stoyanchev"],"pdf_url":"https://arxiv.org/pdf/2403.19603v1.pdf","comment":"5 pages, 2 figures, 3 tables (13 pages, 3 figures, 5 tables including\n  references and appendices), accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2311.11278v2","updated":"2024-03-28T17:25:51Z","published":"2023-11-19T09:41:10Z","title":"Transcending Forgery Specificity with Latent Space Augmentation for\n  Generalizable Deepfake Detection","summary":"  Deepfake detection faces a critical generalization hurdle, with performance\ndeteriorating when there is a mismatch between the distributions of training\nand testing data. A broadly received explanation is the tendency of these\ndetectors to be overfitted to forgery-specific artifacts, rather than learning\nfeatures that are widely applicable across various forgeries. To address this\nissue, we propose a simple yet effective detector called LSDA\n(\\underline{L}atent \\underline{S}pace \\underline{D}ata\n\\underline{A}ugmentation), which is based on a heuristic idea: representations\nwith a wider variety of forgeries should be able to learn a more generalizable\ndecision boundary, thereby mitigating the overfitting of method-specific\nfeatures (see Fig.~\\ref{fig:toy}). Following this idea, we propose to enlarge\nthe forgery space by constructing and simulating variations within and across\nforgery features in the latent space. This approach encompasses the acquisition\nof enriched, domain-specific features and the facilitation of smoother\ntransitions between different forgery types, effectively bridging domain gaps.\nOur approach culminates in refining a binary classifier that leverages the\ndistilled knowledge from the enhanced features, striving for a generalizable\ndeepfake detector. Comprehensive experiments show that our proposed method is\nsurprisingly effective and transcends state-of-the-art detectors across several\nwidely used benchmarks.\n","authors":["Zhiyuan Yan","Yuhao Luo","Siwei Lyu","Qingshan Liu","Baoyuan Wu"],"pdf_url":"https://arxiv.org/pdf/2311.11278v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19600v1","updated":"2024-03-28T17:23:45Z","published":"2024-03-28T17:23:45Z","title":"Enhance Image Classification via Inter-Class Image Mixup with Diffusion\n  Model","summary":"  Text-to-image (T2I) generative models have recently emerged as a powerful\ntool, enabling the creation of photo-realistic images and giving rise to a\nmultitude of applications. However, the effective integration of T2I models\ninto fundamental image classification tasks remains an open question. A\nprevalent strategy to bolster image classification performance is through\naugmenting the training set with synthetic images generated by T2I models. In\nthis study, we scrutinize the shortcomings of both current generative and\nconventional data augmentation techniques. Our analysis reveals that these\nmethods struggle to produce images that are both faithful (in terms of\nforeground objects) and diverse (in terms of background contexts) for\ndomain-specific concepts. To tackle this challenge, we introduce an innovative\ninter-class data augmentation method known as Diff-Mix\n(https://github.com/Zhicaiwww/Diff-Mix), which enriches the dataset by\nperforming image translations between classes. Our empirical results\ndemonstrate that Diff-Mix achieves a better balance between faithfulness and\ndiversity, leading to a marked improvement in performance across diverse image\nclassification scenarios, including few-shot, conventional, and long-tail\nclassifications for domain-specific datasets.\n","authors":["Zhicai Wang","Longhui Wei","Tan Wang","Heyu Chen","Yanbin Hao","Xiang Wang","Xiangnan He","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2403.19600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17048v2","updated":"2024-03-28T17:23:15Z","published":"2023-11-28T18:55:37Z","title":"Zero-shot Referring Expression Comprehension via Structural Similarity\n  Between Images and Captions","summary":"  Zero-shot referring expression comprehension aims at localizing bounding\nboxes in an image corresponding to provided textual prompts, which requires:\n(i) a fine-grained disentanglement of complex visual scene and textual context,\nand (ii) a capacity to understand relationships among disentangled entities.\nUnfortunately, existing large vision-language alignment (VLA) models, e.g.,\nCLIP, struggle with both aspects so cannot be directly used for this task. To\nmitigate this gap, we leverage large foundation models to disentangle both\nimages and texts into triplets in the format of (subject, predicate, object).\nAfter that, grounding is accomplished by calculating the structural similarity\nmatrix between visual and textual triplets with a VLA model, and subsequently\npropagate it to an instance-level similarity matrix. Furthermore, to equip VLA\nmodels with the ability of relationship understanding, we design a\ntriplet-matching objective to fine-tune the VLA models on a collection of\ncurated dataset containing abundant entity relationships. Experiments\ndemonstrate that our visual grounding performance increase of up to 19.5% over\nthe SOTA zero-shot model on RefCOCO/+/g. On the more challenging Who's Waldo\ndataset, our zero-shot approach achieves comparable accuracy to the fully\nsupervised model. Code is available at\nhttps://github.com/Show-han/Zeroshot_REC.\n","authors":["Zeyu Han","Fangrui Zhu","Qianru Lao","Huaizu Jiang"],"pdf_url":"https://arxiv.org/pdf/2311.17048v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.19596v1","updated":"2024-03-28T17:20:39Z","published":"2024-03-28T17:20:39Z","title":"LocCa: Visual Pretraining with Location-aware Captioners","summary":"  Image captioning has been shown as an effective pretraining method similar to\ncontrastive pretraining. However, the incorporation of location-aware\ninformation into visual pretraining remains an area with limited research. In\nthis paper, we propose a simple visual pretraining method with location-aware\ncaptioners (LocCa). LocCa uses a simple image captioner task interface, to\nteach a model to read out rich information, i.e. bounding box coordinates, and\ncaptions, conditioned on the image pixel input. Thanks to the multitask\ncapabilities of an encoder-decoder architecture, we show that an image\ncaptioner can easily handle multiple tasks during pretraining. Our experiments\ndemonstrate that LocCa outperforms standard captioners significantly on\nlocalization downstream tasks while maintaining comparable performance on\nholistic tasks.\n","authors":["Bo Wan","Michael Tschannen","Yongqin Xian","Filip Pavetic","Ibrahim Alabdulmohsin","Xiao Wang","André Susano Pinto","Andreas Steiner","Lucas Beyer","Xiaohua Zhai"],"pdf_url":"https://arxiv.org/pdf/2403.19596v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19595v1","updated":"2024-03-28T17:19:16Z","published":"2024-03-28T17:19:16Z","title":"Situation Awareness for Driver-Centric Driving Style Adaptation","summary":"  There is evidence that the driving style of an autonomous vehicle is\nimportant to increase the acceptance and trust of the passengers. The driving\nsituation has been found to have a significant influence on human driving\nbehavior. However, current driving style models only partially incorporate\ndriving environment information, limiting the alignment between an agent and\nthe given situation. Therefore, we propose a situation-aware driving style\nmodel based on different visual feature encoders pretrained on fleet data, as\nwell as driving behavior predictors, which are adapted to the driving style of\na specific driver. Our experiments show that the proposed method outperforms\nstatic driving styles significantly and forms plausible situation clusters.\nFurthermore, we found that feature encoders pretrained on our dataset lead to\nmore precise driving behavior modeling. In contrast, feature encoders\npretrained supervised and unsupervised on different data sources lead to more\nspecific situation clusters, which can be utilized to constrain and control the\ndriving style adaptation for specific situations. Moreover, in a real-world\nsetting, where driving style adaptation is happening iteratively, we found the\nMLP-based behavior predictors achieve good performance initially but suffer\nfrom catastrophic forgetting. In contrast, behavior predictors based on\nsituationdependent statistics can learn iteratively from continuous data\nstreams by design. Overall, our experiments show that important information for\ndriving behavior prediction is contained within the visual feature encoder. The\ndataset is publicly available at\nhuggingface.co/datasets/jHaselberger/SADC-Situation-Awareness-for-Driver-Centric-Driving-Style-Adaptation.\n","authors":["Johann Haselberger","Bonifaz Stuhr","Bernhard Schick","Steffen Müller"],"pdf_url":"https://arxiv.org/pdf/2403.19595v1.pdf","comment":"14 pages, 6 figures. This work has been submitted to the IEEE for\n  possible publication. Copyright may be transferred without notice, after\n  which this version may no longer be accessible"},{"id":"http://arxiv.org/abs/2403.19593v1","updated":"2024-03-28T17:15:23Z","published":"2024-03-28T17:15:23Z","title":"Frame by Familiar Frame: Understanding Replication in Video Diffusion\n  Models","summary":"  Building on the momentum of image generation diffusion models, there is an\nincreasing interest in video-based diffusion models. However, video generation\nposes greater challenges due to its higher-dimensional nature, the scarcity of\ntraining data, and the complex spatiotemporal relationships involved. Image\ngeneration models, due to their extensive data requirements, have already\nstrained computational resources to their limits. There have been instances of\nthese models reproducing elements from the training samples, leading to\nconcerns and even legal disputes over sample replication. Video diffusion\nmodels, which operate with even more constrained datasets and are tasked with\ngenerating both spatial and temporal content, may be more prone to replicating\nsamples from their training sets. Compounding the issue, these models are often\nevaluated using metrics that inadvertently reward replication. In our paper, we\npresent a systematic investigation into the phenomenon of sample replication in\nvideo diffusion models. We scrutinize various recent diffusion models for video\nsynthesis, assessing their tendency to replicate spatial and temporal content\nin both unconditional and conditional generation scenarios. Our study\nidentifies strategies that are less likely to lead to replication. Furthermore,\nwe propose new evaluation strategies that take replication into account,\noffering a more accurate measure of a model's ability to generate the original\ncontent.\n","authors":["Aimon Rahman","Malsha V. Perera","Vishal M. Patel"],"pdf_url":"https://arxiv.org/pdf/2403.19593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05950v2","updated":"2024-03-28T17:14:53Z","published":"2024-03-09T16:05:31Z","title":"Classifying Objects in 3D Point Clouds Using Recurrent Neural Network: A\n  GRU LSTM Hybrid Approach","summary":"  Accurate classification of objects in 3D point clouds is a significant\nproblem in several applications, such as autonomous navigation and\naugmented/virtual reality scenarios, which has become a research hot spot. In\nthis paper, we presented a deep learning strategy for 3D object classification\nin augmented reality. The proposed approach is a combination of the GRU and\nLSTM. LSTM networks learn longer dependencies well, but due to the number of\ngates, it takes longer to train; on the other hand, GRU networks have a weaker\nperformance than LSTM, but their training speed is much higher than GRU, which\nis The speed is due to its fewer gates. The proposed approach used the\ncombination of speed and accuracy of these two networks. The proposed approach\nachieved an accuracy of 0.99 in the 4,499,0641 points dataset, which includes\neight classes (unlabeled, man-made terrain, natural terrain, high vegetation,\nlow vegetation, buildings, hardscape, scanning artifacts, cars). Meanwhile, the\ntraditional machine learning approaches could achieve a maximum accuracy of\n0.9489 in the best case. Keywords: Point Cloud Classification, Virtual Reality,\nHybrid Model, GRULSTM, GRU, LSTM\n","authors":["Ramin Mousa","Mitra Khezli","Mohamadreza Azadi","Vahid Nikoofard","Saba Hesaraki"],"pdf_url":"https://arxiv.org/pdf/2403.05950v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19589v1","updated":"2024-03-28T17:12:55Z","published":"2024-03-28T17:12:55Z","title":"TOD3Cap: Towards 3D Dense Captioning in Outdoor Scenes","summary":"  3D dense captioning stands as a cornerstone in achieving a comprehensive\nunderstanding of 3D scenes through natural language. It has recently witnessed\nremarkable achievements, particularly in indoor settings. However, the\nexploration of 3D dense captioning in outdoor scenes is hindered by two major\nchallenges: 1) the \\textbf{domain gap} between indoor and outdoor scenes, such\nas dynamics and sparse visual inputs, makes it difficult to directly adapt\nexisting indoor methods; 2) the \\textbf{lack of data} with comprehensive\nbox-caption pair annotations specifically tailored for outdoor scenes. To this\nend, we introduce the new task of outdoor 3D dense captioning. As input, we\nassume a LiDAR point cloud and a set of RGB images captured by the panoramic\ncamera rig. The expected output is a set of object boxes with captions. To\ntackle this task, we propose the TOD3Cap network, which leverages the BEV\nrepresentation to generate object box proposals and integrates Relation\nQ-Former with LLaMA-Adapter to generate rich captions for these objects. We\nalso introduce the TOD3Cap dataset, the largest one to our knowledge for 3D\ndense captioning in outdoor scenes, which contains 2.3M descriptions of 64.3K\noutdoor objects from 850 scenes. Notably, our TOD3Cap network can effectively\nlocalize and caption 3D objects in outdoor scenes, which outperforms baseline\nmethods by a significant margin (+9.6 CiDEr@0.5IoU). Code, data, and models are\npublicly available at https://github.com/jxbbb/TOD3Cap.\n","authors":["Bu Jin","Yupeng Zheng","Pengfei Li","Weize Li","Yuhang Zheng","Sujie Hu","Xinyu Liu","Jinwei Zhu","Zhijie Yan","Haiyang Sun","Kun Zhan","Peng Jia","Xiaoxiao Long","Yilun Chen","Hao Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.19589v1.pdf","comment":"Code, data, and models are publicly available at\n  https://github.com/jxbbb/TOD3Cap"},{"id":"http://arxiv.org/abs/2403.19588v1","updated":"2024-03-28T17:12:39Z","published":"2024-03-28T17:12:39Z","title":"DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs","summary":"  This paper revives Densely Connected Convolutional Networks (DenseNets) and\nreveals the underrated effectiveness over predominant ResNet-style\narchitectures. We believe DenseNets' potential was overlooked due to untouched\ntraining methods and traditional design elements not fully revealing their\ncapabilities. Our pilot study shows dense connections through concatenation are\nstrong, demonstrating that DenseNets can be revitalized to compete with modern\narchitectures. We methodically refine suboptimal components - architectural\nadjustments, block redesign, and improved training recipes towards widening\nDenseNets and boosting memory efficiency while keeping concatenation shortcuts.\nOur models, employing simple architectural elements, ultimately surpass Swin\nTransformer, ConvNeXt, and DeiT-III - key architectures in the residual\nlearning lineage. Furthermore, our models exhibit near state-of-the-art\nperformance on ImageNet-1K, competing with the very recent models and\ndownstream tasks, ADE20k semantic segmentation, and COCO object\ndetection/instance segmentation. Finally, we provide empirical analyses that\nuncover the merits of the concatenation over additive shortcuts, steering a\nrenewed preference towards DenseNet-style designs. Our code is available at\nhttps://github.com/naver-ai/rdnet.\n","authors":["Donghyun Kim","Byeongho Heo","Dongyoon Han"],"pdf_url":"https://arxiv.org/pdf/2403.19588v1.pdf","comment":"Code at https://github.com/naver-ai/rdnet"},{"id":"http://arxiv.org/abs/2403.18346v2","updated":"2024-03-28T17:09:36Z","published":"2024-03-27T08:38:49Z","title":"Quantifying and Mitigating Unimodal Biases in Multimodal Large Language\n  Models: A Causal Perspective","summary":"  Recent advancements in Large Language Models (LLMs) have facilitated the\ndevelopment of Multimodal LLMs (MLLMs). Despite their impressive capabilities,\nMLLMs often suffer from an over-reliance on unimodal biases (e.g., language\nbias and vision bias), leading to incorrect answers in complex multimodal\ntasks. To investigate this issue, we propose a causal framework to interpret\nthe biases in Visual Question Answering (VQA) problems. Within our framework,\nwe devise a causal graph to elucidate the predictions of MLLMs on VQA problems,\nand assess the causal effect of biases through an in-depth causal analysis.\nMotivated by the causal graph, we introduce a novel MORE dataset, consisting of\n12,000 VQA instances. This dataset is designed to challenge MLLMs' abilities,\nnecessitating multi-hop reasoning and the surmounting of unimodal biases.\nFurthermore, we propose two strategies to mitigate unimodal biases and enhance\nMLLMs' reasoning capabilities, including a Decompose-Verify-Answer (DeVA)\nframework for limited-access MLLMs and the refinement of open-source MLLMs\nthrough fine-tuning. Extensive quantitative and qualitative experiments offer\nvaluable insights for future research. Our project page is at\nhttps://opencausalab.github.io/MORE.\n","authors":["Meiqi Chen","Yixin Cao","Yan Zhang","Chaochao Lu"],"pdf_url":"https://arxiv.org/pdf/2403.18346v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19586v1","updated":"2024-03-28T17:08:58Z","published":"2024-03-28T17:08:58Z","title":"TOGS: Gaussian Splatting with Temporal Opacity Offset for Real-Time 4D\n  DSA Rendering","summary":"  Four-dimensional Digital Subtraction Angiography (4D DSA) is a medical\nimaging technique that provides a series of 2D images captured at different\nstages and angles during the process of contrast agent filling blood vessels.\nIt plays a significant role in the diagnosis of cerebrovascular diseases.\nImproving the rendering quality and speed under sparse sampling is important\nfor observing the status and location of lesions. The current methods exhibit\ninadequate rendering quality in sparse views and suffer from slow rendering\nspeed. To overcome these limitations, we propose TOGS, a Gaussian splatting\nmethod with opacity offset over time, which can effectively improve the\nrendering quality and speed of 4D DSA. We introduce an opacity offset table for\neach Gaussian to model the temporal variations in the radiance of the contrast\nagent. By interpolating the opacity offset table, the opacity variation of the\nGaussian at different time points can be determined. This enables us to render\nthe 2D DSA image at that specific moment. Additionally, we introduced a Smooth\nloss term in the loss function to mitigate overfitting issues that may arise in\nthe model when dealing with sparse view scenarios. During the training phase,\nwe randomly prune Gaussians, thereby reducing the storage overhead of the\nmodel. The experimental results demonstrate that compared to previous methods,\nthis model achieves state-of-the-art reconstruction quality under the same\nnumber of training views. Additionally, it enables real-time rendering while\nmaintaining low storage overhead. The code will be publicly available.\n","authors":["Shuai Zhang","Huangxuan Zhao","Zhenghong Zhou","Guanjun Wu","Chuansheng Zheng","Xinggang Wang","Wenyu Liu"],"pdf_url":"https://arxiv.org/pdf/2403.19586v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07330v2","updated":"2024-03-28T17:07:38Z","published":"2023-12-12T14:45:45Z","title":"Learned representation-guided diffusion models for large-image\n  generation","summary":"  To synthesize high-fidelity samples, diffusion models typically require\nauxiliary data to guide the generation process. However, it is impractical to\nprocure the painstaking patch-level annotation effort required in specialized\ndomains like histopathology and satellite imagery; it is often performed by\ndomain experts and involves hundreds of millions of patches. Modern-day\nself-supervised learning (SSL) representations encode rich semantic and visual\ninformation. In this paper, we posit that such representations are expressive\nenough to act as proxies to fine-grained human labels. We introduce a novel\napproach that trains diffusion models conditioned on embeddings from SSL. Our\ndiffusion models successfully project these features back to high-quality\nhistopathology and remote sensing images. In addition, we construct larger\nimages by assembling spatially consistent patches inferred from SSL embeddings,\npreserving long-range dependencies. Augmenting real data by generating\nvariations of real images improves downstream classifier accuracy for\npatch-level and larger, image-scale classification tasks. Our models are\neffective even on datasets not encountered during training, demonstrating their\nrobustness and generalizability. Generating images from learned embeddings is\nagnostic to the source of the embeddings. The SSL embeddings used to generate a\nlarge image can either be extracted from a reference image, or sampled from an\nauxiliary model conditioned on any related modality (e.g. class labels, text,\ngenomic data). As proof of concept, we introduce the text-to-large image\nsynthesis paradigm where we successfully synthesize large pathology and\nsatellite images out of text descriptions.\n","authors":["Alexandros Graikos","Srikar Yellapragada","Minh-Quan Le","Saarthak Kapse","Prateek Prasanna","Joel Saltz","Dimitris Samaras"],"pdf_url":"https://arxiv.org/pdf/2312.07330v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17113v2","updated":"2024-03-28T17:07:28Z","published":"2023-11-28T12:05:41Z","title":"Human Gaussian Splatting: Real-time Rendering of Animatable Avatars","summary":"  This work addresses the problem of real-time rendering of photorealistic\nhuman body avatars learned from multi-view videos. While the classical\napproaches to model and render virtual humans generally use a textured mesh,\nrecent research has developed neural body representations that achieve\nimpressive visual quality. However, these models are difficult to render in\nreal-time and their quality degrades when the character is animated with body\nposes different than the training observations. We propose an animatable human\nmodel based on 3D Gaussian Splatting, that has recently emerged as a very\nefficient alternative to neural radiance fields. The body is represented by a\nset of gaussian primitives in a canonical space which is deformed with a coarse\nto fine approach that combines forward skinning and local non-rigid refinement.\nWe describe how to learn our Human Gaussian Splatting (HuGS) model in an\nend-to-end fashion from multi-view observations, and evaluate it against the\nstate-of-the-art approaches for novel pose synthesis of clothed body. Our\nmethod achieves 1.5 dB PSNR improvement over the state-of-the-art on THuman4\ndataset while being able to render in real-time (80 fps for 512x512\nresolution).\n","authors":["Arthur Moreau","Jifei Song","Helisa Dhamo","Richard Shaw","Yiren Zhou","Eduardo Pérez-Pellitero"],"pdf_url":"https://arxiv.org/pdf/2311.17113v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.19584v1","updated":"2024-03-28T17:07:02Z","published":"2024-03-28T17:07:02Z","title":"Img2Loc: Revisiting Image Geolocalization using Multi-modality\n  Foundation Models and Image-based Retrieval-Augmented Generation","summary":"  Geolocating precise locations from images presents a challenging problem in\ncomputer vision and information retrieval.Traditional methods typically employ\neither classification, which dividing the Earth surface into grid cells and\nclassifying images accordingly, or retrieval, which identifying locations by\nmatching images with a database of image-location pairs. However,\nclassification-based approaches are limited by the cell size and cannot yield\nprecise predictions, while retrieval-based systems usually suffer from poor\nsearch quality and inadequate coverage of the global landscape at varied scale\nand aggregation levels. To overcome these drawbacks, we present Img2Loc, a\nnovel system that redefines image geolocalization as a text generation task.\nThis is achieved using cutting-edge large multi-modality models like GPT4V or\nLLaVA with retrieval augmented generation. Img2Loc first employs CLIP-based\nrepresentations to generate an image-based coordinate query database. It then\nuniquely combines query results with images itself, forming elaborate prompts\ncustomized for LMMs. When tested on benchmark datasets such as Im2GPS3k and\nYFCC4k, Img2Loc not only surpasses the performance of previous state-of-the-art\nmodels but does so without any model training.\n","authors":["Zhongliang Zhou","Jielu Zhang","Zihan Guan","Mengxuan Hu","Ni Lao","Lan Mu","Sheng Li","Gengchen Mai"],"pdf_url":"https://arxiv.org/pdf/2403.19584v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18028v2","updated":"2024-03-28T17:06:15Z","published":"2024-03-26T18:29:39Z","title":"Predicting Species Occurrence Patterns from Partial Observations","summary":"  To address the interlinked biodiversity and climate crises, we need an\nunderstanding of where species occur and how these patterns are changing.\nHowever, observational data on most species remains very limited, and the\namount of data available varies greatly between taxonomic groups. We introduce\nthe problem of predicting species occurrence patterns given (a) satellite\nimagery, and (b) known information on the occurrence of other species. To\nevaluate algorithms on this task, we introduce SatButterfly, a dataset of\nsatellite images, environmental data and observational data for butterflies,\nwhich is designed to pair with the existing SatBird dataset of bird\nobservational data. To address this task, we propose a general model, R-Tran,\nfor predicting species occurrence patterns that enables the use of partial\nobservational data wherever found. We find that R-Tran outperforms other\nmethods in predicting species encounter rates with partial information both\nwithin a taxon (birds) and across taxa (birds and butterflies). Our approach\nopens new perspectives to leveraging insights from species with abundant data\nto other species with scarce data, by modelling the ecosystems in which they\nco-occur.\n","authors":["Hager Radi Abdelwahed","Mélisande Teng","David Rolnick"],"pdf_url":"https://arxiv.org/pdf/2403.18028v2.pdf","comment":"Tackling Climate Change with Machine Learning workshop at ICLR 2024"},{"id":"http://arxiv.org/abs/2403.19580v1","updated":"2024-03-28T17:05:04Z","published":"2024-03-28T17:05:04Z","title":"OV-Uni3DETR: Towards Unified Open-Vocabulary 3D Object Detection via\n  Cycle-Modality Propagation","summary":"  In the current state of 3D object detection research, the severe scarcity of\nannotated 3D data, substantial disparities across different data modalities,\nand the absence of a unified architecture, have impeded the progress towards\nthe goal of universality. In this paper, we propose \\textbf{OV-Uni3DETR}, a\nunified open-vocabulary 3D detector via cycle-modality propagation. Compared\nwith existing 3D detectors, OV-Uni3DETR offers distinct advantages: 1)\nOpen-vocabulary 3D detection: During training, it leverages various accessible\ndata, especially extensive 2D detection images, to boost training diversity.\nDuring inference, it can detect both seen and unseen classes. 2) Modality\nunifying: It seamlessly accommodates input data from any given modality,\neffectively addressing scenarios involving disparate modalities or missing\nsensor information, thereby supporting test-time modality switching. 3) Scene\nunifying: It provides a unified multi-modal model architecture for diverse\nscenes collected by distinct sensors. Specifically, we propose the\ncycle-modality propagation, aimed at propagating knowledge bridging 2D and 3D\nmodalities, to support the aforementioned functionalities. 2D semantic\nknowledge from large-vocabulary learning guides novel class discovery in the 3D\ndomain, and 3D geometric knowledge provides localization supervision for 2D\ndetection images. OV-Uni3DETR achieves the state-of-the-art performance on\nvarious scenarios, surpassing existing methods by more than 6\\% on average. Its\nperformance using only RGB images is on par with or even surpasses that of\nprevious point cloud based methods. Code and pre-trained models will be\nreleased later.\n","authors":["Zhenyu Wang","Yali Li","Taichi Liu","Hengshuang Zhao","Shengjin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.19580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19579v1","updated":"2024-03-28T17:04:07Z","published":"2024-03-28T17:04:07Z","title":"The Bad Batches: Enhancing Self-Supervised Learning in Image\n  Classification Through Representative Batch Curation","summary":"  The pursuit of learning robust representations without human supervision is a\nlongstanding challenge. The recent advancements in self-supervised contrastive\nlearning approaches have demonstrated high performance across various\nrepresentation learning challenges. However, current methods depend on the\nrandom transformation of training examples, resulting in some cases of\nunrepresentative positive pairs that can have a large impact on learning. This\nlimitation not only impedes the convergence of the learning process but the\nrobustness of the learnt representation as well as requiring larger batch sizes\nto improve robustness to such bad batches. This paper attempts to alleviate the\ninfluence of false positive and false negative pairs by employing pairwise\nsimilarity calculations through the Fr\\'echet ResNet Distance (FRD), thereby\nobtaining robust representations from unlabelled data. The effectiveness of the\nproposed method is substantiated by empirical results, where a linear\nclassifier trained on self-supervised contrastive representations achieved an\nimpressive 87.74\\% top-1 accuracy on STL10 and 99.31\\% on the Flower102\ndataset. These results emphasize the potential of the proposed approach in\npushing the boundaries of the state-of-the-art in self-supervised contrastive\nlearning, particularly for image classification tasks.\n","authors":["Ozgu Goksu","Nicolas Pugeault"],"pdf_url":"https://arxiv.org/pdf/2403.19579v1.pdf","comment":"8 Pages, 4 figures, IEEE WCCI 2024 Conference"},{"id":"http://arxiv.org/abs/2402.19470v2","updated":"2024-03-28T16:52:45Z","published":"2024-02-29T18:57:39Z","title":"Towards Generalizable Tumor Synthesis","summary":"  Tumor synthesis enables the creation of artificial tumors in medical images,\nfacilitating the training of AI models for tumor detection and segmentation.\nHowever, success in tumor synthesis hinges on creating visually realistic\ntumors that are generalizable across multiple organs and, furthermore, the\nresulting AI models being capable of detecting real tumors in images sourced\nfrom different domains (e.g., hospitals). This paper made a progressive stride\ntoward generalizable tumor synthesis by leveraging a critical observation:\nearly-stage tumors (< 2cm) tend to have similar imaging characteristics in\ncomputed tomography (CT), whether they originate in the liver, pancreas, or\nkidneys. We have ascertained that generative AI models, e.g., Diffusion Models,\ncan create realistic tumors generalized to a range of organs even when trained\non a limited number of tumor examples from only one organ. Moreover, we have\nshown that AI models trained on these synthetic tumors can be generalized to\ndetect and segment real tumors from CT volumes, encompassing a broad spectrum\nof patient demographics, imaging protocols, and healthcare facilities.\n","authors":["Qi Chen","Xiaoxi Chen","Haorui Song","Zhiwei Xiong","Alan Yuille","Chen Wei","Zongwei Zhou"],"pdf_url":"https://arxiv.org/pdf/2402.19470v2.pdf","comment":"The IEEE / CVF Computer Vision and Pattern Recognition Conference\n  (CVPR 2024)"},{"id":"http://arxiv.org/abs/2311.17112v2","updated":"2024-03-28T16:51:18Z","published":"2023-11-28T11:23:34Z","title":"Parameter Efficient Fine-tuning via Cross Block Orchestration for\n  Segment Anything Model","summary":"  Parameter-efficient fine-tuning (PEFT) is an effective methodology to unleash\nthe potential of large foundation models in novel scenarios with limited\ntraining data. In the computer vision community, PEFT has shown effectiveness\nin image classification, but little research has studied its ability for image\nsegmentation. Fine-tuning segmentation models usually require a heavier\nadjustment of parameters to align the proper projection directions in the\nparameter space for new scenarios. This raises a challenge to existing PEFT\nalgorithms, as they often inject a limited number of individual parameters into\neach block, which prevents substantial adjustment of the projection direction\nof the parameter space due to the limitation of Hidden Markov Chain along\nblocks. In this paper, we equip PEFT with a cross-block orchestration mechanism\nto enable the adaptation of the Segment Anything Model (SAM) to various\ndownstream scenarios. We introduce a novel inter-block communication module,\nwhich integrates a learnable relation matrix to facilitate communication among\ndifferent coefficient sets of each PEFT block's parameter space. Moreover, we\npropose an intra-block enhancement module, which introduces a linear projection\nhead whose weights are generated from a hyper-complex layer, further enhancing\nthe impact of the adjustment of projection directions on the entire parameter\nspace. Extensive experiments on diverse benchmarks demonstrate that our\nproposed approach consistently improves the segmentation performance\nsignificantly on novel scenarios with only around 1K additional parameters.\n","authors":["Zelin Peng","Zhengqin Xu","Zhilin Zeng","Lingxi Xie","Qi Tian","Wei Shen"],"pdf_url":"https://arxiv.org/pdf/2311.17112v2.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2312.02137v2","updated":"2024-03-28T16:50:37Z","published":"2023-12-04T18:56:22Z","title":"MANUS: Markerless Grasp Capture using Articulated 3D Gaussians","summary":"  Understanding how we grasp objects with our hands has important applications\nin areas like robotics and mixed reality. However, this challenging problem\nrequires accurate modeling of the contact between hands and objects. To capture\ngrasps, existing methods use skeletons, meshes, or parametric models that does\nnot represent hand shape accurately resulting in inaccurate contacts. We\npresent MANUS, a method for Markerless Hand-Object Grasp Capture using\nArticulated 3D Gaussians. We build a novel articulated 3D Gaussians\nrepresentation that extends 3D Gaussian splatting for high-fidelity\nrepresentation of articulating hands. Since our representation uses Gaussian\nprimitives, it enables us to efficiently and accurately estimate contacts\nbetween the hand and the object. For the most accurate results, our method\nrequires tens of camera views that current datasets do not provide. We\ntherefore build MANUS-Grasps, a new dataset that contains hand-object grasps\nviewed from 50+ cameras across 30+ scenes, 3 subjects, and comprising over 7M\nframes. In addition to extensive qualitative results, we also show that our\nmethod outperforms others on a quantitative contact evaluation method that uses\npaint transfer from the object to the hand.\n","authors":["Chandradeep Pokhariya","Ishaan N Shah","Angela Xing","Zekun Li","Kefan Chen","Avinash Sharma","Srinath Sridhar"],"pdf_url":"https://arxiv.org/pdf/2312.02137v2.pdf","comment":"IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR)\n  2024"},{"id":"http://arxiv.org/abs/2312.11598v3","updated":"2024-03-28T16:49:40Z","published":"2023-12-18T18:16:52Z","title":"SkillDiffuser: Interpretable Hierarchical Planning via Skill\n  Abstractions in Diffusion-Based Task Execution","summary":"  Diffusion models have demonstrated strong potential for robotic trajectory\nplanning. However, generating coherent trajectories from high-level\ninstructions remains challenging, especially for long-range composition tasks\nrequiring multiple sequential skills. We propose SkillDiffuser, an end-to-end\nhierarchical planning framework integrating interpretable skill learning with\nconditional diffusion planning to address this problem. At the higher level,\nthe skill abstraction module learns discrete, human-understandable skill\nrepresentations from visual observations and language instructions. These\nlearned skill embeddings are then used to condition the diffusion model to\ngenerate customized latent trajectories aligned with the skills. This allows\ngenerating diverse state trajectories that adhere to the learnable skills. By\nintegrating skill learning with conditional trajectory generation,\nSkillDiffuser produces coherent behavior following abstract instructions across\ndiverse tasks. Experiments on multi-task robotic manipulation benchmarks like\nMeta-World and LOReL demonstrate state-of-the-art performance and\nhuman-interpretable skill representations from SkillDiffuser. More\nvisualization results and information could be found on our website.\n","authors":["Zhixuan Liang","Yao Mu","Hengbo Ma","Masayoshi Tomizuka","Mingyu Ding","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2312.11598v3.pdf","comment":"Accepted by CVPR 2024. Camera ready version. Project page:\n  https://skilldiffuser.github.io/"},{"id":"http://arxiv.org/abs/2403.16385v2","updated":"2024-03-28T16:45:44Z","published":"2024-03-25T03:02:27Z","title":"Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators\n  for Reasoning-Based Chart VQA","summary":"  Understanding data visualizations like charts and plots requires reasoning\nabout both visual elements and numerics. Although strong in extractive\nquestions, current chart visual question answering (chart VQA) models suffer on\ncomplex reasoning questions. In this work, we address the lack of reasoning\nability by data augmentation. We leverage Large Language Models (LLMs), which\nhave shown to have strong reasoning ability, as an automatic data annotator\nthat generates question-answer annotations for chart images. The key innovation\nin our method lies in the Synthesize Step-by-Step strategy: our LLM-based data\ngenerator learns to decompose the complex question into step-by-step\nsub-questions (rationales), which are then used to derive the final answer\nusing external tools, i.e. Python. This step-wise generation procedure is\ntrained on synthetic data generated using a template-based QA generation\npipeline. Experimental results highlight the significance of the proposed\nstep-by-step generation. By training with the LLM-augmented data (LAMENDA), we\nsignificantly enhance the chart VQA models, achieving the state-of-the-art\naccuracy on the ChartQA and PlotQA datasets. In particular, our approach\nimproves the accuracy of the previous state-of-the-art approach from 38% to 54%\non the human-written questions in the ChartQA dataset, which needs strong\nreasoning. We hope our work underscores the potential of synthetic data and\nencourages further exploration of data augmentation using LLMs for\nreasoning-heavy tasks.\n","authors":["Zhuowan Li","Bhavan Jasani","Peng Tang","Shabnam Ghadar"],"pdf_url":"https://arxiv.org/pdf/2403.16385v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.19554v1","updated":"2024-03-28T16:38:04Z","published":"2024-03-28T16:38:04Z","title":"Cross-Attention is Not Always Needed: Dynamic Cross-Attention for\n  Audio-Visual Dimensional Emotion Recognition","summary":"  In video-based emotion recognition, audio and visual modalities are often\nexpected to have a complementary relationship, which is widely explored using\ncross-attention. However, they may also exhibit weak complementary\nrelationships, resulting in poor representations of audio-visual features, thus\ndegrading the performance of the system. To address this issue, we propose\nDynamic Cross-Attention (DCA) that can dynamically select cross-attended or\nunattended features on the fly based on their strong or weak complementary\nrelationship with each other, respectively. Specifically, a simple yet\nefficient gating layer is designed to evaluate the contribution of the\ncross-attention mechanism and choose cross-attended features only when they\nexhibit a strong complementary relationship, otherwise unattended features. We\nevaluate the performance of the proposed approach on the challenging RECOLA and\nAff-Wild2 datasets. We also compare the proposed approach with other variants\nof cross-attention and show that the proposed model consistently improves the\nperformance on both datasets.\n","authors":["R. Gnana Praveen","Jahangir Alam"],"pdf_url":"https://arxiv.org/pdf/2403.19554v1.pdf","comment":"Accepted at IEEE ICME2024"},{"id":"http://arxiv.org/abs/2403.19549v1","updated":"2024-03-28T16:32:06Z","published":"2024-03-28T16:32:06Z","title":"GlORIE-SLAM: Globally Optimized RGB-only Implicit Encoding Point Cloud\n  SLAM","summary":"  Recent advancements in RGB-only dense Simultaneous Localization and Mapping\n(SLAM) have predominantly utilized grid-based neural implicit encodings and/or\nstruggle to efficiently realize global map and pose consistency. To this end,\nwe propose an efficient RGB-only dense SLAM system using a flexible neural\npoint cloud scene representation that adapts to keyframe poses and depth\nupdates, without needing costly backpropagation. Another critical challenge of\nRGB-only SLAM is the lack of geometric priors. To alleviate this issue, with\nthe aid of a monocular depth estimator, we introduce a novel DSPO layer for\nbundle adjustment which optimizes the pose and depth of keyframes along with\nthe scale of the monocular depth. Finally, our system benefits from loop\nclosure and online global bundle adjustment and performs either better or\ncompetitive to existing dense neural RGB SLAM methods in tracking, mapping and\nrendering accuracy on the Replica, TUM-RGBD and ScanNet datasets. The source\ncode will be made available.\n","authors":["Ganlin Zhang","Erik Sandström","Youmin Zhang","Manthan Patel","Luc Van Gool","Martin R. Oswald"],"pdf_url":"https://arxiv.org/pdf/2403.19549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15981v2","updated":"2024-03-28T16:21:30Z","published":"2024-03-24T02:15:14Z","title":"Exploring Accurate 3D Phenotyping in Greenhouse through Neural Radiance\n  Fields","summary":"  Accurate collection of plant phenotyping is critical to optimising\nsustainable farming practices in precision agriculture. Traditional phenotyping\nin controlled laboratory environments, while valuable, falls short in\nunderstanding plant growth under real-world conditions. Emerging sensor and\ndigital technologies offer a promising approach for direct phenotyping of\nplants in farm environments. This study investigates a learning-based\nphenotyping method using the Neural Radiance Field to achieve accurate in-situ\nphenotyping of pepper plants in greenhouse environments. To quantitatively\nevaluate the performance of this method, traditional point cloud registration\non 3D scanning data is implemented for comparison. Experimental result shows\nthat NeRF(Neural Radiance Fields) achieves competitive accuracy compared to the\n3D scanning methods. The mean distance error between the scanner-based method\nand the NeRF-based method is 0.865mm. This study shows that the learning-based\nNeRF method achieves similar accuracy to 3D scanning-based methods but with\nimproved scalability and robustness.\n","authors":["Junhong Zhao","Wei Ying","Yaoqiang Pan","Zhenfeng Yi","Chao Chen","Kewei Hu","Hanwen Kang"],"pdf_url":"https://arxiv.org/pdf/2403.15981v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.08989v3","updated":"2024-03-28T16:17:43Z","published":"2022-04-13T21:20:42Z","title":"Efficient Deep Learning-based Estimation of the Vital Signs on\n  Smartphones","summary":"  With the increasing use of smartphones in our daily lives, these devices have\nbecome capable of performing many complex tasks. Concerning the need for\ncontinuous monitoring of vital signs, especially for the elderly or those with\ncertain types of diseases, the development of algorithms that can estimate\nvital signs using smartphones has attracted researchers worldwide. In\nparticular, researchers have been exploring ways to estimate vital signs, such\nas heart rate, oxygen saturation levels, and respiratory rate, using algorithms\nthat can be run on smartphones. However, many of these algorithms require\nmultiple pre-processing steps that might introduce some implementation\noverheads or require the design of a couple of hand-crafted stages to obtain an\noptimal result. To address this issue, this research proposes a novel\nend-to-end solution to mobile-based vital sign estimation using deep learning\nthat eliminates the need for pre-processing. By using a fully convolutional\narchitecture, the proposed model has much fewer parameters and less\ncomputational complexity compared to the architectures that use fully-connected\nlayers as the prediction heads. This also reduces the risk of overfitting.\nAdditionally, a public dataset for vital sign estimation, which includes 62\nvideos collected from 35 men and 27 women, is provided. Overall, the proposed\nend-to-end approach promises significantly improved efficiency and performance\nfor on-device health monitoring on readily available consumer electronics.\n","authors":["Taha Samavati","Mahdi Farvardin","Aboozar Ghaffari"],"pdf_url":"https://arxiv.org/pdf/2204.08989v3.pdf","comment":"10 pages, 8 figures, 11 tables"},{"id":"http://arxiv.org/abs/2403.19539v1","updated":"2024-03-28T16:13:22Z","published":"2024-03-28T16:13:22Z","title":"De-confounded Data-free Knowledge Distillation for Handling Distribution\n  Shifts","summary":"  Data-Free Knowledge Distillation (DFKD) is a promising task to train\nhigh-performance small models to enhance actual deployment without relying on\nthe original training data. Existing methods commonly avoid relying on private\ndata by utilizing synthetic or sampled data. However, a long-overlooked issue\nis that the severe distribution shifts between their substitution and original\ndata, which manifests as huge differences in the quality of images and class\nproportions. The harmful shifts are essentially the confounder that\nsignificantly causes performance bottlenecks. To tackle the issue, this paper\nproposes a novel perspective with causal inference to disentangle the student\nmodels from the impact of such shifts. By designing a customized causal graph,\nwe first reveal the causalities among the variables in the DFKD task.\nSubsequently, we propose a Knowledge Distillation Causal Intervention (KDCI)\nframework based on the backdoor adjustment to de-confound the confounder. KDCI\ncan be flexibly combined with most existing state-of-the-art baselines.\nExperiments in combination with six representative DFKD methods demonstrate the\neffectiveness of our KDCI, which can obviously help existing methods under\nalmost all settings, \\textit{e.g.}, improving the baseline by up to 15.54\\%\naccuracy on the CIFAR-100 dataset.\n","authors":["Yuzheng Wang","Dingkang Yang","Zhaoyu Chen","Yang Liu","Siao Liu","Wenqiang Zhang","Lihua Zhang","Lizhe Qi"],"pdf_url":"https://arxiv.org/pdf/2403.19539v1.pdf","comment":"Accepted by CVPR24"},{"id":"http://arxiv.org/abs/2403.19534v1","updated":"2024-03-28T16:07:55Z","published":"2024-03-28T16:07:55Z","title":"Locate, Assign, Refine: Taming Customized Image Inpainting with\n  Text-Subject Guidance","summary":"  Prior studies have made significant progress in image inpainting guided by\neither text or subject image. However, the research on editing with their\ncombined guidance is still in the early stages. To tackle this challenge, we\npresent LAR-Gen, a novel approach for image inpainting that enables seamless\ninpainting of masked scene images, incorporating both the textual prompts and\nspecified subjects. Our approach adopts a coarse-to-fine manner to ensure\nsubject identity preservation and local semantic coherence. The process\ninvolves (i) Locate: concatenating the noise with masked scene image to achieve\nprecise regional editing, (ii) Assign: employing decoupled cross-attention\nmechanism to accommodate multi-modal guidance, and (iii) Refine: using a novel\nRefineNet to supplement subject details. Additionally, to address the issue of\nscarce training data, we introduce a novel data construction pipeline. This\npipeline extracts substantial pairs of data consisting of local text prompts\nand corresponding visual instances from a vast image dataset, leveraging\npublicly available large models. Extensive experiments and varied application\nscenarios demonstrate the superiority of LAR-Gen in terms of both identity\npreservation and text semantic consistency. Project page can be found at\n\\url{https://ali-vilab.github.io/largen-page/}.\n","authors":["Yulin Pan","Chaojie Mao","Zeyinzi Jiang","Zhen Han","Jingfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.19534v1.pdf","comment":"22 pages, 14 figures"},{"id":"http://arxiv.org/abs/2403.19527v1","updated":"2024-03-28T16:02:03Z","published":"2024-03-28T16:02:03Z","title":"Instance-Adaptive and Geometric-Aware Keypoint Learning for\n  Category-Level 6D Object Pose Estimation","summary":"  Category-level 6D object pose estimation aims to estimate the rotation,\ntranslation and size of unseen instances within specific categories. In this\narea, dense correspondence-based methods have achieved leading performance.\nHowever, they do not explicitly consider the local and global geometric\ninformation of different instances, resulting in poor generalization ability to\nunseen instances with significant shape variations. To deal with this problem,\nwe propose a novel Instance-Adaptive and Geometric-Aware Keypoint Learning\nmethod for category-level 6D object pose estimation (AG-Pose), which includes\ntwo key designs: (1) The first design is an Instance-Adaptive Keypoint\nDetection module, which can adaptively detect a set of sparse keypoints for\nvarious instances to represent their geometric structures. (2) The second\ndesign is a Geometric-Aware Feature Aggregation module, which can efficiently\nintegrate the local and global geometric information into keypoint features.\nThese two modules can work together to establish robust keypoint-level\ncorrespondences for unseen instances, thus enhancing the generalization ability\nof the model.Experimental results on CAMERA25 and REAL275 datasets show that\nthe proposed AG-Pose outperforms state-of-the-art methods by a large margin\nwithout category-specific shape priors.\n","authors":["Xiao Lin","Wenfei Yang","Yuan Gao","Tianzhu Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.19527v1.pdf","comment":"Accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2403.19522v1","updated":"2024-03-28T15:57:20Z","published":"2024-03-28T15:57:20Z","title":"Model Stock: All we need is just a few fine-tuned models","summary":"  This paper introduces an efficient fine-tuning method for large pre-trained\nmodels, offering strong in-distribution (ID) and out-of-distribution (OOD)\nperformance. Breaking away from traditional practices that need a multitude of\nfine-tuned models for averaging, our approach employs significantly fewer\nmodels to achieve final weights yet yield superior accuracy. Drawing from key\ninsights in the weight space of fine-tuned weights, we uncover a strong link\nbetween the performance and proximity to the center of weight space. Based on\nthis, we introduce a method that approximates a center-close weight using only\ntwo fine-tuned models, applicable during or after training. Our innovative\nlayer-wise weight averaging technique surpasses state-of-the-art model methods\nsuch as Model Soup, utilizing only two fine-tuned models. This strategy can be\naptly coined Model Stock, highlighting its reliance on selecting a minimal\nnumber of models to draw a more optimized-averaged model. We demonstrate the\nefficacy of Model Stock with fine-tuned models based upon pre-trained CLIP\narchitectures, achieving remarkable performance on both ID and OOD tasks on the\nstandard benchmarks, all while barely bringing extra computational demands. Our\ncode and pre-trained models are available at\nhttps://github.com/naver-ai/model-stock.\n","authors":["Dong-Hwan Jang","Sangdoo Yun","Dongyoon Han"],"pdf_url":"https://arxiv.org/pdf/2403.19522v1.pdf","comment":"Code at https://github.com/naver-ai/model-stock"},{"id":"http://arxiv.org/abs/2401.01286v4","updated":"2024-03-28T15:56:55Z","published":"2024-01-02T16:54:58Z","title":"A Comprehensive Study of Knowledge Editing for Large Language Models","summary":"  Large Language Models (LLMs) have shown extraordinary capabilities in\nunderstanding and generating text that closely mirrors human communication.\nHowever, a primary limitation lies in the significant computational demands\nduring training, arising from their extensive parameterization. This challenge\nis further intensified by the dynamic nature of the world, necessitating\nfrequent updates to LLMs to correct outdated information or integrate new\nknowledge, thereby ensuring their continued relevance. Note that many\napplications demand continual model adjustments post-training to address\ndeficiencies or undesirable behaviors. There is an increasing interest in\nefficient, lightweight methods for on-the-fly model modifications. To this end,\nrecent years have seen a burgeoning in the techniques of knowledge editing for\nLLMs, which aim to efficiently modify LLMs' behaviors within specific domains\nwhile preserving overall performance across various inputs. In this paper, we\nfirst define the knowledge editing problem and then provide a comprehensive\nreview of cutting-edge approaches. Drawing inspiration from educational and\ncognitive research theories, we propose a unified categorization criterion that\nclassifies knowledge editing methods into three groups: resorting to external\nknowledge, merging knowledge into the model, and editing intrinsic knowledge.\nFurthermore, we introduce a new benchmark, KnowEdit, for a comprehensive\nempirical evaluation of representative knowledge editing approaches.\nAdditionally, we provide an in-depth analysis of knowledge location, which can\ngive a deeper understanding of the knowledge structures inherent within LLMs.\nFinally, we discuss several potential applications of knowledge editing,\noutlining its broad and impactful implications.\n","authors":["Ningyu Zhang","Yunzhi Yao","Bozhong Tian","Peng Wang","Shumin Deng","Mengru Wang","Zekun Xi","Shengyu Mao","Jintian Zhang","Yuansheng Ni","Siyuan Cheng","Ziwen Xu","Xin Xu","Jia-Chen Gu","Yong Jiang","Pengjun Xie","Fei Huang","Lei Liang","Zhiqiang Zhang","Xiaowei Zhu","Jun Zhou","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2401.01286v4.pdf","comment":"Ongoing work; 52 pages, 282 citations; benchmark is available at\n  https://huggingface.co/datasets/zjunlp/KnowEdit code is available at\n  https://github.com/zjunlp/EasyEdit paper list is available at\n  https://github.com/zjunlp/KnowledgeEditingPapers"},{"id":"http://arxiv.org/abs/2309.13610v2","updated":"2024-03-28T15:52:16Z","published":"2023-09-24T11:19:13Z","title":"VisionKG: Unleashing the Power of Visual Datasets via Knowledge Graph","summary":"  The availability of vast amounts of visual data with heterogeneous features\nis a key factor for developing, testing, and benchmarking of new computer\nvision (CV) algorithms and architectures. Most visual datasets are created and\ncurated for specific tasks or with limited image data distribution for very\nspecific situations, and there is no unified approach to manage and access them\nacross diverse sources, tasks, and taxonomies. This not only creates\nunnecessary overheads when building robust visual recognition systems, but also\nintroduces biases into learning systems and limits the capabilities of\ndata-centric AI. To address these problems, we propose the Vision Knowledge\nGraph (VisionKG), a novel resource that interlinks, organizes and manages\nvisual datasets via knowledge graphs and Semantic Web technologies. It can\nserve as a unified framework facilitating simple access and querying of\nstate-of-the-art visual datasets, regardless of their heterogeneous formats and\ntaxonomies. One of the key differences between our approach and existing\nmethods is that ours is knowledge-based rather than metadatabased. It enhances\nthe enrichment of the semantics at both image and instance levels and offers\nvarious data retrieval and exploratory services via SPARQL. VisionKG currently\ncontains 519 million RDF triples that describe approximately 40 million\nentities, and are accessible at https://vision.semkg.org and through APIs. With\nthe integration of 30 datasets and four popular CV tasks, we demonstrate its\nusefulness across various scenarios when working with CV pipelines.\n","authors":["Jicheng Yuan","Anh Le-Tuan","Manh Nguyen-Duc","Trung-Kien Tran","Manfred Hauswirth","Danh Le-Phuoc"],"pdf_url":"https://arxiv.org/pdf/2309.13610v2.pdf","comment":"Accepted at ESWC 2024"},{"id":"http://arxiv.org/abs/2312.02069v2","updated":"2024-03-28T15:51:05Z","published":"2023-12-04T17:28:35Z","title":"GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians","summary":"  We introduce GaussianAvatars, a new method to create photorealistic head\navatars that are fully controllable in terms of expression, pose, and\nviewpoint. The core idea is a dynamic 3D representation based on 3D Gaussian\nsplats that are rigged to a parametric morphable face model. This combination\nfacilitates photorealistic rendering while allowing for precise animation\ncontrol via the underlying parametric model, e.g., through expression transfer\nfrom a driving sequence or by manually changing the morphable model parameters.\nWe parameterize each splat by a local coordinate frame of a triangle and\noptimize for explicit displacement offset to obtain a more accurate geometric\nrepresentation. During avatar reconstruction, we jointly optimize for the\nmorphable model parameters and Gaussian splat parameters in an end-to-end\nfashion. We demonstrate the animation capabilities of our photorealistic avatar\nin several challenging scenarios. For instance, we show reenactments from a\ndriving video, where our method outperforms existing works by a significant\nmargin.\n","authors":["Shenhan Qian","Tobias Kirschstein","Liam Schoneveld","Davide Davoli","Simon Giebenhain","Matthias Nießner"],"pdf_url":"https://arxiv.org/pdf/2312.02069v2.pdf","comment":"Project page: https://shenhanqian.github.io/gaussian-avatars"},{"id":"http://arxiv.org/abs/2308.16682v2","updated":"2024-03-28T15:49:42Z","published":"2023-08-31T12:36:50Z","title":"DiffusionPoser: Real-time Human Motion Reconstruction From Arbitrary\n  Sparse Sensors Using Autoregressive Diffusion","summary":"  Motion capture from a limited number of body-worn sensors, such as inertial\nmeasurement units (IMUs) and pressure insoles, has important applications in\nhealth, human performance, and entertainment. Recent work has focused on\naccurately reconstructing whole-body motion from a specific sensor\nconfiguration using six IMUs. While a common goal across applications is to use\nthe minimal number of sensors to achieve required accuracy, the optimal\narrangement of the sensors might differ from application to application. We\npropose a single diffusion model, DiffusionPoser, which reconstructs human\nmotion in real-time from an arbitrary combination of sensors, including IMUs\nplaced at specified locations, and, pressure insoles. Unlike existing methods,\nour model grants users the flexibility to determine the number and arrangement\nof sensors tailored to the specific activity of interest, without the need for\nretraining. A novel autoregressive inferencing scheme ensures real-time motion\nreconstruction that closely aligns with measured sensor signals. The generative\nnature of DiffusionPoser ensures realistic behavior, even for\ndegrees-of-freedom not directly measured. Qualitative results can be found on\nour website: https://diffusionposer.github.io/.\n","authors":["Tom Van Wouwe","Seunghwan Lee","Antoine Falisse","Scott Delp","C. Karen Liu"],"pdf_url":"https://arxiv.org/pdf/2308.16682v2.pdf","comment":"accepted at CVPR2024"},{"id":"http://arxiv.org/abs/2403.19517v1","updated":"2024-03-28T15:48:16Z","published":"2024-03-28T15:48:16Z","title":"XScale-NVS: Cross-Scale Novel View Synthesis with Hash Featurized\n  Manifold","summary":"  We propose XScale-NVS for high-fidelity cross-scale novel view synthesis of\nreal-world large-scale scenes. Existing representations based on explicit\nsurface suffer from discretization resolution or UV distortion, while implicit\nvolumetric representations lack scalability for large scenes due to the\ndispersed weight distribution and surface ambiguity. In light of the above\nchallenges, we introduce hash featurized manifold, a novel hash-based\nfeaturization coupled with a deferred neural rendering framework. This approach\nfully unlocks the expressivity of the representation by explicitly\nconcentrating the hash entries on the 2D manifold, thus effectively\nrepresenting highly detailed contents independent of the discretization\nresolution. We also introduce a novel dataset, namely GigaNVS, to benchmark\ncross-scale, high-resolution novel view synthesis of realworld large-scale\nscenes. Our method significantly outperforms competing baselines on various\nreal-world scenes, yielding an average LPIPS that is 40% lower than prior\nstate-of-the-art on the challenging GigaNVS benchmark. Please see our project\npage at: xscalenvs.github.io.\n","authors":["Guangyu Wang","Jinzhi Zhang","Fan Wang","Ruqi Huang","Lu Fang"],"pdf_url":"https://arxiv.org/pdf/2403.19517v1.pdf","comment":"Accepted to CVPR 2024. Project page: xscalenvs.github.io/"},{"id":"http://arxiv.org/abs/2403.19514v1","updated":"2024-03-28T15:45:03Z","published":"2024-03-28T15:45:03Z","title":"CDIMC-net: Cognitive Deep Incomplete Multi-view Clustering Network","summary":"  In recent years, incomplete multi-view clustering, which studies the\nchallenging multi-view clustering problem on missing views, has received\ngrowing research interests. Although a series of methods have been proposed to\naddress this issue, the following problems still exist: 1) Almost all of the\nexisting methods are based on shallow models, which is difficult to obtain\ndiscriminative common representations. 2) These methods are generally sensitive\nto noise or outliers since the negative samples are treated equally as the\nimportant samples. In this paper, we propose a novel incomplete multi-view\nclustering network, called Cognitive Deep Incomplete Multi-view Clustering\nNetwork (CDIMC-net), to address these issues. Specifically, it captures the\nhigh-level features and local structure of each view by incorporating the\nview-specific deep encoders and graph embedding strategy into a framework.\nMoreover, based on the human cognition, i.e., learning from easy to hard, it\nintroduces a self-paced strategy to select the most confident samples for model\ntraining, which can reduce the negative influence of outliers. Experimental\nresults on several incomplete datasets show that CDIMC-net outperforms the\nstate-of-the-art incomplete multi-view clustering methods.\n","authors":["Jie Wen","Zheng Zhang","Yong Xu","Bob Zhang","Lunke Fei","Guo-Sen Xie"],"pdf_url":"https://arxiv.org/pdf/2403.19514v1.pdf","comment":"Accepted by IJCAI 2020"},{"id":"http://arxiv.org/abs/2403.19508v1","updated":"2024-03-28T15:41:43Z","published":"2024-03-28T15:41:43Z","title":"Debiasing Cardiac Imaging with Controlled Latent Diffusion Models","summary":"  The progress in deep learning solutions for disease diagnosis and prognosis\nbased on cardiac magnetic resonance imaging is hindered by highly imbalanced\nand biased training data. To address this issue, we propose a method to\nalleviate imbalances inherent in datasets through the generation of synthetic\ndata based on sensitive attributes such as sex, age, body mass index, and\nhealth condition. We adopt ControlNet based on a denoising diffusion\nprobabilistic model to condition on text assembled from patient metadata and\ncardiac geometry derived from segmentation masks using a large-cohort study,\nspecifically, the UK Biobank. We assess our method by evaluating the realism of\nthe generated images using established quantitative metrics. Furthermore, we\nconduct a downstream classification task aimed at debiasing a classifier by\nrectifying imbalances within underrepresented groups through synthetically\ngenerated samples. Our experiments demonstrate the effectiveness of the\nproposed approach in mitigating dataset imbalances, such as the scarcity of\nyounger patients or individuals with normal BMI level suffering from heart\nfailure. This work represents a major step towards the adoption of synthetic\ndata for the development of fair and generalizable models for medical\nclassification tasks. Notably, we conduct all our experiments using a single,\nconsumer-level GPU to highlight the feasibility of our approach within\nresource-constrained environments. Our code is available at\nhttps://github.com/faildeny/debiasing-cardiac-mri.\n","authors":["Grzegorz Skorupko","Richard Osuala","Zuzanna Szafranowska","Kaisar Kushibar","Nay Aung","Steffen E Petersen","Karim Lekadir","Polyxeni Gkontra"],"pdf_url":"https://arxiv.org/pdf/2403.19508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.02496v3","updated":"2024-03-28T15:33:42Z","published":"2023-07-04T08:00:31Z","title":"Learning to reconstruct the bubble distribution with conductivity maps\n  using Invertible Neural Networks and Error Diffusion","summary":"  Electrolysis is crucial for eco-friendly hydrogen production, but gas bubbles\ngenerated during the process hinder reactions, reduce cell efficiency, and\nincrease energy consumption. Additionally, these gas bubbles cause changes in\nthe conductivity inside the cell, resulting in corresponding variations in the\ninduced magnetic field around the cell. Therefore, measuring these gas\nbubble-induced magnetic field fluctuations using external magnetic sensors and\nsolving the inverse problem of Biot-Savart Law allows for estimating the\nconductivity in the cell and, thus, bubble size and location. However,\ndetermining high-resolution conductivity maps from only a few induced magnetic\nfield measurements is an ill-posed inverse problem. To overcome this, we\nexploit Invertible Neural Networks (INNs) to reconstruct the conductivity\nfield. Our qualitative results and quantitative evaluation using random error\ndiffusion show that INN achieves far superior performance compared to Tikhonov\nregularization.\n","authors":["Nishant Kumar","Lukas Krause","Thomas Wondrak","Sven Eckert","Kerstin Eckert","Stefan Gumhold"],"pdf_url":"https://arxiv.org/pdf/2307.02496v3.pdf","comment":"Accepted for Oral presentation at WCIPT11 (11th World Congress on\n  Industrial Process Tomography)"},{"id":"http://arxiv.org/abs/2403.19501v1","updated":"2024-03-28T15:31:36Z","published":"2024-03-28T15:31:36Z","title":"RELI11D: A Comprehensive Multimodal Human Motion Dataset and Method","summary":"  Comprehensive capturing of human motions requires both accurate captures of\ncomplex poses and precise localization of the human within scenes. Most of the\nHPE datasets and methods primarily rely on RGB, LiDAR, or IMU data. However,\nsolely using these modalities or a combination of them may not be adequate for\nHPE, particularly for complex and fast movements. For holistic human motion\nunderstanding, we present RELI11D, a high-quality multimodal human motion\ndataset involves LiDAR, IMU system, RGB camera, and Event camera. It records\nthe motions of 10 actors performing 5 sports in 7 scenes, including 3.32 hours\nof synchronized LiDAR point clouds, IMU measurement data, RGB videos and Event\nsteams. Through extensive experiments, we demonstrate that the RELI11D presents\nconsiderable challenges and opportunities as it contains many rapid and complex\nmotions that require precise location. To address the challenge of integrating\ndifferent modalities, we propose LEIR, a multimodal baseline that effectively\nutilizes LiDAR Point Cloud, Event stream, and RGB through our cross-attention\nfusion strategy. We show that LEIR exhibits promising results for rapid motions\nand daily motions and that utilizing the characteristics of multiple modalities\ncan indeed improve HPE performance. Both the dataset and source code will be\nreleased publicly to the research community, fostering collaboration and\nenabling further exploration in this field.\n","authors":["Ming Yan","Yan Zhang","Shuqiang Cai","Shuqi Fan","Xincheng Lin","Yudi Dai","Siqi Shen","Chenglu Wen","Lan Xu","Yuexin Ma","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.19501v1.pdf","comment":"CVPR2024, Project website: http://www.lidarhumanmotion.net/reli11d/"},{"id":"http://arxiv.org/abs/2403.19497v1","updated":"2024-03-28T15:27:34Z","published":"2024-03-28T15:27:34Z","title":"Surface-based parcellation and vertex-wise analysis of ultra\n  high-resolution ex vivo 7 tesla MRI in neurodegenerative diseases","summary":"  Magnetic resonance imaging (MRI) is the standard modality to understand human\nbrain structure and function in vivo (antemortem). Decades of research in human\nneuroimaging has led to the widespread development of methods and tools to\nprovide automated volume-based segmentations and surface-based parcellations\nwhich help localize brain functions to specialized anatomical regions. Recently\nex vivo (postmortem) imaging of the brain has opened-up avenues to study brain\nstructure at sub-millimeter ultra high-resolution revealing details not\npossible to observe with in vivo MRI. Unfortunately, there has been limited\nmethodological development in ex vivo MRI primarily due to lack of datasets and\nlimited centers with such imaging resources. Therefore, in this work, we\npresent one-of-its-kind dataset of 82 ex vivo T2w whole brain hemispheres MRI\nat 0.3 mm isotropic resolution spanning Alzheimer's disease and related\ndementias. We adapted and developed a fast and easy-to-use automated\nsurface-based pipeline to parcellate, for the first time, ultra high-resolution\nex vivo brain tissue at the native subject space resolution using the\nDesikan-Killiany-Tourville (DKT) brain atlas. This allows us to perform\nvertex-wise analysis in the template space and thereby link morphometry\nmeasures with pathology measurements derived from histology. We will\nopen-source our dataset docker container, Jupyter notebooks for ready-to-use\nout-of-the-box set of tools and command line options to advance ex vivo MRI\nclinical brain imaging research on the project webpage.\n","authors":["Pulkit Khandelwal","Michael Tran Duong","Constanza Fuentes","Amanda Denning","Winifred Trotman","Ranjit Ittyerah","Alejandra Bahena","Theresa Schuck","Marianna Gabrielyan","Karthik Prabhakaran","Daniel Ohm","Gabor Mizsei","John Robinson","Monica Munoz","John Detre","Edward Lee","David Irwin","Corey McMillan","M. Dylan Tisdall","Sandhitsu Das","David Wolk","Paul A. Yushkevich"],"pdf_url":"https://arxiv.org/pdf/2403.19497v1.pdf","comment":"Under review at MICCAI 2024"},{"id":"http://arxiv.org/abs/2403.19495v1","updated":"2024-03-28T15:27:13Z","published":"2024-03-28T15:27:13Z","title":"CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians","summary":"  The field of 3D reconstruction from images has rapidly evolved in the past\nfew years, first with the introduction of Neural Radiance Field (NeRF) and more\nrecently with 3D Gaussian Splatting (3DGS). The latter provides a significant\nedge over NeRF in terms of the training and inference speed, as well as the\nreconstruction quality. Although 3DGS works well for dense input images, the\nunstructured point-cloud like representation quickly overfits to the more\nchallenging setup of extremely sparse input images (e.g., 3 images), creating a\nrepresentation that appears as a jumble of needles from novel views. To address\nthis issue, we propose regularized optimization and depth-based initialization.\nOur key idea is to introduce a structured Gaussian representation that can be\ncontrolled in 2D image space. We then constraint the Gaussians, in particular\ntheir position, and prevent them from moving independently during optimization.\nSpecifically, we introduce single and multiview constraints through an implicit\nconvolutional decoder and a total variation loss, respectively. With the\ncoherency introduced to the Gaussians, we further constrain the optimization\nthrough a flow-based loss function. To support our regularized optimization, we\npropose an approach to initialize the Gaussians using monocular depth estimates\nat each input view. We demonstrate significant improvements compared to the\nstate-of-the-art sparse-view NeRF-based approaches on a variety of scenes.\n","authors":["Avinash Paliwal","Wei Ye","Jinhui Xiong","Dmytro Kotovenko","Rakesh Ranjan","Vikas Chandra","Nima Khademi Kalantari"],"pdf_url":"https://arxiv.org/pdf/2403.19495v1.pdf","comment":"Project page: https://people.engr.tamu.edu/nimak/Papers/CoherentGS"},{"id":"http://arxiv.org/abs/2403.14472v2","updated":"2024-03-28T15:24:17Z","published":"2024-03-21T15:18:30Z","title":"Detoxifying Large Language Models via Knowledge Editing","summary":"  This paper investigates using knowledge editing techniques to detoxify Large\nLanguage Models (LLMs). We construct a benchmark, SafeEdit, which covers nine\nunsafe categories with various powerful attack prompts and equips comprehensive\nmetrics for systematic evaluation. We conduct experiments with several\nknowledge editing approaches, indicating that knowledge editing has the\npotential to efficiently detoxify LLMs with limited impact on general\nperformance. Then, we propose a simple yet effective baseline, dubbed\nDetoxifying with Intraoperative Neural Monitoring (DINM), to diminish the\ntoxicity of LLMs within a few tuning steps via only one instance. We further\nprovide an in-depth analysis of the internal mechanism for various detoxify\napproaches, demonstrating that previous methods like SFT and DPO may merely\nsuppress the activations of toxic parameters, while DINM mitigates the toxicity\nof the toxic parameters to a certain extent, making permanent adjustments. We\nhope that these insights could shed light on future work of developing\ndetoxifying approaches and the underlying knowledge mechanisms of LLMs. Code\nand benchmark are available at https://github.com/zjunlp/EasyEdit.\n","authors":["Mengru Wang","Ningyu Zhang","Ziwen Xu","Zekun Xi","Shumin Deng","Yunzhi Yao","Qishen Zhang","Linyi Yang","Jindong Wang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14472v2.pdf","comment":"Ongoing work. Project website:\n  https://zjunlp.github.io/project/SafeEdit Due to the specificity of the\n  knowledge editing setting, we revise Tables 1 and 3 to present a fair\n  comparison of experimental results. More experimental results will be updated\n  soon"},{"id":"http://arxiv.org/abs/2403.17608v2","updated":"2024-03-28T15:24:16Z","published":"2024-03-26T11:39:00Z","title":"Fake or JPEG? Revealing Common Biases in Generated Image Detection\n  Datasets","summary":"  The widespread adoption of generative image models has highlighted the urgent\nneed to detect artificial content, which is a crucial step in combating\nwidespread manipulation and misinformation. Consequently, numerous detectors\nand associated datasets have emerged. However, many of these datasets\ninadvertently introduce undesirable biases, thereby impacting the effectiveness\nand evaluation of detectors. In this paper, we emphasize that many datasets for\nAI-generated image detection contain biases related to JPEG compression and\nimage size. Using the GenImage dataset, we demonstrate that detectors indeed\nlearn from these undesired factors. Furthermore, we show that removing the\nnamed biases substantially increases robustness to JPEG compression and\nsignificantly alters the cross-generator performance of evaluated detectors.\nSpecifically, it leads to more than 11 percentage points increase in\ncross-generator performance for ResNet50 and Swin-T detectors on the GenImage\ndataset, achieving state-of-the-art results.\n  We provide the dataset and source codes of this paper on the anonymous\nwebsite: https://www.unbiased-genimage.org\n","authors":["Patrick Grommelt","Louis Weiss","Franz-Josef Pfreundt","Janis Keuper"],"pdf_url":"https://arxiv.org/pdf/2403.17608v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19492v1","updated":"2024-03-28T15:23:52Z","published":"2024-03-28T15:23:52Z","title":"Segmentation tool for images of cracks","summary":"  Safety-critical infrastructures, such as bridges, are periodically inspected\nto check for existing damage, such as fatigue cracks and corrosion, and to\nguarantee the safe use of the infrastructure. Visual inspection is the most\nfrequent type of general inspection, despite the fact that its detection\ncapability is rather limited, especially for fatigue cracks. Machine learning\nalgorithms can be used for augmenting the capability of classical visual\ninspection of bridge structures, however, the implementation of such an\nalgorithm requires a massive annotated training dataset, which is\ntime-consuming to produce. This paper proposes a semi-automatic crack\nsegmentation tool that eases the manual segmentation of cracks on images needed\nto create a training dataset for a machine learning algorithm. Also, it can be\nused to measure the geometry of the crack. This tool makes use of an image\nprocessing algorithm, which was initially developed for the analysis of\nvascular systems on retinal images. The algorithm relies on a multi-orientation\nwavelet transform, which is applied to the image to construct the so-called\n\"orientation scores\", i.e. a modified version of the image. Afterwards, the\nfiltered orientation scores are used to formulate an optimal path problem that\nidentifies the crack. The globally optimal path between manually selected crack\nendpoints is computed, using a state-of-the-art geometric tracking method. The\npixel-wise segmentation is done afterwards using the obtained crack path. The\nproposed method outperforms fully automatic methods and shows potential to be\nan adequate alternative to the manual data annotation.\n","authors":["Andrii Kompanets","Remco Duits","Davide Leonetti","Nicky van den Berg","H. H."," Snijder"],"pdf_url":"https://arxiv.org/pdf/2403.19492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19490v1","updated":"2024-03-28T15:22:29Z","published":"2024-03-28T15:22:29Z","title":"Jointly Training and Pruning CNNs via Learnable Agent Guidance and\n  Alignment","summary":"  Structural model pruning is a prominent approach used for reducing the\ncomputational cost of Convolutional Neural Networks (CNNs) before their\ndeployment on resource-constrained devices. Yet, the majority of proposed ideas\nrequire a pretrained model before pruning, which is costly to secure. In this\npaper, we propose a novel structural pruning approach to jointly learn the\nweights and structurally prune architectures of CNN models. The core element of\nour method is a Reinforcement Learning (RL) agent whose actions determine the\npruning ratios of the CNN model's layers, and the resulting model's accuracy\nserves as its reward. We conduct the joint training and pruning by iteratively\ntraining the model's weights and the agent's policy, and we regularize the\nmodel's weights to align with the selected structure by the agent. The evolving\nmodel's weights result in a dynamic reward function for the agent, which\nprevents using prominent episodic RL methods with stationary environment\nassumption for our purpose. We address this challenge by designing a mechanism\nto model the complex changing dynamics of the reward function and provide a\nrepresentation of it to the RL agent. To do so, we take a learnable embedding\nfor each training epoch and employ a recurrent model to calculate a\nrepresentation of the changing environment. We train the recurrent model and\nembeddings using a decoder model to reconstruct observed rewards. Such a design\nempowers our agent to effectively leverage episodic observations along with the\nenvironment representations to learn a proper policy to determine performant\nsub-networks of the CNN model. Our extensive experiments on CIFAR-10 and\nImageNet using ResNets and MobileNets demonstrate the effectiveness of our\nmethod.\n","authors":["Alireza Ganjdanesh","Shangqian Gao","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2403.19490v1.pdf","comment":"IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR\n  2024"},{"id":"http://arxiv.org/abs/2311.16516v4","updated":"2024-03-28T15:15:04Z","published":"2023-11-27T18:20:03Z","title":"Segment Every Out-of-Distribution Object","summary":"  Semantic segmentation models, while effective for in-distribution categories,\nface challenges in real-world deployment due to encountering\nout-of-distribution (OoD) objects. Detecting these OoD objects is crucial for\nsafety-critical applications. Existing methods rely on anomaly scores, but\nchoosing a suitable threshold for generating masks presents difficulties and\ncan lead to fragmentation and inaccuracy. This paper introduces a method to\nconvert anomaly \\textbf{S}core \\textbf{T}o segmentation \\textbf{M}ask, called\nS2M, a simple and effective framework for OoD detection in semantic\nsegmentation. Unlike assigning anomaly scores to pixels, S2M directly segments\nthe entire OoD object. By transforming anomaly scores into prompts for a\npromptable segmentation model, S2M eliminates the need for threshold selection.\nExtensive experiments demonstrate that S2M outperforms the state-of-the-art by\napproximately 20% in IoU and 40% in mean F1 score, on average, across various\nbenchmarks including Fishyscapes, Segment-Me-If-You-Can, and RoadAnomaly\ndatasets.\n","authors":["Wenjie Zhao","Jia Li","Xin Dong","Yu Xiang","Yunhui Guo"],"pdf_url":"https://arxiv.org/pdf/2311.16516v4.pdf","comment":"20 pages, 14 figures"},{"id":"http://arxiv.org/abs/2403.19474v1","updated":"2024-03-28T15:01:58Z","published":"2024-03-28T15:01:58Z","title":"SG-PGM: Partial Graph Matching Network with Semantic Geometric Fusion\n  for 3D Scene Graph Alignment and Its Downstream Tasks","summary":"  Scene graphs have been recently introduced into 3D spatial understanding as a\ncomprehensive representation of the scene. The alignment between 3D scene\ngraphs is the first step of many downstream tasks such as scene graph aided\npoint cloud registration, mosaicking, overlap checking, and robot navigation.\nIn this work, we treat 3D scene graph alignment as a partial graph-matching\nproblem and propose to solve it with a graph neural network. We reuse the\ngeometric features learned by a point cloud registration method and associate\nthe clustered point-level geometric features with the node-level semantic\nfeature via our designed feature fusion module. Partial matching is enabled by\nusing a learnable method to select the top-k similar node pairs. Subsequent\ndownstream tasks such as point cloud registration are achieved by running a\npre-trained registration network within the matched regions. We further propose\na point-matching rescoring method, that uses the node-wise alignment of the 3D\nscene graph to reweight the matching candidates from a pre-trained point cloud\nregistration method. It reduces the false point correspondences estimated\nespecially in low-overlapping cases. Experiments show that our method improves\nthe alignment accuracy by 10~20% in low-overlap and random transformation\nscenarios and outperforms the existing work in multiple downstream tasks.\n","authors":["Yaxu Xie","Alain Pagani","Didier Stricker"],"pdf_url":"https://arxiv.org/pdf/2403.19474v1.pdf","comment":"16 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.15905v3","updated":"2024-03-28T15:00:04Z","published":"2024-03-23T18:19:02Z","title":"Towards Low-Energy Adaptive Personalization for Resource-Constrained\n  Devices","summary":"  The personalization of machine learning (ML) models to address data drift is\na significant challenge in the context of Internet of Things (IoT)\napplications. Presently, most approaches focus on fine-tuning either the full\nbase model or its last few layers to adapt to new data, while often neglecting\nenergy costs. However, various types of data drift exist, and fine-tuning the\nfull base model or the last few layers may not result in optimal performance in\ncertain scenarios. We propose Target Block Fine-Tuning (TBFT), a low-energy\nadaptive personalization framework designed for resource-constrained devices.\nWe categorize data drift and personalization into three types: input-level,\nfeature-level, and output-level. For each type, we fine-tune different blocks\nof the model to achieve optimal performance with reduced energy costs.\nSpecifically, input-, feature-, and output-level correspond to fine-tuning the\nfront, middle, and rear blocks of the model. We evaluate TBFT on a ResNet\nmodel, three datasets, three different training sizes, and a Raspberry Pi.\nCompared with the $Block Avg$, where each block is fine-tuned individually and\ntheir performance improvements are averaged, TBFT exhibits an improvement in\nmodel accuracy by an average of 15.30% whilst saving 41.57% energy consumption\non average compared with full fine-tuning.\n","authors":["Yushan Huang","Josh Millar","Yuxuan Long","Yuchen Zhao","Hamed Hadaddi"],"pdf_url":"https://arxiv.org/pdf/2403.15905v3.pdf","comment":"Accepetd to The 4th Workshop on Machine Learning and Systems\n  (EuroMLSys '24)"},{"id":"http://arxiv.org/abs/2403.19473v1","updated":"2024-03-28T14:59:56Z","published":"2024-03-28T14:59:56Z","title":"Benchmarking Implicit Neural Representation and Geometric Rendering in\n  Real-Time RGB-D SLAM","summary":"  Implicit neural representation (INR), in combination with geometric\nrendering, has recently been employed in real-time dense RGB-D SLAM. Despite\nactive research endeavors being made, there lacks a unified protocol for fair\nevaluation, impeding the evolution of this area. In this work, we establish, to\nour knowledge, the first open-source benchmark framework to evaluate the\nperformance of a wide spectrum of commonly used INRs and rendering functions\nfor mapping and localization. The goal of our benchmark is to 1) gain an\nintuition of how different INRs and rendering functions impact mapping and\nlocalization and 2) establish a unified evaluation protocol w.r.t. the design\nchoices that may impact the mapping and localization. With the framework, we\nconduct a large suite of experiments, offering various insights in choosing the\nINRs and geometric rendering functions: for example, the dense feature grid\noutperforms other INRs (e.g. tri-plane and hash grid), even when geometric and\ncolor features are jointly encoded for memory efficiency. To extend the\nfindings into the practical scenario, a hybrid encoding strategy is proposed to\nbring the best of the accuracy and completion from the grid-based and\ndecomposition-based INRs. We further propose explicit hybrid encoding for\nhigh-fidelity dense grid mapping to comply with the RGB-D SLAM system that puts\nthe premise on robustness and computation efficiency.\n","authors":["Tongyan Hua","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.19473v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18361v2","updated":"2024-03-28T14:59:44Z","published":"2024-03-27T08:53:13Z","title":"ViTAR: Vision Transformer with Any Resolution","summary":"  This paper tackles a significant challenge faced by Vision Transformers\n(ViTs): their constrained scalability across different image resolutions.\nTypically, ViTs experience a performance decline when processing resolutions\ndifferent from those seen during training. Our work introduces two key\ninnovations to address this issue. Firstly, we propose a novel module for\ndynamic resolution adjustment, designed with a single Transformer block,\nspecifically to achieve highly efficient incremental token integration.\nSecondly, we introduce fuzzy positional encoding in the Vision Transformer to\nprovide consistent positional awareness across multiple resolutions, thereby\npreventing overfitting to any single training resolution. Our resulting model,\nViTAR (Vision Transformer with Any Resolution), demonstrates impressive\nadaptability, achieving 83.3\\% top-1 accuracy at a 1120x1120 resolution and\n80.4\\% accuracy at a 4032x4032 resolution, all while reducing computational\ncosts. ViTAR also shows strong performance in downstream tasks such as instance\nand semantic segmentation and can easily combined with self-supervised learning\ntechniques like Masked AutoEncoder. Our work provides a cost-effective solution\nfor enhancing the resolution scalability of ViTs, paving the way for more\nversatile and efficient high-resolution image processing.\n","authors":["Qihang Fan","Quanzeng You","Xiaotian Han","Yongfei Liu","Yunzhe Tao","Huaibo Huang","Ran He","Hongxia Yang"],"pdf_url":"https://arxiv.org/pdf/2403.18361v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17216v2","updated":"2024-03-28T14:58:59Z","published":"2023-11-28T20:40:45Z","title":"Self-Discovering Interpretable Diffusion Latent Directions for\n  Responsible Text-to-Image Generation","summary":"  Diffusion-based models have gained significant popularity for text-to-image\ngeneration due to their exceptional image-generation capabilities. A risk with\nthese models is the potential generation of inappropriate content, such as\nbiased or harmful images. However, the underlying reasons for generating such\nundesired content from the perspective of the diffusion model's internal\nrepresentation remain unclear. Previous work interprets vectors in an\ninterpretable latent space of diffusion models as semantic concepts. However,\nexisting approaches cannot discover directions for arbitrary concepts, such as\nthose related to inappropriate concepts. In this work, we propose a novel\nself-supervised approach to find interpretable latent directions for a given\nconcept. With the discovered vectors, we further propose a simple approach to\nmitigate inappropriate generation. Extensive experiments have been conducted to\nverify the effectiveness of our mitigation approach, namely, for fair\ngeneration, safe generation, and responsible text-enhancing generation. Project\npage: \\url{https://interpretdiffusion.github.io}.\n","authors":["Hang Li","Chengzhi Shen","Philip Torr","Volker Tresp","Jindong Gu"],"pdf_url":"https://arxiv.org/pdf/2311.17216v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.19467v1","updated":"2024-03-28T14:47:32Z","published":"2024-03-28T14:47:32Z","title":"Beyond Talking -- Generating Holistic 3D Human Dyadic Motion for\n  Communication","summary":"  In this paper, we introduce an innovative task focused on human\ncommunication, aiming to generate 3D holistic human motions for both speakers\nand listeners. Central to our approach is the incorporation of factorization to\ndecouple audio features and the combination of textual semantic information,\nthereby facilitating the creation of more realistic and coordinated movements.\nWe separately train VQ-VAEs with respect to the holistic motions of both\nspeaker and listener. We consider the real-time mutual influence between the\nspeaker and the listener and propose a novel chain-like transformer-based\nauto-regressive model specifically designed to characterize real-world\ncommunication scenarios effectively which can generate the motions of both the\nspeaker and the listener simultaneously. These designs ensure that the results\nwe generate are both coordinated and diverse. Our approach demonstrates\nstate-of-the-art performance on two benchmark datasets. Furthermore, we\nintroduce the HoCo holistic communication dataset, which is a valuable resource\nfor future research. Our HoCo dataset and code will be released for research\npurposes upon acceptance.\n","authors":["Mingze Sun","Chao Xu","Xinyu Jiang","Yang Liu","Baigui Sun","Ruqi Huang"],"pdf_url":"https://arxiv.org/pdf/2403.19467v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19456v1","updated":"2024-03-28T14:27:36Z","published":"2024-03-28T14:27:36Z","title":"Break-for-Make: Modular Low-Rank Adaptations for Composable\n  Content-Style Customization","summary":"  Personalized generation paradigms empower designers to customize visual\nintellectual properties with the help of textual descriptions by tuning or\nadapting pre-trained text-to-image models on a few images. Recent works explore\napproaches for concurrently customizing both content and detailed visual style\nappearance. However, these existing approaches often generate images where the\ncontent and style are entangled. In this study, we reconsider the customization\nof content and style concepts from the perspective of parameter space\nconstruction. Unlike existing methods that utilize a shared parameter space for\ncontent and style, we propose a learning framework that separates the parameter\nspace to facilitate individual learning of content and style, thereby enabling\ndisentangled content and style. To achieve this goal, we introduce \"partly\nlearnable projection\" (PLP) matrices to separate the original adapters into\ndivided sub-parameter spaces. We propose \"break-for-make\" customization\nlearning pipeline based on PLP, which is simple yet effective. We break the\noriginal adapters into \"up projection\" and \"down projection\", train content and\nstyle PLPs individually with the guidance of corresponding textual prompts in\nthe separate adapters, and maintain generalization by employing a\nmulti-correspondence projection learning strategy. Based on the adapters broken\napart for separate training content and style, we then make the entity\nparameter space by reconstructing the content and style PLPs matrices, followed\nby fine-tuning the combined adapter to generate the target object with the\ndesired appearance. Experiments on various styles, including textures,\nmaterials, and artistic style, show that our method outperforms\nstate-of-the-art single/multiple concept learning pipelines in terms of\ncontent-style-prompt alignment.\n","authors":["Yu Xu","Fan Tang","Juan Cao","Yuxin Zhang","Oliver Deussen","Weiming Dong","Jintao Li","Tong-Yee Lee"],"pdf_url":"https://arxiv.org/pdf/2403.19456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.03008v2","updated":"2024-03-28T14:16:09Z","published":"2023-09-06T13:54:31Z","title":"Sparse 3D Reconstruction via Object-Centric Ray Sampling","summary":"  We propose a novel method for 3D object reconstruction from a sparse set of\nviews captured from a 360-degree calibrated camera rig. We represent the object\nsurface through a hybrid model that uses both an MLP-based neural\nrepresentation and a triangle mesh. A key contribution in our work is a novel\nobject-centric sampling scheme of the neural representation, where rays are\nshared among all views. This efficiently concentrates and reduces the number of\nsamples used to update the neural model at each iteration. This sampling scheme\nrelies on the mesh representation to ensure also that samples are\nwell-distributed along its normals. The rendering is then performed efficiently\nby a differentiable renderer. We demonstrate that this sampling scheme results\nin a more effective training of the neural representation, does not require the\nadditional supervision of segmentation masks, yields state of the art 3D\nreconstructions, and works with sparse views on the Google's Scanned Objects,\nTank and Temples and MVMC Car datasets. Code available at:\nhttps://github.com/llukmancerkezi/ROSTER\n","authors":["Llukman Cerkezi","Paolo Favaro"],"pdf_url":"https://arxiv.org/pdf/2309.03008v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19444v1","updated":"2024-03-28T14:15:13Z","published":"2024-03-28T14:15:13Z","title":"Transparent and Clinically Interpretable AI for Lung Cancer Detection in\n  Chest X-Rays","summary":"  The rapidly advancing field of Explainable Artificial Intelligence (XAI) aims\nto tackle the issue of trust regarding the use of complex black-box deep\nlearning models in real-world applications. Existing post-hoc XAI techniques\nhave recently been shown to have poor performance on medical data, producing\nunreliable explanations which are infeasible for clinical use. To address this,\nwe propose an ante-hoc approach based on concept bottleneck models which\nintroduces for the first time clinical concepts into the classification\npipeline, allowing the user valuable insight into the decision-making process.\nOn a large public dataset of chest X-rays and associated medical reports, we\nfocus on the binary classification task of lung cancer detection. Our approach\nyields improved classification performance in lung cancer detection when\ncompared to baseline deep learning models (F1 > 0.9), while also generating\nclinically relevant and more reliable explanations than existing techniques. We\nevaluate our approach against post-hoc image XAI techniques LIME and SHAP, as\nwell as CXR-LLaVA, a recent textual XAI tool which operates in the context of\nquestion answering on chest X-rays.\n","authors":["Amy Rafferty","Rishi Ramaesh","Ajitha Rajan"],"pdf_url":"https://arxiv.org/pdf/2403.19444v1.pdf","comment":"12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.09412v2","updated":"2024-03-28T14:10:08Z","published":"2024-03-14T14:03:29Z","title":"OpenGraph: Open-Vocabulary Hierarchical 3D Graph Representation in\n  Large-Scale Outdoor Environments","summary":"  Environment representations endowed with sophisticated semantics are pivotal\nfor facilitating seamless interaction between robots and humans, enabling them\nto effectively carry out various tasks. Open-vocabulary maps, powered by\nVisual-Language models (VLMs), possess inherent advantages, including zero-shot\nlearning and support for open-set classes. However, existing open-vocabulary\nmaps are primarily designed for small-scale environments, such as desktops or\nrooms, and are typically geared towards limited-area tasks involving robotic\nindoor navigation or in-place manipulation. They face challenges in direct\ngeneralization to outdoor environments characterized by numerous objects and\ncomplex tasks, owing to limitations in both understanding level and map\nstructure. In this work, we propose OpenGraph, the first open-vocabulary\nhierarchical graph representation designed for large-scale outdoor\nenvironments. OpenGraph initially extracts instances and their captions from\nvisual images, enhancing textual reasoning by encoding them. Subsequently, it\nachieves 3D incremental object-centric mapping with feature embedding by\nprojecting images onto LiDAR point clouds. Finally, the environment is\nsegmented based on lane graph connectivity to construct a hierarchical graph.\nValidation results from public dataset SemanticKITTI demonstrate that OpenGraph\nachieves the highest segmentation and query accuracy. The source code of\nOpenGraph is publicly available at https://github.com/BIT-DYN/OpenGraph.\n","authors":["Yinan Deng","Jiahui Wang","Jingyu Zhao","Xinyu Tian","Guangyan Chen","Yi Yang","Yufeng Yue"],"pdf_url":"https://arxiv.org/pdf/2403.09412v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19438v1","updated":"2024-03-28T14:07:13Z","published":"2024-03-28T14:07:13Z","title":"SubjectDrive: Scaling Generative Data in Autonomous Driving via Subject\n  Control","summary":"  Autonomous driving progress relies on large-scale annotated datasets. In this\nwork, we explore the potential of generative models to produce vast quantities\nof freely-labeled data for autonomous driving applications and present\nSubjectDrive, the first model proven to scale generative data production in a\nway that could continuously improve autonomous driving applications. We\ninvestigate the impact of scaling up the quantity of generative data on the\nperformance of downstream perception models and find that enhancing data\ndiversity plays a crucial role in effectively scaling generative data\nproduction. Therefore, we have developed a novel model equipped with a subject\ncontrol mechanism, which allows the generative model to leverage diverse\nexternal data sources for producing varied and useful data. Extensive\nevaluations confirm SubjectDrive's efficacy in generating scalable autonomous\ndriving training data, marking a significant step toward revolutionizing data\nproduction methods in this field.\n","authors":["Binyuan Huang","Yuqing Wen","Yucheng Zhao","Yaosi Hu","Yingfei Liu","Fan Jia","Weixin Mao","Tiancai Wang","Chi Zhang","Chang Wen Chen","Zhenzhong Chen","Xiangyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.19438v1.pdf","comment":"Project page: https://subjectdrive.github.io/"},{"id":"http://arxiv.org/abs/2403.19435v1","updated":"2024-03-28T14:04:17Z","published":"2024-03-28T14:04:17Z","title":"BAMM: Bidirectional Autoregressive Motion Model","summary":"  Generating human motion from text has been dominated by denoising motion\nmodels either through diffusion or generative masking process. However, these\nmodels face great limitations in usability by requiring prior knowledge of the\nmotion length. Conversely, autoregressive motion models address this limitation\nby adaptively predicting motion endpoints, at the cost of degraded generation\nquality and editing capabilities. To address these challenges, we propose\nBidirectional Autoregressive Motion Model (BAMM), a novel text-to-motion\ngeneration framework. BAMM consists of two key components: (1) a motion\ntokenizer that transforms 3D human motion into discrete tokens in latent space,\nand (2) a masked self-attention transformer that autoregressively predicts\nrandomly masked tokens via a hybrid attention masking strategy. By unifying\ngenerative masked modeling and autoregressive modeling, BAMM captures rich and\nbidirectional dependencies among motion tokens, while learning the\nprobabilistic mapping from textual inputs to motion outputs with\ndynamically-adjusted motion sequence length. This feature enables BAMM to\nsimultaneously achieving high-quality motion generation with enhanced usability\nand built-in motion editability. Extensive experiments on HumanML3D and KIT-ML\ndatasets demonstrate that BAMM surpasses current state-of-the-art methods in\nboth qualitative and quantitative measures.\n","authors":["Ekkasit Pinyoanuntapong","Muhammad Usama Saleem","Pu Wang","Minwoo Lee","Srijan Das","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2403.19435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19428v1","updated":"2024-03-28T13:58:05Z","published":"2024-03-28T13:58:05Z","title":"Burst Super-Resolution with Diffusion Models for Improving Perceptual\n  Quality","summary":"  While burst LR images are useful for improving the SR image quality compared\nwith a single LR image, prior SR networks accepting the burst LR images are\ntrained in a deterministic manner, which is known to produce a blurry SR image.\nIn addition, it is difficult to perfectly align the burst LR images, making the\nSR image more blurry. Since such blurry images are perceptually degraded, we\naim to reconstruct the sharp high-fidelity boundaries. Such high-fidelity\nimages can be reconstructed by diffusion models. However, prior SR methods\nusing the diffusion model are not properly optimized for the burst SR task.\nSpecifically, the reverse process starting from a random sample is not\noptimized for image enhancement and restoration methods, including burst SR. In\nour proposed method, on the other hand, burst LR features are used to\nreconstruct the initial burst SR image that is fed into an intermediate step in\nthe diffusion model. This reverse process from the intermediate step 1) skips\ndiffusion steps for reconstructing the global structure of the image and 2)\nfocuses on steps for refining detailed textures. Our experimental results\ndemonstrate that our method can improve the scores of the perceptual quality\nmetrics. Code: https://github.com/placerkyo/BSRD\n","authors":["Kyotaro Tokoro","Kazutoshi Akita","Norimichi Ukita"],"pdf_url":"https://arxiv.org/pdf/2403.19428v1.pdf","comment":"Accepted to IJCNN 2024 (International Joint Conference on Neural\n  Networks)"},{"id":"http://arxiv.org/abs/2403.19425v1","updated":"2024-03-28T13:56:26Z","published":"2024-03-28T13:56:26Z","title":"A Robust Ensemble Algorithm for Ischemic Stroke Lesion Segmentation:\n  Generalizability and Clinical Utility Beyond the ISLES Challenge","summary":"  Diffusion-weighted MRI (DWI) is essential for stroke diagnosis, treatment\ndecisions, and prognosis. However, image and disease variability hinder the\ndevelopment of generalizable AI algorithms with clinical value. We address this\ngap by presenting a novel ensemble algorithm derived from the 2022 Ischemic\nStroke Lesion Segmentation (ISLES) challenge. ISLES'22 provided 400 patient\nscans with ischemic stroke from various medical centers, facilitating the\ndevelopment of a wide range of cutting-edge segmentation algorithms by the\nresearch community. Through collaboration with leading teams, we combined\ntop-performing algorithms into an ensemble model that overcomes the limitations\nof individual solutions. Our ensemble model achieved superior ischemic lesion\ndetection and segmentation accuracy on our internal test set compared to\nindividual algorithms. This accuracy generalized well across diverse image and\ndisease variables. Furthermore, the model excelled in extracting clinical\nbiomarkers. Notably, in a Turing-like test, neuroradiologists consistently\npreferred the algorithm's segmentations over manual expert efforts,\nhighlighting increased comprehensiveness and precision. Validation using a\nreal-world external dataset (N=1686) confirmed the model's generalizability.\nThe algorithm's outputs also demonstrated strong correlations with clinical\nscores (admission NIHSS and 90-day mRS) on par with or exceeding expert-derived\nresults, underlining its clinical relevance. This study offers two key\nfindings. First, we present an ensemble algorithm\n(https://github.com/Tabrisrei/ISLES22_Ensemble) that detects and segments\nischemic stroke lesions on DWI across diverse scenarios on par with expert\n(neuro)radiologists. Second, we show the potential for biomedical challenge\noutputs to extend beyond the challenge's initial objectives, demonstrating\ntheir real-world clinical applicability.\n","authors":["Ezequiel de la Rosa","Mauricio Reyes","Sook-Lei Liew","Alexandre Hutton","Roland Wiest","Johannes Kaesmacher","Uta Hanning","Arsany Hakim","Richard Zubal","Waldo Valenzuela","David Robben","Diana M. Sima","Vincenzo Anania","Arne Brys","James A. Meakin","Anne Mickan","Gabriel Broocks","Christian Heitkamp","Shengbo Gao","Kongming Liang","Ziji Zhang","Md Mahfuzur Rahman Siddiquee","Andriy Myronenko","Pooya Ashtari","Sabine Van Huffel","Hyun-su Jeong","Chi-ho Yoon","Chulhong Kim","Jiayu Huo","Sebastien Ourselin","Rachel Sparks","Albert Clèrigues","Arnau Oliver","Xavier Lladó","Liam Chalcroft","Ioannis Pappas","Jeroen Bertels","Ewout Heylen","Juliette Moreau","Nima Hatami","Carole Frindel","Abdul Qayyum","Moona Mazher","Domenec Puig","Shao-Chieh Lin","Chun-Jung Juan","Tianxi Hu","Lyndon Boone","Maged Goubran","Yi-Jui Liu","Susanne Wegener","Florian Kofler","Ivan Ezhov","Suprosanna Shit","Moritz R. Hernandez Petzsche","Bjoern Menze","Jan S. Kirschke","Benedikt Wiestler"],"pdf_url":"https://arxiv.org/pdf/2403.19425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.19507v3","updated":"2024-03-28T13:51:37Z","published":"2023-05-31T02:35:41Z","title":"Manifold Constraint Regularization for Remote Sensing Image Generation","summary":"  Generative Adversarial Networks (GANs) have shown notable accomplishments in\nremote sensing domain. However, this paper reveals that their performance on\nremote sensing images falls short when compared to their impressive results\nwith natural images. This study identifies a previously overlooked issue: GANs\nexhibit a heightened susceptibility to overfitting on remote sensing images.To\naddress this challenge, this paper analyzes the characteristics of remote\nsensing images and proposes manifold constraint regularization, a novel\napproach that tackles overfitting of GANs on remote sensing images for the\nfirst time. Our method includes a new measure for evaluating the structure of\nthe data manifold. Leveraging this measure, we propose the manifold constraint\nregularization term, which not only alleviates the overfitting problem, but\nalso promotes alignment between the generated and real data manifolds, leading\nto enhanced quality in the generated images. The effectiveness and versatility\nof this method have been corroborated through extensive validation on various\nremote sensing datasets and GAN models. The proposed method not only enhances\nthe quality of the generated images, reflected in a 3.13\\% improvement in\nFrechet Inception Distance (FID) score, but also boosts the performance of the\nGANs on downstream tasks, evidenced by a 3.76\\% increase in classification\naccuracy.\n","authors":["Xingzhe Su","Changwen Zheng","Wenwen Qiang","Fengge Wu","Junsuo Zhao","Fuchun Sun","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2305.19507v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08471v2","updated":"2024-03-28T13:47:42Z","published":"2023-10-09T20:18:10Z","title":"WinSyn: A High Resolution Testbed for Synthetic Data","summary":"  We present WinSyn, a unique dataset and testbed for creating high-quality\nsynthetic data with procedural modeling techniques. The dataset contains\nhigh-resolution photographs of windows, selected from locations around the\nworld, with 89,318 individual window crops showcasing diverse geometric and\nmaterial characteristics. We evaluate a procedural model by training semantic\nsegmentation networks on both synthetic and real images and then comparing\ntheir performances on a shared test set of real images. Specifically, we\nmeasure the difference in mean Intersection over Union (mIoU) and determine the\neffective number of real images to match synthetic data's training performance.\nWe design a baseline procedural model as a benchmark and provide 21,290\nsynthetically generated images. By tuning the procedural model, key factors are\nidentified which significantly influence the model's fidelity in replicating\nreal-world scenarios. Importantly, we highlight the challenge of procedural\nmodeling using current techniques, especially in their ability to replicate the\nspatial semantics of real-world scenarios. This insight is critical because of\nthe potential of procedural models to bridge to hidden scene aspects such as\ndepth, reflectivity, material properties, and lighting conditions.\n","authors":["Tom Kelly","John Femiani","Peter Wonka"],"pdf_url":"https://arxiv.org/pdf/2310.08471v2.pdf","comment":"cvpr version"},{"id":"http://arxiv.org/abs/2403.19417v1","updated":"2024-03-28T13:47:19Z","published":"2024-03-28T13:47:19Z","title":"OAKINK2: A Dataset of Bimanual Hands-Object Manipulation in Complex Task\n  Completion","summary":"  We present OAKINK2, a dataset of bimanual object manipulation tasks for\ncomplex daily activities. In pursuit of constructing the complex tasks into a\nstructured representation, OAKINK2 introduces three level of abstraction to\norganize the manipulation tasks: Affordance, Primitive Task, and Complex Task.\nOAKINK2 features on an object-centric perspective for decoding the complex\ntasks, treating them as a sequence of object affordance fulfillment. The first\nlevel, Affordance, outlines the functionalities that objects in the scene can\nafford, the second level, Primitive Task, describes the minimal interaction\nunits that humans interact with the object to achieve its affordance, and the\nthird level, Complex Task, illustrates how Primitive Tasks are composed and\ninterdependent. OAKINK2 dataset provides multi-view image streams and precise\npose annotations for the human body, hands and various interacting objects.\nThis extensive collection supports applications such as interaction\nreconstruction and motion synthesis. Based on the 3-level abstraction of\nOAKINK2, we explore a task-oriented framework for Complex Task Completion\n(CTC). CTC aims to generate a sequence of bimanual manipulation to achieve task\nobjectives. Within the CTC framework, we employ Large Language Models (LLMs) to\ndecompose the complex task objectives into sequences of Primitive Tasks and\nhave developed a Motion Fulfillment Model that generates bimanual hand motion\nfor each Primitive Task. OAKINK2 datasets and models are available at\nhttps://oakink.net/v2.\n","authors":["Xinyu Zhan","Lixin Yang","Yifei Zhao","Kangrui Mao","Hanlin Xu","Zenan Lin","Kailin Li","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2403.19417v1.pdf","comment":"To be appeared in CVPR 2024. 26 pages"},{"id":"http://arxiv.org/abs/2403.05369v4","updated":"2024-03-28T13:41:05Z","published":"2024-03-08T15:00:44Z","title":"Frequency-Adaptive Dilated Convolution for Semantic Segmentation","summary":"  Dilated convolution, which expands the receptive field by inserting gaps\nbetween its consecutive elements, is widely employed in computer vision. In\nthis study, we propose three strategies to improve individual phases of dilated\nconvolution from the view of spectrum analysis. Departing from the conventional\npractice of fixing a global dilation rate as a hyperparameter, we introduce\nFrequency-Adaptive Dilated Convolution (FADC), which dynamically adjusts\ndilation rates spatially based on local frequency components. Subsequently, we\ndesign two plug-in modules to directly enhance effective bandwidth and\nreceptive field size. The Adaptive Kernel (AdaKern) module decomposes\nconvolution weights into low-frequency and high-frequency components,\ndynamically adjusting the ratio between these components on a per-channel\nbasis. By increasing the high-frequency part of convolution weights, AdaKern\ncaptures more high-frequency components, thereby improving effective bandwidth.\nThe Frequency Selection (FreqSelect) module optimally balances high- and\nlow-frequency components in feature representations through spatially variant\nreweighting. It suppresses high frequencies in the background to encourage FADC\nto learn a larger dilation, thereby increasing the receptive field for an\nexpanded scope. Extensive experiments on segmentation and object detection\nconsistently validate the efficacy of our approach. The code is publicly\navailable at \\url{https://github.com/Linwei-Chen/FADC}.\n","authors":["Linwei Chen","Lin Gu","Ying Fu"],"pdf_url":"https://arxiv.org/pdf/2403.05369v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19415v1","updated":"2024-03-28T13:39:55Z","published":"2024-03-28T13:39:55Z","title":"Brain-Shift: Unsupervised Pseudo-Healthy Brain Synthesis for Novel\n  Biomarker Extraction in Chronic Subdural Hematoma","summary":"  Chronic subdural hematoma (cSDH) is a common neurological condition\ncharacterized by the accumulation of blood between the brain and the dura\nmater. This accumulation of blood can exert pressure on the brain, potentially\nleading to fatal outcomes. Treatment options for cSDH are limited to invasive\nsurgery or non-invasive management. Traditionally, the midline shift,\nhand-measured by experts from an ideal sagittal plane, and the hematoma volume\nhave been the primary metrics for quantifying and analyzing cSDH. However,\nthese approaches do not quantify the local 3D brain deformation caused by cSDH.\nWe propose a novel method using anatomy-aware unsupervised diffeomorphic\npseudo-healthy synthesis to generate brain deformation fields. The deformation\nfields derived from this process are utilized to extract biomarkers that\nquantify the shift in the brain due to cSDH. We use CT scans of 121 patients\nfor training and validation of our method and find that our metrics allow the\nidentification of patients who require surgery. Our results indicate that\nautomatically obtained brain deformation fields might contain prognostic value\nfor personalized cSDH treatment. Our implementation is available on:\ngithub.com/Barisimre/brain-morphing\n","authors":["Baris Imre","Elina Thibeau-Sutre","Jorieke Reimer","Kuan Kho","Jelmer M. Wolterink"],"pdf_url":"https://arxiv.org/pdf/2403.19415v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19412v1","updated":"2024-03-28T13:36:00Z","published":"2024-03-28T13:36:00Z","title":"A Simple and Effective Point-based Network for Event Camera 6-DOFs Pose\n  Relocalization","summary":"  Event cameras exhibit remarkable attributes such as high dynamic range,\nasynchronicity, and low latency, making them highly suitable for vision tasks\nthat involve high-speed motion in challenging lighting conditions. These\ncameras implicitly capture movement and depth information in events, making\nthem appealing sensors for Camera Pose Relocalization (CPR) tasks.\nNevertheless, existing CPR networks based on events neglect the pivotal\nfine-grained temporal information in events, resulting in unsatisfactory\nperformance. Moreover, the energy-efficient features are further compromised by\nthe use of excessively complex models, hindering efficient deployment on edge\ndevices. In this paper, we introduce PEPNet, a simple and effective point-based\nnetwork designed to regress six degrees of freedom (6-DOFs) event camera poses.\nWe rethink the relationship between the event camera and CPR tasks, leveraging\nthe raw Point Cloud directly as network input to harness the high-temporal\nresolution and inherent sparsity of events. PEPNet is adept at abstracting the\nspatial and implicit temporal features through hierarchical structure and\nexplicit temporal features by Attentive Bi-directional Long Short-Term Memory\n(A-Bi-LSTM). By employing a carefully crafted lightweight design, PEPNet\ndelivers state-of-the-art (SOTA) performance on both indoor and outdoor\ndatasets with meager computational resources. Specifically, PEPNet attains a\nsignificant 38% and 33% performance improvement on the random split IJRR and\nM3ED datasets, respectively. Moreover, the lightweight design version\nPEPNet$_{tiny}$ accomplishes results comparable to the SOTA while employing a\nmere 0.5% of the parameters.\n","authors":["Hongwei Ren","Jiadong Zhu","Yue Zhou","Haotian FU","Yulong Huang","Bojun Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.19412v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.19407v1","updated":"2024-03-28T13:32:49Z","published":"2024-03-28T13:32:49Z","title":"Towards Temporally Consistent Referring Video Object Segmentation","summary":"  Referring Video Object Segmentation (R-VOS) methods face challenges in\nmaintaining consistent object segmentation due to temporal context variability\nand the presence of other visually similar objects. We propose an end-to-end\nR-VOS paradigm that explicitly models temporal instance consistency alongside\nthe referring segmentation. Specifically, we introduce a novel hybrid memory\nthat facilitates inter-frame collaboration for robust spatio-temporal matching\nand propagation. Features of frames with automatically generated high-quality\nreference masks are propagated to segment the remaining frames based on\nmulti-granularity association to achieve temporally consistent R-VOS.\nFurthermore, we propose a new Mask Consistency Score (MCS) metric to evaluate\nthe temporal consistency of video segmentation. Extensive experiments\ndemonstrate that our approach enhances temporal consistency by a significant\nmargin, leading to top-ranked performance on popular R-VOS benchmarks, i.e.,\nRef-YouTube-VOS (67.1%) and Ref-DAVIS17 (65.6%).\n","authors":["Bo Miao","Mohammed Bennamoun","Yongsheng Gao","Mubarak Shah","Ajmal Mian"],"pdf_url":"https://arxiv.org/pdf/2403.19407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.18331v2","updated":"2024-03-28T13:27:33Z","published":"2023-11-30T08:02:49Z","title":"MRFP: Learning Generalizable Semantic Segmentation from Sim-2-Real with\n  Multi-Resolution Feature Perturbation","summary":"  Deep neural networks have shown exemplary performance on semantic scene\nunderstanding tasks on source domains, but due to the absence of style\ndiversity during training, enhancing performance on unseen target domains using\nonly single source domain data remains a challenging task. Generation of\nsimulated data is a feasible alternative to retrieving large style-diverse\nreal-world datasets as it is a cumbersome and budget-intensive process.\nHowever, the large domain-specfic inconsistencies between simulated and\nreal-world data pose a significant generalization challenge in semantic\nsegmentation. In this work, to alleviate this problem, we propose a novel\nMultiResolution Feature Perturbation (MRFP) technique to randomize\ndomain-specific fine-grained features and perturb style of coarse features. Our\nexperimental results on various urban-scene segmentation datasets clearly\nindicate that, along with the perturbation of style-information, perturbation\nof fine-feature components is paramount to learn domain invariant robust\nfeature maps for semantic segmentation models. MRFP is a simple and\ncomputationally efficient, transferable module with no additional learnable\nparameters or objective functions, that helps state-of-the-art deep neural\nnetworks to learn robust domain invariant features for simulation-to-real\nsemantic segmentation.\n","authors":["Sumanth Udupa","Prajwal Gurunath","Aniruddh Sikdar","Suresh Sundaram"],"pdf_url":"https://arxiv.org/pdf/2311.18331v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2311.11908v3","updated":"2024-03-28T13:16:50Z","published":"2023-11-20T16:40:29Z","title":"Continual Learning: Applications and the Road Forward","summary":"  Continual learning is a subfield of machine learning, which aims to allow\nmachine learning models to continuously learn on new data, by accumulating\nknowledge without forgetting what was learned in the past. In this work, we\ntake a step back, and ask: \"Why should one care about continual learning in the\nfirst place?\". We set the stage by examining recent continual learning papers\npublished at four major machine learning conferences, and show that\nmemory-constrained settings dominate the field. Then, we discuss five open\nproblems in machine learning, and even though they might seem unrelated to\ncontinual learning at first sight, we show that continual learning will\ninevitably be part of their solution. These problems are model editing,\npersonalization and specialization, on-device learning, faster (re-)training\nand reinforcement learning. Finally, by comparing the desiderata from these\nunsolved problems and the current assumptions in continual learning, we\nhighlight and discuss four future directions for continual learning research.\nWe hope that this work offers an interesting perspective on the future of\ncontinual learning, while displaying its potential value and the paths we have\nto pursue in order to make it successful. This work is the result of the many\ndiscussions the authors had at the Dagstuhl seminar on Deep Continual Learning,\nin March 2023.\n","authors":["Eli Verwimp","Rahaf Aljundi","Shai Ben-David","Matthias Bethge","Andrea Cossu","Alexander Gepperth","Tyler L. Hayes","Eyke Hüllermeier","Christopher Kanan","Dhireesha Kudithipudi","Christoph H. Lampert","Martin Mundt","Razvan Pascanu","Adrian Popescu","Andreas S. Tolias","Joost van de Weijer","Bing Liu","Vincenzo Lomonaco","Tinne Tuytelaars","Gido M. van de Ven"],"pdf_url":"https://arxiv.org/pdf/2311.11908v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19386v1","updated":"2024-03-28T12:51:15Z","published":"2024-03-28T12:51:15Z","title":"PointCloud-Text Matching: Benchmark Datasets and a Baseline","summary":"  In this paper, we present and study a new instance-level retrieval task:\nPointCloud-Text Matching~(PTM), which aims to find the exact cross-modal\ninstance that matches a given point-cloud query or text query. PTM could be\napplied to various scenarios, such as indoor/urban-canyon localization and\nscene retrieval. However, there exists no suitable and targeted dataset for PTM\nin practice. Therefore, we construct three new PTM benchmark datasets, namely\n3D2T-SR, 3D2T-NR, and 3D2T-QA. We observe that the data is challenging and with\nnoisy correspondence due to the sparsity, noise, or disorder of point clouds\nand the ambiguity, vagueness, or incompleteness of texts, which make existing\ncross-modal matching methods ineffective for PTM. To tackle these challenges,\nwe propose a PTM baseline, named Robust PointCloud-Text Matching method (RoMa).\nRoMa consists of two modules: a Dual Attention Perception module (DAP) and a\nRobust Negative Contrastive Learning module (RNCL). Specifically, DAP leverages\ntoken-level and feature-level attention to adaptively focus on useful local and\nglobal features, and aggregate them into common representations, thereby\nreducing the adverse impact of noise and ambiguity. To handle noisy\ncorrespondence, RNCL divides negative pairs, which are much less error-prone\nthan positive pairs, into clean and noisy subsets, and assigns them forward and\nreverse optimization directions respectively, thus enhancing robustness against\nnoisy correspondence. We conduct extensive experiments on our benchmarks and\ndemonstrate the superiority of our RoMa.\n","authors":["Yanglin Feng","Yang Qin","Dezhong Peng","Hongyuan Zhu","Xi Peng","Peng Hu"],"pdf_url":"https://arxiv.org/pdf/2403.19386v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02051v2","updated":"2024-03-28T12:41:14Z","published":"2023-12-04T17:09:52Z","title":"TimeChat: A Time-sensitive Multimodal Large Language Model for Long\n  Video Understanding","summary":"  This work proposes TimeChat, a time-sensitive multimodal large language model\nspecifically designed for long video understanding. Our model incorporates two\nkey architectural contributions: (1) a timestamp-aware frame encoder that binds\nvisual content with the timestamp of each frame, and (2) a sliding video\nQ-Former that produces a video token sequence of varying lengths to accommodate\nvideos of various durations. Additionally, we construct an instruction-tuning\ndataset, encompassing 6 tasks and a total of 125K instances, to further enhance\nTimeChat's instruction-following performance. Experiment results across various\nvideo understanding tasks, such as dense captioning, temporal grounding, and\nhighlight detection, demonstrate TimeChat's strong zero-shot temporal\nlocalization and reasoning capabilities. For example, it achieves +9.2 F1 score\nand +2.8 CIDEr on YouCook2, +5.8 HIT@1 on QVHighlights, and +27.5 R@1 (IoU=0.5)\non Charades-STA, compared to state-of-the-art video large language models,\nholding the potential to serve as a versatile video assistant for long-form\nvideo comprehension tasks and satisfy realistic user requirements.\n","authors":["Shuhuai Ren","Linli Yao","Shicheng Li","Xu Sun","Lu Hou"],"pdf_url":"https://arxiv.org/pdf/2312.02051v2.pdf","comment":"CVPR 2024 camera-ready version, code is available at\n  https://github.com/RenShuhuai-Andy/TimeChat"},{"id":"http://arxiv.org/abs/2403.19376v1","updated":"2024-03-28T12:38:21Z","published":"2024-03-28T12:38:21Z","title":"NIGHT -- Non-Line-of-Sight Imaging from Indirect Time of Flight Data","summary":"  The acquisition of objects outside the Line-of-Sight of cameras is a very\nintriguing but also extremely challenging research topic. Recent works showed\nthe feasibility of this idea exploiting transient imaging data produced by\ncustom direct Time of Flight sensors. In this paper, for the first time, we\ntackle this problem using only data from an off-the-shelf indirect Time of\nFlight sensor without any further hardware requirement. We introduced a Deep\nLearning model able to re-frame the surfaces where light bounces happen as a\nvirtual mirror. This modeling makes the task easier to handle and also\nfacilitates the construction of annotated training data. From the obtained data\nit is possible to retrieve the depth information of the hidden scene. We also\nprovide a first-in-its-kind synthetic dataset for the task and demonstrate the\nfeasibility of the proposed idea over it.\n","authors":["Matteo Caligiuri","Adriano Simonetto","Gianluca Agresti","Pietro Zanuttigh"],"pdf_url":"https://arxiv.org/pdf/2403.19376v1.pdf","comment":"Submitted to ECCV 24, 17 pages, 6 figures, 2 tables"},{"id":"http://arxiv.org/abs/2403.19366v1","updated":"2024-03-28T12:28:58Z","published":"2024-03-28T12:28:58Z","title":"Infrared Small Target Detection with Scale and Location Sensitivity","summary":"  Recently, infrared small target detection (IRSTD) has been dominated by\ndeep-learning-based methods. However, these methods mainly focus on the design\nof complex model structures to extract discriminative features, leaving the\nloss functions for IRSTD under-explored. For example, the widely used\nIntersection over Union (IoU) and Dice losses lack sensitivity to the scales\nand locations of targets, limiting the detection performance of detectors. In\nthis paper, we focus on boosting detection performance with a more effective\nloss but a simpler model structure. Specifically, we first propose a novel\nScale and Location Sensitive (SLS) loss to handle the limitations of existing\nlosses: 1) for scale sensitivity, we compute a weight for the IoU loss based on\ntarget scales to help the detector distinguish targets with different scales:\n2) for location sensitivity, we introduce a penalty term based on the center\npoints of targets to help the detector localize targets more precisely. Then,\nwe design a simple Multi-Scale Head to the plain U-Net (MSHNet). By applying\nSLS loss to each scale of the predictions, our MSHNet outperforms existing\nstate-of-the-art methods by a large margin. In addition, the detection\nperformance of existing detectors can be further improved when trained with our\nSLS loss, demonstrating the effectiveness and generalization of our SLS loss.\nThe code is available at https://github.com/ying-fu/MSHNet.\n","authors":["Qiankun Liu","Rui Liu","Bolun Zheng","Hongkui Wang","Ying Fu"],"pdf_url":"https://arxiv.org/pdf/2403.19366v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2304.03198v6","updated":"2024-03-28T12:07:44Z","published":"2023-04-06T16:21:56Z","title":"RFAConv: Innovating Spatial Attention and Standard Convolutional\n  Operation","summary":"  Spatial attention has been widely used to improve the performance of\nconvolutional neural networks. However, it has certain limitations. In this\npaper, we propose a new perspective on the effectiveness of spatial attention,\nwhich is that the spatial attention mechanism essentially solves the problem of\nconvolutional kernel parameter sharing. However, the information contained in\nthe attention map generated by spatial attention is not sufficient for\nlarge-size convolutional kernels. Therefore, we propose a novel attention\nmechanism called Receptive-Field Attention (RFA). Existing spatial attention,\nsuch as Convolutional Block Attention Module (CBAM) and Coordinated Attention\n(CA) focus only on spatial features, which does not fully address the problem\nof convolutional kernel parameter sharing. In contrast, RFA not only focuses on\nthe receptive-field spatial feature but also provides effective attention\nweights for large-size convolutional kernels. The Receptive-Field Attention\nconvolutional operation (RFAConv), developed by RFA, represents a new approach\nto replace the standard convolution operation. It offers nearly negligible\nincrement of computational cost and parameters, while significantly improving\nnetwork performance. We conducted a series of experiments on ImageNet-1k, COCO,\nand VOC datasets to demonstrate the superiority of our approach. Of particular\nimportance, we believe that it is time to shift focus from spatial features to\nreceptive-field spatial features for current spatial attention mechanisms. In\nthis way, we can further improve network performance and achieve even better\nresults. The code and pre-trained models for the relevant tasks can be found at\nhttps://github.com/Liuchen1997/RFAConv.\n","authors":["Xin Zhang","Chen Liu","Degang Yang","Tingting Song","Yichen Ye","Ke Li","Yingze Song"],"pdf_url":"https://arxiv.org/pdf/2304.03198v6.pdf","comment":"12 pages, 11figures"},{"id":"http://arxiv.org/abs/2306.16324v2","updated":"2024-03-28T12:05:23Z","published":"2023-06-28T15:58:53Z","title":"DoseDiff: Distance-aware Diffusion Model for Dose Prediction in\n  Radiotherapy","summary":"  Treatment planning, which is a critical component of the radiotherapy\nworkflow, is typically carried out by a medical physicist in a time-consuming\ntrial-and-error manner. Previous studies have proposed knowledge-based or\ndeep-learning-based methods for predicting dose distribution maps to assist\nmedical physicists in improving the efficiency of treatment planning. However,\nthese dose prediction methods usually fail to effectively utilize distance\ninformation between surrounding tissues and targets or organs-at-risk (OARs).\nMoreover, they are poor at maintaining the distribution characteristics of ray\npaths in the predicted dose distribution maps, resulting in a loss of valuable\ninformation. In this paper, we propose a distance-aware diffusion model\n(DoseDiff) for precise prediction of dose distribution. We define dose\nprediction as a sequence of denoising steps, wherein the predicted dose\ndistribution map is generated with the conditions of the computed tomography\n(CT) image and signed distance maps (SDMs). The SDMs are obtained by distance\ntransformation from the masks of targets or OARs, which provide the distance\nfrom each pixel in the image to the outline of the targets or OARs. We further\npropose a multi-encoder and multi-scale fusion network (MMFNet) that\nincorporates multi-scale and transformer-based fusion modules to enhance\ninformation fusion between the CT image and SDMs at the feature level. We\nevaluate our model on two in-house datasets and a public dataset, respectively.\nThe results demonstrate that our DoseDiff method outperforms state-of-the-art\ndose prediction methods in terms of both quantitative performance and visual\nquality.\n","authors":["Yiwen Zhang","Chuanpu Li","Liming Zhong","Zeli Chen","Wei Yang","Xuetao Wang"],"pdf_url":"https://arxiv.org/pdf/2306.16324v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19336v1","updated":"2024-03-28T11:52:42Z","published":"2024-03-28T11:52:42Z","title":"IVLMap: Instance-Aware Visual Language Grounding for Consumer Robot\n  Navigation","summary":"  Vision-and-Language Navigation (VLN) is a challenging task that requires a\nrobot to navigate in photo-realistic environments with human natural language\npromptings. Recent studies aim to handle this task by constructing the semantic\nspatial map representation of the environment, and then leveraging the strong\nability of reasoning in large language models for generalizing code for guiding\nthe robot navigation. However, these methods face limitations in instance-level\nand attribute-level navigation tasks as they cannot distinguish different\ninstances of the same object. To address this challenge, we propose a new\nmethod, namely, Instance-aware Visual Language Map (IVLMap), to empower the\nrobot with instance-level and attribute-level semantic mapping, where it is\nautonomously constructed by fusing the RGBD video data collected from the robot\nagent with special-designed natural language map indexing in the bird's-in-eye\nview. Such indexing is instance-level and attribute-level. In particular, when\nintegrated with a large language model, IVLMap demonstrates the capability to\ni) transform natural language into navigation targets with instance and\nattribute information, enabling precise localization, and ii) accomplish\nzero-shot end-to-end navigation tasks based on natural language commands.\nExtensive navigation experiments are conducted. Simulation results illustrate\nthat our method can achieve an average improvement of 14.4\\% in navigation\naccuracy. Code and demo are released at https://ivlmap.github.io/.\n","authors":["Jiacui Huang","Hongtao Zhang","Mingbo Zhao","Zhou Wu"],"pdf_url":"https://arxiv.org/pdf/2403.19336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19334v1","updated":"2024-03-28T11:50:23Z","published":"2024-03-28T11:50:23Z","title":"Test-Time Domain Generalization for Face Anti-Spoofing","summary":"  Face Anti-Spoofing (FAS) is pivotal in safeguarding facial recognition\nsystems against presentation attacks. While domain generalization (DG) methods\nhave been developed to enhance FAS performance, they predominantly focus on\nlearning domain-invariant features during training, which may not guarantee\ngeneralizability to unseen data that differs largely from the source\ndistributions. Our insight is that testing data can serve as a valuable\nresource to enhance the generalizability beyond mere evaluation for DG FAS. In\nthis paper, we introduce a novel Test-Time Domain Generalization (TTDG)\nframework for FAS, which leverages the testing data to boost the model's\ngeneralizability. Our method, consisting of Test-Time Style Projection (TTSP)\nand Diverse Style Shifts Simulation (DSSS), effectively projects the unseen\ndata to the seen domain space. In particular, we first introduce the innovative\nTTSP to project the styles of the arbitrarily unseen samples of the testing\ndistribution to the known source space of the training distributions. We then\ndesign the efficient DSSS to synthesize diverse style shifts via learnable\nstyle bases with two specifically designed losses in a hyperspherical feature\nspace. Our method eliminates the need for model updates at the test time and\ncan be seamlessly integrated into not only the CNN but also ViT backbones.\nComprehensive experiments on widely used cross-domain FAS benchmarks\ndemonstrate our method's state-of-the-art performance and effectiveness.\n","authors":["Qianyu Zhou","Ke-Yue Zhang","Taiping Yao","Xuequan Lu","Shouhong Ding","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2403.19334v1.pdf","comment":"Accepted to IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2024"},{"id":"http://arxiv.org/abs/2403.18339v2","updated":"2024-03-28T11:46:25Z","published":"2024-03-27T08:28:14Z","title":"H2ASeg: Hierarchical Adaptive Interaction and Weighting Network for\n  Tumor Segmentation in PET/CT Images","summary":"  Positron emission tomography (PET) combined with computed tomography (CT)\nimaging is routinely used in cancer diagnosis and prognosis by providing\ncomplementary information. Automatically segmenting tumors in PET/CT images can\nsignificantly improve examination efficiency. Traditional multi-modal\nsegmentation solutions mainly rely on concatenation operations for modality\nfusion, which fail to effectively model the non-linear dependencies between PET\nand CT modalities. Recent studies have investigated various approaches to\noptimize the fusion of modality-specific features for enhancing joint\nrepresentations. However, modality-specific encoders used in these methods\noperate independently, inadequately leveraging the synergistic relationships\ninherent in PET and CT modalities, for example, the complementarity between\nsemantics and structure. To address these issues, we propose a Hierarchical\nAdaptive Interaction and Weighting Network termed H2ASeg to explore the\nintrinsic cross-modal correlations and transfer potential complementary\ninformation. Specifically, we design a Modality-Cooperative Spatial Attention\n(MCSA) module that performs intra- and inter-modal interactions globally and\nlocally. Additionally, a Target-Aware Modality Weighting (TAMW) module is\ndeveloped to highlight tumor-related features within multi-modal features,\nthereby refining tumor segmentation. By embedding these modules across\ndifferent layers, H2ASeg can hierarchically model cross-modal correlations,\nenabling a nuanced understanding of both semantic and structural tumor\nfeatures. Extensive experiments demonstrate the superiority of H2ASeg,\noutperforming state-of-the-art methods on AutoPet-II and Hecktor2022\nbenchmarks. The code is released at https://github.com/JinPLu/H2ASeg.\n","authors":["Jinpeng Lu","Jingyun Chen","Linghan Cai","Songhan Jiang","Yongbing Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.18339v2.pdf","comment":"10 pages,4 figures"},{"id":"http://arxiv.org/abs/2311.15596v2","updated":"2024-03-28T11:35:55Z","published":"2023-11-27T07:44:25Z","title":"EgoThink: Evaluating First-Person Perspective Thinking Capability of\n  Vision-Language Models","summary":"  Vision-language models (VLMs) have recently shown promising results in\ntraditional downstream tasks. Evaluation studies have emerged to assess their\nabilities, with the majority focusing on the third-person perspective, and only\na few addressing specific tasks from the first-person perspective. However, the\ncapability of VLMs to \"think\" from a first-person perspective, a crucial\nattribute for advancing autonomous agents and robotics, remains largely\nunexplored. To bridge this research gap, we introduce EgoThink, a novel visual\nquestion-answering benchmark that encompasses six core capabilities with twelve\ndetailed dimensions. The benchmark is constructed using selected clips from\negocentric videos, with manually annotated question-answer pairs containing\nfirst-person information. To comprehensively assess VLMs, we evaluate eighteen\npopular VLMs on EgoThink. Moreover, given the open-ended format of the answers,\nwe use GPT-4 as the automatic judge to compute single-answer grading.\nExperimental results indicate that although GPT-4V leads in numerous\ndimensions, all evaluated VLMs still possess considerable potential for\nimprovement in first-person perspective tasks. Meanwhile, enlarging the number\nof trainable parameters has the most significant impact on model performance on\nEgoThink. In conclusion, EgoThink serves as a valuable addition to existing\nevaluation benchmarks for VLMs, providing an indispensable resource for future\nresearch in the realm of embodied artificial intelligence and robotics.\n","authors":["Sijie Cheng","Zhicheng Guo","Jingwen Wu","Kechen Fang","Peng Li","Huaping Liu","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2311.15596v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19326v1","updated":"2024-03-28T11:33:02Z","published":"2024-03-28T11:33:02Z","title":"MedBN: Robust Test-Time Adaptation against Malicious Test Samples","summary":"  Test-time adaptation (TTA) has emerged as a promising solution to address\nperformance decay due to unforeseen distribution shifts between training and\ntest data. While recent TTA methods excel in adapting to test data variations,\nsuch adaptability exposes a model to vulnerability against malicious examples,\nan aspect that has received limited attention. Previous studies have uncovered\nsecurity vulnerabilities within TTA even when a small proportion of the test\nbatch is maliciously manipulated. In response to the emerging threat, we\npropose median batch normalization (MedBN), leveraging the robustness of the\nmedian for statistics estimation within the batch normalization layer during\ntest-time inference. Our method is algorithm-agnostic, thus allowing seamless\nintegration with existing TTA frameworks. Our experimental results on benchmark\ndatasets, including CIFAR10-C, CIFAR100-C and ImageNet-C, consistently\ndemonstrate that MedBN outperforms existing approaches in maintaining robust\nperformance across different attack scenarios, encompassing both instant and\ncumulative attacks. Through extensive experiments, we show that our approach\nsustains the performance even in the absence of attacks, achieving a practical\nbalance between robustness and performance.\n","authors":["Hyejin Park","Jeongyeon Hwang","Sunung Mun","Sangdon Park","Jungseul Ok"],"pdf_url":"https://arxiv.org/pdf/2403.19326v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.19322v1","updated":"2024-03-28T11:26:30Z","published":"2024-03-28T11:26:30Z","title":"Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models","summary":"  The surge of Multimodal Large Language Models (MLLMs), given their prominent\nemergent capabilities in instruction following and reasoning, has greatly\nadvanced the field of visual reasoning. However, constrained by their\nnon-lossless image tokenization, most MLLMs fall short of comprehensively\ncapturing details of text and objects, especially in high-resolution images. To\naddress this, we propose P2G, a novel framework for plug-and-play grounding of\nreasoning in MLLMs. Specifically, P2G exploits the tool-usage potential of\nMLLMs to employ expert agents to achieve on-the-fly grounding to critical\nvisual and textual objects of image, thus achieving deliberate reasoning via\nmultimodal prompting. We further create P2GB, a benchmark aimed at assessing\nMLLMs' ability to understand inter-object relationships and text in challenging\nhigh-resolution images. Comprehensive experiments on visual reasoning tasks\ndemonstrate the superiority of P2G. Noteworthy, P2G achieved comparable\nperformance with GPT-4V on P2GB, with a 7B backbone. Our work highlights the\npotential of plug-and-play grounding of reasoning and opens up a promising\nalternative beyond model scaling.\n","authors":["Jiaxing Chen","Yuxuan Liu","Dehu Li","Xiang An","Ziyong Feng","Yongle Zhao","Yin Xie"],"pdf_url":"https://arxiv.org/pdf/2403.19322v1.pdf","comment":"14 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.19319v1","updated":"2024-03-28T11:22:53Z","published":"2024-03-28T11:22:53Z","title":"Mesh2NeRF: Direct Mesh Supervision for Neural Radiance Field\n  Representation and Generation","summary":"  We present Mesh2NeRF, an approach to derive ground-truth radiance fields from\ntextured meshes for 3D generation tasks. Many 3D generative approaches\nrepresent 3D scenes as radiance fields for training. Their ground-truth\nradiance fields are usually fitted from multi-view renderings from a\nlarge-scale synthetic 3D dataset, which often results in artifacts due to\nocclusions or under-fitting issues. In Mesh2NeRF, we propose an analytic\nsolution to directly obtain ground-truth radiance fields from 3D meshes,\ncharacterizing the density field with an occupancy function featuring a defined\nsurface thickness, and determining view-dependent color through a reflection\nfunction considering both the mesh and environment lighting. Mesh2NeRF extracts\naccurate radiance fields which provides direct supervision for training\ngenerative NeRFs and single scene representation. We validate the effectiveness\nof Mesh2NeRF across various tasks, achieving a noteworthy 3.12dB improvement in\nPSNR for view synthesis in single scene representation on the ABO dataset, a\n0.69 PSNR enhancement in the single-view conditional generation of ShapeNet\nCars, and notably improved mesh extraction from NeRF in the unconditional\ngeneration of Objaverse Mugs.\n","authors":["Yujin Chen","Yinyu Nie","Benjamin Ummenhofer","Reiner Birkl","Michael Paulitsch","Matthias Müller","Matthias Nießner"],"pdf_url":"https://arxiv.org/pdf/2403.19319v1.pdf","comment":"Project page: https://terencecyj.github.io/projects/Mesh2NeRF/ Video:\n  https://youtu.be/oufv1N3f7iY"},{"id":"http://arxiv.org/abs/2403.19316v1","updated":"2024-03-28T11:17:00Z","published":"2024-03-28T11:17:00Z","title":"Hypergraph-based Multi-View Action Recognition using Event Cameras","summary":"  Action recognition from video data forms a cornerstone with wide-ranging\napplications. Single-view action recognition faces limitations due to its\nreliance on a single viewpoint. In contrast, multi-view approaches capture\ncomplementary information from various viewpoints for improved accuracy.\nRecently, event cameras have emerged as innovative bio-inspired sensors,\nleading to advancements in event-based action recognition. However, existing\nworks predominantly focus on single-view scenarios, leaving a gap in multi-view\nevent data exploitation, particularly in challenges like information deficit\nand semantic misalignment. To bridge this gap, we introduce HyperMV, a\nmulti-view event-based action recognition framework. HyperMV converts discrete\nevent data into frame-like representations and extracts view-related features\nusing a shared convolutional network. By treating segments as vertices and\nconstructing hyperedges using rule-based and KNN-based strategies, a multi-view\nhypergraph neural network that captures relationships across viewpoint and\ntemporal features is established. The vertex attention hypergraph propagation\nis also introduced for enhanced feature fusion. To prompt research in this\narea, we present the largest multi-view event-based action dataset\n$\\text{THU}^{\\text{MV-EACT}}\\text{-50}$, comprising 50 actions from 6\nviewpoints, which surpasses existing datasets by over tenfold. Experimental\nresults show that HyperMV significantly outperforms baselines in both\ncross-subject and cross-view scenarios, and also exceeds the state-of-the-arts\nin frame-based multi-view action recognition.\n","authors":["Yue Gao","Jiaxuan Lu","Siqi Li","Yipeng Li","Shaoyi Du"],"pdf_url":"https://arxiv.org/pdf/2403.19316v1.pdf","comment":"Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI 2024)"},{"id":"http://arxiv.org/abs/2403.19314v1","updated":"2024-03-28T11:12:33Z","published":"2024-03-28T11:12:33Z","title":"Total-Decom: Decomposed 3D Scene Reconstruction with Minimal Interaction","summary":"  Scene reconstruction from multi-view images is a fundamental problem in\ncomputer vision and graphics. Recent neural implicit surface reconstruction\nmethods have achieved high-quality results; however, editing and manipulating\nthe 3D geometry of reconstructed scenes remains challenging due to the absence\nof naturally decomposed object entities and complex object/background\ncompositions. In this paper, we present Total-Decom, a novel method for\ndecomposed 3D reconstruction with minimal human interaction. Our approach\nseamlessly integrates the Segment Anything Model (SAM) with hybrid\nimplicit-explicit neural surface representations and a mesh-based\nregion-growing technique for accurate 3D object decomposition. Total-Decom\nrequires minimal human annotations while providing users with real-time control\nover the granularity and quality of decomposition. We extensively evaluate our\nmethod on benchmark datasets and demonstrate its potential for downstream\napplications, such as animation and scene editing. The code is available at\n\\href{https://github.com/CVMI-Lab/Total-Decom.git}{https://github.com/CVMI-Lab/Total-Decom.git}.\n","authors":["Xiaoyang Lyu","Chirui Chang","Peng Dai","Yang-tian Sun","Xiaojuang Qi"],"pdf_url":"https://arxiv.org/pdf/2403.19314v1.pdf","comment":"8 pages, 7 figures, accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2211.01579v3","updated":"2024-03-28T10:53:54Z","published":"2022-11-03T04:19:27Z","title":"Data-free Defense of Black Box Models Against Adversarial Attacks","summary":"  Several companies often safeguard their trained deep models (i.e., details of\narchitecture, learnt weights, training details etc.) from third-party users by\nexposing them only as black boxes through APIs. Moreover, they may not even\nprovide access to the training data due to proprietary reasons or sensitivity\nconcerns. In this work, we propose a novel defense mechanism for black box\nmodels against adversarial attacks in a data-free set up. We construct\nsynthetic data via generative model and train surrogate network using model\nstealing techniques. To minimize adversarial contamination on perturbed\nsamples, we propose 'wavelet noise remover' (WNR) that performs discrete\nwavelet decomposition on input images and carefully select only a few important\ncoefficients determined by our 'wavelet coefficient selection module' (WCSM).\nTo recover the high-frequency content of the image after noise removal via WNR,\nwe further train a 'regenerator' network with an objective to retrieve the\ncoefficients such that the reconstructed image yields similar to original\npredictions on the surrogate model. At test time, WNR combined with trained\nregenerator network is prepended to the black box network, resulting in a high\nboost in adversarial accuracy. Our method improves the adversarial accuracy on\nCIFAR-10 by 38.98% and 32.01% on state-of-the-art Auto Attack compared to\nbaseline, even when the attacker uses surrogate architecture (Alexnet-half and\nAlexnet) similar to the black box architecture (Alexnet) with same model\nstealing strategy as defender. The code is available at\nhttps://github.com/vcl-iisc/data-free-black-box-defense\n","authors":["Gaurav Kumar Nayak","Inder Khatri","Ruchit Rawal","Anirban Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2211.01579v3.pdf","comment":"CVPR Workshop (Under Review)"},{"id":"http://arxiv.org/abs/2403.19306v1","updated":"2024-03-28T10:42:49Z","published":"2024-03-28T10:42:49Z","title":"Sparse Generation: Making Pseudo Labels Sparse for weakly supervision\n  with points","summary":"  In recent years, research on point weakly supervised object detection (PWSOD)\nmethods in the field of computer vision has attracted people's attention.\nHowever, existing pseudo labels generation methods perform poorly in a small\namount of supervised annotation data and dense object detection tasks. We\nconsider the generation of weakly supervised pseudo labels as the result of\nmodel's sparse output, and propose a method called Sparse Generation to make\npseudo labels sparse. It constructs dense tensors through the relationship\nbetween data and detector model, optimizes three of its parameters, and obtains\na sparse tensor via coordinated calculation, thereby indirectly obtaining\nhigher quality pseudo labels, and solving the model's density problem in the\nsituation of only a small amount of supervised annotation data can be used. On\ntwo broadly used open-source datasets (RSOD, SIMD) and a self-built dataset\n(Bullet-Hole), the experimental results showed that the proposed method has a\nsignificant advantage in terms of overall performance metrics, comparing to\nthat state-of-the-art method.\n","authors":["Tian Ma","Chuyang Shang","Wanzhu Ren","Yuancheng Li","Jiiayi Yang","Jiali Qian"],"pdf_url":"https://arxiv.org/pdf/2403.19306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19294v1","updated":"2024-03-28T10:31:23Z","published":"2024-03-28T10:31:23Z","title":"FlowDepth: Decoupling Optical Flow for Self-Supervised Monocular Depth\n  Estimation","summary":"  Self-supervised multi-frame methods have currently achieved promising results\nin depth estimation. However, these methods often suffer from mismatch problems\ndue to the moving objects, which break the static assumption. Additionally,\nunfairness can occur when calculating photometric errors in high-freq or\nlow-texture regions of the images. To address these issues, existing approaches\nuse additional semantic priori black-box networks to separate moving objects\nand improve the model only at the loss level. Therefore, we propose FlowDepth,\nwhere a Dynamic Motion Flow Module (DMFM) decouples the optical flow by a\nmechanism-based approach and warps the dynamic regions thus solving the\nmismatch problem. For the unfairness of photometric errors caused by high-freq\nand low-texture regions, we use Depth-Cue-Aware Blur (DCABlur) and Cost-Volume\nsparsity loss respectively at the input and the loss level to solve the\nproblem. Experimental results on the KITTI and Cityscapes datasets show that\nour method outperforms the state-of-the-art methods.\n","authors":["Yiyang Sun","Zhiyuan Xu","Xiaonian Wang","Jing Yao"],"pdf_url":"https://arxiv.org/pdf/2403.19294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19278v1","updated":"2024-03-28T10:02:08Z","published":"2024-03-28T10:02:08Z","title":"CAT: Exploiting Inter-Class Dynamics for Domain Adaptive Object\n  Detection","summary":"  Domain adaptive object detection aims to adapt detection models to domains\nwhere annotated data is unavailable. Existing methods have been proposed to\naddress the domain gap using the semi-supervised student-teacher framework.\nHowever, a fundamental issue arises from the class imbalance in the labelled\ntraining set, which can result in inaccurate pseudo-labels. The relationship\nbetween classes, especially where one class is a majority and the other\nminority, has a large impact on class bias. We propose Class-Aware Teacher\n(CAT) to address the class bias issue in the domain adaptation setting. In our\nwork, we approximate the class relationships with our Inter-Class Relation\nmodule (ICRm) and exploit it to reduce the bias within the model. In this way,\nwe are able to apply augmentations to highly related classes, both inter- and\nintra-domain, to boost the performance of minority classes while having minimal\nimpact on majority classes. We further reduce the bias by implementing a\nclass-relation weight to our classification loss. Experiments conducted on\nvarious datasets and ablation studies show that our method is able to address\nthe class bias in the domain adaptation setting. On the Cityscapes to Foggy\nCityscapes dataset, we attained a 52.5 mAP, a substantial improvement over the\n51.2 mAP achieved by the state-of-the-art method.\n","authors":["Mikhail Kennerley","Jian-Gang Wang","Bharadwaj Veeravalli","Robby T. Tan"],"pdf_url":"https://arxiv.org/pdf/2403.19278v1.pdf","comment":"Accepted into CVPR 2024"},{"id":"http://arxiv.org/abs/2307.10924v2","updated":"2024-03-28T09:54:38Z","published":"2023-07-20T14:51:28Z","title":"Intrinsic Image Decomposition Using Point Cloud Representation","summary":"  The purpose of intrinsic decomposition is to separate an image into its\nalbedo (reflective properties) and shading components (illumination\nproperties). This is challenging because it's an ill-posed problem.\nConventional approaches primarily concentrate on 2D imagery and fail to fully\nexploit the capabilities of 3D data representation. 3D point clouds offer a\nmore comprehensive format for representing scenes, as they combine geometric\nand color information effectively. To this end, in this paper, we introduce\nPoint Intrinsic Net (PoInt-Net), which leverages 3D point cloud data to\nconcurrently estimate albedo and shading maps. The merits of PoInt-Net include\nthe following aspects. First, the model is efficient, achieving consistent\nperformance across point clouds of any size with training only required on\nsmall-scale point clouds. Second, it exhibits remarkable robustness; even when\ntrained exclusively on datasets comprising individual objects, PoInt-Net\ndemonstrates strong generalization to unseen objects and scenes. Third, it\ndelivers superior accuracy over conventional 2D approaches, demonstrating\nenhanced performance across various metrics on different datasets. (Code\nReleased)\n","authors":["Xiaoyan Xing","Konrad Groh","Sezer Karaoglu","Theo Gevers"],"pdf_url":"https://arxiv.org/pdf/2307.10924v2.pdf","comment":"Code: https://github.com/xyxingx/PoInt-Net"},{"id":"http://arxiv.org/abs/2403.17633v2","updated":"2024-03-28T09:47:45Z","published":"2024-03-26T12:08:14Z","title":"UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object\n  Detection with Sparse LiDAR and Large Domain Gaps","summary":"  In this study, we address a gap in existing unsupervised domain adaptation\napproaches on LiDAR-based 3D object detection, which have predominantly\nconcentrated on adapting between established, high-density autonomous driving\ndatasets. We focus on sparser point clouds, capturing scenarios from different\nperspectives: not just from vehicles on the road but also from mobile robots on\nsidewalks, which encounter significantly different environmental conditions and\nsensor configurations. We introduce Unsupervised Adversarial Domain Adaptation\nfor 3D Object Detection (UADA3D). UADA3D does not depend on pre-trained source\nmodels or teacher-student architectures. Instead, it uses an adversarial\napproach to directly learn domain-invariant features. We demonstrate its\nefficacy in various adaptation scenarios, showing significant improvements in\nboth self-driving car and mobile robot domains. Our code is open-source and\nwill be available soon.\n","authors":["Maciej K Wozniak","Mattias Hansson","Marko Thiel","Patric Jensfelt"],"pdf_url":"https://arxiv.org/pdf/2403.17633v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19265v1","updated":"2024-03-28T09:44:20Z","published":"2024-03-28T09:44:20Z","title":"Neural Fields for 3D Tracking of Anatomy and Surgical Instruments in\n  Monocular Laparoscopic Video Clips","summary":"  Laparoscopic video tracking primarily focuses on two target types: surgical\ninstruments and anatomy. The former could be used for skill assessment, while\nthe latter is necessary for the projection of virtual overlays. Where\ninstrument and anatomy tracking have often been considered two separate\nproblems, in this paper, we propose a method for joint tracking of all\nstructures simultaneously. Based on a single 2D monocular video clip, we train\na neural field to represent a continuous spatiotemporal scene, used to create\n3D tracks of all surfaces visible in at least one frame. Due to the small size\nof instruments, they generally cover a small part of the image only, resulting\nin decreased tracking accuracy. Therefore, we propose enhanced class weighting\nto improve the instrument tracks. We evaluate tracking on video clips from\nlaparoscopic cholecystectomies, where we find mean tracking accuracies of 92.4%\nfor anatomical structures and 87.4% for instruments. Additionally, we assess\nthe quality of depth maps obtained from the method's scene reconstructions. We\nshow that these pseudo-depths have comparable quality to a state-of-the-art\npre-trained depth estimator. On laparoscopic videos in the SCARED dataset, the\nmethod predicts depth with an MAE of 2.9 mm and a relative error of 9.2%. These\nresults show the feasibility of using neural fields for monocular 3D\nreconstruction of laparoscopic scenes.\n","authors":["Beerend G. A. Gerats","Jelmer M. Wolterink","Seb P. Mol","Ivo A. M. J. Broeders"],"pdf_url":"https://arxiv.org/pdf/2403.19265v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03795v3","updated":"2024-03-28T09:40:08Z","published":"2023-12-06T14:13:54Z","title":"AnimatableDreamer: Text-Guided Non-rigid 3D Model Generation and\n  Reconstruction with Canonical Score Distillation","summary":"  Advances in 3D generation have facilitated sequential 3D model generation\n(a.k.a 4D generation), yet its application for animatable objects with large\nmotion remains scarce. Our work proposes AnimatableDreamer, a text-to-4D\ngeneration framework capable of generating diverse categories of non-rigid\nobjects on skeletons extracted from a monocular video. At its core,\nAnimatableDreamer is equipped with our novel optimization design dubbed\nCanonical Score Distillation (CSD), which lifts 2D diffusion for temporal\nconsistent 4D generation. CSD, designed from a score gradient perspective,\ngenerates a canonical model with warp-robustness across different\narticulations. Notably, it also enhances the authenticity of bones and skinning\nby integrating inductive priors from a diffusion model. Furthermore, with\nmulti-view distillation, CSD infers invisible regions, thereby improving the\nfidelity of monocular non-rigid reconstruction. Extensive experiments\ndemonstrate the capability of our method in generating high-flexibility\ntext-guided 3D models from the monocular video, while also showing improved\nreconstruction performance over existing non-rigid reconstruction methods.\n","authors":["Xinzhou Wang","Yikai Wang","Junliang Ye","Zhengyi Wang","Fuchun Sun","Pengkun Liu","Ling Wang","Kai Sun","Xintong Wang","Bin He"],"pdf_url":"https://arxiv.org/pdf/2312.03795v3.pdf","comment":"Project page: https://animatabledreamer.github.io/"},{"id":"http://arxiv.org/abs/2311.15977v2","updated":"2024-03-28T09:31:05Z","published":"2023-11-27T16:23:01Z","title":"Text2Loc: 3D Point Cloud Localization from Natural Language","summary":"  We tackle the problem of 3D point cloud localization based on a few natural\nlinguistic descriptions and introduce a novel neural network, Text2Loc, that\nfully interprets the semantic relationship between points and text. Text2Loc\nfollows a coarse-to-fine localization pipeline: text-submap global place\nrecognition, followed by fine localization. In global place recognition,\nrelational dynamics among each textual hint are captured in a hierarchical\ntransformer with max-pooling (HTM), whereas a balance between positive and\nnegative pairs is maintained using text-submap contrastive learning. Moreover,\nwe propose a novel matching-free fine localization method to further refine the\nlocation predictions, which completely removes the need for complicated\ntext-instance matching and is lighter, faster, and more accurate than previous\nmethods. Extensive experiments show that Text2Loc improves the localization\naccuracy by up to $2\\times$ over the state-of-the-art on the KITTI360Pose\ndataset. Our project page is publicly available at\n\\url{https://yan-xia.github.io/projects/text2loc/}.\n","authors":["Yan Xia","Letian Shi","Zifeng Ding","João F. Henriques","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2311.15977v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.19254v1","updated":"2024-03-28T09:21:00Z","published":"2024-03-28T09:21:00Z","title":"Imperceptible Protection against Style Imitation from Diffusion Models","summary":"  Recent progress in diffusion models has profoundly enhanced the fidelity of\nimage generation. However, this has raised concerns about copyright\ninfringements. While prior methods have introduced adversarial perturbations to\nprevent style imitation, most are accompanied by the degradation of artworks'\nvisual quality. Recognizing the importance of maintaining this, we develop a\nvisually improved protection method that preserves its protection capability.\nTo this end, we create a perceptual map to identify areas most sensitive to\nhuman eyes. We then adjust the protection intensity guided by an instance-aware\nrefinement. We also integrate a perceptual constraints bank to further improve\nthe imperceptibility. Results show that our method substantially elevates the\nquality of the protected image without compromising on protection efficacy.\n","authors":["Namhyuk Ahn","Wonhyuk Ahn","KiYoon Yoo","Daesik Kim","Seung-Hun Nam"],"pdf_url":"https://arxiv.org/pdf/2403.19254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08080v2","updated":"2024-03-28T09:20:33Z","published":"2023-10-12T07:10:12Z","title":"RT-SRTS: Angle-Agnostic Real-Time Simultaneous 3D Reconstruction and\n  Tumor Segmentation from Single X-Ray Projection","summary":"  Radiotherapy is one of the primary treatment methods for tumors, but the\norgan movement caused by respiration limits its accuracy. Recently, 3D imaging\nfrom a single X-ray projection has received extensive attention as a promising\napproach to address this issue. However, current methods can only reconstruct\n3D images without directly locating the tumor and are only validated for\nfixed-angle imaging, which fails to fully meet the requirements of motion\ncontrol in radiotherapy. In this study, a novel imaging method RT-SRTS is\nproposed which integrates 3D imaging and tumor segmentation into one network\nbased on multi-task learning (MTL) and achieves real-time simultaneous 3D\nreconstruction and tumor segmentation from a single X-ray projection at any\nangle. Furthermore, the attention enhanced calibrator (AEC) and\nuncertain-region elaboration (URE) modules have been proposed to aid feature\nextraction and improve segmentation accuracy. The proposed method was evaluated\non fifteen patient cases and compared with three state-of-the-art methods. It\nnot only delivers superior 3D reconstruction but also demonstrates commendable\ntumor segmentation results. Simultaneous reconstruction and segmentation can be\ncompleted in approximately 70 ms, significantly faster than the required time\nthreshold for real-time tumor tracking. The efficacies of both AEC and URE have\nalso been validated in ablation studies. The code of work is available at\nhttps://github.com/ZywooSimple/RT-SRTS.\n","authors":["Miao Zhu","Qiming Fu","Bo Liu","Mengxi Zhang","Bojian Li","Xiaoyan Luo","Fugen Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.08080v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.11342v2","updated":"2024-03-28T09:20:19Z","published":"2023-04-22T07:48:17Z","title":"NaviNeRF: NeRF-based 3D Representation Disentanglement by Latent\n  Semantic Navigation","summary":"  3D representation disentanglement aims to identify, decompose, and manipulate\nthe underlying explanatory factors of 3D data, which helps AI fundamentally\nunderstand our 3D world. This task is currently under-explored and poses great\nchallenges: (i) the 3D representations are complex and in general contains much\nmore information than 2D image; (ii) many 3D representations are not well\nsuited for gradient-based optimization, let alone disentanglement. To address\nthese challenges, we use NeRF as a differentiable 3D representation, and\nintroduce a self-supervised Navigation to identify interpretable semantic\ndirections in the latent space. To our best knowledge, this novel method,\ndubbed NaviNeRF, is the first work to achieve fine-grained 3D disentanglement\nwithout any priors or supervisions. Specifically, NaviNeRF is built upon the\ngenerative NeRF pipeline, and equipped with an Outer Navigation Branch and an\nInner Refinement Branch. They are complementary -- the outer navigation is to\nidentify global-view semantic directions, and the inner refinement dedicates to\nfine-grained attributes. A synergistic loss is further devised to coordinate\ntwo branches. Extensive experiments demonstrate that NaviNeRF has a superior\nfine-grained 3D disentanglement ability than the previous 3D-aware models. Its\nperformance is also comparable to editing-oriented models relying on semantic\nor geometry priors.\n","authors":["Baao Xie","Bohan Li","Zequn Zhang","Junting Dong","Xin Jin","Jingyu Yang","Wenjun Zeng"],"pdf_url":"https://arxiv.org/pdf/2304.11342v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19243v1","updated":"2024-03-28T08:58:20Z","published":"2024-03-28T08:58:20Z","title":"Sine Activated Low-Rank Matrices for Parameter Efficient Learning","summary":"  Low-rank decomposition has emerged as a vital tool for enhancing parameter\nefficiency in neural network architectures, gaining traction across diverse\napplications in machine learning. These techniques significantly lower the\nnumber of parameters, striking a balance between compactness and performance.\nHowever, a common challenge has been the compromise between parameter\nefficiency and the accuracy of the model, where reduced parameters often lead\nto diminished accuracy compared to their full-rank counterparts. In this work,\nwe propose a novel theoretical framework that integrates a sinusoidal function\nwithin the low-rank decomposition process. This approach not only preserves the\nbenefits of the parameter efficiency characteristic of low-rank methods but\nalso increases the decomposition's rank, thereby enhancing model accuracy. Our\nmethod proves to be an adaptable enhancement for existing low-rank models, as\nevidenced by its successful application in Vision Transformers (ViT), Large\nLanguage Models (LLMs), Neural Radiance Fields (NeRF), and 3D shape modeling.\nThis demonstrates the wide-ranging potential and efficiency of our proposed\ntechnique.\n","authors":["Yiping Ji","Hemanth Saratchandran","Cameron Gordon","Zeyu Zhang","Simon Lucey"],"pdf_url":"https://arxiv.org/pdf/2403.19243v1.pdf","comment":"The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2403.19242v1","updated":"2024-03-28T08:54:40Z","published":"2024-03-28T08:54:40Z","title":"RTracker: Recoverable Tracking via PN Tree Structured Memory","summary":"  Existing tracking methods mainly focus on learning better target\nrepresentation or developing more robust prediction models to improve tracking\nperformance. While tracking performance has significantly improved, the target\nloss issue occurs frequently due to tracking failures, complete occlusion, or\nout-of-view situations. However, considerably less attention is paid to the\nself-recovery issue of tracking methods, which is crucial for practical\napplications. To this end, we propose a recoverable tracking framework,\nRTracker, that uses a tree-structured memory to dynamically associate a tracker\nand a detector to enable self-recovery ability. Specifically, we propose a\nPositive-Negative Tree-structured memory to chronologically store and maintain\npositive and negative target samples. Upon the PN tree memory, we develop\ncorresponding walking rules for determining the state of the target and define\na set of control flows to unite the tracker and the detector in different\ntracking scenarios. Our core idea is to use the support samples of positive and\nnegative target categories to establish a relative distance-based criterion for\na reliable assessment of target loss. The favorable performance in comparison\nagainst the state-of-the-art methods on numerous challenging benchmarks\ndemonstrates the effectiveness of the proposed algorithm.\n","authors":["Yuqing Huang","Xin Li","Zikun Zhou","Yaowei Wang","Zhenyu He","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2403.19242v1.pdf","comment":"accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.19238v1","updated":"2024-03-28T08:49:35Z","published":"2024-03-28T08:49:35Z","title":"Taming Lookup Tables for Efficient Image Retouching","summary":"  The widespread use of high-definition screens in edge devices, such as\nend-user cameras, smartphones, and televisions, is spurring a significant\ndemand for image enhancement. Existing enhancement models often optimize for\nhigh performance while falling short of reducing hardware inference time and\npower consumption, especially on edge devices with constrained computing and\nstorage resources. To this end, we propose Image Color Enhancement Lookup Table\n(ICELUT) that adopts LUTs for extremely efficient edge inference, without any\nconvolutional neural network (CNN). During training, we leverage pointwise\n(1x1) convolution to extract color information, alongside a split fully\nconnected layer to incorporate global information. Both components are then\nseamlessly converted into LUTs for hardware-agnostic deployment. ICELUT\nachieves near-state-of-the-art performance and remarkably low power\nconsumption. We observe that the pointwise network structure exhibits robust\nscalability, upkeeping the performance even with a heavily downsampled 32x32\ninput image. These enable ICELUT, the first-ever purely LUT-based image\nenhancer, to reach an unprecedented speed of 0.4ms on GPU and 7ms on CPU, at\nleast one order faster than any CNN solution. Codes are available at\nhttps://github.com/Stephen0808/ICELUT.\n","authors":["Sidi Yang","Binxiao Huang","Mingdeng Cao","Yatai Ji","Hanzhong Guo","Ngai Wong","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2403.19238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.19654v2","updated":"2024-03-28T08:47:14Z","published":"2023-10-30T15:38:43Z","title":"MCAD: Multi-teacher Cross-modal Alignment Distillation for efficient\n  image-text retrieval","summary":"  Due to the success of large-scale visual-language pretraining (VLP) models\nand the widespread use of image-text retrieval in industry areas, it is now\ncritically necessary to reduce the model size and streamline their\nmobile-device deployment. Single- and dual-stream model structures are commonly\nused in image-text retrieval with the goal of closing the semantic gap between\ntextual and visual modalities. While single-stream models use deep feature\nfusion to achieve more accurate cross-model alignment, dual-stream models are\nbetter at offline indexing and fast inference.We propose a Multi-teacher\nCross-modality Alignment Distillation (MCAD) technique to integrate the\nadvantages of single- and dual-stream models. By incorporating the fused\nsingle-stream features into the image and text features of the dual-stream\nmodel, we formulate new modified teacher similarity distributions and features.\nThen, we conduct both distribution and feature distillation to boost the\ncapability of the student dual-stream model, achieving high retrieval\nperformance without increasing inference complexity.Extensive experiments\ndemonstrate the remarkable performance and high efficiency of MCAD on\nimage-text retrieval tasks. Furthermore, we implement a lightweight CLIP model\non Snapdragon/Dimensity chips with only $\\sim$100M running memory and\n$\\sim$8.0ms search latency, achieving the mobile-device application of VLP\nmodels.\n","authors":["Youbo Lei","Feifei He","Chen Chen","Yingbin Mo","Si Jia Li","Defeng Xie","Haonan Lu"],"pdf_url":"https://arxiv.org/pdf/2310.19654v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19235v1","updated":"2024-03-28T08:47:02Z","published":"2024-03-28T08:47:02Z","title":"DreamSalon: A Staged Diffusion Framework for Preserving Identity-Context\n  in Editable Face Generation","summary":"  While large-scale pre-trained text-to-image models can synthesize diverse and\nhigh-quality human-centered images, novel challenges arise with a nuanced task\nof \"identity fine editing\": precisely modifying specific features of a subject\nwhile maintaining its inherent identity and context. Existing personalization\nmethods either require time-consuming optimization or learning additional\nencoders, adept in \"identity re-contextualization\". However, they often\nstruggle with detailed and sensitive tasks like human face editing. To address\nthese challenges, we introduce DreamSalon, a noise-guided, staged-editing\nframework, uniquely focusing on detailed image manipulations and\nidentity-context preservation. By discerning editing and boosting stages via\nthe frequency and gradient of predicted noises, DreamSalon first performs\ndetailed manipulations on specific features in the editing stage, guided by\nhigh-frequency information, and then employs stochastic denoising in the\nboosting stage to improve image quality. For more precise editing, DreamSalon\nsemantically mixes source and target textual prompts, guided by differences in\ntheir embedding covariances, to direct the model's focus on specific\nmanipulation areas. Our experiments demonstrate DreamSalon's ability to\nefficiently and faithfully edit fine details on human faces, outperforming\nexisting methods both qualitatively and quantitatively.\n","authors":["Haonan Lin","Mengmeng Wang","Yan Chen","Wenbin An","Yuzhe Yao","Guang Dai","Qianying Wang","Yong Liu","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2403.19235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19232v1","updated":"2024-03-28T08:44:36Z","published":"2024-03-28T08:44:36Z","title":"AZ-NAS: Assembling Zero-Cost Proxies for Network Architecture Search","summary":"  Training-free network architecture search (NAS) aims to discover\nhigh-performing networks with zero-cost proxies, capturing network\ncharacteristics related to the final performance. However, network rankings\nestimated by previous training-free NAS methods have shown weak correlations\nwith the performance. To address this issue, we propose AZ-NAS, a novel\napproach that leverages the ensemble of various zero-cost proxies to enhance\nthe correlation between a predicted ranking of networks and the ground truth\nsubstantially in terms of the performance. To achieve this, we introduce four\nnovel zero-cost proxies that are complementary to each other, analyzing\ndistinct traits of architectures in the views of expressivity, progressivity,\ntrainability, and complexity. The proxy scores can be obtained simultaneously\nwithin a single forward and backward pass, making an overall NAS process highly\nefficient. In order to integrate the rankings predicted by our proxies\neffectively, we introduce a non-linear ranking aggregation method that\nhighlights the networks highly-ranked consistently across all the proxies.\nExperimental results conclusively demonstrate the efficacy and efficiency of\nAZ-NAS, outperforming state-of-the-art methods on standard benchmarks, all\nwhile maintaining a reasonable runtime cost.\n","authors":["Junghyup Lee","Bumsub Ham"],"pdf_url":"https://arxiv.org/pdf/2403.19232v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2401.11874v2","updated":"2024-03-28T08:40:08Z","published":"2024-01-22T12:00:37Z","title":"Detect-Order-Construct: A Tree Construction based Approach for\n  Hierarchical Document Structure Analysis","summary":"  Document structure analysis (aka document layout analysis) is crucial for\nunderstanding the physical layout and logical structure of documents, with\napplications in information retrieval, document summarization, knowledge\nextraction, etc. In this paper, we concentrate on Hierarchical Document\nStructure Analysis (HDSA) to explore hierarchical relationships within\nstructured documents created using authoring software employing hierarchical\nschemas, such as LaTeX, Microsoft Word, and HTML. To comprehensively analyze\nhierarchical document structures, we propose a tree construction based approach\nthat addresses multiple subtasks concurrently, including page object detection\n(Detect), reading order prediction of identified objects (Order), and the\nconstruction of intended hierarchical structure (Construct). We present an\neffective end-to-end solution based on this framework to demonstrate its\nperformance. To assess our approach, we develop a comprehensive benchmark\ncalled Comp-HRDoc, which evaluates the above subtasks simultaneously. Our\nend-to-end system achieves state-of-the-art performance on two large-scale\ndocument layout analysis datasets (PubLayNet and DocLayNet), a high-quality\nhierarchical document structure reconstruction dataset (HRDoc), and our\nComp-HRDoc benchmark. The Comp-HRDoc benchmark will be released to facilitate\nfurther research in this field.\n","authors":["Jiawei Wang","Kai Hu","Zhuoyao Zhong","Lei Sun","Qiang Huo"],"pdf_url":"https://arxiv.org/pdf/2401.11874v2.pdf","comment":"Submitted to Pattern Recognition"},{"id":"http://arxiv.org/abs/2403.19225v1","updated":"2024-03-28T08:39:44Z","published":"2024-03-28T08:39:44Z","title":"Efficient and Effective Weakly-Supervised Action Segmentation via\n  Action-Transition-Aware Boundary Alignment","summary":"  Weakly-supervised action segmentation is a task of learning to partition a\nlong video into several action segments, where training videos are only\naccompanied by transcripts (ordered list of actions). Most of existing methods\nneed to infer pseudo segmentation for training by serial alignment between all\nframes and the transcript, which is time-consuming and hard to be parallelized\nwhile training. In this work, we aim to escape from this inefficient alignment\nwith massive but redundant frames, and instead to directly localize a few\naction transitions for pseudo segmentation generation, where a transition\nrefers to the change from an action segment to its next adjacent one in the\ntranscript. As the true transitions are submerged in noisy boundaries due to\nintra-segment visual variation, we propose a novel Action-Transition-Aware\nBoundary Alignment (ATBA) framework to efficiently and effectively filter out\nnoisy boundaries and detect transitions. In addition, to boost the semantic\nlearning in the case that noise is inevitably present in the pseudo\nsegmentation, we also introduce video-level losses to utilize the trusted\nvideo-level supervision. Extensive experiments show the effectiveness of our\napproach on both performance and training speed.\n","authors":["Angchi Xu","Wei-Shi Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.19225v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.19221v1","updated":"2024-03-28T08:35:46Z","published":"2024-03-28T08:35:46Z","title":"Towards Multimodal Video Paragraph Captioning Models Robust to Missing\n  Modality","summary":"  Video paragraph captioning (VPC) involves generating detailed narratives for\nlong videos, utilizing supportive modalities such as speech and event\nboundaries. However, the existing models are constrained by the assumption of\nconstant availability of a single auxiliary modality, which is impractical\ngiven the diversity and unpredictable nature of real-world scenarios. To this\nend, we propose a Missing-Resistant framework MR-VPC that effectively harnesses\nall available auxiliary inputs and maintains resilience even in the absence of\ncertain modalities. Under this framework, we propose the Multimodal VPC (MVPC)\narchitecture integrating video, speech, and event boundary inputs in a unified\nmanner to process various auxiliary inputs. Moreover, to fortify the model\nagainst incomplete data, we introduce DropAM, a data augmentation strategy that\nrandomly omits auxiliary inputs, paired with DistillAM, a regularization target\nthat distills knowledge from teacher models trained on modality-complete data,\nenabling efficient learning in modality-deficient environments. Through\nexhaustive experimentation on YouCook2 and ActivityNet Captions, MR-VPC has\nproven to deliver superior performance on modality-complete and\nmodality-missing test data. This work highlights the significance of developing\nresilient VPC models and paves the way for more adaptive, robust multimodal\nvideo understanding.\n","authors":["Sishuo Chen","Lei Li","Shuhuai Ren","Rundong Gao","Yuanxin Liu","Xiaohan Bi","Xu Sun","Lu Hou"],"pdf_url":"https://arxiv.org/pdf/2403.19221v1.pdf","comment":"Code available at https://github.com/lancopku/MR-VPC"},{"id":"http://arxiv.org/abs/2403.19220v1","updated":"2024-03-28T08:34:04Z","published":"2024-03-28T08:34:04Z","title":"GeoAuxNet: Towards Universal 3D Representation Learning for Multi-sensor\n  Point Clouds","summary":"  Point clouds captured by different sensors such as RGB-D cameras and LiDAR\npossess non-negligible domain gaps. Most existing methods design different\nnetwork architectures and train separately on point clouds from various\nsensors. Typically, point-based methods achieve outstanding performances on\neven-distributed dense point clouds from RGB-D cameras, while voxel-based\nmethods are more efficient for large-range sparse LiDAR point clouds. In this\npaper, we propose geometry-to-voxel auxiliary learning to enable voxel\nrepresentations to access point-level geometric information, which supports\nbetter generalisation of the voxel-based backbone with additional\ninterpretations of multi-sensor point clouds. Specifically, we construct\nhierarchical geometry pools generated by a voxel-guided dynamic point network,\nwhich efficiently provide auxiliary fine-grained geometric information adapted\nto different stages of voxel features. We conduct experiments on joint\nmulti-sensor datasets to demonstrate the effectiveness of GeoAuxNet. Enjoying\nelaborate geometric information, our method outperforms other models\ncollectively trained on multi-sensor datasets, and achieve competitive results\nwith the-state-of-art experts on each single dataset.\n","authors":["Shengjun Zhang","Xin Fei","Yueqi Duan"],"pdf_url":"https://arxiv.org/pdf/2403.19220v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2311.13120v3","updated":"2024-03-28T08:30:56Z","published":"2023-11-22T02:46:57Z","title":"Multi-modal In-Context Learning Makes an Ego-evolving Scene Text\n  Recognizer","summary":"  Scene text recognition (STR) in the wild frequently encounters challenges\nwhen coping with domain variations, font diversity, shape deformations, etc. A\nstraightforward solution is performing model fine-tuning tailored to a specific\nscenario, but it is computationally intensive and requires multiple model\ncopies for various scenarios. Recent studies indicate that large language\nmodels (LLMs) can learn from a few demonstration examples in a training-free\nmanner, termed \"In-Context Learning\" (ICL). Nevertheless, applying LLMs as a\ntext recognizer is unacceptably resource-consuming. Moreover, our pilot\nexperiments on LLMs show that ICL fails in STR, mainly attributed to the\ninsufficient incorporation of contextual information from diverse samples in\nthe training stage. To this end, we introduce E$^2$STR, a STR model trained\nwith context-rich scene text sequences, where the sequences are generated via\nour proposed in-context training strategy. E$^2$STR demonstrates that a\nregular-sized model is sufficient to achieve effective ICL capabilities in STR.\nExtensive experiments show that E$^2$STR exhibits remarkable training-free\nadaptation in various scenarios and outperforms even the fine-tuned\nstate-of-the-art approaches on public benchmarks. The code is released at\nhttps://github.com/bytedance/E2STR .\n","authors":["Zhen Zhao","Jingqun Tang","Chunhui Lin","Binghong Wu","Can Huang","Hao Liu","Xin Tan","Zhizhong Zhang","Yuan Xie"],"pdf_url":"https://arxiv.org/pdf/2311.13120v3.pdf","comment":"Accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2402.05608v3","updated":"2024-03-28T08:28:44Z","published":"2024-02-08T12:08:42Z","title":"Scalable Diffusion Models with State Space Backbone","summary":"  This paper presents a new exploration into a category of diffusion models\nbuilt upon state space architecture. We endeavor to train diffusion models for\nimage data, wherein the traditional U-Net backbone is supplanted by a state\nspace backbone, functioning on raw patches or latent space. Given its notable\nefficacy in accommodating long-range dependencies, Diffusion State Space Models\n(DiS) are distinguished by treating all inputs including time, condition, and\nnoisy image patches as tokens. Our assessment of DiS encompasses both\nunconditional and class-conditional image generation scenarios, revealing that\nDiS exhibits comparable, if not superior, performance to CNN-based or\nTransformer-based U-Net architectures of commensurate size. Furthermore, we\nanalyze the scalability of DiS, gauged by the forward pass complexity\nquantified in Gflops. DiS models with higher Gflops, achieved through\naugmentation of depth/width or augmentation of input tokens, consistently\ndemonstrate lower FID. In addition to demonstrating commendable scalability\ncharacteristics, DiS-H/2 models in latent space achieve performance levels akin\nto prior diffusion models on class-conditional ImageNet benchmarks at the\nresolution of 256$\\times$256 and 512$\\times$512, while significantly reducing\nthe computational burden. The code and models are available at:\nhttps://github.com/feizc/DiS.\n","authors":["Zhengcong Fei","Mingyuan Fan","Changqian Yu","Junshi Huang"],"pdf_url":"https://arxiv.org/pdf/2402.05608v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00096v2","updated":"2024-03-28T08:25:27Z","published":"2023-11-30T13:32:43Z","title":"OST: Refining Text Knowledge with Optimal Spatio-Temporal Descriptor for\n  General Video Recognition","summary":"  Due to the resource-intensive nature of training vision-language models on\nexpansive video data, a majority of studies have centered on adapting\npre-trained image-language models to the video domain. Dominant pipelines\npropose to tackle the visual discrepancies with additional temporal learners\nwhile overlooking the substantial discrepancy for web-scaled descriptive\nnarratives and concise action category names, leading to less distinct semantic\nspace and potential performance limitations. In this work, we prioritize the\nrefinement of text knowledge to facilitate generalizable video recognition. To\naddress the limitations of the less distinct semantic space of category names,\nwe prompt a large language model (LLM) to augment action class names into\nSpatio-Temporal Descriptors thus bridging the textual discrepancy and serving\nas a knowledge base for general recognition. Moreover, to assign the best\ndescriptors with different video instances, we propose Optimal Descriptor\nSolver, forming the video recognition problem as solving the optimal matching\nflow across frame-level representations and descriptors. Comprehensive\nevaluations in zero-shot, few-shot, and fully supervised video recognition\nhighlight the effectiveness of our approach. Our best model achieves a\nstate-of-the-art zero-shot accuracy of 75.1% on Kinetics-600.\n","authors":["Tongjia Chen","Hongshan Yu","Zhengeng Yang","Zechuan Li","Wei Sun","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2312.00096v2.pdf","comment":"Technical report. Project Page: https://tomchen-ctj.github.io/OST/"},{"id":"http://arxiv.org/abs/2308.12532v6","updated":"2024-03-28T08:23:02Z","published":"2023-08-24T03:43:02Z","title":"FedSOL: Stabilized Orthogonal Learning with Proximal Restrictions in\n  Federated Learning","summary":"  Federated Learning (FL) aggregates locally trained models from individual\nclients to construct a global model. While FL enables learning a model with\ndata privacy, it often suffers from significant performance degradation when\nclients have heterogeneous data distributions. This data heterogeneity causes\nthe model to forget the global knowledge acquired from previously sampled\nclients after being trained on local datasets. Although the introduction of\nproximal objectives in local updates helps to preserve global knowledge, it can\nalso hinder local learning by interfering with local objectives. To address\nthis problem, we propose a novel method, Federated Stabilized Orthogonal\nLearning (FedSOL), which adopts an orthogonal learning strategy to balance the\ntwo conflicting objectives. FedSOL is designed to identify gradients of local\nobjectives that are inherently orthogonal to directions affecting the proximal\nobjective. Specifically, FedSOL targets parameter regions where learning on the\nlocal objective is minimally influenced by proximal weight perturbations. Our\nexperiments demonstrate that FedSOL consistently achieves state-of-the-art\nperformance across various scenarios.\n","authors":["Gihun Lee","Minchan Jeong","Sangmook Kim","Jaehoon Oh","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2308.12532v6.pdf","comment":"The IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  2024 (CVPR 2024)"},{"id":"http://arxiv.org/abs/2403.19213v1","updated":"2024-03-28T08:21:56Z","published":"2024-03-28T08:21:56Z","title":"Learning Multiple Representations with Inconsistency-Guided Detail\n  Regularization for Mask-Guided Matting","summary":"  Mask-guided matting networks have achieved significant improvements and have\nshown great potential in practical applications in recent years. However,\nsimply learning matting representation from synthetic and\nlack-of-real-world-diversity matting data, these approaches tend to overfit\nlow-level details in wrong regions, lack generalization to objects with complex\nstructures and real-world scenes such as shadows, as well as suffer from\ninterference of background lines or textures. To address these challenges, in\nthis paper, we propose a novel auxiliary learning framework for mask-guided\nmatting models, incorporating three auxiliary tasks: semantic segmentation,\nedge detection, and background line detection besides matting, to learn\ndifferent and effective representations from different types of data and\nannotations. Our framework and model introduce the following key aspects: (1)\nto learn real-world adaptive semantic representation for objects with diverse\nand complex structures under real-world scenes, we introduce extra semantic\nsegmentation and edge detection tasks on more diverse real-world data with\nsegmentation annotations; (2) to avoid overfitting on low-level details, we\npropose a module to utilize the inconsistency between learned segmentation and\nmatting representations to regularize detail refinement; (3) we propose a novel\nbackground line detection task into our auxiliary learning framework, to\nsuppress interference of background lines or textures. In addition, we propose\na high-quality matting benchmark, Plant-Mat, to evaluate matting methods on\ncomplex structures. Extensively quantitative and qualitative results show that\nour approach outperforms state-of-the-art mask-guided methods.\n","authors":["Weihao Jiang","Zhaozhi Xie","Yuxiang Lu","Longjie Qi","Jingyong Cai","Hiroyuki Uchiyama","Bin Chen","Yue Ding","Hongtao Lu"],"pdf_url":"https://arxiv.org/pdf/2403.19213v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03441v4","updated":"2024-03-28T08:09:07Z","published":"2023-12-06T11:50:14Z","title":"UFineBench: Towards Text-based Person Retrieval with Ultra-fine\n  Granularity","summary":"  Existing text-based person retrieval datasets often have relatively\ncoarse-grained text annotations. This hinders the model to comprehend the\nfine-grained semantics of query texts in real scenarios. To address this\nproblem, we contribute a new benchmark named \\textbf{UFineBench} for text-based\nperson retrieval with ultra-fine granularity.\n  Firstly, we construct a new \\textbf{dataset} named UFine6926. We collect a\nlarge number of person images and manually annotate each image with two\ndetailed textual descriptions, averaging 80.8 words each. The average word\ncount is three to four times that of the previous datasets. In addition of\nstandard in-domain evaluation, we also propose a special \\textbf{evaluation\nparadigm} more representative of real scenarios. It contains a new evaluation\nset with cross domains, cross textual granularity and cross textual styles,\nnamed UFine3C, and a new evaluation metric for accurately measuring retrieval\nability, named mean Similarity Distribution (mSD). Moreover, we propose CFAM, a\nmore efficient \\textbf{algorithm} especially designed for text-based person\nretrieval with ultra fine-grained texts. It achieves fine granularity mining by\nadopting a shared cross-modal granularity decoder and hard negative match\nmechanism.\n  With standard in-domain evaluation, CFAM establishes competitive performance\nacross various datasets, especially on our ultra fine-grained UFine6926.\nFurthermore, by evaluating on UFine3C, we demonstrate that training on our\nUFine6926 significantly improves generalization to real scenarios compared with\nother coarse-grained datasets. The dataset and code will be made publicly\navailable at \\url{https://github.com/Zplusdragon/UFineBench}.\n","authors":["Jialong Zuo","Hanyu Zhou","Ying Nie","Feng Zhang","Tianyu Guo","Nong Sang","Yunhe Wang","Changxin Gao"],"pdf_url":"https://arxiv.org/pdf/2312.03441v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19205v1","updated":"2024-03-28T08:06:48Z","published":"2024-03-28T08:06:48Z","title":"From Activation to Initialization: Scaling Insights for Optimizing\n  Neural Fields","summary":"  In the realm of computer vision, Neural Fields have gained prominence as a\ncontemporary tool harnessing neural networks for signal representation. Despite\nthe remarkable progress in adapting these networks to solve a variety of\nproblems, the field still lacks a comprehensive theoretical framework. This\narticle aims to address this gap by delving into the intricate interplay\nbetween initialization and activation, providing a foundational basis for the\nrobust optimization of Neural Fields. Our theoretical insights reveal a\ndeep-seated connection among network initialization, architectural choices, and\nthe optimization process, emphasizing the need for a holistic approach when\ndesigning cutting-edge Neural Fields.\n","authors":["Hemanth Saratchandran","Sameera Ramasinghe","Simon Lucey"],"pdf_url":"https://arxiv.org/pdf/2403.19205v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.11956v3","updated":"2024-03-28T08:04:51Z","published":"2024-03-18T16:52:49Z","title":"Subjective-Aligned Dataset and Metric for Text-to-Video Quality\n  Assessment","summary":"  With the rapid development of generative models, Artificial\nIntelligence-Generated Contents (AIGC) have exponentially increased in daily\nlives. Among them, Text-to-Video (T2V) generation has received widespread\nattention. Though many T2V models have been released for generating high\nperceptual quality videos, there is still lack of a method to evaluate the\nquality of these videos quantitatively. To solve this issue, we establish the\nlargest-scale Text-to-Video Quality Assessment DataBase (T2VQA-DB) to date. The\ndataset is composed of 10,000 videos generated by 9 different T2V models. We\nalso conduct a subjective study to obtain each video's corresponding mean\nopinion score. Based on T2VQA-DB, we propose a novel transformer-based model\nfor subjective-aligned Text-to-Video Quality Assessment (T2VQA). The model\nextracts features from text-video alignment and video fidelity perspectives,\nthen it leverages the ability of a large language model to give the prediction\nscore. Experimental results show that T2VQA outperforms existing T2V metrics\nand SOTA video quality assessment models. Quantitative analysis indicates that\nT2VQA is capable of giving subjective-align predictions, validating its\neffectiveness. The dataset and code will be released at\nhttps://github.com/QMME/T2VQA.\n","authors":["Tengchuan Kou","Xiaohong Liu","Zicheng Zhang","Chunyi Li","Haoning Wu","Xiongkuo Min","Guangtao Zhai","Ning Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11956v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18807v2","updated":"2024-03-28T08:01:34Z","published":"2024-03-27T17:53:30Z","title":"ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth\n  Estimation","summary":"  In the absence of parallax cues, a learning-based single image depth\nestimation (SIDE) model relies heavily on shading and contextual cues in the\nimage. While this simplicity is attractive, it is necessary to train such\nmodels on large and varied datasets, which are difficult to capture. It has\nbeen shown that using embeddings from pre-trained foundational models, such as\nCLIP, improves zero shot transfer in several applications. Taking inspiration\nfrom this, in our paper we explore the use of global image priors generated\nfrom a pre-trained ViT model to provide more detailed contextual information.\nWe argue that the embedding vector from a ViT model, pre-trained on a large\ndataset, captures greater relevant information for SIDE than the usual route of\ngenerating pseudo image captions, followed by CLIP based text embeddings. Based\non this idea, we propose a new SIDE model using a diffusion backbone which is\nconditioned on ViT embeddings. Our proposed design establishes a new\nstate-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of\n0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on\nKITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to\n0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model\ntrained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%)\nover NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%,\n18%, 45%, 9%) by ZoeDepth. The code is available at\nhttps://ecodepth-iitd.github.io\n","authors":["Suraj Patni","Aradhye Agarwal","Chetan Arora"],"pdf_url":"https://arxiv.org/pdf/2403.18807v2.pdf","comment":"IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n  2024"},{"id":"http://arxiv.org/abs/2403.19203v1","updated":"2024-03-28T08:00:14Z","published":"2024-03-28T08:00:14Z","title":"Single-Shared Network with Prior-Inspired Loss for Parameter-Efficient\n  Multi-Modal Imaging Skin Lesion Classification","summary":"  In this study, we introduce a multi-modal approach that efficiently\nintegrates multi-scale clinical and dermoscopy features within a single\nnetwork, thereby substantially reducing model parameters. The proposed method\nincludes three novel fusion schemes.\n  Firstly, unlike current methods that usually employ two individual models for\nfor clinical and dermoscopy modalities, we verified that multimodal feature can\nbe learned by sharing the parameters of encoder while leaving the individual\nmodal-specific classifiers.\n  Secondly, the shared cross-attention module can replace the individual one to\nefficiently interact between two modalities at multiple layers.\n  Thirdly, different from current methods that equally optimize dermoscopy and\nclinical branches, inspired by prior knowledge that dermoscopy images play a\nmore significant role than clinical images, we propose a novel biased loss.\nThis loss guides the single-shared network to prioritize dermoscopy information\nover clinical information, implicitly learning a better joint feature\nrepresentation for the modal-specific task.\n  Extensive experiments on a well-recognized Seven-Point Checklist (SPC)\ndataset and a collected dataset demonstrate the effectiveness of our method on\nboth CNN and Transformer structures. Furthermore, our method exhibits\nsuperiority in both accuracy and model parameters compared to currently\nadvanced methods.\n","authors":["Peng Tang","Tobias Lasser"],"pdf_url":"https://arxiv.org/pdf/2403.19203v1.pdf","comment":"This paper have submitted to Journal for review"},{"id":"http://arxiv.org/abs/2403.14760v2","updated":"2024-03-28T07:46:49Z","published":"2024-03-21T18:02:20Z","title":"Can 3D Vision-Language Models Truly Understand Natural Language?","summary":"  Rapid advancements in 3D vision-language (3D-VL) tasks have opened up new\navenues for human interaction with embodied agents or robots using natural\nlanguage. Despite this progress, we find a notable limitation: existing 3D-VL\nmodels exhibit sensitivity to the styles of language input, struggling to\nunderstand sentences with the same semantic meaning but written in different\nvariants. This observation raises a critical question: Can 3D vision-language\nmodels truly understand natural language? To test the language\nunderstandability of 3D-VL models, we first propose a language robustness task\nfor systematically assessing 3D-VL models across various tasks, benchmarking\ntheir performance when presented with different language style variants.\nImportantly, these variants are commonly encountered in applications requiring\ndirect interaction with humans, such as embodied robotics, given the diversity\nand unpredictability of human language. We propose a 3D Language Robustness\nDataset, designed based on the characteristics of human language, to facilitate\nthe systematic study of robustness. Our comprehensive evaluation uncovers a\nsignificant drop in the performance of all existing models across various 3D-VL\ntasks. Even the state-of-the-art 3D-LLM fails to understand some variants of\nthe same sentences. Further in-depth analysis suggests that the existing models\nhave a fragile and biased fusion module, which stems from the low diversity of\nthe existing dataset. Finally, we propose a training-free module driven by LLM,\nwhich improves language robustness. Datasets and code will be available at\ngithub.\n","authors":["Weipeng Deng","Runyu Ding","Jihan Yang","Jiahui Liu","Yijiang Li","Xiaojuan Qi","Edith Ngai"],"pdf_url":"https://arxiv.org/pdf/2403.14760v2.pdf","comment":"https://github.com/VincentDENGP/3D-LR"},{"id":"http://arxiv.org/abs/2403.19193v1","updated":"2024-03-28T07:43:49Z","published":"2024-03-28T07:43:49Z","title":"Text Data-Centric Image Captioning with Interactive Prompts","summary":"  Supervised image captioning approaches have made great progress, but it is\nchallenging to collect high-quality human-annotated image-text data. Recently,\nlarge-scale vision and language models (e.g., CLIP) and large-scale generative\nlanguage models (e.g., GPT-2) have shown strong performances in various tasks,\nwhich also provide some new solutions for image captioning with web paired\ndata, unpaired data or even text-only data. Among them, the mainstream solution\nis to project image embeddings into the text embedding space with the\nassistance of consistent representations between image-text pairs from the CLIP\nmodel. However, the current methods still face several challenges in adapting\nto the diversity of data configurations in a unified solution, accurately\nestimating image-text embedding bias, and correcting unsatisfactory prediction\nresults in the inference stage. This paper proposes a new Text data-centric\napproach with Interactive Prompts for image Captioning, named TIPCap. 1) We\nconsider four different settings which gradually reduce the dependence on\npaired data. 2) We construct a mapping module driven by multivariate Gaussian\ndistribution to mitigate the modality gap, which is applicable to the above\nfour different settings. 3) We propose a prompt interaction module that can\nincorporate optional prompt information before generating captions. Extensive\nexperiments show that our TIPCap outperforms other weakly or unsupervised image\ncaptioning methods and achieves a new state-of-the-art performance on two\nwidely used datasets, i.e., MS-COCO and Flickr30K.\n","authors":["Yiyu Wang","Hao Luo","Jungang Xu","Yingfei Sun","Fan Wang"],"pdf_url":"https://arxiv.org/pdf/2403.19193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15955v2","updated":"2024-03-28T07:30:25Z","published":"2024-03-23T23:22:54Z","title":"Finding needles in a haystack: A Black-Box Approach to Invisible\n  Watermark Detection","summary":"  In this paper, we propose WaterMark Detection (WMD), the first invisible\nwatermark detection method under a black-box and annotation-free setting. WMD\nis capable of detecting arbitrary watermarks within a given reference dataset\nusing a clean non-watermarked dataset as a reference, without relying on\nspecific decoding methods or prior knowledge of the watermarking techniques. We\ndevelop WMD using foundations of offset learning, where a clean non-watermarked\ndataset enables us to isolate the influence of only watermarked samples in the\nreference dataset. Our comprehensive evaluations demonstrate the effectiveness\nof WMD, significantly outperforming naive detection methods, which only yield\nAUC scores around 0.5. In contrast, WMD consistently achieves impressive\ndetection AUC scores, surpassing 0.9 in most single-watermark datasets and\nexceeding 0.7 in more challenging multi-watermark scenarios across diverse\ndatasets and watermarking methods. As invisible watermarks become increasingly\nprevalent, while specific decoding techniques remain undisclosed, our approach\nprovides a versatile solution and establishes a path toward increasing\naccountability, transparency, and trust in our digital visual content.\n","authors":["Minzhou Pan","Zhengting Wang","Xin Dong","Vikash Sehwag","Lingjuan Lyu","Xue Lin"],"pdf_url":"https://arxiv.org/pdf/2403.15955v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.09911v3","updated":"2024-03-28T07:16:11Z","published":"2023-08-19T05:34:13Z","title":"Noisy-Correspondence Learning for Text-to-Image Person Re-identification","summary":"  Text-to-image person re-identification (TIReID) is a compelling topic in the\ncross-modal community, which aims to retrieve the target person based on a\ntextual query. Although numerous TIReID methods have been proposed and achieved\npromising performance, they implicitly assume the training image-text pairs are\ncorrectly aligned, which is not always the case in real-world scenarios. In\npractice, the image-text pairs inevitably exist under-correlated or even\nfalse-correlated, a.k.a noisy correspondence (NC), due to the low quality of\nthe images and annotation errors. To address this problem, we propose a novel\nRobust Dual Embedding method (RDE) that can learn robust visual-semantic\nassociations even with NC. Specifically, RDE consists of two main components:\n1) A Confident Consensus Division (CCD) module that leverages the dual-grained\ndecisions of dual embedding modules to obtain a consensus set of clean training\ndata, which enables the model to learn correct and reliable visual-semantic\nassociations. 2) A Triplet Alignment Loss (TAL) relaxes the conventional\nTriplet Ranking loss with the hardest negative samples to a log-exponential\nupper bound over all negative ones, thus preventing the model collapse under NC\nand can also focus on hard-negative samples for promising performance. We\nconduct extensive experiments on three public benchmarks, namely CUHK-PEDES,\nICFG-PEDES, and RSTPReID, to evaluate the performance and robustness of our\nRDE. Our method achieves state-of-the-art results both with and without\nsynthetic noisy correspondences on all three datasets. Code is available at\nhttps://github.com/QinYang79/RDE.\n","authors":["Yang Qin","Yingke Chen","Dezhong Peng","Xi Peng","Joey Tianyi Zhou","Peng Hu"],"pdf_url":"https://arxiv.org/pdf/2308.09911v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.13102v2","updated":"2024-03-28T07:13:53Z","published":"2023-12-20T15:20:25Z","title":"SpecNeRF: Gaussian Directional Encoding for Specular Reflections","summary":"  Neural radiance fields have achieved remarkable performance in modeling the\nappearance of 3D scenes. However, existing approaches still struggle with the\nview-dependent appearance of glossy surfaces, especially under complex lighting\nof indoor environments. Unlike existing methods, which typically assume distant\nlighting like an environment map, we propose a learnable Gaussian directional\nencoding to better model the view-dependent effects under near-field lighting\nconditions. Importantly, our new directional encoding captures the\nspatially-varying nature of near-field lighting and emulates the behavior of\nprefiltered environment maps. As a result, it enables the efficient evaluation\nof preconvolved specular color at any 3D location with varying roughness\ncoefficients. We further introduce a data-driven geometry prior that helps\nalleviate the shape radiance ambiguity in reflection modeling. We show that our\nGaussian directional encoding and geometry prior significantly improve the\nmodeling of challenging specular reflections in neural radiance fields, which\nhelps decompose appearance into more physically meaningful components.\n","authors":["Li Ma","Vasu Agrawal","Haithem Turki","Changil Kim","Chen Gao","Pedro Sander","Michael Zollhöfer","Christian Richardt"],"pdf_url":"https://arxiv.org/pdf/2312.13102v2.pdf","comment":"Accepted to CVPR2024, Project page:\n  https://limacv.github.io/SpecNeRF_web/"},{"id":"http://arxiv.org/abs/2403.19177v1","updated":"2024-03-28T07:01:11Z","published":"2024-03-28T07:01:11Z","title":"Rethinking Information Loss in Medical Image Segmentation with\n  Various-sized Targets","summary":"  Medical image segmentation presents the challenge of segmenting various-size\ntargets, demanding the model to effectively capture both local and global\ninformation. Despite recent efforts using CNNs and ViTs to predict annotations\nof different scales, these approaches often struggle to effectively balance the\ndetection of targets across varying sizes. Simply utilizing local information\nfrom CNNs and global relationships from ViTs without considering potential\nsignificant divergence in latent feature distributions may result in\nsubstantial information loss. To address this issue, in this paper, we will\nintroduce a novel Stagger Network (SNet) and argues that a well-designed fusion\nstructure can mitigate the divergence in latent feature distributions between\nCNNs and ViTs, thereby reducing information loss. Specifically, to emphasize\nboth global dependencies and local focus, we design a Parallel Module to bridge\nthe semantic gap. Meanwhile, we propose the Stagger Module, trying to fuse the\nselected features that are more semantically similar. An Information Recovery\nModule is further adopted to recover complementary information back to the\nnetwork. As a key contribution, we theoretically analyze that the proposed\nparallel and stagger strategies would lead to less information loss, thus\ncertifying the SNet's rationale. Experimental results clearly proved that the\nproposed SNet excels comparisons with recent SOTAs in segmenting on the Synapse\ndataset where targets are in various sizes. Besides, it also demonstrates\nsuperiority on the ACDC and the MoNuSeg datasets where targets are with more\nconsistent dimensions.\n","authors":["Tianyi Liu","Zhaorui Tan","Kaizhu Huang","Haochuan Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.19177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16169v3","updated":"2024-03-28T06:56:45Z","published":"2024-03-24T14:24:13Z","title":"Gaze-guided Hand-Object Interaction Synthesis: Benchmark and Method","summary":"  Gaze plays a crucial role in revealing human attention and intention,\nshedding light on the cognitive processes behind human actions. The integration\nof gaze guidance with the dynamics of hand-object interactions boosts the\naccuracy of human motion prediction. However, the lack of datasets that capture\nthe intricate relationship and consistency among gaze, hand, and object\nmovements remains a substantial hurdle. In this paper, we introduce the first\nGaze-guided Hand-Object Interaction dataset, GazeHOI, and present a novel task\nfor synthesizing gaze-guided hand-object interactions. Our dataset, GazeHOI,\nfeatures simultaneous 3D modeling of gaze, hand, and object interactions,\ncomprising 479 sequences with an average duration of 19.1 seconds, 812\nsub-sequences, and 33 objects of various sizes. We propose a hierarchical\nframework centered on a gaze-guided hand-object interaction diffusion model,\nnamed GHO-Diffusion. In the pre-diffusion phase, we separate gaze conditions\ninto spatial-temporal features and goal pose conditions at different levels of\ninformation granularity. During the diffusion phase, two gaze-conditioned\ndiffusion models are stacked to simplify the complex synthesis of hand-object\nmotions. Here, the object motion diffusion model generates sequences of object\nmotions based on gaze conditions, while the hand motion diffusion model\nproduces hand motions based on the generated object motion. To improve\nfine-grained goal pose alignment, we introduce a Spherical Gaussian constraint\nto guide the denoising step. In the subsequent post-diffusion phase, we\noptimize the generated hand motions using contact consistency. Our extensive\nexperiments highlight the uniqueness of our dataset and the effectiveness of\nour approach.\n","authors":["Jie Tian","Lingxiao Yang","Ran Ji","Yuexin Ma","Lan Xu","Jingyi Yu","Ye Shi","Jingya Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16169v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19174v1","updated":"2024-03-28T06:46:45Z","published":"2024-03-28T06:46:45Z","title":"Algorithmic Ways of Seeing: Using Object Detection to Facilitate Art\n  Exploration","summary":"  This Research through Design paper explores how object detection may be\napplied to a large digital art museum collection to facilitate new ways of\nencountering and experiencing art. We present the design and evaluation of an\ninteractive application called SMKExplore, which allows users to explore a\nmuseum's digital collection of paintings by browsing through objects detected\nin the images, as a novel form of open-ended exploration. We provide three\ncontributions. First, we show how an object detection pipeline can be\nintegrated into a design process for visual exploration. Second, we present the\ndesign and development of an app that enables exploration of an art museum's\ncollection. Third, we offer reflections on future possibilities for museums and\nHCI researchers to incorporate object detection techniques into the\ndigitalization of museums.\n","authors":["Louie Søs Meyer","Johanne Engel Aaen","Anitamalina Regitse Tranberg","Peter Kun","Matthias Freiberger","Sebastian Risi","Anders Sundnes Løvlie"],"pdf_url":"https://arxiv.org/pdf/2403.19174v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00365v2","updated":"2024-03-28T06:38:55Z","published":"2023-12-31T01:39:38Z","title":"HQ-VAE: Hierarchical Discrete Representation Learning with Variational\n  Bayes","summary":"  Vector quantization (VQ) is a technique to deterministically learn features\nwith discrete codebook representations. It is commonly performed with a\nvariational autoencoding model, VQ-VAE, which can be further extended to\nhierarchical structures for making high-fidelity reconstructions. However, such\nhierarchical extensions of VQ-VAE often suffer from the codebook/layer collapse\nissue, where the codebook is not efficiently used to express the data, and\nhence degrades reconstruction accuracy. To mitigate this problem, we propose a\nnovel unified framework to stochastically learn hierarchical discrete\nrepresentation on the basis of the variational Bayes framework, called\nhierarchically quantized variational autoencoder (HQ-VAE). HQ-VAE naturally\ngeneralizes the hierarchical variants of VQ-VAE, such as VQ-VAE-2 and\nresidual-quantized VAE (RQ-VAE), and provides them with a Bayesian training\nscheme. Our comprehensive experiments on image datasets show that HQ-VAE\nenhances codebook usage and improves reconstruction performance. We also\nvalidated HQ-VAE in terms of its applicability to a different modality with an\naudio dataset.\n","authors":["Yuhta Takida","Yukara Ikemiya","Takashi Shibuya","Kazuki Shimada","Woosung Choi","Chieh-Hsin Lai","Naoki Murata","Toshimitsu Uesaka","Kengo Uchida","Wei-Hsiang Liao","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2401.00365v2.pdf","comment":"34 pages with 17 figures, accepted for TMLR"},{"id":"http://arxiv.org/abs/2403.19164v1","updated":"2024-03-28T06:22:45Z","published":"2024-03-28T06:22:45Z","title":"RecDiffusion: Rectangling for Image Stitching with Diffusion Models","summary":"  Image stitching from different captures often results in non-rectangular\nboundaries, which is often considered unappealing. To solve non-rectangular\nboundaries, current solutions involve cropping, which discards image content,\ninpainting, which can introduce unrelated content, or warping, which can\ndistort non-linear features and introduce artifacts. To overcome these issues,\nwe introduce a novel diffusion-based learning framework, \\textbf{RecDiffusion},\nfor image stitching rectangling. This framework combines Motion Diffusion\nModels (MDM) to generate motion fields, effectively transitioning from the\nstitched image's irregular borders to a geometrically corrected intermediary.\nFollowed by Content Diffusion Models (CDM) for image detail refinement.\nNotably, our sampling process utilizes a weighted map to identify regions\nneeding correction during each iteration of CDM. Our RecDiffusion ensures\ngeometric accuracy and overall visual appeal, surpassing all previous methods\nin both quantitative and qualitative measures when evaluated on public\nbenchmarks. Code is released at https://github.com/lhaippp/RecDiffusion.\n","authors":["Tianhao Zhou","Haipeng Li","Ziyi Wang","Ao Luo","Chen-Lin Zhang","Jiajun Li","Bing Zeng","Shuaicheng Liu"],"pdf_url":"https://arxiv.org/pdf/2403.19164v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10522v5","updated":"2024-03-28T06:20:10Z","published":"2023-11-17T13:43:43Z","title":"Enhancing Object Coherence in Layout-to-Image Synthesis","summary":"  Layout-to-image synthesis is an emerging technique in conditional image\ngeneration. It aims to generate complex scenes, where users require fine\ncontrol over the layout of the objects in a scene. However, it remains\nchallenging to control the object coherence, including semantic coherence\n(e.g., the cat looks at the flowers or not) and physical coherence (e.g., the\nhand and the racket should not be misaligned). In this paper, we propose a\nnovel diffusion model with effective global semantic fusion (GSF) and\nself-similarity feature enhancement modules to guide the object coherence for\nthis task. For semantic coherence, we argue that the image caption contains\nrich information for defining the semantic relationship within the objects in\nthe images. Instead of simply employing cross-attention between captions and\ngenerated images, which addresses the highly relevant layout restriction and\nsemantic coherence separately and thus leads to unsatisfying results shown in\nour experiments, we develop GSF to fuse the supervision from the layout\nrestriction and semantic coherence requirement and exploit it to guide the\nimage synthesis process. Moreover, to improve the physical coherence, we\ndevelop a Self-similarity Coherence Attention (SCA) module to explicitly\nintegrate local contextual physical coherence into each pixel's generation\nprocess. Specifically, we adopt a self-similarity map to encode the coherence\nrestrictions and employ it to extract coherent features from text embedding.\nThrough visualization of our self-similarity map, we explore the essence of\nSCA, revealing that its effectiveness is not only in capturing reliable\nphysical coherence patterns but also in enhancing complex texture generation.\nExtensive experiments demonstrate the superiority of our proposed method in\nboth image generation quality and controllability.\n","authors":["Yibin Wang","Weizhong Zhang","Jianwei Zheng","Cheng Jin"],"pdf_url":"https://arxiv.org/pdf/2311.10522v5.pdf","comment":"GitHub: https://github.com/CodeGoat24/EOCNet"},{"id":"http://arxiv.org/abs/2403.19163v1","updated":"2024-03-28T06:18:12Z","published":"2024-03-28T06:18:12Z","title":"D'OH: Decoder-Only random Hypernetworks for Implicit Neural\n  Representations","summary":"  Deep implicit functions have been found to be an effective tool for\nefficiently encoding all manner of natural signals. Their attractiveness stems\nfrom their ability to compactly represent signals with little to no off-line\ntraining data. Instead, they leverage the implicit bias of deep networks to\ndecouple hidden redundancies within the signal. In this paper, we explore the\nhypothesis that additional compression can be achieved by leveraging the\nredundancies that exist between layers. We propose to use a novel run-time\ndecoder-only hypernetwork - that uses no offline training data - to better\nmodel this cross-layer parameter redundancy. Previous applications of\nhyper-networks with deep implicit functions have applied feed-forward\nencoder/decoder frameworks that rely on large offline datasets that do not\ngeneralize beyond the signals they were trained on. We instead present a\nstrategy for the initialization of run-time deep implicit functions for\nsingle-instance signals through a Decoder-Only randomly projected Hypernetwork\n(D'OH). By directly changing the dimension of a latent code to approximate a\ntarget implicit neural architecture, we provide a natural way to vary the\nmemory footprint of neural representations without the costly need for neural\narchitecture search on a space of alternative low-rate structures.\n","authors":["Cameron Gordon","Lachlan Ewen MacDonald","Hemanth Saratchandran","Simon Lucey"],"pdf_url":"https://arxiv.org/pdf/2403.19163v1.pdf","comment":"29 pages, 17 figures"},{"id":"http://arxiv.org/abs/2403.19203v1","updated":"2024-03-28T08:00:14Z","published":"2024-03-28T08:00:14Z","title":"Single-Shared Network with Prior-Inspired Loss for Parameter-Efficient\n  Multi-Modal Imaging Skin Lesion Classification","summary":"  In this study, we introduce a multi-modal approach that efficiently\nintegrates multi-scale clinical and dermoscopy features within a single\nnetwork, thereby substantially reducing model parameters. The proposed method\nincludes three novel fusion schemes. Firstly, unlike current methods that\nusually employ two individual models for for clinical and dermoscopy\nmodalities, we verified that multimodal feature can be learned by sharing the\nparameters of encoder while leaving the individual modal-specific classifiers.\nSecondly, the shared cross-attention module can replace the individual one to\nefficiently interact between two modalities at multiple layers. Thirdly,\ndifferent from current methods that equally optimize dermoscopy and clinical\nbranches, inspired by prior knowledge that dermoscopy images play a more\nsignificant role than clinical images, we propose a novel biased loss. This\nloss guides the single-shared network to prioritize dermoscopy information over\nclinical information, implicitly learning a better joint feature representation\nfor the modal-specific task. Extensive experiments on a well-recognized\nSeven-Point Checklist (SPC) dataset and a collected dataset demonstrate the\neffectiveness of our method on both CNN and Transformer structures.\nFurthermore, our method exhibits superiority in both accuracy and model\nparameters compared to currently advanced methods.\n","authors":["Peng Tang","Tobias Lasser"],"pdf_url":"https://arxiv.org/pdf/2403.19203v1.pdf","comment":"This paper have submitted to Journal for review"},{"id":"http://arxiv.org/abs/2403.19885v1","updated":"2024-03-28T23:51:51Z","published":"2024-03-28T23:51:51Z","title":"Towards Long Term SLAM on Thermal Imagery","summary":"  Visual SLAM with thermal imagery, and other low contrast visually degraded\nenvironments such as underwater, or in areas dominated by snow and ice, remain\na difficult problem for many state of the art (SOTA) algorithms. In addition to\nchallenging front-end data association, thermal imagery presents an additional\ndifficulty for long term relocalization and map reuse. The relative\ntemperatures of objects in thermal imagery change dramatically from day to\nnight. Feature descriptors typically used for relocalization in SLAM are unable\nto maintain consistency over these diurnal changes. We show that learned\nfeature descriptors can be used within existing Bag of Word based localization\nschemes to dramatically improve place recognition across large temporal gaps in\nthermal imagery. In order to demonstrate the effectiveness of our trained\nvocabulary, we have developed a baseline SLAM system, integrating learned\nfeatures and matching into a classical SLAM algorithm. Our system demonstrates\ngood local tracking on challenging thermal imagery, and relocalization that\novercomes dramatic day to night thermal appearance changes. Our code and\ndatasets are available here:\nhttps://github.com/neufieldrobotics/IRSLAM_Baseline\n","authors":["Colin Keil","Aniket Gupta","Pushyami Kaveti","Hanumant Singh"],"pdf_url":"https://arxiv.org/pdf/2403.19885v1.pdf","comment":"8 pages, 7 figures, Submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2403.19882v1","updated":"2024-03-28T23:31:59Z","published":"2024-03-28T23:31:59Z","title":"Enhancing Efficiency in Vision Transformer Networks: Design Techniques\n  and Insights","summary":"  Intrigued by the inherent ability of the human visual system to identify\nsalient regions in complex scenes, attention mechanisms have been seamlessly\nintegrated into various Computer Vision (CV) tasks. Building upon this\nparadigm, Vision Transformer (ViT) networks exploit attention mechanisms for\nimproved efficiency. This review navigates the landscape of redesigned\nattention mechanisms within ViTs, aiming to enhance their performance. This\npaper provides a comprehensive exploration of techniques and insights for\ndesigning attention mechanisms, systematically reviewing recent literature in\nthe field of CV. This survey begins with an introduction to the theoretical\nfoundations and fundamental concepts underlying attention mechanisms. We then\npresent a systematic taxonomy of various attention mechanisms within ViTs,\nemploying redesigned approaches. A multi-perspective categorization is proposed\nbased on their application, objectives, and the type of attention applied. The\nanalysis includes an exploration of the novelty, strengths, weaknesses, and an\nin-depth evaluation of the different proposed strategies. This culminates in\nthe development of taxonomies that highlight key properties and contributions.\nFinally, we gather the reviewed studies along with their available open-source\nimplementations at our\n\\href{https://github.com/mindflow-institue/Awesome-Attention-Mechanism-in-Medical-Imaging}{GitHub}\\footnote{\\url{https://github.com/xmindflow/Awesome-Attention-Mechanism-in-Medical-Imaging}}.\nWe aim to regularly update it with the most recent relevant papers.\n","authors":["Moein Heidari","Reza Azad","Sina Ghorbani Kolahi","René Arimond","Leon Niggemeier","Alaa Sulaiman","Afshin Bozorgpour","Ehsan Khodapanah Aghdam","Amirhossein Kazerouni","Ilker Hacihaliloglu","Dorit Merhof"],"pdf_url":"https://arxiv.org/pdf/2403.19882v1.pdf","comment":"Submitted to Computational Visual Media Journal"},{"id":"http://arxiv.org/abs/2403.19880v1","updated":"2024-03-28T23:26:45Z","published":"2024-03-28T23:26:45Z","title":"Vision-Language Synthetic Data Enhances Echocardiography Downstream\n  Tasks","summary":"  High-quality, large-scale data is essential for robust deep learning models\nin medical applications, particularly ultrasound image analysis. Diffusion\nmodels facilitate high-fidelity medical image generation, reducing the costs\nassociated with acquiring and annotating new images. This paper utilizes recent\nvision-language models to produce diverse and realistic synthetic\nechocardiography image data, preserving key features of the original images\nguided by textual and semantic label maps. Specifically, we investigate three\npotential avenues: unconditional generation, generation guided by text, and a\nhybrid approach incorporating both textual and semantic supervision. We show\nthat the rich contextual information present in the synthesized data\npotentially enhances the accuracy and interpretability of downstream tasks,\nsuch as echocardiography segmentation and classification with improved metrics\nand faster convergence. Our implementation with checkpoints, prompts, and the\ncreated synthetic dataset will be publicly available at\n\\href{https://github.com/Pooria90/DiffEcho}{GitHub}.\n","authors":["Pooria Ashrafian","Milad Yazdani","Moein Heidari","Dena Shahriari","Ilker Hacihaliloglu"],"pdf_url":"https://arxiv.org/pdf/2403.19880v1.pdf","comment":"Submitted as a conference paper to MICCAI 2024"},{"id":"http://arxiv.org/abs/2311.17076v2","updated":"2024-03-28T23:02:27Z","published":"2023-11-27T22:23:27Z","title":"Compositional Chain-of-Thought Prompting for Large Multimodal Models","summary":"  The combination of strong visual backbones and Large Language Model (LLM)\nreasoning has led to Large Multimodal Models (LMMs) becoming the current\nstandard for a wide range of vision and language (VL) tasks. However, recent\nresearch has shown that even the most advanced LMMs still struggle to capture\naspects of compositional visual reasoning, such as attributes and relationships\nbetween objects. One solution is to utilize scene graphs (SGs)--a formalization\nof objects and their relations and attributes that has been extensively used as\na bridge between the visual and textual domains. Yet, scene graph data requires\nscene graph annotations, which are expensive to collect and thus not easily\nscalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic\nforgetting of the pretraining objective. To overcome this, inspired by\nchain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a\nnovel zero-shot Chain-of-Thought prompting method that utilizes SG\nrepresentations in order to extract compositional knowledge from an LMM.\nSpecifically, we first generate an SG using the LMM, and then use that SG in\nthe prompt to produce a response. Through extensive experiments, we find that\nthe proposed CCoT approach not only improves LMM performance on several vision\nand language VL compositional benchmarks but also improves the performance of\nseveral popular LMMs on general multimodal benchmarks, without the need for\nfine-tuning or annotated ground-truth SGs. Code:\nhttps://github.com/chancharikmitra/CCoT\n","authors":["Chancharik Mitra","Brandon Huang","Trevor Darrell","Roei Herzig"],"pdf_url":"https://arxiv.org/pdf/2311.17076v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01779v3","updated":"2024-03-28T22:27:12Z","published":"2023-10-03T04:01:27Z","title":"HallE-Control: Controlling Object Hallucination in Large Multimodal\n  Models","summary":"  Current Large Multimodal Models (LMMs) achieve remarkable progress, yet there\nremains significant uncertainty regarding their ability to accurately apprehend\nvisual details, that is, in performing detailed captioning. To address this, we\nintroduce $\\textit{CCEval}$, a GPT-4 assisted evaluation method for detailed\ncaptioning. Interestingly, while LMMs demonstrate minimal object existence\nhallucination in existing VQA benchmarks, our proposed evaluation reveals\ncontinued susceptibility to such hallucinations. In this paper, we make the\nfirst attempt to investigate such hallucination from different aspects,\nincluding image resolution, the language decoder size, and instruction data\namount, quality, granularity. Our findings underscore the unwarranted inference\nwhen the language description includes details at a finer object granularity\nthan what the vision module can ground or verify, thus inducing hallucination.\nTo control such hallucinations, we further attribute the reliability of\ncaptioning to contextual knowledge (involving only contextually grounded\nobjects) and parametric knowledge (containing inferred objects by the model).\nThus, we introduce $\\textit{HallE-Control}$, a controllable LMM in terms of\n$\\textbf{Hall}$ucination in object $\\textbf{E}$xistence. HallE-Control can\ncondition the captioning to shift between (i) exclusively depicting contextual\nknowledge for grounded objects and (ii) blending it with parametric knowledge\nto imagine inferred objects. Our method reduces hallucination by 44% compared\nto LLaVA$_{7B}$ and maintains the object coverage.\n","authors":["Bohan Zhai","Shijia Yang","Chenfeng Xu","Sheng Shen","Kurt Keutzer","Chunyuan Li","Manling Li"],"pdf_url":"https://arxiv.org/pdf/2310.01779v3.pdf","comment":"Our code is publicly available at\n  https://github.com/bronyayang/HallE_Control"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2403.19651v1","updated":"2024-03-28T17:59:20Z","published":"2024-03-28T17:59:20Z","title":"MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions","summary":"  Image retrieval, i.e., finding desired images given a reference image,\ninherently encompasses rich, multi-faceted search intents that are difficult to\ncapture solely using image-based measures. Recent work leverages text\ninstructions to allow users to more freely express their search intents.\nHowever, existing work primarily focuses on image pairs that are visually\nsimilar and/or can be characterized by a small set of pre-defined relations.\nThe core thesis of this paper is that text instructions can enable retrieving\nimages with richer relations beyond visual similarity. To show this, we\nintroduce MagicLens, a series of self-supervised image retrieval models that\nsupport open-ended instructions. MagicLens is built on a key novel insight:\nimage pairs that naturally occur on the same web pages contain a wide range of\nimplicit relations (e.g., inside view of), and we can bring those implicit\nrelations explicit by synthesizing instructions via large multimodal models\n(LMMs) and large language models (LLMs). Trained on 36.7M (query image,\ninstruction, target image) triplets with rich semantic relations mined from the\nweb, MagicLens achieves comparable or better results on eight benchmarks of\nvarious image retrieval tasks than prior state-of-the-art (SOTA) methods.\nRemarkably, it outperforms previous SOTA but with a 50X smaller model size on\nmultiple benchmarks. Additional human analyses on a 1.4M-image unseen corpus\nfurther demonstrate the diversity of search intents supported by MagicLens.\n","authors":["Kai Zhang","Yi Luan","Hexiang Hu","Kenton Lee","Siyuan Qiao","Wenhu Chen","Yu Su","Ming-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2403.19651v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2403.19546v1","updated":"2024-03-28T16:27:26Z","published":"2024-03-28T16:27:26Z","title":"Croissant: A Metadata Format for ML-Ready Datasets","summary":"  Data is a critical resource for Machine Learning (ML), yet working with data\nremains a key friction point. This paper introduces Croissant, a metadata\nformat for datasets that simplifies how data is used by ML tools and\nframeworks. Croissant makes datasets more discoverable, portable and\ninteroperable, thereby addressing significant challenges in ML data management\nand responsible AI. Croissant is already supported by several popular dataset\nrepositories, spanning hundreds of thousands of datasets, ready to be loaded\ninto the most popular ML frameworks.\n","authors":["Mubashara Akhtar","Omar Benjelloun","Costanza Conforti","Joan Giner-Miguelez","Nitisha Jain","Michael Kuchnik","Quentin Lhoest","Pierre Marcenac","Manil Maskey","Peter Mattson","Luis Oala","Pierre Ruyssen","Rajat Shinde","Elena Simperl","Goeffry Thomas","Slava Tykhonov","Joaquin Vanschoren","Steffen Vogler","Carole-Jean Wu"],"pdf_url":"https://arxiv.org/pdf/2403.19546v1.pdf","comment":"Preprint. Contributors listed in alphabetical order"},{"id":"http://arxiv.org/abs/2306.17563v2","updated":"2024-03-28T13:59:09Z","published":"2023-06-30T11:32:25Z","title":"Large Language Models are Effective Text Rankers with Pairwise Ranking\n  Prompting","summary":"  Ranking documents using Large Language Models (LLMs) by directly feeding the\nquery and candidate documents into the prompt is an interesting and practical\nproblem. However, researchers have found it difficult to outperform fine-tuned\nbaseline rankers on benchmark datasets. We analyze pointwise and listwise\nranking prompts used by existing methods and argue that off-the-shelf LLMs do\nnot fully understand these challenging ranking formulations. In this paper, we\npropose to significantly reduce the burden on LLMs by using a new technique\ncalled Pairwise Ranking Prompting (PRP). Our results are the first in the\nliterature to achieve state-of-the-art ranking performance on standard\nbenchmarks using moderate-sized open-sourced LLMs. On TREC-DL 2019&2020, PRP\nbased on the Flan-UL2 model with 20B parameters performs favorably with the\nprevious best approach in the literature, which is based on the blackbox\ncommercial GPT-4 that has 50x (estimated) model size, while outperforming other\nLLM-based solutions, such as InstructGPT which has 175B parameters, by over 10%\nfor all ranking metrics. By using the same prompt template on seven BEIR tasks,\nPRP outperforms supervised baselines and outperforms the blackbox commercial\nChatGPT solution by 4.2% and pointwise LLM-based solutions by more than 10% on\naverage NDCG@10. Furthermore, we propose several variants of PRP to improve\nefficiency and show that it is possible to achieve competitive results even\nwith linear complexity.\n","authors":["Zhen Qin","Rolf Jagerman","Kai Hui","Honglei Zhuang","Junru Wu","Le Yan","Jiaming Shen","Tianqi Liu","Jialu Liu","Donald Metzler","Xuanhui Wang","Michael Bendersky"],"pdf_url":"https://arxiv.org/pdf/2306.17563v2.pdf","comment":"Accepted to NAACL 2024. Corrected results of RankT5 on TREC-DL19"},{"id":"http://arxiv.org/abs/2307.15464v7","updated":"2024-03-28T13:28:13Z","published":"2023-07-28T10:34:47Z","title":"Framework to Automatically Determine the Quality of Open Data Catalogs","summary":"  Data catalogs play a crucial role in modern data-driven organizations by\nfacilitating the discovery, understanding, and utilization of diverse data\nassets. However, ensuring their quality and reliability is complex, especially\nin open and large-scale data environments. This paper proposes a framework to\nautomatically determine the quality of open data catalogs, addressing the need\nfor efficient and reliable quality assessment mechanisms. Our framework can\nanalyze various core quality dimensions, such as accuracy, completeness,\nconsistency, scalability, and timeliness, offer several alternatives for the\nassessment of compatibility and similarity across such catalogs as well as the\nimplementation of a set of non-core quality dimensions such as provenance,\nreadability, and licensing. The goal is to empower data-driven organizations to\nmake informed decisions based on trustworthy and well-curated data assets. The\nsource code that illustrates our approach can be downloaded from\nhttps://www.github.com/jorge-martinez-gil/dataq/.\n","authors":["Jorge Martinez-Gil"],"pdf_url":"https://arxiv.org/pdf/2307.15464v7.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2306.10367v2","updated":"2024-03-28T12:31:00Z","published":"2023-06-17T14:57:35Z","title":"Query2GMM: Learning Representation with Gaussian Mixture Model for\n  Reasoning over Knowledge Graphs","summary":"  Logical query answering over Knowledge Graphs (KGs) is a fundamental yet\ncomplex task. A promising approach to achieve this is to embed queries and\nentities jointly into the same embedding space. Research along this line\nsuggests that using multi-modal distribution to represent answer entities is\nmore suitable than uni-modal distribution, as a single query may contain\nmultiple disjoint answer subsets due to the compositional nature of multi-hop\nqueries and the varying latent semantics of relations. However, existing\nmethods based on multi-modal distribution roughly represent each subset without\ncapturing its accurate cardinality, or even degenerate into uni-modal\ndistribution learning during the reasoning process due to the lack of an\neffective similarity measure. To better model queries with diversified answers,\nwe propose Query2GMM for answering logical queries over knowledge graphs. In\nQuery2GMM, we present the GMM embedding to represent each query using a\nunivariate Gaussian Mixture Model (GMM). Each subset of a query is encoded by\nits cardinality, semantic center and dispersion degree, allowing for precise\nrepresentation of multiple subsets. Then we design specific neural networks for\neach operator to handle the inherent complexity that comes with multi-modal\ndistribution while alleviating the cascading errors. Last, we design a new\nsimilarity measure to assess the relationships between an entity and a query's\nmulti-answer subsets, enabling effective multi-modal distribution learning for\nreasoning. Comprehensive experimental results show that Query2GMM outperforms\nthe best competitor by an absolute average of $6.35\\%$.\n","authors":["Yuhan Wu","Yuanyuan Xu","Wenjie Zhang","Xiwei Xu","Ying Zhang"],"pdf_url":"https://arxiv.org/pdf/2306.10367v2.pdf","comment":"10 pages, 4 figures, accepted by WWW 2024"},{"id":"http://arxiv.org/abs/2403.19347v1","updated":"2024-03-28T12:05:15Z","published":"2024-03-28T12:05:15Z","title":"Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual\n  User Behaviors","summary":"  With the rise of large language models (LLMs), recent works have leveraged\nLLMs to improve the performance of click-through rate (CTR) prediction.\nHowever, we argue that a critical obstacle remains in deploying LLMs for\npractical use: the efficiency of LLMs when processing long textual user\nbehaviors. As user sequences grow longer, the current efficiency of LLMs is\ninadequate for training on billions of users and items. To break through the\nefficiency barrier of LLMs, we propose Behavior Aggregated Hierarchical\nEncoding (BAHE) to enhance the efficiency of LLM-based CTR modeling.\nSpecifically, BAHE proposes a novel hierarchical architecture that decouples\nthe encoding of user behaviors from inter-behavior interactions. Firstly, to\nprevent computational redundancy from repeated encoding of identical user\nbehaviors, BAHE employs the LLM's pre-trained shallow layers to extract\nembeddings of the most granular, atomic user behaviors from extensive user\nsequences and stores them in the offline database. Subsequently, the deeper,\ntrainable layers of the LLM facilitate intricate inter-behavior interactions,\nthereby generating comprehensive user embeddings. This separation allows the\nlearning of high-level user representations to be independent of low-level\nbehavior encoding, significantly reducing computational complexity. Finally,\nthese refined user embeddings, in conjunction with correspondingly processed\nitem embeddings, are incorporated into the CTR model to compute the CTR scores.\nExtensive experimental results show that BAHE reduces training time and memory\nby five times for CTR models using LLMs, especially with longer user sequences.\nBAHE has been deployed in a real-world system, allowing for daily updates of 50\nmillion CTR data on 8 A100 GPUs, making LLMs practical for industrial CTR\nprediction.\n","authors":["Binzong Geng","Zhaoxin Huan","Xiaolu Zhang","Yong He","Liang Zhang","Fajie Yuan","Jun Zhou","Linjian Mo"],"pdf_url":"https://arxiv.org/pdf/2403.19347v1.pdf","comment":"Accepted by the 47th International ACM SIGIR Conference on Research\n  and Development in Information Retrieval (SIGIR), 2024"},{"id":"http://arxiv.org/abs/2403.19345v1","updated":"2024-03-28T12:02:45Z","published":"2024-03-28T12:02:45Z","title":"Intelligent Classification and Personalized Recommendation of E-commerce\n  Products Based on Machine Learning","summary":"  With the rapid evolution of the Internet and the exponential proliferation of\ninformation, users encounter information overload and the conundrum of choice.\nPersonalized recommendation systems play a pivotal role in alleviating this\nburden by aiding users in filtering and selecting information tailored to their\npreferences and requirements. Such systems not only enhance user experience and\nsatisfaction but also furnish opportunities for businesses and platforms to\naugment user engagement, sales, and advertising efficacy.This paper undertakes\na comparative analysis between the operational mechanisms of traditional\ne-commerce commodity classification systems and personalized recommendation\nsystems. It delineates the significance and application of personalized\nrecommendation systems across e-commerce, content information, and media\ndomains. Furthermore, it delves into the challenges confronting personalized\nrecommendation systems in e-commerce, including data privacy, algorithmic bias,\nscalability, and the cold start problem. Strategies to address these challenges\nare elucidated.Subsequently, the paper outlines a personalized recommendation\nsystem leveraging the BERT model and nearest neighbor algorithm, specifically\ntailored to address the exigencies of the eBay e-commerce platform. The\nefficacy of this recommendation system is substantiated through manual\nevaluation, and a practical application operational guide and structured output\nrecommendation results are furnished to ensure the system's operability and\nscalability.\n","authors":["Kangming Xu","Huiming Zhou","Haotian Zheng","Mingwei Zhu","Qi Xin"],"pdf_url":"https://arxiv.org/pdf/2403.19345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04260v2","updated":"2024-03-28T11:55:32Z","published":"2024-03-07T06:49:37Z","title":"Can Small Language Models be Good Reasoners for Sequential\n  Recommendation?","summary":"  Large language models (LLMs) open up new horizons for sequential\nrecommendations, owing to their remarkable language comprehension and\ngeneration capabilities. However, there are still numerous challenges that\nshould be addressed to successfully implement sequential recommendations\nempowered by LLMs. Firstly, user behavior patterns are often complex, and\nrelying solely on one-step reasoning from LLMs may lead to incorrect or\ntask-irrelevant responses. Secondly, the prohibitively resource requirements of\nLLM (e.g., ChatGPT-175B) are overwhelmingly high and impractical for real\nsequential recommender systems. In this paper, we propose a novel Step-by-step\nknowLedge dIstillation fraMework for recommendation (SLIM), paving a promising\npath for sequential recommenders to enjoy the exceptional reasoning\ncapabilities of LLMs in a \"slim\" (i.e., resource-efficient) manner. We\nintroduce CoT prompting based on user behavior sequences for the larger teacher\nmodel. The rationales generated by the teacher model are then utilized as\nlabels to distill the downstream smaller student model (e.g., LLaMA2-7B). In\nthis way, the student model acquires the step-by-step reasoning capabilities in\nrecommendation tasks. We encode the generated rationales from the student model\ninto a dense vector, which empowers recommendation in both ID-based and\nID-agnostic scenarios. Extensive experiments demonstrate the effectiveness of\nSLIM over state-of-the-art baselines, and further analysis showcasing its\nability to generate meaningful recommendation reasoning at affordable costs.\n","authors":["Yuling Wang","Changxin Tian","Binbin Hu","Yanhua Yu","Ziqi Liu","Zhiqiang Zhang","Jun Zhou","Liang Pang","Xiao Wang"],"pdf_url":"https://arxiv.org/pdf/2403.04260v2.pdf","comment":"Accepted by TheWebConf (WWW) 2024"},{"id":"http://arxiv.org/abs/2403.18025v2","updated":"2024-03-28T11:01:21Z","published":"2024-03-26T18:23:16Z","title":"Improving Pre-trained Language Model Sensitivity via Mask Specific\n  losses: A case study on Biomedical NER","summary":"  Adapting language models (LMs) to novel domains is often achieved through\nfine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning\nintroduces new knowledge into an LM, enabling it to comprehend and efficiently\nperform a target domain task. Fine-tuning can however be inadvertently\ninsensitive if it ignores the wide array of disparities (e.g in word meaning)\nbetween source and target domains. For instance, words such as chronic and\npressure may be treated lightly in social conversations, however, clinically,\nthese words are usually an expression of concern. To address insensitive\nfine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach\nthat efficiently acquires target domain knowledge by appropriately weighting\nthe importance of domain-specific terms (DS-terms) during fine-tuning. MSLM\njointly masks DS-terms and generic words, then learns mask-specific losses by\nensuring LMs incur larger penalties for inaccurately predicting DS-terms\ncompared to generic words. Results of our analysis show that MSLM improves LMs\nsensitivity and detection of DS-terms. We empirically show that an optimal\nmasking rate not only depends on the LM, but also on the dataset and the length\nof sequences. Our proposed masking strategy outperforms advanced masking\nstrategies such as span- and PMI-based masking.\n","authors":["Micheal Abaho","Danushka Bollegala","Gary Leeming","Dan Joyce","Iain E Buchan"],"pdf_url":"https://arxiv.org/pdf/2403.18025v2.pdf","comment":"Paper alrerady accepted for publishing by the NAACL 2024 conference\n  (main conference paper)"},{"id":"http://arxiv.org/abs/2403.19302v1","updated":"2024-03-28T10:40:22Z","published":"2024-03-28T10:40:22Z","title":"Generate then Retrieve: Conversational Response Retrieval Using LLMs as\n  Answer and Query Generators","summary":"  CIS is a prominent area in IR that focuses on developing interactive\nknowledge assistants. These systems must adeptly comprehend the user's\ninformation requirements within the conversational context and retrieve the\nrelevant information. To this aim, the existing approaches model the user's\ninformation needs with one query called rewritten query and use this query for\npassage retrieval. In this paper, we propose three different methods for\ngenerating multiple queries to enhance the retrieval. In these methods, we\nleverage the capabilities of large language models (LLMs) in understanding the\nuser's information need and generating an appropriate response, to generate\nmultiple queries. We implement and evaluate the proposed models utilizing\nvarious LLMs including GPT-4 and Llama-2 chat in zero-shot and few-shot\nsettings. In addition, we propose a new benchmark for TREC iKAT based on gpt\n3.5 judgments. Our experiments reveal the effectiveness of our proposed models\non the TREC iKAT dataset.\n","authors":["Zahra Abbasiantaeb","Mohammad Aliannejadi"],"pdf_url":"https://arxiv.org/pdf/2403.19302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19276v1","updated":"2024-03-28T10:01:35Z","published":"2024-03-28T10:01:35Z","title":"Enhanced Bayesian Personalized Ranking for Robust Hard Negative Sampling\n  in Recommender Systems","summary":"  In implicit collaborative filtering, hard negative mining techniques are\ndeveloped to accelerate and enhance the recommendation model learning. However,\nthe inadvertent selection of false negatives remains a major concern in hard\nnegative sampling, as these false negatives can provide incorrect information\nand mislead the model learning. To date, only a small number of studies have\nbeen committed to solve the false negative problem, primarily focusing on\ndesigning sophisticated sampling algorithms to filter false negatives. In\ncontrast, this paper shifts its focus to refining the loss function. We find\nthat the original Bayesian Personalized Ranking (BPR), initially designed for\nuniform negative sampling, is inadequate in adapting to hard sampling\nscenarios. Hence, we introduce an enhanced Bayesian Personalized Ranking\nobjective, named as Hard-BPR, which is specifically crafted for dynamic hard\nnegative sampling to mitigate the influence of false negatives. This method is\nsimple yet efficient for real-world deployment. Extensive experiments conducted\non three real-world datasets demonstrate the effectiveness and robustness of\nour approach, along with the enhanced ability to distinguish false negatives.\n","authors":["Kexin Shi","Jing Zhang","Linjiajie Fang","Wenjia Wang","Bingyi Jing"],"pdf_url":"https://arxiv.org/pdf/2403.19276v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2312.10370v2","updated":"2024-03-28T09:12:21Z","published":"2023-12-16T08:08:36Z","title":"Do Similar Entities have Similar Embeddings?","summary":"  Knowledge graph embedding models (KGEMs) developed for link prediction learn\nvector representations for entities in a knowledge graph, known as embeddings.\nA common tacit assumption is the KGE entity similarity assumption, which states\nthat these KGEMs retain the graph's structure within their embedding space,\n\\textit{i.e.}, position similar entities within the graph close to one another.\nThis desirable property make KGEMs widely used in downstream tasks such as\nrecommender systems or drug repurposing. Yet, the relation of entity similarity\nand similarity in the embedding space has rarely been formally evaluated.\nTypically, KGEMs are assessed based on their sole link prediction capabilities,\nusing ranked-based metrics such as Hits@K or Mean Rank. This paper challenges\nthe prevailing assumption that entity similarity in the graph is inherently\nmirrored in the embedding space. Therefore, we conduct extensive experiments to\nmeasure the capability of KGEMs to cluster similar entities together, and\ninvestigate the nature of the underlying factors. Moreover, we study if\ndifferent KGEMs expose a different notion of similarity. Datasets, pre-trained\nembeddings and code are available at:\nhttps://github.com/nicolas-hbt/similar-embeddings/.\n","authors":["Nicolas Hubert","Heiko Paulheim","Armelle Brun","Davy Monticolo"],"pdf_url":"https://arxiv.org/pdf/2312.10370v2.pdf","comment":"Accepted at ESWC 2024"},{"id":"http://arxiv.org/abs/2403.19216v1","updated":"2024-03-28T08:27:44Z","published":"2024-03-28T08:27:44Z","title":"Are Large Language Models Good at Utility Judgments?","summary":"  Retrieval-augmented generation (RAG) is considered to be a promising approach\nto alleviate the hallucination issue of large language models (LLMs), and it\nhas received widespread attention from researchers recently. Due to the\nlimitation in the semantic understanding of retrieval models, the success of\nRAG heavily lies on the ability of LLMs to identify passages with utility.\nRecent efforts have explored the ability of LLMs to assess the relevance of\npassages in retrieval, but there has been limited work on evaluating the\nutility of passages in supporting question answering. In this work, we conduct\na comprehensive study about the capabilities of LLMs in utility evaluation for\nopen-domain QA. Specifically, we introduce a benchmarking procedure and\ncollection of candidate passages with different characteristics, facilitating a\nseries of experiments with five representative LLMs. Our experiments reveal\nthat: (i) well-instructed LLMs can distinguish between relevance and utility,\nand that LLMs are highly receptive to newly generated counterfactual passages.\nMoreover, (ii) we scrutinize key factors that affect utility judgments in the\ninstruction design. And finally, (iii) to verify the efficacy of utility\njudgments in practical retrieval augmentation applications, we delve into LLMs'\nQA capabilities using the evidence judged with utility and direct dense\nretrieval results. (iv) We propose a k-sampling, listwise approach to reduce\nthe dependency of LLMs on the sequence of input passages, thereby facilitating\nsubsequent answer generation. We believe that the way we formalize and study\nthe problem along with our findings contributes to a critical assessment of\nretrieval-augmented LLMs. Our code and benchmark can be found at\n\\url{https://github.com/ict-bigdatalab/utility_judgments}.\n","authors":["Hengran Zhang","Ruqing Zhang","Jiafeng Guo","Maarten de Rijke","Yixing Fan","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.19216v1.pdf","comment":"Acctepted by SIGIR2024"},{"id":"http://arxiv.org/abs/2211.13912v2","updated":"2024-03-28T07:44:12Z","published":"2022-11-25T06:03:09Z","title":"Enhancing Recommender Systems: A Strategy to Mitigate False Negative\n  Impact","summary":"  In implicit collaborative filtering (CF) task of recommender systems, recent\nworks mainly focus on model structure design with promising techniques like\ngraph neural networks (GNNs). Effective and efficient negative sampling methods\nthat suit these models, however, remain underdeveloped. One challenge is that\nexisting hard negative samplers tend to suffer from severer over-fitting in\nmodel training. In this work, we first study the reason behind the\nover-fitting, and illustrate it with the incorrect selection of false negative\ninstances with the support of experiments. In addition, we empirically observe\na counter-intuitive phenomenon, that is, polluting hard negative samples'\nembeddings with a quite large proportional of positive samples' embeddings will\nlead to remarkable performance gains for prediction accuracy. On top of this\nfinding, we present a novel negative sampling strategy, i.e.,\npositive-dominated negative synthesizing (PDNS). Moreover, we provide\ntheoretical analysis and derive a simple equivalent algorithm of PDNS, where\nonly a soft factor is added in the loss function. Comprehensive experiments on\nthree real-world datasets demonstrate the superiority of our proposed method in\nterms of both effectiveness and robustness.\n","authors":["Kexin Shi","Yun Zhang","Bingyi Jing","Wenjia Wang"],"pdf_url":"https://arxiv.org/pdf/2211.13912v2.pdf","comment":"9 pages, 16 figures"},{"id":"http://arxiv.org/abs/2403.19181v1","updated":"2024-03-28T07:22:16Z","published":"2024-03-28T07:22:16Z","title":"Make Large Language Model a Better Ranker","summary":"  The evolution of Large Language Models (LLMs) has significantly enhanced\ncapabilities across various fields, leading to a paradigm shift in how\nRecommender Systems (RSs) are conceptualized and developed. However, existing\nresearch primarily focuses on point-wise and pair-wise recommendation\nparadigms. These approaches prove inefficient in LLM-based recommenders due to\nthe high computational cost of utilizing Large Language Models. While some\nstudies have delved into list-wise approaches, they fall short in ranking\ntasks. This shortfall is attributed to the misalignment between the objectives\nof ranking and language generation. To this end, this paper introduces the\nLanguage Model Framework with Aligned Listwise Ranking Objectives (ALRO). ALRO\nis designed to bridge the gap between the capabilities of LLMs and the nuanced\nrequirements of ranking tasks within recommender systems. A key feature of ALRO\nis the introduction of soft lambda loss, an adaptation of lambda loss tailored\nto suit language generation tasks. Additionally, ALRO incorporates a\npermutation-sensitive learning mechanism that addresses position bias, a\nprevalent issue in generative models, without imposing additional computational\nburdens during inference. Our evaluative studies reveal that ALRO outperforms\nexisting embedding-based recommendation methods and the existing LLM-based\nrecommendation baselines, highlighting its efficacy.\n","authors":["Wenshuo Chao","Zhi Zheng","Hengshu Zhu","Hao Liu"],"pdf_url":"https://arxiv.org/pdf/2403.19181v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.19056v3","updated":"2024-03-28T06:49:56Z","published":"2023-10-29T16:04:10Z","title":"MILL: Mutual Verification with Large Language Models for Zero-Shot Query\n  Expansion","summary":"  Query expansion, pivotal in search engines, enhances the representation of\nuser information needs with additional terms. While existing methods expand\nqueries using retrieved or generated contextual documents, each approach has\nnotable limitations. Retrieval-based methods often fail to accurately capture\nsearch intent, particularly with brief or ambiguous queries. Generation-based\nmethods, utilizing large language models (LLMs), generally lack corpus-specific\nknowledge and entail high fine-tuning costs. To address these gaps, we propose\na novel zero-shot query expansion framework utilizing LLMs for mutual\nverification. Specifically, we first design a query-query-document generation\nmethod, leveraging LLMs' zero-shot reasoning ability to produce diverse\nsub-queries and corresponding documents. Then, a mutual verification process\nsynergizes generated and retrieved documents for optimal expansion. Our\nproposed method is fully zero-shot, and extensive experiments on three public\nbenchmark datasets are conducted to demonstrate its effectiveness over existing\nmethods. Our code is available online at\nhttps://github.com/Applied-Machine-Learning-Lab/MILL to ease reproduction.\n","authors":["Pengyue Jia","Yiding Liu","Xiangyu Zhao","Xiaopeng Li","Changying Hao","Shuaiqiang Wang","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2310.19056v3.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2403.18479v2","updated":"2024-03-28T06:19:50Z","published":"2024-03-27T11:49:55Z","title":"Lightweight Embeddings for Graph Collaborative Filtering","summary":"  Graph neural networks (GNNs) are currently one of the most performant\ncollaborative filtering methods. Meanwhile, owing to the use of an embedding\ntable to represent each user/item as a distinct vector, GNN-based recommenders\nhave inherited the long-standing defect of parameter inefficiency. As a common\npractice for scalable embeddings, parameter sharing enables the use of fewer\nembedding vectors (i.e., meta-embeddings). When assigning meta-embeddings, most\nexisting methods are a heuristically designed, predefined mapping from each\nuser's/item's ID to the corresponding meta-embedding indexes, thus simplifying\nthe optimization problem into learning only the meta-embeddings. However, in\nthe context of GNN-based collaborative filtering, such a fixed mapping omits\nthe semantic correlations between entities that are evident in the user-item\ninteraction graph, leading to suboptimal recommendation performance. To this\nend, we propose Lightweight Embeddings for Graph Collaborative Filtering\n(LEGCF), a parameter-efficient embedding framework dedicated to GNN-based\nrecommenders. LEGCF innovatively introduces an assignment matrix as an extra\nlearnable component on top of meta-embeddings. To jointly optimize these two\nheavily entangled components, aside from learning the meta-embeddings by\nminimizing the recommendation loss, LEGCF further performs efficient assignment\nupdate by enforcing a novel semantic similarity constraint and finding its\nclosed-form solution based on matrix pseudo-inverse. The meta-embeddings and\nassignment matrix are alternately updated, where the latter is sparsified on\nthe fly to ensure negligible storage overhead. Extensive experiments on three\nbenchmark datasets have verified LEGCF's smallest trade-off between size and\nperformance, with consistent accuracy gain over state-of-the-art baselines. The\ncodebase of LEGCF is available in https://github.com/xurong-liang/LEGCF.\n","authors":["Xurong Liang","Tong Chen","Lizhen Cui","Yang Wang","Meng Wang","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2403.18479v2.pdf","comment":"Accepted by SIGIR '24"},{"id":"http://arxiv.org/abs/2403.17740v2","updated":"2024-03-28T04:40:59Z","published":"2024-03-26T14:29:34Z","title":"All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating\n  Prediction","summary":"  Cold-start rating prediction is a fundamental problem in recommender systems\nthat has been extensively studied. Many methods have been proposed that exploit\nexplicit relations among existing data, such as collaborative filtering, social\nrecommendations and heterogeneous information network, to alleviate the data\ninsufficiency issue for cold-start users and items. However, the explicit\nrelations constructed based on data between different roles may be unreliable\nand irrelevant, which limits the performance ceiling of the specific\nrecommendation task. Motivated by this, in this paper, we propose a flexible\nframework dubbed heterogeneous interaction rating network (HIRE). HIRE dose not\nsolely rely on the pre-defined interaction pattern or the manually constructed\nheterogeneous information network. Instead, we devise a Heterogeneous\nInteraction Module (HIM) to jointly model the heterogeneous interactions and\ndirectly infer the important interactions via the observed data. In the\nexperiments, we evaluate our model under three cold-start settings on three\nreal-world datasets. The experimental results show that HIRE outperforms other\nbaselines by a large margin. Furthermore, we visualize the inferred\ninteractions of HIRE to confirm the contribution of our model.\n","authors":["Shuheng Fang","Kangfei Zhao","Yu Rong","Zhixun Li","Jeffrey Xu Yu"],"pdf_url":"https://arxiv.org/pdf/2403.17740v2.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.11624v2","updated":"2024-03-28T04:11:28Z","published":"2024-03-18T09:56:00Z","title":"Dual-Channel Multiplex Graph Neural Networks for Recommendation","summary":"  Efficient recommender systems play a crucial role in accurately capturing\nuser and item attributes that mirror individual preferences. Some existing\nrecommendation techniques have started to shift their focus towards modeling\nvarious types of interaction relations between users and items in real-world\nrecommendation scenarios, such as clicks, marking favorites, and purchases on\nonline shopping platforms. Nevertheless, these approaches still grapple with\ntwo significant shortcomings: (1) Insufficient modeling and exploitation of the\nimpact of various behavior patterns formed by multiplex relations between users\nand items on representation learning, and (2) ignoring the effect of different\nrelations in the behavior patterns on the target relation in recommender system\nscenarios. In this study, we introduce a novel recommendation framework,\nDual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the\naforementioned challenges. It incorporates an explicit behavior pattern\nrepresentation learner to capture the behavior patterns composed of multiplex\nuser-item interaction relations, and includes a relation chain representation\nlearning and a relation chain-aware encoder to discover the impact of various\nauxiliary relations on the target relation, the dependencies between different\nrelations, and mine the appropriate order of relations in a behavior pattern.\nExtensive experiments on three real-world datasets demonstrate that our \\model\nsurpasses various state-of-the-art recommendation methods. It outperforms the\nbest baselines by 10.06\\% and 12.15\\% on average across all datasets in terms\nof R@10 and N@10 respectively.\n","authors":["Xiang Li","Chaofan Fu","Zhongying Zhao","Guanjie Zheng","Chao Huang","Junyu Dong","Yanwei Yu"],"pdf_url":"https://arxiv.org/pdf/2403.11624v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2006.14774v5","updated":"2024-03-28T03:24:11Z","published":"2020-06-26T03:22:24Z","title":"Co-Designing Statistical MIMO Radar and In-band Full-Duplex Multi-User\n  MIMO Communications -- Part I: Signal Processing","summary":"  We consider a spectral sharing problem in which a statistical (or widely\ndistributed) multiple-input multiple-output (MIMO) radar and an in-band\nfull-duplex (IBFD) multi-user MIMO (MU-MIMO) communications system concurrently\noperate within the same frequency band. Prior works on joint\nMIMO-radar-MIMO-communications (MRMC) systems largely focus on either colocated\nMIMO radars, half-duplex MIMO communications, single-user scenarios, omit\npractical constraints (clutter, uplink [UL]/downlink [DL] transmit powers,\nUL/DL quality-of-service, and peak-to-average-power ratio), or MRMC\nco-existence that employs separate transmit/receive units. The purpose of this\nand companion papers (Part II and III) is to co-design an MRMC framework that\naddresses all of these issues. In this paper, we propose signal processing for\na distributed IBFD MRMC, where radar receiver is designed to additionally\nexploit the downlink communications signals reflected from a radar target.\nExtensive numerical experiments show that our methods improve radar target\ndetection over conventional codes and yield a higher achievable data rate than\nstandard precoders. The following companion paper (Part II) describes the\ntheory and procedure of our algorithm to solve the non-convex design problem.\nThe final companion paper (Part II) considers the case of multiple targets and\nexamines the tracking performance of our MRMC system.\n","authors":["Jiawei Liu","Kumar Vijay Mishra","Mohammad Saquib"],"pdf_url":"https://arxiv.org/pdf/2006.14774v5.pdf","comment":"23 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2403.19063v1","updated":"2024-03-28T00:03:54Z","published":"2024-03-28T00:03:54Z","title":"Instruction-based Hypergraph Pretraining","summary":"  Pretraining has been widely explored to augment the adaptability of graph\nlearning models to transfer knowledge from large datasets to a downstream task,\nsuch as link prediction or classification. However, the gap between training\nobjectives and the discrepancy between data distributions in pretraining and\ndownstream tasks hinders the transfer of the pretrained knowledge. Inspired by\ninstruction-based prompts widely used in pretrained language models, we\nintroduce instructions into graph pretraining. In this paper, we propose a\nnovel pretraining framework named Instruction-based Hypergraph Pretraining. To\novercome the discrepancy between pretraining and downstream tasks, text-based\ninstructions are applied to provide explicit guidance on specific tasks for\nrepresentation learning. Compared to learnable prompts, whose effectiveness\ndepends on the quality and the diversity of training data, text-based\ninstructions intrinsically encapsulate task information and support the model\nto generalize beyond the structure seen during pretraining. To capture\nhigh-order relations with task information in a context-aware manner, a novel\nprompting hypergraph convolution layer is devised to integrate instructions\ninto information propagation in hypergraphs. Extensive experiments conducted on\nthree public datasets verify the superiority of IHP in various scenarios.\n","authors":["Mingdai Yang","Zhiwei Liu","Liangwei Yang","Xiaolong Liu","Chen Wang","Hao Peng","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2403.19063v1.pdf","comment":"Accepted by SIGIR'24"},{"id":"http://arxiv.org/abs/2403.19841v1","updated":"2024-03-28T21:28:28Z","published":"2024-03-28T21:28:28Z","title":"Dealing with Missing Modalities in Multimodal Recommendation: a Feature\n  Propagation-based Approach","summary":"  Multimodal recommender systems work by augmenting the representation of the\nproducts in the catalogue through multimodal features extracted from images,\ntextual descriptions, or audio tracks characterising such products.\nNevertheless, in real-world applications, only a limited percentage of products\ncome with multimodal content to extract meaningful features from, making it\nhard to provide accurate recommendations. To the best of our knowledge, very\nfew attention has been put into the problem of missing modalities in multimodal\nrecommendation so far. To this end, our paper comes as a preliminary attempt to\nformalise and address such an issue. Inspired by the recent advances in graph\nrepresentation learning, we propose to re-sketch the missing modalities problem\nas a problem of missing graph node features to apply the state-of-the-art\nfeature propagation algorithm eventually. Technically, we first project the\nuser-item graph into an item-item one based on co-interactions. Then,\nleveraging the multimodal similarities among co-interacted items, we apply a\nmodified version of the feature propagation technique to impute the missing\nmultimodal features. Adopted as a pre-processing stage for two recent\nmultimodal recommender systems, our simple approach performs better than other\nshallower solutions on three popular datasets.\n","authors":["Daniele Malitesta","Emanuele Rossi","Claudio Pomo","Fragkiskos D. Malliaros","Tommaso Di Noia"],"pdf_url":"https://arxiv.org/pdf/2403.19841v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.01818v3","updated":"2024-03-28T20:38:01Z","published":"2022-06-03T21:01:48Z","title":"QAGCN: Answering Multi-Relation Questions via Single-Step Implicit\n  Reasoning over Knowledge Graphs","summary":"  Multi-relation question answering (QA) is a challenging task, where given\nquestions usually require long reasoning chains in KGs that consist of multiple\nrelations. Recently, methods with explicit multi-step reasoning over KGs have\nbeen prominently used in this task and have demonstrated promising performance.\nExamples include methods that perform stepwise label propagation through KG\ntriples and methods that navigate over KG triples based on reinforcement\nlearning. A main weakness of these methods is that their reasoning mechanisms\nare usually complex and difficult to implement or train. In this paper, we\nargue that multi-relation QA can be achieved via end-to-end single-step\nimplicit reasoning, which is simpler, more efficient, and easier to adopt. We\npropose QAGCN -- a Question-Aware Graph Convolutional Network (GCN)-based\nmethod that includes a novel GCN architecture with controlled\nquestion-dependent message propagation for the implicit reasoning. Extensive\nexperiments have been conducted, where QAGCN achieved competitive and even\nsuperior performance compared to state-of-the-art explicit-reasoning methods.\nOur code and pre-trained models are available in the repository:\nhttps://github.com/ruijie-wang-uzh/QAGCN\n","authors":["Ruijie Wang","Luca Rossetto","Michael Cochez","Abraham Bernstein"],"pdf_url":"https://arxiv.org/pdf/2206.01818v3.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2403.19648v1","updated":"2024-03-28T17:56:56Z","published":"2024-03-28T17:56:56Z","title":"Human-compatible driving partners through data-regularized self-play\n  reinforcement learning","summary":"  A central challenge for autonomous vehicles is coordinating with humans.\nTherefore, incorporating realistic human agents is essential for scalable\ntraining and evaluation of autonomous driving systems in simulation. Simulation\nagents are typically developed by imitating large-scale, high-quality datasets\nof human driving. However, pure imitation learning agents empirically have high\ncollision rates when executed in a multi-agent closed-loop setting. To build\nagents that are realistic and effective in closed-loop settings, we propose\nHuman-Regularized PPO (HR-PPO), a multi-agent algorithm where agents are\ntrained through self-play with a small penalty for deviating from a human\nreference policy. In contrast to prior work, our approach is RL-first and only\nuses 30 minutes of imperfect human demonstrations. We evaluate agents in a\nlarge set of multi-agent traffic scenes. Results show our HR-PPO agents are\nhighly effective in achieving goals, with a success rate of 93%, an off-road\nrate of 3.5%, and a collision rate of 3%. At the same time, the agents drive in\na human-like manner, as measured by their similarity to existing human driving\nlogs. We also find that HR-PPO agents show considerable improvements on proxy\nmeasures for coordination with human driving, particularly in highly\ninteractive scenarios. We open-source our code and trained agents at\nhttps://github.com/Emerge-Lab/nocturne_lab and provide demonstrations of agent\nbehaviors at https://sites.google.com/view/driving-partners.\n","authors":["Daphne Cornelisse","Eugene Vinitsky"],"pdf_url":"https://arxiv.org/pdf/2403.19648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12031v2","updated":"2024-03-28T17:56:28Z","published":"2024-03-18T17:59:04Z","title":"RouterBench: A Benchmark for Multi-LLM Routing System","summary":"  As the range of applications for Large Language Models (LLMs) continues to\ngrow, the demand for effective serving solutions becomes increasingly critical.\nDespite the versatility of LLMs, no single model can optimally address all\ntasks and applications, particularly when balancing performance with cost. This\nlimitation has led to the development of LLM routing systems, which combine the\nstrengths of various models to overcome the constraints of individual LLMs.\nYet, the absence of a standardized benchmark for evaluating the performance of\nLLM routers hinders progress in this area. To bridge this gap, we present\nRouterBench, a novel evaluation framework designed to systematically assess the\nefficacy of LLM routing systems, along with a comprehensive dataset comprising\nover 405k inference outcomes from representative LLMs to support the\ndevelopment of routing strategies. We further propose a theoretical framework\nfor LLM routing, and deliver a comparative analysis of various routing\napproaches through RouterBench, highlighting their potentials and limitations\nwithin our evaluation framework. This work not only formalizes and advances the\ndevelopment of LLM routing systems but also sets a standard for their\nassessment, paving the way for more accessible and economically viable LLM\ndeployments. The code and data are available at\nhttps://github.com/withmartian/routerbench.\n","authors":["Qitian Jason Hu","Jacob Bieker","Xiuyu Li","Nan Jiang","Benjamin Keigwin","Gaurav Ranganath","Kurt Keutzer","Shriyash Kaustubh Upadhyay"],"pdf_url":"https://arxiv.org/pdf/2403.12031v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19647v1","updated":"2024-03-28T17:56:07Z","published":"2024-03-28T17:56:07Z","title":"Sparse Feature Circuits: Discovering and Editing Interpretable Causal\n  Graphs in Language Models","summary":"  We introduce methods for discovering and applying sparse feature circuits.\nThese are causally implicated subnetworks of human-interpretable features for\nexplaining language model behaviors. Circuits identified in prior work consist\nof polysemantic and difficult-to-interpret units like attention heads or\nneurons, rendering them unsuitable for many downstream applications. In\ncontrast, sparse feature circuits enable detailed understanding of\nunanticipated mechanisms. Because they are based on fine-grained units, sparse\nfeature circuits are useful for downstream tasks: We introduce SHIFT, where we\nimprove the generalization of a classifier by ablating features that a human\njudges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised\nand scalable interpretability pipeline by discovering thousands of sparse\nfeature circuits for automatically discovered model behaviors.\n","authors":["Samuel Marks","Can Rager","Eric J. Michaud","Yonatan Belinkov","David Bau","Aaron Mueller"],"pdf_url":"https://arxiv.org/pdf/2403.19647v1.pdf","comment":"Code and data at https://github.com/saprmarks/feature-circuits.\n  Demonstration at https://feature-circuits.xyz"},{"id":"http://arxiv.org/abs/2403.11687v2","updated":"2024-03-28T17:56:05Z","published":"2024-03-18T11:37:53Z","title":"Nonsmooth Implicit Differentiation: Deterministic and Stochastic\n  Convergence Rates","summary":"  We study the problem of efficiently computing the derivative of the\nfixed-point of a parametric nondifferentiable contraction map. This problem has\nwide applications in machine learning, including hyperparameter optimization,\nmeta-learning and data poisoning attacks. We analyze two popular approaches:\niterative differentiation (ITD) and approximate implicit differentiation (AID).\nA key challenge behind the nonsmooth setting is that the chain rule does not\nhold anymore. Building upon the recent work by Bolte et al. (2022), who proved\nlinear convergence of nondifferentiable ITD, we provide an improved linear rate\nfor ITD and a slightly better rate for AID, both in the deterministic case. We\nfurther introduce NSID, a new stochastic method to compute the implicit\nderivative when the fixed point is defined as the composition of an outer map\nand an inner map which is accessible only through a stochastic unbiased\nestimator. We establish rates for the convergence of NSID, encompassing the\nbest available rates in the smooth setting. We present illustrative experiments\nconfirming our analysis.\n","authors":["Riccardo Grazzi","Massimiliano Pontil","Saverio Salzo"],"pdf_url":"https://arxiv.org/pdf/2403.11687v2.pdf","comment":"Removed the assumption on the conservative derivative of the fixed\n  point map having a product structure: the product of partial conservative\n  derivatives is not conservative in general"},{"id":"http://arxiv.org/abs/2403.19631v1","updated":"2024-03-28T17:47:19Z","published":"2024-03-28T17:47:19Z","title":"Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in\n  Language Models","summary":"  Large Language Models (LLMs) have shown proficiency in question-answering\ntasks but often struggle to integrate real-time knowledge updates, leading to\npotentially outdated or inaccurate responses. This problem becomes even more\nchallenging when dealing with multi-hop questions since they require LLMs to\nupdate and integrate multiple knowledge pieces relevant to the questions. To\ntackle the problem, we propose the Retrieval-Augmented model Editing (RAE)\nframework tailored for multi-hop question answering. RAE first retrieves edited\nfacts and then refines the language model through in-context learning.\nSpecifically, our retrieval approach, based on mutual information maximization,\nleverages the reasoning abilities of LLMs to identify chain facts that na\\\"ive\nsimilarity-based searches might miss. Additionally, our framework incorporates\na pruning strategy to eliminate redundant information from the retrieved facts,\nwhich enhances the editing accuracy and mitigates the hallucination problem.\nOur framework is supported by theoretical justification for its fact retrieval\nefficacy. Finally, comprehensive evaluation across various LLMs validates RAE's\nability in providing accurate answers with updated knowledge.\n","authors":["Yucheng Shi","Qiaoyu Tan","Xuansheng Wu","Shaochen Zhong","Kaixiong Zhou","Ninghao Liu"],"pdf_url":"https://arxiv.org/pdf/2403.19631v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2403.19629v1","updated":"2024-03-28T17:46:25Z","published":"2024-03-28T17:46:25Z","title":"Metric Learning from Limited Pairwise Preference Comparisons","summary":"  We study metric learning from preference comparisons under the ideal point\nmodel, in which a user prefers an item over another if it is closer to their\nlatent ideal item. These items are embedded into $\\mathbb{R}^d$ equipped with\nan unknown Mahalanobis distance shared across users. While recent work shows\nthat it is possible to simultaneously recover the metric and ideal items given\n$\\mathcal{O}(d)$ pairwise comparisons per user, in practice we often have a\nlimited budget of $o(d)$ comparisons. We study whether the metric can still be\nrecovered, even though it is known that learning individual ideal items is now\nno longer possible. We show that in general, $o(d)$ comparisons reveals no\ninformation about the metric, even with infinitely many users. However, when\ncomparisons are made over items that exhibit low-dimensional structure, each\nuser can contribute to learning the metric restricted to a low-dimensional\nsubspace so that the metric can be jointly identified. We present a\ndivide-and-conquer approach that achieves this, and provide theoretical\nrecovery guarantees and empirical validation.\n","authors":["Zhi Wang","Geelon So","Ramya Korlakai Vinayak"],"pdf_url":"https://arxiv.org/pdf/2403.19629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19625v1","updated":"2024-03-28T17:45:03Z","published":"2024-03-28T17:45:03Z","title":"Top-$k$ Classification and Cardinality-Aware Prediction","summary":"  We present a detailed study of top-$k$ classification, the task of predicting\nthe $k$ most probable classes for an input, extending beyond single-class\nprediction. We demonstrate that several prevalent surrogate loss functions in\nmulti-class classification, such as comp-sum and constrained losses, are\nsupported by $H$-consistency bounds with respect to the top-$k$ loss. These\nbounds guarantee consistency in relation to the hypothesis set $H$, providing\nstronger guarantees than Bayes-consistency due to their non-asymptotic and\nhypothesis-set specific nature. To address the trade-off between accuracy and\ncardinality $k$, we further introduce cardinality-aware loss functions through\ninstance-dependent cost-sensitive learning. For these functions, we derive\ncost-sensitive comp-sum and constrained surrogate losses, establishing their\n$H$-consistency bounds and Bayes-consistency. Minimizing these losses leads to\nnew cardinality-aware algorithms for top-$k$ classification. We report the\nresults of extensive experiments on CIFAR-100, ImageNet, CIFAR-10, and SVHN\ndatasets demonstrating the effectiveness and benefit of these algorithms.\n","authors":["Anqi Mao","Mehryar Mohri","Yutao Zhong"],"pdf_url":"https://arxiv.org/pdf/2403.19625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19620v1","updated":"2024-03-28T17:40:15Z","published":"2024-03-28T17:40:15Z","title":"Collaborative Interactive Evolution of Art in the Latent Space of Deep\n  Generative Models","summary":"  Generative Adversarial Networks (GANs) have shown great success in generating\nhigh quality images and are thus used as one of the main approaches to generate\nart images. However, usually the image generation process involves sampling\nfrom the latent space of the learned art representations, allowing little\ncontrol over the output. In this work, we first employ GANs that are trained to\nproduce creative images using an architecture known as Creative Adversarial\nNetworks (CANs), then, we employ an evolutionary approach to navigate within\nthe latent space of the models to discover images. We use automatic aesthetic\nand collaborative interactive human evaluation metrics to assess the generated\nimages. In the human interactive evaluation case, we propose a collaborative\nevaluation based on the assessments of several participants. Furthermore, we\nalso experiment with an intelligent mutation operator that aims to improve the\nquality of the images through local search based on an aesthetic measure. We\nevaluate the effectiveness of this approach by comparing the results produced\nby the automatic and collaborative interactive evolution. The results show that\nthe proposed approach can generate highly attractive art images when the\nevolution is guided by collaborative human feedback.\n","authors":["Ole Hall","Anil Yaman"],"pdf_url":"https://arxiv.org/pdf/2403.19620v1.pdf","comment":"Preprint. The Version of Record of this contribution is to be\n  published in the proceedings of the 13th International Conference on\n  Artificial Intelligence in Music, Sound, Art and Design (EvoMUSART) 2024"},{"id":"http://arxiv.org/abs/2304.09224v2","updated":"2024-03-28T17:36:50Z","published":"2023-04-18T18:23:20Z","title":"Quantum machine learning for image classification","summary":"  Image classification, a pivotal task in multiple industries, faces\ncomputational challenges due to the burgeoning volume of visual data. This\nresearch addresses these challenges by introducing two quantum machine learning\nmodels that leverage the principles of quantum mechanics for effective\ncomputations. Our first model, a hybrid quantum neural network with parallel\nquantum circuits, enables the execution of computations even in the noisy\nintermediate-scale quantum era, where circuits with a large number of qubits\nare currently infeasible. This model demonstrated a record-breaking\nclassification accuracy of 99.21% on the full MNIST dataset, surpassing the\nperformance of known quantum-classical models, while having eight times fewer\nparameters than its classical counterpart. Also, the results of testing this\nhybrid model on a Medical MNIST (classification accuracy over 99%), and on\nCIFAR-10 (classification accuracy over 82%), can serve as evidence of the\ngeneralizability of the model and highlights the efficiency of quantum layers\nin distinguishing common features of input data. Our second model introduces a\nhybrid quantum neural network with a Quanvolutional layer, reducing image\nresolution via a convolution process. The model matches the performance of its\nclassical counterpart, having four times fewer trainable parameters, and\noutperforms a classical model with equal weight parameters. These models\nrepresent advancements in quantum machine learning research and illuminate the\npath towards more accurate image classification systems.\n","authors":["Arsenii Senokosov","Alexandr Sedykh","Asel Sagingalieva","Basil Kyriacou","Alexey Melnikov"],"pdf_url":"https://arxiv.org/pdf/2304.09224v2.pdf","comment":"13 pages, 10 figures, 1 table"},{"id":"http://arxiv.org/abs/2403.19612v1","updated":"2024-03-28T17:32:01Z","published":"2024-03-28T17:32:01Z","title":"ILPO-NET: Network for the invariant recognition of arbitrary volumetric\n  patterns in 3D","summary":"  Effective recognition of spatial patterns and learning their hierarchy is\ncrucial in modern spatial data analysis. Volumetric data applications seek\ntechniques ensuring invariance not only to shifts but also to pattern\nrotations. While traditional methods can readily achieve translational\ninvariance, rotational invariance possesses multiple challenges and remains an\nactive area of research. Here, we present ILPO-Net (Invariant to Local Patterns\nOrientation Network), a novel approach that handles arbitrarily shaped patterns\nwith the convolutional operation inherently invariant to local spatial pattern\norientations using the Wigner matrix expansions. Our architecture seamlessly\nintegrates the new convolution operator and, when benchmarked on diverse\nvolumetric datasets such as MedMNIST and CATH, demonstrates superior\nperformance over the baselines with significantly reduced parameter counts - up\nto 1000 times fewer in the case of MedMNIST. Beyond these demonstrations,\nILPO-Net's rotational invariance paves the way for other applications across\nmultiple disciplines. Our code is publicly available at\nhttps://gricad-gitlab.univ-grenoble-alpes.fr/GruLab/ILPONet.\n","authors":["Dmitrii Zhemchuzhnikov","Sergei Grudinin"],"pdf_url":"https://arxiv.org/pdf/2403.19612v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19605v1","updated":"2024-03-28T17:28:06Z","published":"2024-03-28T17:28:06Z","title":"Data-Adaptive Tradeoffs among Multiple Risks in Distribution-Free\n  Prediction","summary":"  Decision-making pipelines are generally characterized by tradeoffs among\nvarious risk functions. It is often desirable to manage such tradeoffs in a\ndata-adaptive manner. As we demonstrate, if this is done naively, state-of-the\nart uncertainty quantification methods can lead to significant violations of\nputative risk guarantees.\n  To address this issue, we develop methods that permit valid control of risk\nwhen threshold and tradeoff parameters are chosen adaptively. Our methodology\nsupports monotone and nearly-monotone risks, but otherwise makes no\ndistributional assumptions.\n  To illustrate the benefits of our approach, we carry out numerical\nexperiments on synthetic data and the large-scale vision dataset MS-COCO.\n","authors":["Drew T. Nguyen","Reese Pathak","Anastasios N. Angelopoulos","Stephen Bates","Michael I. Jordan"],"pdf_url":"https://arxiv.org/pdf/2403.19605v1.pdf","comment":"27 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.19591v1","updated":"2024-03-28T17:13:47Z","published":"2024-03-28T17:13:47Z","title":"Genetic Quantization-Aware Approximation for Non-Linear Operations in\n  Transformers","summary":"  Non-linear functions are prevalent in Transformers and their lightweight\nvariants, incurring substantial and frequently underestimated hardware costs.\nPrevious state-of-the-art works optimize these operations by piece-wise linear\napproximation and store the parameters in look-up tables (LUT), but most of\nthem require unfriendly high-precision arithmetics such as FP/INT 32 and lack\nconsideration of integer-only INT quantization. This paper proposed a genetic\nLUT-Approximation algorithm namely GQA-LUT that can automatically determine the\nparameters with quantization awareness. The results demonstrate that GQA-LUT\nachieves negligible degradation on the challenging semantic segmentation task\nfor both vanilla and linear Transformer models. Besides, proposed GQA-LUT\nenables the employment of INT8-based LUT-Approximation that achieves an area\nsavings of 81.3~81.7% and a power reduction of 79.3~80.2% compared to the\nhigh-precision FP/INT 32 alternatives. Code is available at https://\ngithub.com/PingchengDong/GQA-LUT.\n","authors":["Pingcheng Dong","Yonghao Tan","Dong Zhang","Tianwei Ni","Xuejiao Liu","Yu Liu","Peng Luo","Luhong Liang","Shih-Yang Liu","Xijie Huang","Huaiyu Zhu","Yun Pan","Fengwei An","Kwang-Ting Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.19591v1.pdf","comment":"61st ACM/IEEE Design Automation Conference (DAC) 2024"},{"id":"http://arxiv.org/abs/2403.19588v1","updated":"2024-03-28T17:12:39Z","published":"2024-03-28T17:12:39Z","title":"DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs","summary":"  This paper revives Densely Connected Convolutional Networks (DenseNets) and\nreveals the underrated effectiveness over predominant ResNet-style\narchitectures. We believe DenseNets' potential was overlooked due to untouched\ntraining methods and traditional design elements not fully revealing their\ncapabilities. Our pilot study shows dense connections through concatenation are\nstrong, demonstrating that DenseNets can be revitalized to compete with modern\narchitectures. We methodically refine suboptimal components - architectural\nadjustments, block redesign, and improved training recipes towards widening\nDenseNets and boosting memory efficiency while keeping concatenation shortcuts.\nOur models, employing simple architectural elements, ultimately surpass Swin\nTransformer, ConvNeXt, and DeiT-III - key architectures in the residual\nlearning lineage. Furthermore, our models exhibit near state-of-the-art\nperformance on ImageNet-1K, competing with the very recent models and\ndownstream tasks, ADE20k semantic segmentation, and COCO object\ndetection/instance segmentation. Finally, we provide empirical analyses that\nuncover the merits of the concatenation over additive shortcuts, steering a\nrenewed preference towards DenseNet-style designs. Our code is available at\nhttps://github.com/naver-ai/rdnet.\n","authors":["Donghyun Kim","Byeongho Heo","Dongyoon Han"],"pdf_url":"https://arxiv.org/pdf/2403.19588v1.pdf","comment":"Code at https://github.com/naver-ai/rdnet"},{"id":"http://arxiv.org/abs/2403.18028v2","updated":"2024-03-28T17:06:15Z","published":"2024-03-26T18:29:39Z","title":"Predicting Species Occurrence Patterns from Partial Observations","summary":"  To address the interlinked biodiversity and climate crises, we need an\nunderstanding of where species occur and how these patterns are changing.\nHowever, observational data on most species remains very limited, and the\namount of data available varies greatly between taxonomic groups. We introduce\nthe problem of predicting species occurrence patterns given (a) satellite\nimagery, and (b) known information on the occurrence of other species. To\nevaluate algorithms on this task, we introduce SatButterfly, a dataset of\nsatellite images, environmental data and observational data for butterflies,\nwhich is designed to pair with the existing SatBird dataset of bird\nobservational data. To address this task, we propose a general model, R-Tran,\nfor predicting species occurrence patterns that enables the use of partial\nobservational data wherever found. We find that R-Tran outperforms other\nmethods in predicting species encounter rates with partial information both\nwithin a taxon (birds) and across taxa (birds and butterflies). Our approach\nopens new perspectives to leveraging insights from species with abundant data\nto other species with scarce data, by modelling the ecosystems in which they\nco-occur.\n","authors":["Hager Radi Abdelwahed","Mélisande Teng","David Rolnick"],"pdf_url":"https://arxiv.org/pdf/2403.18028v2.pdf","comment":"Tackling Climate Change with Machine Learning workshop at ICLR 2024"},{"id":"http://arxiv.org/abs/2403.19578v1","updated":"2024-03-28T17:04:00Z","published":"2024-03-28T17:04:00Z","title":"Keypoint Action Tokens Enable In-Context Imitation Learning in Robotics","summary":"  We show that off-the-shelf text-based Transformers, with no additional\ntraining, can perform few-shot in-context visual imitation learning, mapping\nvisual observations to action sequences that emulate the demonstrator's\nbehaviour. We achieve this by transforming visual observations (inputs) and\ntrajectories of actions (outputs) into sequences of tokens that a\ntext-pretrained Transformer (GPT-4 Turbo) can ingest and generate, via a\nframework we call Keypoint Action Tokens (KAT). Despite being trained only on\nlanguage, we show that these Transformers excel at translating tokenised visual\nkeypoint observations into action trajectories, performing on par or better\nthan state-of-the-art imitation learning (diffusion policies) in the low-data\nregime on a suite of real-world, everyday tasks. Rather than operating in the\nlanguage domain as is typical, KAT leverages text-based Transformers to operate\nin the vision and action domains to learn general patterns in demonstration\ndata for highly efficient imitation learning, indicating promising new avenues\nfor repurposing natural language models for embodied tasks. Videos are\navailable at https://www.robot-learning.uk/keypoint-action-tokens.\n","authors":["Norman Di Palo","Edward Johns"],"pdf_url":"https://arxiv.org/pdf/2403.19578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08737v2","updated":"2024-03-28T16:59:24Z","published":"2023-03-15T16:21:50Z","title":"Evaluating gesture generation in a large-scale open challenge: The GENEA\n  Challenge 2022","summary":"  This paper reports on the second GENEA Challenge to benchmark data-driven\nautomatic co-speech gesture generation. Participating teams used the same\nspeech and motion dataset to build gesture-generation systems. Motion generated\nby all these systems was rendered to video using a standardised visualisation\npipeline and evaluated in several large, crowdsourced user studies. Unlike when\ncomparing different research papers, differences in results are here only due\nto differences between methods, enabling direct comparison between systems. The\ndataset was based on 18 hours of full-body motion capture, including fingers,\nof different persons engaging in a dyadic conversation. Ten teams participated\nin the challenge across two tiers: full-body and upper-body gesticulation. For\neach tier, we evaluated both the human-likeness of the gesture motion and its\nappropriateness for the specific speech signal. Our evaluations decouple\nhuman-likeness from gesture appropriateness, which has been a difficult problem\nin the field.\n  The evaluation results show some synthetic gesture conditions being rated as\nsignificantly more human-like than 3D human motion capture. To the best of our\nknowledge, this has not been demonstrated before. On the other hand, all\nsynthetic motion is found to be vastly less appropriate for the speech than the\noriginal motion-capture recordings. We also find that conventional objective\nmetrics do not correlate well with subjective human-likeness ratings in this\nlarge evaluation. The one exception is the Fr\\'echet gesture distance (FGD),\nwhich achieves a Kendall's tau rank correlation of around $-0.5$. Based on the\nchallenge results we formulate numerous recommendations for system building and\nevaluation.\n","authors":["Taras Kucherenko","Pieter Wolfert","Youngwoo Yoon","Carla Viegas","Teodor Nikolov","Mihail Tsakov","Gustav Eje Henter"],"pdf_url":"https://arxiv.org/pdf/2303.08737v2.pdf","comment":"The first three authors made equal contributions and share joint\n  first authorship. Accepted for publication in the ACM Transactions on\n  Graphics (TOG).Please see https://youngwoo-yoon.github.io/GENEAchallenge2022/\n  for all challenge materials. arXiv admin note: text overlap with\n  arXiv:2208.10441"},{"id":"http://arxiv.org/abs/2402.19212v4","updated":"2024-03-28T16:59:03Z","published":"2024-02-29T14:41:31Z","title":"Deep Reinforcement Learning: A Convex Optimization Approach","summary":"  In this paper, we consider reinforcement learning of nonlinear systems with\ncontinuous state and action spaces. We present an episodic learning algorithm,\nwhere we for each episode use convex optimization to find a two-layer neural\nnetwork approximation of the optimal $Q$-function. The convex optimization\napproach guarantees that the weights calculated at each episode are optimal,\nwith respect to the given sampled states and actions of the current episode.\nFor stable nonlinear systems, we show that the algorithm converges and that the\nconverging parameters of the trained neural network can be made arbitrarily\nclose to the optimal neural network parameters. In particular, if the\nregularization parameter is $\\rho$ and the time horizon is $T$, then the\nparameters of the trained neural network converge to $w$, where the distance\nbetween $w$ from the optimal parameters $w^\\star$ is bounded by\n$\\mathcal{O}(\\rho T^{-1})$. That is, when the number of episodes goes to\ninfinity, there exists a constant $C$ such that \\[\\|w-w^\\star\\| \\le\nC\\cdot\\frac{\\rho}{T}.\\] In particular, our algorithm converges arbitrarily\nclose to the optimal neural network parameters as the time horizon increases or\nas the regularization parameter decreases.\n","authors":["Ather Gattami"],"pdf_url":"https://arxiv.org/pdf/2402.19212v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19572v1","updated":"2024-03-28T16:56:39Z","published":"2024-03-28T16:56:39Z","title":"Swarm Characteristics Classification Using Neural Networks","summary":"  Understanding the characteristics of swarming autonomous agents is critical\nfor defense and security applications. This article presents a study on using\nsupervised neural network time series classification (NN TSC) to predict key\nattributes and tactics of swarming autonomous agents for military contexts.\nSpecifically, NN TSC is applied to infer two binary attributes - communication\nand proportional navigation - which combine to define four mutually exclusive\nswarm tactics. We identify a gap in literature on using NNs for swarm\nclassification and demonstrate the effectiveness of NN TSC in rapidly deducing\nintelligence about attacking swarms to inform counter-maneuvers. Through\nsimulated swarm-vs-swarm engagements, we evaluate NN TSC performance in terms\nof observation window requirements, noise robustness, and scalability to swarm\nsize. Key findings show NNs can predict swarm behaviors with 97% accuracy using\nshort observation windows of 20 time steps, while also demonstrating graceful\ndegradation down to 80% accuracy under 50% noise, as well as excellent\nscalability to swarm sizes from 10 to 100 agents. These capabilities are\npromising for real-time decision-making support in defense scenarios by rapidly\ninferring insights about swarm behavior.\n","authors":["Donald W. Peltier III","Isaac Kaminer","Abram Clark","Marko Orescanin"],"pdf_url":"https://arxiv.org/pdf/2403.19572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.15865v5","updated":"2024-03-28T16:56:06Z","published":"2023-06-28T01:41:30Z","title":"Differentially Private Distributed Estimation and Learning","summary":"  We study distributed estimation and learning problems in a networked\nenvironment where agents exchange information to estimate unknown statistical\nproperties of random variables from their privately observed samples. The\nagents can collectively estimate the unknown quantities by exchanging\ninformation about their private observations, but they also face privacy risks.\nOur novel algorithms extend the existing distributed estimation literature and\nenable the participating agents to estimate a complete sufficient statistic\nfrom private signals acquired offline or online over time and to preserve the\nprivacy of their signals and network neighborhoods. This is achieved through\nlinear aggregation schemes with adjusted randomization schemes that add noise\nto the exchanged estimates subject to differential privacy (DP) constraints,\nboth in an offline and online manner. We provide convergence rate analysis and\ntight finite-time convergence bounds. We show that the noise that minimizes the\nconvergence time to the best estimates is the Laplace noise, with parameters\ncorresponding to each agent's sensitivity to their signal and network\ncharacteristics. Our algorithms are amenable to dynamic topologies and\nbalancing privacy and accuracy trade-offs. Finally, to supplement and validate\nour theoretical results, we run experiments on real-world data from the US\nPower Grid Network and electric consumption data from German Households to\nestimate the average power consumption of power stations and households under\nall privacy regimes and show that our method outperforms existing first-order,\nprivacy-aware, distributed optimization methods.\n","authors":["Marios Papachristou","M. Amin Rahimian"],"pdf_url":"https://arxiv.org/pdf/2306.15865v5.pdf","comment":"Accepted for publication at IISE Transactions (Special issue on\n  Federated, Distributed Learning and Analytics)"},{"id":"http://arxiv.org/abs/2403.19570v1","updated":"2024-03-28T16:52:47Z","published":"2024-03-28T16:52:47Z","title":"GrINd: Grid Interpolation Network for Scattered Observations","summary":"  Predicting the evolution of spatiotemporal physical systems from sparse and\nscattered observational data poses a significant challenge in various\nscientific domains. Traditional methods rely on dense grid-structured data,\nlimiting their applicability in scenarios with sparse observations. To address\nthis challenge, we introduce GrINd (Grid Interpolation Network for Scattered\nObservations), a novel network architecture that leverages the high-performance\nof grid-based models by mapping scattered observations onto a high-resolution\ngrid using a Fourier Interpolation Layer. In the high-resolution space, a\nNeuralPDE-class model predicts the system's state at future timepoints using\ndifferentiable ODE solvers and fully convolutional neural networks\nparametrizing the system's dynamics. We empirically evaluate GrINd on the\nDynaBench benchmark dataset, comprising six different physical systems observed\nat scattered locations, demonstrating its state-of-the-art performance compared\nto existing models. GrINd offers a promising approach for forecasting physical\nsystems from sparse, scattered observational data, extending the applicability\nof deep learning methods to real-world scenarios with limited data\navailability.\n","authors":["Andrzej Dulny","Paul Heinisch","Andreas Hotho","Anna Krause"],"pdf_url":"https://arxiv.org/pdf/2403.19570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11598v3","updated":"2024-03-28T16:49:40Z","published":"2023-12-18T18:16:52Z","title":"SkillDiffuser: Interpretable Hierarchical Planning via Skill\n  Abstractions in Diffusion-Based Task Execution","summary":"  Diffusion models have demonstrated strong potential for robotic trajectory\nplanning. However, generating coherent trajectories from high-level\ninstructions remains challenging, especially for long-range composition tasks\nrequiring multiple sequential skills. We propose SkillDiffuser, an end-to-end\nhierarchical planning framework integrating interpretable skill learning with\nconditional diffusion planning to address this problem. At the higher level,\nthe skill abstraction module learns discrete, human-understandable skill\nrepresentations from visual observations and language instructions. These\nlearned skill embeddings are then used to condition the diffusion model to\ngenerate customized latent trajectories aligned with the skills. This allows\ngenerating diverse state trajectories that adhere to the learnable skills. By\nintegrating skill learning with conditional trajectory generation,\nSkillDiffuser produces coherent behavior following abstract instructions across\ndiverse tasks. Experiments on multi-task robotic manipulation benchmarks like\nMeta-World and LOReL demonstrate state-of-the-art performance and\nhuman-interpretable skill representations from SkillDiffuser. More\nvisualization results and information could be found on our website.\n","authors":["Zhixuan Liang","Yao Mu","Hengbo Ma","Masayoshi Tomizuka","Mingyu Ding","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2312.11598v3.pdf","comment":"Accepted by CVPR 2024. Camera ready version. Project page:\n  https://skilldiffuser.github.io/"},{"id":"http://arxiv.org/abs/2403.19561v1","updated":"2024-03-28T16:46:53Z","published":"2024-03-28T16:46:53Z","title":"Self-Improved Learning for Scalable Neural Combinatorial Optimization","summary":"  The end-to-end neural combinatorial optimization (NCO) method shows promising\nperformance in solving complex combinatorial optimization problems without the\nneed for expert design. However, existing methods struggle with large-scale\nproblems, hindering their practical applicability. To overcome this limitation,\nthis work proposes a novel Self-Improved Learning (SIL) method for better\nscalability of neural combinatorial optimization. Specifically, we develop an\nefficient self-improved mechanism that enables direct model training on\nlarge-scale problem instances without any labeled data. Powered by an\ninnovative local reconstruction approach, this method can iteratively generate\nbetter solutions by itself as pseudo-labels to guide efficient model training.\nIn addition, we design a linear complexity attention mechanism for the model to\nefficiently handle large-scale combinatorial problem instances with low\ncomputation overhead. Comprehensive experiments on the Travelling Salesman\nProblem (TSP) and the Capacitated Vehicle Routing Problem (CVRP) with up to\n100K nodes in both uniform and real-world distributions demonstrate the\nsuperior scalability of our method.\n","authors":["Fu Luo","Xi Lin","Zhenkun Wang","Tong Xialiang","Mingxuan Yuan","Qingfu Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.19561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14694v2","updated":"2024-03-28T16:37:06Z","published":"2023-03-26T11:43:29Z","title":"A stability theorem for bigraded persistence barcodes","summary":"  We define bigraded persistent homology modules and bigraded barcodes of a\nfinite pseudo-metric space X using the ordinary and double homology of the\nmoment-angle complex associated with the Vietoris-Rips filtration of X. We\nprove a stability theorem for the bigraded persistent double homology modules\nand barcodes.\n","authors":["Anthony Bahri","Ivan Limonchenko","Taras Panov","Jongbaek Song","Donald Stanley"],"pdf_url":"https://arxiv.org/pdf/2303.14694v2.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2310.10500v2","updated":"2024-03-28T16:30:07Z","published":"2023-10-16T15:20:12Z","title":"Few-Shot Learning Patterns in Financial Time-Series for Trend-Following\n  Strategies","summary":"  Forecasting models for systematic trading strategies do not adapt quickly\nwhen financial market conditions rapidly change, as was seen in the advent of\nthe COVID-19 pandemic in 2020, causing many forecasting models to take\nloss-making positions. To deal with such situations, we propose a novel\ntime-series trend-following forecaster that can quickly adapt to new market\nconditions, referred to as regimes. We leverage recent developments from the\ndeep learning community and use few-shot learning. We propose the Cross\nAttentive Time-Series Trend Network -- X-Trend -- which takes positions\nattending over a context set of financial time-series regimes. X-Trend\ntransfers trends from similar patterns in the context set to make forecasts,\nthen subsequently takes positions for a new distinct target regime. By quickly\nadapting to new financial regimes, X-Trend increases Sharpe ratio by 18.9% over\na neural forecaster and 10-fold over a conventional Time-series Momentum\nstrategy during the turbulent market period from 2018 to 2023. Our strategy\nrecovers twice as quickly from the COVID-19 drawdown compared to the\nneural-forecaster. X-Trend can also take zero-shot positions on novel unseen\nfinancial assets obtaining a 5-fold Sharpe ratio increase versus a neural\ntime-series trend forecaster over the same period. Furthermore, the\ncross-attention mechanism allows us to interpret the relationship between\nforecasts and patterns in the context set.\n","authors":["Kieran Wood","Samuel Kessler","Stephen J. Roberts","Stefan Zohren"],"pdf_url":"https://arxiv.org/pdf/2310.10500v2.pdf","comment":"minor edits"},{"id":"http://arxiv.org/abs/2403.19546v1","updated":"2024-03-28T16:27:26Z","published":"2024-03-28T16:27:26Z","title":"Croissant: A Metadata Format for ML-Ready Datasets","summary":"  Data is a critical resource for Machine Learning (ML), yet working with data\nremains a key friction point. This paper introduces Croissant, a metadata\nformat for datasets that simplifies how data is used by ML tools and\nframeworks. Croissant makes datasets more discoverable, portable and\ninteroperable, thereby addressing significant challenges in ML data management\nand responsible AI. Croissant is already supported by several popular dataset\nrepositories, spanning hundreds of thousands of datasets, ready to be loaded\ninto the most popular ML frameworks.\n","authors":["Mubashara Akhtar","Omar Benjelloun","Costanza Conforti","Joan Giner-Miguelez","Nitisha Jain","Michael Kuchnik","Quentin Lhoest","Pierre Marcenac","Manil Maskey","Peter Mattson","Luis Oala","Pierre Ruyssen","Rajat Shinde","Elena Simperl","Goeffry Thomas","Slava Tykhonov","Joaquin Vanschoren","Steffen Vogler","Carole-Jean Wu"],"pdf_url":"https://arxiv.org/pdf/2403.19546v1.pdf","comment":"Preprint. Contributors listed in alphabetical order"},{"id":"http://arxiv.org/abs/2204.08989v3","updated":"2024-03-28T16:17:43Z","published":"2022-04-13T21:20:42Z","title":"Efficient Deep Learning-based Estimation of the Vital Signs on\n  Smartphones","summary":"  With the increasing use of smartphones in our daily lives, these devices have\nbecome capable of performing many complex tasks. Concerning the need for\ncontinuous monitoring of vital signs, especially for the elderly or those with\ncertain types of diseases, the development of algorithms that can estimate\nvital signs using smartphones has attracted researchers worldwide. In\nparticular, researchers have been exploring ways to estimate vital signs, such\nas heart rate, oxygen saturation levels, and respiratory rate, using algorithms\nthat can be run on smartphones. However, many of these algorithms require\nmultiple pre-processing steps that might introduce some implementation\noverheads or require the design of a couple of hand-crafted stages to obtain an\noptimal result. To address this issue, this research proposes a novel\nend-to-end solution to mobile-based vital sign estimation using deep learning\nthat eliminates the need for pre-processing. By using a fully convolutional\narchitecture, the proposed model has much fewer parameters and less\ncomputational complexity compared to the architectures that use fully-connected\nlayers as the prediction heads. This also reduces the risk of overfitting.\nAdditionally, a public dataset for vital sign estimation, which includes 62\nvideos collected from 35 men and 27 women, is provided. Overall, the proposed\nend-to-end approach promises significantly improved efficiency and performance\nfor on-device health monitoring on readily available consumer electronics.\n","authors":["Taha Samavati","Mahdi Farvardin","Aboozar Ghaffari"],"pdf_url":"https://arxiv.org/pdf/2204.08989v3.pdf","comment":"10 pages, 8 figures, 11 tables"},{"id":"http://arxiv.org/abs/2301.13375v2","updated":"2024-03-28T16:08:43Z","published":"2023-01-31T02:39:52Z","title":"Optimal Transport Perturbations for Safe Reinforcement Learning with\n  Robustness Guarantees","summary":"  Robustness and safety are critical for the trustworthy deployment of deep\nreinforcement learning. Real-world decision making applications require\nalgorithms that can guarantee robust performance and safety in the presence of\ngeneral environment disturbances, while making limited assumptions on the data\ncollection process during training. In order to accomplish this goal, we\nintroduce a safe reinforcement learning framework that incorporates robustness\nthrough the use of an optimal transport cost uncertainty set. We provide an\nefficient implementation based on applying Optimal Transport Perturbations to\nconstruct worst-case virtual state transitions, which does not impact data\ncollection during training and does not require detailed simulator access. In\nexperiments on continuous control tasks with safety constraints, our approach\ndemonstrates robust performance while significantly improving safety at\ndeployment time compared to standard safe reinforcement learning.\n","authors":["James Queeney","Erhan Can Ozcan","Ioannis Ch. Paschalidis","Christos G. Cassandras"],"pdf_url":"https://arxiv.org/pdf/2301.13375v2.pdf","comment":"Transactions on Machine Learning Research (TMLR), 2024"},{"id":"http://arxiv.org/abs/2403.19530v1","updated":"2024-03-28T16:06:06Z","published":"2024-03-28T16:06:06Z","title":"Detecting Financial Bots on the Ethereum Blockchain","summary":"  The integration of bots in Distributed Ledger Technologies (DLTs) fosters\nefficiency and automation. However, their use is also associated with predatory\ntrading and market manipulation, and can pose threats to system integrity. It\nis therefore essential to understand the extent of bot deployment in DLTs;\ndespite this, current detection systems are predominantly rule-based and lack\nflexibility. In this study, we present a novel approach that utilizes machine\nlearning for the detection of financial bots on the Ethereum platform. First,\nwe systematize existing scientific literature and collect anecdotal evidence to\nestablish a taxonomy for financial bots, comprising 7 categories and 24\nsubcategories. Next, we create a ground-truth dataset consisting of 133 human\nand 137 bot addresses. Third, we employ both unsupervised and supervised\nmachine learning algorithms to detect bots deployed on Ethereum. The\nhighest-performing clustering algorithm is a Gaussian Mixture Model with an\naverage cluster purity of 82.6%, while the highest-performing model for binary\nclassification is a Random Forest with an accuracy of 83%. Our machine\nlearning-based detection mechanism contributes to understanding the Ethereum\necosystem dynamics by providing additional insights into the current bot\nlandscape.\n","authors":["Thomas Niedermayer","Pietro Saggese","Bernhard Haslhofer"],"pdf_url":"https://arxiv.org/pdf/2403.19530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19522v1","updated":"2024-03-28T15:57:20Z","published":"2024-03-28T15:57:20Z","title":"Model Stock: All we need is just a few fine-tuned models","summary":"  This paper introduces an efficient fine-tuning method for large pre-trained\nmodels, offering strong in-distribution (ID) and out-of-distribution (OOD)\nperformance. Breaking away from traditional practices that need a multitude of\nfine-tuned models for averaging, our approach employs significantly fewer\nmodels to achieve final weights yet yield superior accuracy. Drawing from key\ninsights in the weight space of fine-tuned weights, we uncover a strong link\nbetween the performance and proximity to the center of weight space. Based on\nthis, we introduce a method that approximates a center-close weight using only\ntwo fine-tuned models, applicable during or after training. Our innovative\nlayer-wise weight averaging technique surpasses state-of-the-art model methods\nsuch as Model Soup, utilizing only two fine-tuned models. This strategy can be\naptly coined Model Stock, highlighting its reliance on selecting a minimal\nnumber of models to draw a more optimized-averaged model. We demonstrate the\nefficacy of Model Stock with fine-tuned models based upon pre-trained CLIP\narchitectures, achieving remarkable performance on both ID and OOD tasks on the\nstandard benchmarks, all while barely bringing extra computational demands. Our\ncode and pre-trained models are available at\nhttps://github.com/naver-ai/model-stock.\n","authors":["Dong-Hwan Jang","Sangdoo Yun","Dongyoon Han"],"pdf_url":"https://arxiv.org/pdf/2403.19522v1.pdf","comment":"Code at https://github.com/naver-ai/model-stock"},{"id":"http://arxiv.org/abs/2401.01286v4","updated":"2024-03-28T15:56:55Z","published":"2024-01-02T16:54:58Z","title":"A Comprehensive Study of Knowledge Editing for Large Language Models","summary":"  Large Language Models (LLMs) have shown extraordinary capabilities in\nunderstanding and generating text that closely mirrors human communication.\nHowever, a primary limitation lies in the significant computational demands\nduring training, arising from their extensive parameterization. This challenge\nis further intensified by the dynamic nature of the world, necessitating\nfrequent updates to LLMs to correct outdated information or integrate new\nknowledge, thereby ensuring their continued relevance. Note that many\napplications demand continual model adjustments post-training to address\ndeficiencies or undesirable behaviors. There is an increasing interest in\nefficient, lightweight methods for on-the-fly model modifications. To this end,\nrecent years have seen a burgeoning in the techniques of knowledge editing for\nLLMs, which aim to efficiently modify LLMs' behaviors within specific domains\nwhile preserving overall performance across various inputs. In this paper, we\nfirst define the knowledge editing problem and then provide a comprehensive\nreview of cutting-edge approaches. Drawing inspiration from educational and\ncognitive research theories, we propose a unified categorization criterion that\nclassifies knowledge editing methods into three groups: resorting to external\nknowledge, merging knowledge into the model, and editing intrinsic knowledge.\nFurthermore, we introduce a new benchmark, KnowEdit, for a comprehensive\nempirical evaluation of representative knowledge editing approaches.\nAdditionally, we provide an in-depth analysis of knowledge location, which can\ngive a deeper understanding of the knowledge structures inherent within LLMs.\nFinally, we discuss several potential applications of knowledge editing,\noutlining its broad and impactful implications.\n","authors":["Ningyu Zhang","Yunzhi Yao","Bozhong Tian","Peng Wang","Shumin Deng","Mengru Wang","Zekun Xi","Shengyu Mao","Jintian Zhang","Yuansheng Ni","Siyuan Cheng","Ziwen Xu","Xin Xu","Jia-Chen Gu","Yong Jiang","Pengjun Xie","Fei Huang","Lei Liang","Zhiqiang Zhang","Xiaowei Zhu","Jun Zhou","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2401.01286v4.pdf","comment":"Ongoing work; 52 pages, 282 citations; benchmark is available at\n  https://huggingface.co/datasets/zjunlp/KnowEdit code is available at\n  https://github.com/zjunlp/EasyEdit paper list is available at\n  https://github.com/zjunlp/KnowledgeEditingPapers"},{"id":"http://arxiv.org/abs/2403.19521v1","updated":"2024-03-28T15:54:59Z","published":"2024-03-28T15:54:59Z","title":"Interpreting Key Mechanisms of Factual Recall in Transformer-Based\n  Language Models","summary":"  In this paper, we deeply explore the mechanisms employed by Transformer-based\nlanguage models in factual recall tasks. In zero-shot scenarios, given a prompt\nlike \"The capital of France is,\" task-specific attention heads extract the\ntopic entity, such as \"France,\" from the context and pass it to subsequent MLPs\nto recall the required answer such as \"Paris.\" We introduce a novel analysis\nmethod aimed at decomposing the outputs of the MLP into components\nunderstandable by humans. Through this method, we quantify the function of the\nMLP layer following these task-specific heads. In the residual stream, it\neither erases or amplifies the information originating from individual heads.\nMoreover, it generates a component that redirects the residual stream towards\nthe direction of its expected answer. These zero-shot mechanisms are also\nemployed in few-shot scenarios. Additionally, we observed a widely existent\nanti-overconfidence mechanism in the final layer of models, which suppresses\ncorrect predictions. We mitigate this suppression by leveraging our\ninterpretation to improve factual recall performance. Our interpretations have\nbeen evaluated across various language models, from the GPT-2 families to 1.3B\nOPT, and across tasks covering different domains of factual knowledge.\n","authors":["Ang Lv","Kaiyi Zhang","Yuhan Chen","Yulong Wang","Lifeng Liu","Ji-Rong Wen","Jian Xie","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2403.19521v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.10226v2","updated":"2024-03-28T15:53:48Z","published":"2023-08-20T10:43:50Z","title":"Machine Learning-Powered Combinatorial Clock Auction","summary":"  We study the design of iterative combinatorial auctions (ICAs). The main\nchallenge in this domain is that the bundle space grows exponentially in the\nnumber of items. To address this, several papers have recently proposed machine\nlearning (ML)-based preference elicitation algorithms that aim to elicit only\nthe most important information from bidders. However, from a practical point of\nview, the main shortcoming of this prior work is that those designs elicit\nbidders' preferences via value queries (i.e., ``What is your value for the\nbundle $\\{A,B\\}$?''). In most real-world ICA domains, value queries are\nconsidered impractical, since they impose an unrealistically high cognitive\nburden on bidders, which is why they are not used in practice. In this paper,\nwe address this shortcoming by designing an ML-powered combinatorial clock\nauction that elicits information from the bidders only via demand queries\n(i.e., ``At prices $p$, what is your most preferred bundle of items?''). We\nmake two key technical contributions: First, we present a novel method for\ntraining an ML model on demand queries. Second, based on those trained ML\nmodels, we introduce an efficient method for determining the demand query with\nthe highest clearing potential, for which we also provide a theoretical\nfoundation. We experimentally evaluate our ML-based demand query mechanism in\nseveral spectrum auction domains and compare it against the most established\nreal-world ICA: the combinatorial clock auction (CCA). Our mechanism\nsignificantly outperforms the CCA in terms of efficiency in all domains, it\nachieves higher efficiency in a significantly reduced number of rounds, and,\nusing linear prices, it exhibits vastly higher clearing potential. Thus, with\nthis paper we bridge the gap between research and practice and propose the\nfirst practical ML-powered ICA.\n","authors":["Ermis Soumalias","Jakob Weissteiner","Jakob Heiss","Sven Seuken"],"pdf_url":"https://arxiv.org/pdf/2308.10226v2.pdf","comment":"AAAI 2024 (8 pages + appendix)"},{"id":"http://arxiv.org/abs/2403.19516v1","updated":"2024-03-28T15:47:13Z","published":"2024-03-28T15:47:13Z","title":"Maximum Likelihood Estimation on Stochastic Blockmodels for Directed\n  Graph Clustering","summary":"  This paper studies the directed graph clustering problem through the lens of\nstatistics, where we formulate clustering as estimating underlying communities\nin the directed stochastic block model (DSBM). We conduct the maximum\nlikelihood estimation (MLE) on the DSBM and thereby ascertain the most probable\ncommunity assignment given the observed graph structure. In addition to the\nstatistical point of view, we further establish the equivalence between this\nMLE formulation and a novel flow optimization heuristic, which jointly\nconsiders two important directed graph statistics: edge density and edge\norientation. Building on this new formulation of directed clustering, we\nintroduce two efficient and interpretable directed clustering algorithms, a\nspectral clustering algorithm and a semidefinite programming based clustering\nalgorithm. We provide a theoretical upper bound on the number of misclustered\nvertices of the spectral clustering algorithm using tools from matrix\nperturbation theory. We compare, both quantitatively and qualitatively, our\nproposed algorithms with existing directed clustering methods on both synthetic\nand real-world data, thus providing further ground to our theoretical\ncontributions.\n","authors":["Mihai Cucuringu","Xiaowen Dong","Ning Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.19516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19514v1","updated":"2024-03-28T15:45:03Z","published":"2024-03-28T15:45:03Z","title":"CDIMC-net: Cognitive Deep Incomplete Multi-view Clustering Network","summary":"  In recent years, incomplete multi-view clustering, which studies the\nchallenging multi-view clustering problem on missing views, has received\ngrowing research interests. Although a series of methods have been proposed to\naddress this issue, the following problems still exist: 1) Almost all of the\nexisting methods are based on shallow models, which is difficult to obtain\ndiscriminative common representations. 2) These methods are generally sensitive\nto noise or outliers since the negative samples are treated equally as the\nimportant samples. In this paper, we propose a novel incomplete multi-view\nclustering network, called Cognitive Deep Incomplete Multi-view Clustering\nNetwork (CDIMC-net), to address these issues. Specifically, it captures the\nhigh-level features and local structure of each view by incorporating the\nview-specific deep encoders and graph embedding strategy into a framework.\nMoreover, based on the human cognition, i.e., learning from easy to hard, it\nintroduces a self-paced strategy to select the most confident samples for model\ntraining, which can reduce the negative influence of outliers. Experimental\nresults on several incomplete datasets show that CDIMC-net outperforms the\nstate-of-the-art incomplete multi-view clustering methods.\n","authors":["Jie Wen","Zheng Zhang","Yong Xu","Bob Zhang","Lunke Fei","Guo-Sen Xie"],"pdf_url":"https://arxiv.org/pdf/2403.19514v1.pdf","comment":"Accepted by IJCAI 2020"},{"id":"http://arxiv.org/abs/2403.17919v2","updated":"2024-03-28T15:44:39Z","published":"2024-03-26T17:55:02Z","title":"LISA: Layerwise Importance Sampling for Memory-Efficient Large Language\n  Model Fine-Tuning","summary":"  The machine learning community has witnessed impressive advancements since\nthe first appearance of large language models (LLMs), yet their huge memory\nconsumption has become a major roadblock to large-scale training. Parameter\nEfficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been\nproposed to alleviate this problem, but their performance still fails to match\nfull parameter training in most large-scale fine-tuning settings. Attempting to\ncomplement this deficiency, we investigate layerwise properties of LoRA on\nfine-tuning tasks and observe an uncommon skewness of weight norms across\ndifferent layers. Utilizing this key observation, a surprisingly simple\ntraining strategy is discovered, which outperforms both LoRA and full parameter\ntraining in a wide range of settings with memory costs as low as LoRA. We name\nit Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA,\nwhich applies the idea of importance sampling to different layers in LLMs and\nrandomly freeze most middle layers during optimization. Experimental results\nshow that with similar or less GPU memory consumption, LISA surpasses LoRA or\neven full parameter tuning in downstream fine-tuning tasks, where LISA\nconsistently outperforms LoRA by over $11\\%$-$37\\%$ in terms of MT-Bench\nscores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or\nbetter performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating\nits effectiveness across different domains.\n","authors":["Rui Pan","Xiang Liu","Shizhe Diao","Renjie Pi","Jipeng Zhang","Chi Han","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.17919v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19508v1","updated":"2024-03-28T15:41:43Z","published":"2024-03-28T15:41:43Z","title":"Debiasing Cardiac Imaging with Controlled Latent Diffusion Models","summary":"  The progress in deep learning solutions for disease diagnosis and prognosis\nbased on cardiac magnetic resonance imaging is hindered by highly imbalanced\nand biased training data. To address this issue, we propose a method to\nalleviate imbalances inherent in datasets through the generation of synthetic\ndata based on sensitive attributes such as sex, age, body mass index, and\nhealth condition. We adopt ControlNet based on a denoising diffusion\nprobabilistic model to condition on text assembled from patient metadata and\ncardiac geometry derived from segmentation masks using a large-cohort study,\nspecifically, the UK Biobank. We assess our method by evaluating the realism of\nthe generated images using established quantitative metrics. Furthermore, we\nconduct a downstream classification task aimed at debiasing a classifier by\nrectifying imbalances within underrepresented groups through synthetically\ngenerated samples. Our experiments demonstrate the effectiveness of the\nproposed approach in mitigating dataset imbalances, such as the scarcity of\nyounger patients or individuals with normal BMI level suffering from heart\nfailure. This work represents a major step towards the adoption of synthetic\ndata for the development of fair and generalizable models for medical\nclassification tasks. Notably, we conduct all our experiments using a single,\nconsumer-level GPU to highlight the feasibility of our approach within\nresource-constrained environments. Our code is available at\nhttps://github.com/faildeny/debiasing-cardiac-mri.\n","authors":["Grzegorz Skorupko","Richard Osuala","Zuzanna Szafranowska","Kaisar Kushibar","Nay Aung","Steffen E Petersen","Karim Lekadir","Polyxeni Gkontra"],"pdf_url":"https://arxiv.org/pdf/2403.19508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19507v1","updated":"2024-03-28T15:41:41Z","published":"2024-03-28T15:41:41Z","title":"SineNet: Learning Temporal Dynamics in Time-Dependent Partial\n  Differential Equations","summary":"  We consider using deep neural networks to solve time-dependent partial\ndifferential equations (PDEs), where multi-scale processing is crucial for\nmodeling complex, time-evolving dynamics. While the U-Net architecture with\nskip connections is commonly used by prior studies to enable multi-scale\nprocessing, our analysis shows that the need for features to evolve across\nlayers results in temporally misaligned features in skip connections, which\nlimits the model's performance. To address this limitation, we propose SineNet,\nconsisting of multiple sequentially connected U-shaped network blocks, referred\nto as waves. In SineNet, high-resolution features are evolved progressively\nthrough multiple stages, thereby reducing the amount of misalignment within\neach stage. We furthermore analyze the role of skip connections in enabling\nboth parallel and sequential processing of multi-scale information. Our method\nis rigorously tested on multiple PDE datasets, including the Navier-Stokes\nequations and shallow water equations, showcasing the advantages of our\nproposed approach over conventional U-Nets with a comparable parameter budget.\nWe further demonstrate that increasing the number of waves in SineNet while\nmaintaining the same number of parameters leads to a monotonically improved\nperformance. The results highlight the effectiveness of SineNet and the\npotential of our approach in advancing the state-of-the-art in neural PDE\nsolver design. Our code is available as part of AIRS\n(https://github.com/divelab/AIRS).\n","authors":["Xuan Zhang","Jacob Helwig","Yuchao Lin","Yaochen Xie","Cong Fu","Stephan Wojtowytsch","Shuiwang Ji"],"pdf_url":"https://arxiv.org/pdf/2403.19507v1.pdf","comment":"The Twelfth International Conference on Learning Representations"},{"id":"http://arxiv.org/abs/2307.02496v3","updated":"2024-03-28T15:33:42Z","published":"2023-07-04T08:00:31Z","title":"Learning to reconstruct the bubble distribution with conductivity maps\n  using Invertible Neural Networks and Error Diffusion","summary":"  Electrolysis is crucial for eco-friendly hydrogen production, but gas bubbles\ngenerated during the process hinder reactions, reduce cell efficiency, and\nincrease energy consumption. Additionally, these gas bubbles cause changes in\nthe conductivity inside the cell, resulting in corresponding variations in the\ninduced magnetic field around the cell. Therefore, measuring these gas\nbubble-induced magnetic field fluctuations using external magnetic sensors and\nsolving the inverse problem of Biot-Savart Law allows for estimating the\nconductivity in the cell and, thus, bubble size and location. However,\ndetermining high-resolution conductivity maps from only a few induced magnetic\nfield measurements is an ill-posed inverse problem. To overcome this, we\nexploit Invertible Neural Networks (INNs) to reconstruct the conductivity\nfield. Our qualitative results and quantitative evaluation using random error\ndiffusion show that INN achieves far superior performance compared to Tikhonov\nregularization.\n","authors":["Nishant Kumar","Lukas Krause","Thomas Wondrak","Sven Eckert","Kerstin Eckert","Stefan Gumhold"],"pdf_url":"https://arxiv.org/pdf/2307.02496v3.pdf","comment":"Accepted for Oral presentation at WCIPT11 (11th World Congress on\n  Industrial Process Tomography)"},{"id":"http://arxiv.org/abs/2403.19500v1","updated":"2024-03-28T15:29:30Z","published":"2024-03-28T15:29:30Z","title":"Tensor Network-Constrained Kernel Machines as Gaussian Processes","summary":"  Tensor Networks (TNs) have recently been used to speed up kernel machines by\nconstraining the model weights, yielding exponential computational and storage\nsavings. In this paper we prove that the outputs of Canonical Polyadic\nDecomposition (CPD) and Tensor Train (TT)-constrained kernel machines recover a\nGaussian Process (GP), which we fully characterize, when placing i.i.d. priors\nover their parameters. We analyze the convergence of both CPD and\nTT-constrained models, and show how TT yields models exhibiting more GP\nbehavior compared to CPD, for the same number of model parameters. We\nempirically observe this behavior in two numerical experiments where we\nrespectively analyze the convergence to the GP and the performance at\nprediction. We thereby establish a connection between TN-constrained kernel\nmachines and GPs.\n","authors":["Frederiek Wesel","Kim Batselier"],"pdf_url":"https://arxiv.org/pdf/2403.19500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19499v1","updated":"2024-03-28T15:29:19Z","published":"2024-03-28T15:29:19Z","title":"Client-supervised Federated Learning: Towards One-model-for-all\n  Personalization","summary":"  Personalized Federated Learning (PerFL) is a new machine learning paradigm\nthat delivers personalized models for diverse clients under federated learning\nsettings. Most PerFL methods require extra learning processes on a client to\nadapt a globally shared model to the client-specific personalized model using\nits own local data. However, the model adaptation process in PerFL is still an\nopen challenge in the stage of model deployment and test time. This work\ntackles the challenge by proposing a novel federated learning framework to\nlearn only one robust global model to achieve competitive performance to those\npersonalized models on unseen/test clients in the FL system. Specifically, we\ndesign a new Client-Supervised Federated Learning (FedCS) to unravel clients'\nbias on instances' latent representations so that the global model can learn\nboth client-specific and client-agnostic knowledge. Experimental study shows\nthat the FedCS can learn a robust FL global model for the changing data\ndistributions of unseen/test clients. The FedCS's global model can be directly\ndeployed to the test clients while achieving comparable performance to other\npersonalized FL methods that require model adaptation.\n","authors":["Peng Yan","Guodong Long"],"pdf_url":"https://arxiv.org/pdf/2403.19499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16591v2","updated":"2024-03-28T15:27:38Z","published":"2024-03-25T10:06:45Z","title":"Deciphering the Interplay between Local Differential Privacy, Average\n  Bayesian Privacy, and Maximum Bayesian Privacy","summary":"  The swift evolution of machine learning has led to emergence of various\ndefinitions of privacy due to the threats it poses to privacy, including the\nconcept of local differential privacy (LDP). Although widely embraced and\nutilized across numerous domains, this conventional approach to measure privacy\nstill exhibits certain limitations, spanning from failure to prevent\ninferential disclosure to lack of consideration for the adversary's background\nknowledge. In this comprehensive study, we introduce Bayesian privacy and delve\ninto the intricate relationship between LDP and its Bayesian counterparts,\nunveiling novel insights into utility-privacy trade-offs. We introduce a\nframework that encapsulates both attack and defense strategies, highlighting\ntheir interplay and effectiveness. The relationship between LDP and Maximum\nBayesian Privacy (MBP) is first revealed, demonstrating that under uniform\nprior distribution, a mechanism satisfying $\\xi$-LDP will satisfy $\\xi$-MBP and\nconversely $\\xi$-MBP also confers 2$\\xi$-LDP. Our next theoretical contribution\nare anchored in the rigorous definitions and relationships between Average\nBayesian Privacy (ABP) and Maximum Bayesian Privacy (MBP), encapsulated by\nequations $\\epsilon_{p,a} \\leq \\frac{1}{\\sqrt{2}}\\sqrt{(\\epsilon_{p,m} +\n\\epsilon)\\cdot(e^{\\epsilon_{p,m} + \\epsilon} - 1)}$. These relationships\nfortify our understanding of the privacy guarantees provided by various\nmechanisms. Our work not only lays the groundwork for future empirical\nexploration but also promises to facilitate the design of privacy-preserving\nalgorithms, thereby fostering the development of trustworthy machine learning\nsolutions.\n","authors":["Xiaojin Zhang","Yulin Fei","Wei Chen","Hai Jin"],"pdf_url":"https://arxiv.org/pdf/2403.16591v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19494v1","updated":"2024-03-28T15:26:38Z","published":"2024-03-28T15:26:38Z","title":"Regression with Multi-Expert Deferral","summary":"  Learning to defer with multiple experts is a framework where the learner can\nchoose to defer the prediction to several experts. While this problem has\nreceived significant attention in classification contexts, it presents unique\nchallenges in regression due to the infinite and continuous nature of the label\nspace. In this work, we introduce a novel framework of regression with\ndeferral, which involves deferring the prediction to multiple experts. We\npresent a comprehensive analysis for both the single-stage scenario, where\nthere is simultaneous learning of predictor and deferral functions, and the\ntwo-stage scenario, which involves a pre-trained predictor with a learned\ndeferral function. We introduce new surrogate loss functions for both scenarios\nand prove that they are supported by $H$-consistency bounds. These bounds\nprovide consistency guarantees that are stronger than Bayes consistency, as\nthey are non-asymptotic and hypothesis set-specific. Our framework is\nversatile, applying to multiple experts, accommodating any bounded regression\nlosses, addressing both instance-dependent and label-dependent costs, and\nsupporting both single-stage and two-stage methods. A by-product is that our\nsingle-stage formulation includes the recent regression with abstention\nframework (Cheng et al., 2023) as a special case, where only a single expert,\nthe squared loss and a label-independent cost are considered. Minimizing our\nproposed loss functions directly leads to novel algorithms for regression with\ndeferral. We report the results of extensive experiments showing the\neffectiveness of our proposed algorithms.\n","authors":["Anqi Mao","Mehryar Mohri","Yutao Zhong"],"pdf_url":"https://arxiv.org/pdf/2403.19494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14472v2","updated":"2024-03-28T15:24:17Z","published":"2024-03-21T15:18:30Z","title":"Detoxifying Large Language Models via Knowledge Editing","summary":"  This paper investigates using knowledge editing techniques to detoxify Large\nLanguage Models (LLMs). We construct a benchmark, SafeEdit, which covers nine\nunsafe categories with various powerful attack prompts and equips comprehensive\nmetrics for systematic evaluation. We conduct experiments with several\nknowledge editing approaches, indicating that knowledge editing has the\npotential to efficiently detoxify LLMs with limited impact on general\nperformance. Then, we propose a simple yet effective baseline, dubbed\nDetoxifying with Intraoperative Neural Monitoring (DINM), to diminish the\ntoxicity of LLMs within a few tuning steps via only one instance. We further\nprovide an in-depth analysis of the internal mechanism for various detoxify\napproaches, demonstrating that previous methods like SFT and DPO may merely\nsuppress the activations of toxic parameters, while DINM mitigates the toxicity\nof the toxic parameters to a certain extent, making permanent adjustments. We\nhope that these insights could shed light on future work of developing\ndetoxifying approaches and the underlying knowledge mechanisms of LLMs. Code\nand benchmark are available at https://github.com/zjunlp/EasyEdit.\n","authors":["Mengru Wang","Ningyu Zhang","Ziwen Xu","Zekun Xi","Shumin Deng","Yunzhi Yao","Qishen Zhang","Linyi Yang","Jindong Wang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14472v2.pdf","comment":"Ongoing work. Project website:\n  https://zjunlp.github.io/project/SafeEdit Due to the specificity of the\n  knowledge editing setting, we revise Tables 1 and 3 to present a fair\n  comparison of experimental results. More experimental results will be updated\n  soon"},{"id":"http://arxiv.org/abs/2403.17608v2","updated":"2024-03-28T15:24:16Z","published":"2024-03-26T11:39:00Z","title":"Fake or JPEG? Revealing Common Biases in Generated Image Detection\n  Datasets","summary":"  The widespread adoption of generative image models has highlighted the urgent\nneed to detect artificial content, which is a crucial step in combating\nwidespread manipulation and misinformation. Consequently, numerous detectors\nand associated datasets have emerged. However, many of these datasets\ninadvertently introduce undesirable biases, thereby impacting the effectiveness\nand evaluation of detectors. In this paper, we emphasize that many datasets for\nAI-generated image detection contain biases related to JPEG compression and\nimage size. Using the GenImage dataset, we demonstrate that detectors indeed\nlearn from these undesired factors. Furthermore, we show that removing the\nnamed biases substantially increases robustness to JPEG compression and\nsignificantly alters the cross-generator performance of evaluated detectors.\nSpecifically, it leads to more than 11 percentage points increase in\ncross-generator performance for ResNet50 and Swin-T detectors on the GenImage\ndataset, achieving state-of-the-art results.\n  We provide the dataset and source codes of this paper on the anonymous\nwebsite: https://www.unbiased-genimage.org\n","authors":["Patrick Grommelt","Louis Weiss","Franz-Josef Pfreundt","Janis Keuper"],"pdf_url":"https://arxiv.org/pdf/2403.17608v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01786v2","updated":"2024-03-28T15:22:42Z","published":"2024-02-01T21:51:09Z","title":"COA-GPT: Generative Pre-trained Transformers for Accelerated Course of\n  Action Development in Military Operations","summary":"  The development of Courses of Action (COAs) in military operations is\ntraditionally a time-consuming and intricate process. Addressing this\nchallenge, this study introduces COA-GPT, a novel algorithm employing Large\nLanguage Models (LLMs) for rapid and efficient generation of valid COAs.\nCOA-GPT incorporates military doctrine and domain expertise to LLMs through\nin-context learning, allowing commanders to input mission information - in both\ntext and image formats - and receive strategically aligned COAs for review and\napproval. Uniquely, COA-GPT not only accelerates COA development, producing\ninitial COAs within seconds, but also facilitates real-time refinement based on\ncommander feedback. This work evaluates COA-GPT in a military-relevant scenario\nwithin a militarized version of the StarCraft II game, comparing its\nperformance against state-of-the-art reinforcement learning algorithms. Our\nresults demonstrate COA-GPT's superiority in generating strategically sound\nCOAs more swiftly, with added benefits of enhanced adaptability and alignment\nwith commander intentions. COA-GPT's capability to rapidly adapt and update\nCOAs during missions presents a transformative potential for military planning,\nparticularly in addressing planning discrepancies and capitalizing on emergent\nwindows of opportunities.\n","authors":["Vinicius G. Goecks","Nicholas Waytowich"],"pdf_url":"https://arxiv.org/pdf/2402.01786v2.pdf","comment":"Accepted at the NATO Science and Technology Organization Symposium\n  (ICMCIS) organized by the Information Systems Technology (IST) Panel,\n  IST-205-RSY - the ICMCIS, held in Koblenz, Germany, 23-24 April 2024"},{"id":"http://arxiv.org/abs/2402.07946v2","updated":"2024-03-28T15:17:30Z","published":"2024-02-09T16:10:29Z","title":"Re-Envisioning Command and Control","summary":"  Future warfare will require Command and Control (C2) decision-making to occur\nin more complex, fast-paced, ill-structured, and demanding conditions. C2 will\nbe further complicated by operational challenges such as Denied, Degraded,\nIntermittent, and Limited (DDIL) communications and the need to account for\nmany data streams, potentially across multiple domains of operation. Yet,\ncurrent C2 practices -- which stem from the industrial era rather than the\nemerging intelligence era -- are linear and time-consuming. Critically, these\napproaches may fail to maintain overmatch against adversaries on the future\nbattlefield. To address these challenges, we propose a vision for future C2\nbased on robust partnerships between humans and artificial intelligence (AI)\nsystems. This future vision is encapsulated in three operational impacts:\nstreamlining the C2 operations process, maintaining unity of effort, and\ndeveloping adaptive collective knowledge systems. This paper illustrates the\nenvisaged future C2 capabilities, discusses the assumptions that shaped them,\nand describes how the proposed developments could transform C2 in future\nwarfare.\n","authors":["Kaleb McDowell","Ellen Novoseller","Anna Madison","Vinicius G. Goecks","Christopher Kelshaw"],"pdf_url":"https://arxiv.org/pdf/2402.07946v2.pdf","comment":"Accepted at the NATO Science and Technology Organization Symposium\n  (ICMCIS) organized by the Information Systems Technology (IST) Panel,\n  IST-205-RSY - the ICMCIS, held in Koblenz, Germany, 23-24 April 2024"},{"id":"http://arxiv.org/abs/2402.06501v2","updated":"2024-03-28T15:17:01Z","published":"2024-02-09T16:11:04Z","title":"Scalable Interactive Machine Learning for Future Command and Control","summary":"  Future warfare will require Command and Control (C2) personnel to make\ndecisions at shrinking timescales in complex and potentially ill-defined\nsituations. Given the need for robust decision-making processes and\ndecision-support tools, integration of artificial and human intelligence holds\nthe potential to revolutionize the C2 operations process to ensure adaptability\nand efficiency in rapidly changing operational environments. We propose to\nleverage recent promising breakthroughs in interactive machine learning, in\nwhich humans can cooperate with machine learning algorithms to guide machine\nlearning algorithm behavior. This paper identifies several gaps in\nstate-of-the-art science and technology that future work should address to\nextend these approaches to function in complex C2 contexts. In particular, we\ndescribe three research focus areas that together, aim to enable scalable\ninteractive machine learning (SIML): 1) developing human-AI interaction\nalgorithms to enable planning in complex, dynamic situations; 2) fostering\nresilient human-AI teams through optimizing roles, configurations, and trust;\nand 3) scaling algorithms and human-AI teams for flexibility across a range of\npotential contexts and situations.\n","authors":["Anna Madison","Ellen Novoseller","Vinicius G. Goecks","Benjamin T. Files","Nicholas Waytowich","Alfred Yu","Vernon J. Lawhern","Steven Thurman","Christopher Kelshaw","Kaleb McDowell"],"pdf_url":"https://arxiv.org/pdf/2402.06501v2.pdf","comment":"Accepted at the NATO Science and Technology Organization Symposium\n  (ICMCIS) organized by the Information Systems Technology (IST) Panel,\n  IST-205-RSY - the ICMCIS, held in Koblenz, Germany, 23-24 April 2024"},{"id":"http://arxiv.org/abs/2310.01201v3","updated":"2024-03-28T15:09:13Z","published":"2023-10-02T13:42:11Z","title":"SWoTTeD: An Extension of Tensor Decomposition to Temporal Phenotyping","summary":"  Tensor decomposition has recently been gaining attention in the machine\nlearning community for the analysis of individual traces, such as Electronic\nHealth Records (EHR). However, this task becomes significantly more difficult\nwhen the data follows complex temporal patterns. This paper introduces the\nnotion of a temporal phenotype as an arrangement of features over time and it\nproposes SWoTTeD (Sliding Window for Temporal Tensor Decomposition), a novel\nmethod to discover hidden temporal patterns. SWoTTeD integrates several\nconstraints and regularizations to enhance the interpretability of the\nextracted phenotypes. We validate our proposal using both synthetic and\nreal-world datasets, and we present an original usecase using data from the\nGreater Paris University Hospital. The results show that SWoTTeD achieves at\nleast as accurate reconstruction as recent state-of-the-art tensor\ndecomposition models, and extracts temporal phenotypes that are meaningful for\nclinicians.\n","authors":["Hana Sebia","Thomas Guyet","Etienne Audureau"],"pdf_url":"https://arxiv.org/pdf/2310.01201v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19480v1","updated":"2024-03-28T15:08:51Z","published":"2024-03-28T15:08:51Z","title":"$H$-Consistency Guarantees for Regression","summary":"  We present a detailed study of $H$-consistency bounds for regression. We\nfirst present new theorems that generalize the tools previously given to\nestablish $H$-consistency bounds. This generalization proves essential for\nanalyzing $H$-consistency bounds specific to regression. Next, we prove a\nseries of novel $H$-consistency bounds for surrogate loss functions of the\nsquared loss, under the assumption of a symmetric distribution and a bounded\nhypothesis set. This includes positive results for the Huber loss, all $\\ell_p$\nlosses, $p \\geq 1$, the squared $\\epsilon$-insensitive loss, as well as a\nnegative result for the $\\epsilon$-insensitive loss used in squared Support\nVector Regression (SVR). We further leverage our analysis of $H$-consistency\nfor regression and derive principled surrogate losses for adversarial\nregression (Section 5). This readily establishes novel algorithms for\nadversarial regression, for which we report favorable experimental results in\nSection 6.\n","authors":["Anqi Mao","Mehryar Mohri","Yutao Zhong"],"pdf_url":"https://arxiv.org/pdf/2403.19480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15905v3","updated":"2024-03-28T15:00:04Z","published":"2024-03-23T18:19:02Z","title":"Towards Low-Energy Adaptive Personalization for Resource-Constrained\n  Devices","summary":"  The personalization of machine learning (ML) models to address data drift is\na significant challenge in the context of Internet of Things (IoT)\napplications. Presently, most approaches focus on fine-tuning either the full\nbase model or its last few layers to adapt to new data, while often neglecting\nenergy costs. However, various types of data drift exist, and fine-tuning the\nfull base model or the last few layers may not result in optimal performance in\ncertain scenarios. We propose Target Block Fine-Tuning (TBFT), a low-energy\nadaptive personalization framework designed for resource-constrained devices.\nWe categorize data drift and personalization into three types: input-level,\nfeature-level, and output-level. For each type, we fine-tune different blocks\nof the model to achieve optimal performance with reduced energy costs.\nSpecifically, input-, feature-, and output-level correspond to fine-tuning the\nfront, middle, and rear blocks of the model. We evaluate TBFT on a ResNet\nmodel, three datasets, three different training sizes, and a Raspberry Pi.\nCompared with the $Block Avg$, where each block is fine-tuned individually and\ntheir performance improvements are averaged, TBFT exhibits an improvement in\nmodel accuracy by an average of 15.30% whilst saving 41.57% energy consumption\non average compared with full fine-tuning.\n","authors":["Yushan Huang","Josh Millar","Yuxuan Long","Yuchen Zhao","Hamed Hadaddi"],"pdf_url":"https://arxiv.org/pdf/2403.15905v3.pdf","comment":"Accepetd to The 4th Workshop on Machine Learning and Systems\n  (EuroMLSys '24)"},{"id":"http://arxiv.org/abs/2403.19470v1","updated":"2024-03-28T14:54:31Z","published":"2024-03-28T14:54:31Z","title":"Deep decomposition method for the limited aperture inverse obstacle\n  scattering problem","summary":"  In this paper, we consider a deep learning approach to the limited aperture\ninverse obstacle scattering problem. It is well known that traditional deep\nlearning relies solely on data, which may limit its performance for the inverse\nproblem when only indirect observation data and a physical model are available.\nA fundamental question arises in light of these limitations: is it possible to\nenable deep learning to work on inverse problems without labeled data and to be\naware of what it is learning? This work proposes a deep decomposition method\n(DDM) for such purposes, which does not require ground truth labels. It\naccomplishes this by providing physical operators associated with the\nscattering model to the neural network architecture. Additionally, a deep\nlearning based data completion scheme is implemented in DDM to prevent\ndistorting the solution of the inverse problem for limited aperture data.\nFurthermore, apart from addressing the ill-posedness imposed by the inverse\nproblem itself, DDM is a physics-aware machine learning technique that can have\ninterpretability property. The convergence result of DDM is theoretically\nproven. Numerical experiments are presented to demonstrate the validity of the\nproposed DDM even when the incident and observation apertures are extremely\nlimited.\n","authors":["Yunwen Yin","Liang Yan"],"pdf_url":"https://arxiv.org/pdf/2403.19470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.03919v3","updated":"2024-03-28T14:52:27Z","published":"2022-09-05T23:51:07Z","title":"Bi-objective Ranking and Selection Using Stochastic Kriging","summary":"  We consider bi-objective ranking and selection problems, where the goal is to\ncorrectly identify the Pareto optimal solutions among a finite set of\ncandidates for which the two objective outcomes have been observed with\nuncertainty (e.g., after running a multiobjective stochastic simulation\noptimization procedure). When identifying these solutions, the noise perturbing\nthe observed performance may lead to two types of errors: solutions that are\ntruly Pareto-optimal can be wrongly considered dominated, and solutions that\nare truly dominated can be wrongly considered Pareto-optimal. We propose a\nnovel Bayesian bi-objective ranking and selection method that sequentially\nallocates extra samples to competitive solutions, in view of reducing the\nmisclassification errors when identifying the solutions with the best expected\nperformance. The approach uses stochastic kriging to build reliable predictive\ndistributions of the objective outcomes, and exploits this information to\ndecide how to resample. Experimental results show that the proposed method\noutperforms the standard allocation method, as well as a well-known the\nstate-of-the-art algorithm. Moreover, we show that the other competing\nalgorithms also benefit from the use of stochastic kriging information; yet,\nthe proposed method remains superior.\n","authors":["Sebastian Rojas Gonzalez","Juergen Branke","Inneke van Nieuwenhuyse"],"pdf_url":"https://arxiv.org/pdf/2209.03919v3.pdf","comment":"33 pages, 14 figures"},{"id":"http://arxiv.org/abs/2403.19462v1","updated":"2024-03-28T14:34:02Z","published":"2024-03-28T14:34:02Z","title":"Offline Imitation Learning from Multiple Baselines with Applications to\n  Compiler Optimization","summary":"  This work studies a Reinforcement Learning (RL) problem in which we are given\na set of trajectories collected with K baseline policies. Each of these\npolicies can be quite suboptimal in isolation, and have strong performance in\ncomplementary parts of the state space. The goal is to learn a policy which\nperforms as well as the best combination of baselines on the entire state\nspace. We propose a simple imitation learning based algorithm, show a sample\ncomplexity bound on its accuracy and prove that the the algorithm is minimax\noptimal by showing a matching lower bound. Further, we apply the algorithm in\nthe setting of machine learning guided compiler optimization to learn policies\nfor inlining programs with the objective of creating a small binary. We\ndemonstrate that we can learn a policy that outperforms an initial policy\nlearned via standard RL through a few iterations of our approach.\n","authors":["Teodor V. Marinov","Alekh Agarwal","Mircea Trofin"],"pdf_url":"https://arxiv.org/pdf/2403.19462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07728v2","updated":"2024-03-28T14:20:13Z","published":"2024-03-12T15:07:20Z","title":"CAP: A General Algorithm for Online Selective Conformal Prediction with\n  FCR Control","summary":"  We study the problem of post-selection predictive inference in an online\nfashion. To avoid devoting resources to unimportant units, a preliminary\nselection of the current individual before reporting its prediction interval is\ncommon and meaningful in online predictive tasks. Since the online selection\ncauses a temporal multiplicity in the selected prediction intervals, it is\nimportant to control the real-time false coverage-statement rate (FCR) which\nmeasures the overall miscoverage level. We develop a general framework named\nCAP (Calibration after Adaptive Pick) that performs an adaptive pick rule on\nhistorical data to construct a calibration set if the current individual is\nselected and then outputs a conformal prediction interval for the unobserved\nlabel. We provide tractable procedures for constructing the calibration set for\npopular online selection rules. We proved that CAP can achieve an exact\nselection-conditional coverage guarantee in the finite-sample and\ndistribution-free regimes. To account for the distribution shift in online\ndata, we also embed CAP into some recent dynamic conformal prediction\nalgorithms and show that the proposed method can deliver long-run FCR control.\nNumerical results on both synthetic and real data corroborate that CAP can\neffectively control FCR around the target level and yield more narrowed\nprediction intervals over existing baselines across various settings.\n","authors":["Yajie Bao","Yuyang Huo","Haojie Ren","Changliang Zou"],"pdf_url":"https://arxiv.org/pdf/2403.07728v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12659v2","updated":"2024-03-28T14:19:21Z","published":"2024-03-19T11:49:08Z","title":"Graph Neural Networks for Carbon Dioxide Adsorption Prediction in\n  Aluminium-Exchanged Zeolites","summary":"  The ability to efficiently predict adsorption properties of zeolites can be\nof large benefit in accelerating the design process of novel materials. The\nexisting configuration space for these materials is wide, while existing\nmolecular simulation methods are computationally expensive. In this work, we\npropose a model which is 4 to 5 orders of magnitude faster at adsorption\nproperties compared to molecular simulations. To validate the model, we\ngenerated datasets containing various aluminium configurations for the MOR,\nMFI, RHO and ITW zeolites along with their heat of adsorptions and Henry\ncoefficients for CO$_2$, obtained from Monte Carlo simulations. The predictions\nobtained from the Machine Learning model are in agreement with the values\nobtained from the Monte Carlo simulations, confirming that the model can be\nused for property prediction. Furthermore, we show that the model can be used\nfor identifying adsorption sites. Finally, we evaluate the capability of our\nmodel for generating novel zeolite configurations by using it in combination\nwith a genetic algorithm.\n","authors":["Marko Petković","José Manuel Vicent-Luna","Vlado Menkovski","Sofía Calero"],"pdf_url":"https://arxiv.org/pdf/2403.12659v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19448v1","updated":"2024-03-28T14:16:23Z","published":"2024-03-28T14:16:23Z","title":"Fisher-Rao Gradient Flows of Linear Programs and State-Action Natural\n  Policy Gradients","summary":"  Kakade's natural policy gradient method has been studied extensively in the\nlast years showing linear convergence with and without regularization. We study\nanother natural gradient method which is based on the Fisher information matrix\nof the state-action distributions and has received little attention from the\ntheoretical side. Here, the state-action distributions follow the Fisher-Rao\ngradient flow inside the state-action polytope with respect to a linear\npotential. Therefore, we study Fisher-Rao gradient flows of linear programs\nmore generally and show linear convergence with a rate that depends on the\ngeometry of the linear program. Equivalently, this yields an estimate on the\nerror induced by entropic regularization of the linear program which improves\nexisting results. We extend these results and show sublinear convergence for\nperturbed Fisher-Rao gradient flows and natural gradient flows up to an\napproximation error. In particular, these general results cover the case of\nstate-action natural policy gradients.\n","authors":["Johannes Müller","Semih Çaycı","Guido Montúfar"],"pdf_url":"https://arxiv.org/pdf/2403.19448v1.pdf","comment":"27 pages, 4 figures, under review"},{"id":"http://arxiv.org/abs/2403.19444v1","updated":"2024-03-28T14:15:13Z","published":"2024-03-28T14:15:13Z","title":"Transparent and Clinically Interpretable AI for Lung Cancer Detection in\n  Chest X-Rays","summary":"  The rapidly advancing field of Explainable Artificial Intelligence (XAI) aims\nto tackle the issue of trust regarding the use of complex black-box deep\nlearning models in real-world applications. Existing post-hoc XAI techniques\nhave recently been shown to have poor performance on medical data, producing\nunreliable explanations which are infeasible for clinical use. To address this,\nwe propose an ante-hoc approach based on concept bottleneck models which\nintroduces for the first time clinical concepts into the classification\npipeline, allowing the user valuable insight into the decision-making process.\nOn a large public dataset of chest X-rays and associated medical reports, we\nfocus on the binary classification task of lung cancer detection. Our approach\nyields improved classification performance in lung cancer detection when\ncompared to baseline deep learning models (F1 > 0.9), while also generating\nclinically relevant and more reliable explanations than existing techniques. We\nevaluate our approach against post-hoc image XAI techniques LIME and SHAP, as\nwell as CXR-LLaVA, a recent textual XAI tool which operates in the context of\nquestion answering on chest X-rays.\n","authors":["Amy Rafferty","Rishi Ramaesh","Ajitha Rajan"],"pdf_url":"https://arxiv.org/pdf/2403.19444v1.pdf","comment":"12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.19441v1","updated":"2024-03-28T14:11:40Z","published":"2024-03-28T14:11:40Z","title":"A Novel Stochastic Transformer-based Approach for Post-Traumatic Stress\n  Disorder Detection using Audio Recording of Clinical Interviews","summary":"  Post-traumatic stress disorder (PTSD) is a mental disorder that can be\ndeveloped after witnessing or experiencing extremely traumatic events. PTSD can\naffect anyone, regardless of ethnicity, or culture. An estimated one in every\neleven people will experience PTSD during their lifetime. The\nClinician-Administered PTSD Scale (CAPS) and the PTSD Check List for Civilians\n(PCL-C) interviews are gold standards in the diagnosis of PTSD. These\nquestionnaires can be fooled by the subject's responses. This work proposes a\ndeep learning-based approach that achieves state-of-the-art performances for\nPTSD detection using audio recordings during clinical interviews. Our approach\nis based on MFCC low-level features extracted from audio recordings of clinical\ninterviews, followed by deep high-level learning using a Stochastic\nTransformer. Our proposed approach achieves state-of-the-art performances with\nan RMSE of 2.92 on the eDAIC dataset thanks to the stochastic depth, stochastic\ndeep learning layers, and stochastic activation function.\n","authors":["Mamadou Dia","Ghazaleh Khodabandelou","Alice Othmani"],"pdf_url":"https://arxiv.org/pdf/2403.19441v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19442v1","updated":"2024-03-28T14:11:40Z","published":"2024-03-28T14:11:40Z","title":"Exploiting Individual Graph Structures to Enhance Ecological Momentary\n  Assessment (EMA) Forecasting","summary":"  In the evolving field of psychopathology, the accurate assessment and\nforecasting of data derived from Ecological Momentary Assessment (EMA) is\ncrucial. EMA offers contextually-rich psychopathological measurements over\ntime, that practically lead to Multivariate Time Series (MTS) data. Thus, many\nchallenges arise in analysis from the temporal complexities inherent in\nemotional, behavioral, and contextual EMA data as well as their\ninter-dependencies. To address both of these aspects, this research\ninvestigates the performance of Recurrent and Temporal Graph Neural Networks\n(GNNs). Overall, GNNs, by incorporating additional information from graphs\nreflecting the inner relationships between the variables, notably enhance the\nresults by decreasing the Mean Squared Error (MSE) to 0.84 compared to the\nbaseline LSTM model at 1.02. Therefore, the effect of constructing graphs with\ndifferent characteristics on GNN performance is also explored. Additionally,\nGNN-learned graphs, which are dynamically refined during the training process,\nwere evaluated. Using such graphs showed a similarly good performance. Thus,\ngraph learning proved also promising for other GNN methods, potentially\nrefining the pre-defined graphs.\n","authors":["Mandani Ntekouli","Gerasimos Spanakis","Lourens Waldorp","Anne Roefs"],"pdf_url":"https://arxiv.org/pdf/2403.19442v1.pdf","comment":"9 pages, 3 figures, 2024 IEEE 40th International Conference on Data\n  Engineering Workshops"},{"id":"http://arxiv.org/abs/2306.17563v2","updated":"2024-03-28T13:59:09Z","published":"2023-06-30T11:32:25Z","title":"Large Language Models are Effective Text Rankers with Pairwise Ranking\n  Prompting","summary":"  Ranking documents using Large Language Models (LLMs) by directly feeding the\nquery and candidate documents into the prompt is an interesting and practical\nproblem. However, researchers have found it difficult to outperform fine-tuned\nbaseline rankers on benchmark datasets. We analyze pointwise and listwise\nranking prompts used by existing methods and argue that off-the-shelf LLMs do\nnot fully understand these challenging ranking formulations. In this paper, we\npropose to significantly reduce the burden on LLMs by using a new technique\ncalled Pairwise Ranking Prompting (PRP). Our results are the first in the\nliterature to achieve state-of-the-art ranking performance on standard\nbenchmarks using moderate-sized open-sourced LLMs. On TREC-DL 2019&2020, PRP\nbased on the Flan-UL2 model with 20B parameters performs favorably with the\nprevious best approach in the literature, which is based on the blackbox\ncommercial GPT-4 that has 50x (estimated) model size, while outperforming other\nLLM-based solutions, such as InstructGPT which has 175B parameters, by over 10%\nfor all ranking metrics. By using the same prompt template on seven BEIR tasks,\nPRP outperforms supervised baselines and outperforms the blackbox commercial\nChatGPT solution by 4.2% and pointwise LLM-based solutions by more than 10% on\naverage NDCG@10. Furthermore, we propose several variants of PRP to improve\nefficiency and show that it is possible to achieve competitive results even\nwith linear complexity.\n","authors":["Zhen Qin","Rolf Jagerman","Kai Hui","Honglei Zhuang","Junru Wu","Le Yan","Jiaming Shen","Tianqi Liu","Jialu Liu","Donald Metzler","Xuanhui Wang","Michael Bendersky"],"pdf_url":"https://arxiv.org/pdf/2306.17563v2.pdf","comment":"Accepted to NAACL 2024. Corrected results of RankT5 on TREC-DL19"},{"id":"http://arxiv.org/abs/2402.10251v4","updated":"2024-03-28T13:55:31Z","published":"2024-02-15T16:04:11Z","title":"Brant-2: Foundation Model for Brain Signals","summary":"  Foundational models benefit from pre-training on large amounts of unlabeled\ndata and enable strong performance in a wide variety of applications with a\nsmall amount of labeled data. Such models can be particularly effective in\nanalyzing brain signals, as this field encompasses numerous application\nscenarios, and it is costly to perform large-scale annotation. In this work, we\npresent the largest foundation model in brain signals, Brant-2. Compared to\nBrant, a foundation model designed for intracranial neural signals, Brant-2 not\nonly exhibits robustness towards data variations and modeling scales but also\ncan be applied to a broader range of brain neural data. By experimenting on an\nextensive range of tasks, we demonstrate that Brant-2 is adaptive to various\napplication scenarios in brain signals. Further analyses reveal the scalability\nof the Brant-2, validate each component's effectiveness, and showcase our\nmodel's ability to maintain performance in scenarios with scarce labels.\n","authors":["Zhizhang Yuan","Daoze Zhang","Junru Chen","Gefei Gu","Yang Yang"],"pdf_url":"https://arxiv.org/pdf/2402.10251v4.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.19421v1","updated":"2024-03-28T13:52:12Z","published":"2024-03-28T13:52:12Z","title":"Scaling up ridge regression for brain encoding in a massive individual\n  fMRI dataset","summary":"  Brain encoding with neuroimaging data is an established analysis aimed at\npredicting human brain activity directly from complex stimuli features such as\nmovie frames. Typically, these features are the latent space representation\nfrom an artificial neural network, and the stimuli are image, audio, or text\ninputs. Ridge regression is a popular prediction model for brain encoding due\nto its good out-of-sample generalization performance. However, training a ridge\nregression model can be highly time-consuming when dealing with large-scale\ndeep functional magnetic resonance imaging (fMRI) datasets that include many\nspace-time samples of brain activity. This paper evaluates different\nparallelization techniques to reduce the training time of brain encoding with\nridge regression on the CNeuroMod Friends dataset, one of the largest deep fMRI\nresource currently available. With multi-threading, our results show that the\nIntel Math Kernel Library (MKL) significantly outperforms the OpenBLAS library,\nbeing 1.9 times faster using 32 threads on a single machine. We then evaluated\nthe Dask multi-CPU implementation of ridge regression readily available in\nscikit-learn (MultiOutput), and we proposed a new \"batch\" version of Dask\nparallelization, motivated by a time complexity analysis. In line with our\ntheoretical analysis, MultiOutput parallelization was found to be impractical,\ni.e., slower than multi-threading on a single machine. In contrast, the\nBatch-MultiOutput regression scaled well across compute nodes and threads,\nproviding speed-ups of up to 33 times with 8 compute nodes and 32 threads\ncompared to a single-threaded scikit-learn execution. Batch parallelization\nusing Dask thus emerges as a scalable approach for brain encoding with ridge\nregression on high-performance computing systems using scikit-learn and large\nfMRI datasets.\n","authors":["Sana Ahmadi","Pierre Bellec","Tristan Glatard"],"pdf_url":"https://arxiv.org/pdf/2403.19421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19419v1","updated":"2024-03-28T13:50:24Z","published":"2024-03-28T13:50:24Z","title":"Fairness in Ranking: Robustness through Randomization without the\n  Protected Attribute","summary":"  There has been great interest in fairness in machine learning, especially in\nrelation to classification problems. In ranking-related problems, such as in\nonline advertising, recommender systems, and HR automation, much work on\nfairness remains to be done. Two complications arise: first, the protected\nattribute may not be available in many applications. Second, there are multiple\nmeasures of fairness of rankings, and optimization-based methods utilizing a\nsingle measure of fairness of rankings may produce rankings that are unfair\nwith respect to other measures. In this work, we propose a randomized method\nfor post-processing rankings, which do not require the availability of the\nprotected attribute. In an extensive numerical study, we show the robustness of\nour methods with respect to P-Fairness and effectiveness with respect to\nNormalized Discounted Cumulative Gain (NDCG) from the baseline ranking,\nimproving on previously proposed methods.\n","authors":["Andrii Kliachkin","Eleni Psaroudaki","Jakub Marecek","Dimitris Fotakis"],"pdf_url":"https://arxiv.org/pdf/2403.19419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19418v1","updated":"2024-03-28T13:49:43Z","published":"2024-03-28T13:49:43Z","title":"Constants of Motion for Conserved and Non-conserved Dynamics","summary":"  This paper begins with a dynamical model that was obtained by applying a\nmachine learning technique (FJet) to time-series data; this dynamical model is\nthen analyzed with Lie symmetry techniques to obtain constants of motion. This\nanalysis is performed on both the conserved and non-conserved cases of the 1D\nand 2D harmonic oscillators. For the 1D oscillator, constants are found in the\ncases where the system is underdamped, overdamped, and critically damped. The\nnovel existence of such a constant for a non-conserved model is interpreted as\na manifestation of the conservation of energy of the {\\em total} system (i.e.,\noscillator plus dissipative environment). For the 2D oscillator, constants are\nfound for the isotropic and anisotropic cases, including when the frequencies\nare incommensurate; it is also generalized to arbitrary dimensions. In\naddition, a constant is identified which generalizes angular momentum for all\nratios of the frequencies. The approach presented here can produce {\\em\nmultiple} constants of motion from a {\\em single}, generic data set.\n","authors":["Michael F. Zimmer"],"pdf_url":"https://arxiv.org/pdf/2403.19418v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.19405v1","updated":"2024-03-28T13:29:29Z","published":"2024-03-28T13:29:29Z","title":"Tabular Learning: Encoding for Entity and Context Embeddings","summary":"  Examining the effect of different encoding techniques on entity and context\nembeddings, the goal of this work is to challenge commonly used Ordinal\nencoding for tabular learning. Applying different preprocessing methods and\nnetwork architectures over several datasets resulted in a benchmark on how the\nencoders influence the learning outcome of the networks. By keeping the test,\nvalidation and training data consistent, results have shown that ordinal\nencoding is not the most suited encoder for categorical data in terms of\npreprocessing the data and thereafter, classifying the target variable\ncorrectly. A better outcome was achieved, encoding the features based on string\nsimilarities by computing a similarity matrix as input for the network. This is\nthe case for both, entity and context embeddings, where the transformer\narchitecture showed improved performance for Ordinal and Similarity encoding\nwith regard to multi-label classification tasks.\n","authors":["Fredy Reusser"],"pdf_url":"https://arxiv.org/pdf/2403.19405v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19401v1","updated":"2024-03-28T13:24:18Z","published":"2024-03-28T13:24:18Z","title":"Hardness of Learning Boolean Functions from Label Proportions","summary":"  In recent years the framework of learning from label proportions (LLP) has\nbeen gaining importance in machine learning. In this setting, the training\nexamples are aggregated into subsets or bags and only the average label per bag\nis available for learning an example-level predictor. This generalizes\ntraditional PAC learning which is the special case of unit-sized bags. The\ncomputational learning aspects of LLP were studied in recent works (Saket,\nNeurIPS'21; Saket, NeurIPS'22) which showed algorithms and hardness for\nlearning halfspaces in the LLP setting. In this work we focus on the\nintractability of LLP learning Boolean functions. Our first result shows that\ngiven a collection of bags of size at most $2$ which are consistent with an OR\nfunction, it is NP-hard to find a CNF of constantly many clauses which\nsatisfies any constant-fraction of the bags. This is in contrast with the work\nof (Saket, NeurIPS'21) which gave a $(2/5)$-approximation for learning ORs\nusing a halfspace. Thus, our result provides a separation between constant\nclause CNFs and halfspaces as hypotheses for LLP learning ORs.\n  Next, we prove the hardness of satisfying more than $1/2 + o(1)$ fraction of\nsuch bags using a $t$-DNF (i.e. DNF where each term has $\\leq t$ literals) for\nany constant $t$. In usual PAC learning such a hardness was known (Khot-Saket,\nFOCS'08) only for learning noisy ORs. We also study the learnability of\nparities and show that it is NP-hard to satisfy more than $(q/2^{q-1} +\no(1))$-fraction of $q$-sized bags which are consistent with a parity using a\nparity, while a random parity based algorithm achieves a\n$(1/2^{q-2})$-approximation.\n","authors":["Venkatesan Guruswami","Rishi Saket"],"pdf_url":"https://arxiv.org/pdf/2403.19401v1.pdf","comment":"17 pages. Conference version of this paper appeared in FSTTCS 2023"},{"id":"http://arxiv.org/abs/2311.11908v3","updated":"2024-03-28T13:16:50Z","published":"2023-11-20T16:40:29Z","title":"Continual Learning: Applications and the Road Forward","summary":"  Continual learning is a subfield of machine learning, which aims to allow\nmachine learning models to continuously learn on new data, by accumulating\nknowledge without forgetting what was learned in the past. In this work, we\ntake a step back, and ask: \"Why should one care about continual learning in the\nfirst place?\". We set the stage by examining recent continual learning papers\npublished at four major machine learning conferences, and show that\nmemory-constrained settings dominate the field. Then, we discuss five open\nproblems in machine learning, and even though they might seem unrelated to\ncontinual learning at first sight, we show that continual learning will\ninevitably be part of their solution. These problems are model editing,\npersonalization and specialization, on-device learning, faster (re-)training\nand reinforcement learning. Finally, by comparing the desiderata from these\nunsolved problems and the current assumptions in continual learning, we\nhighlight and discuss four future directions for continual learning research.\nWe hope that this work offers an interesting perspective on the future of\ncontinual learning, while displaying its potential value and the paths we have\nto pursue in order to make it successful. This work is the result of the many\ndiscussions the authors had at the Dagstuhl seminar on Deep Continual Learning,\nin March 2023.\n","authors":["Eli Verwimp","Rahaf Aljundi","Shai Ben-David","Matthias Bethge","Andrea Cossu","Alexander Gepperth","Tyler L. Hayes","Eyke Hüllermeier","Christopher Kanan","Dhireesha Kudithipudi","Christoph H. Lampert","Martin Mundt","Razvan Pascanu","Adrian Popescu","Andreas S. Tolias","Joost van de Weijer","Bing Liu","Vincenzo Lomonaco","Tinne Tuytelaars","Gido M. van de Ven"],"pdf_url":"https://arxiv.org/pdf/2311.11908v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04190v2","updated":"2024-03-28T12:56:25Z","published":"2023-10-06T12:09:09Z","title":"On the Two Sides of Redundancy in Graph Neural Networks","summary":"  Message passing neural networks iteratively generate node embeddings by\naggregating information from neighboring nodes. With increasing depth,\ninformation from more distant nodes is included. However, node embeddings may\nbe unable to represent the growing node neighborhoods accurately and the\ninfluence of distant nodes may vanish, a problem referred to as oversquashing.\nInformation redundancy in message passing, i.e., the repetitive exchange and\nencoding of identical information amplifies oversquashing. We develop a novel\naggregation scheme based on neighborhood trees, which allows for controlling\nredundancy by pruning redundant branches of unfolding trees underlying standard\nmessage passing. While the regular structure of unfolding trees allows the\nreuse of intermediate results in a straightforward way, the use of neighborhood\ntrees poses computational challenges. We propose compact representations of\nneighborhood trees and merge them, exploiting computational redundancy by\nidentifying isomorphic subtrees. From this, node and graph embeddings are\ncomputed via a neural architecture inspired by tree canonization techniques.\nOur method is less susceptible to oversquashing than traditional message\npassing neural networks and can improve the accuracy on widely used benchmark\ndatasets.\n","authors":["Franka Bause","Samir Moustafa","Johannes Langguth","Wilfried N. Gansterer","Nils M. Kriege"],"pdf_url":"https://arxiv.org/pdf/2310.04190v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19381v1","updated":"2024-03-28T12:42:25Z","published":"2024-03-28T12:42:25Z","title":"On Uncertainty Quantification for Near-Bayes Optimal Algorithms","summary":"  Bayesian modelling allows for the quantification of predictive uncertainty\nwhich is crucial in safety-critical applications. Yet for many machine learning\n(ML) algorithms, it is difficult to construct or implement their Bayesian\ncounterpart. In this work we present a promising approach to address this\nchallenge, based on the hypothesis that commonly used ML algorithms are\nefficient across a wide variety of tasks and may thus be near Bayes-optimal\nw.r.t. an unknown task distribution. We prove that it is possible to recover\nthe Bayesian posterior defined by the task distribution, which is unknown but\noptimal in this setting, by building a martingale posterior using the\nalgorithm. We further propose a practical uncertainty quantification method\nthat apply to general ML algorithms. Experiments based on a variety of non-NN\nand NN algorithms demonstrate the efficacy of our method.\n","authors":["Ziyu Wang","Chris Holmes"],"pdf_url":"https://arxiv.org/pdf/2403.19381v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19355v1","updated":"2024-03-28T12:11:29Z","published":"2024-03-28T12:11:29Z","title":"Artificial Intelligence (AI) Based Prediction of Mortality, for COVID-19\n  Patients","summary":"  For severely affected COVID-19 patients, it is crucial to identify high-risk\npatients and predict survival and need for intensive care (ICU). Most of the\nproposed models are not well reported making them less reproducible and prone\nto high risk of bias particularly in presence of imbalance data/class. In this\nstudy, the performances of nine machine and deep learning algorithms in\ncombination with two widely used feature selection methods were investigated to\npredict last status representing mortality, ICU requirement, and ventilation\ndays. Fivefold cross-validation was used for training and validation purposes.\nTo minimize bias, the training and testing sets were split maintaining similar\ndistributions. Only 10 out of 122 features were found to be useful in\nprediction modelling with Acute kidney injury during hospitalization feature\nbeing the most important one. The algorithms performances depend on feature\nnumbers and data pre-processing techniques. LSTM performs the best in\npredicting last status and ICU requirement with 90%, 92%, 86% and 95% accuracy,\nsensitivity, specificity, and AUC respectively. DNN performs the best in\npredicting Ventilation days with 88% accuracy. Considering all the factors and\nlimitations including absence of exact time point of clinical onset, LSTM with\ncarefully selected features can accurately predict last status and ICU\nrequirement. DNN performs the best in predicting Ventilation days. Appropriate\nmachine learning algorithm with carefully selected features and balance data\ncan accurately predict mortality, ICU requirement and ventilation support. Such\nmodel can be very useful in emergency and pandemic where prompt and precise\n","authors":["Mahbubunnabi Tamala","Mohammad Marufur Rahmanb","Maryam Alhasimc","Mobarak Al Mulhimd","Mohamed Derichee"],"pdf_url":"https://arxiv.org/pdf/2403.19355v1.pdf","comment":"Submitted to Biocybernetics and Biomedical Engineering, 22 March,\n  2024"},{"id":"http://arxiv.org/abs/2403.19339v1","updated":"2024-03-28T11:57:06Z","published":"2024-03-28T11:57:06Z","title":"An Interactive Human-Machine Learning Interface for Collecting and\n  Learning from Complex Annotations","summary":"  Human-Computer Interaction has been shown to lead to improvements in machine\nlearning systems by boosting model performance, accelerating learning and\nbuilding user confidence. In this work, we aim to alleviate the expectation\nthat human annotators adapt to the constraints imposed by traditional labels by\nallowing for extra flexibility in the form that supervision information is\ncollected. For this, we propose a human-machine learning interface for binary\nclassification tasks which enables human annotators to utilise counterfactual\nexamples to complement standard binary labels as annotations for a dataset.\nFinally we discuss the challenges in future extensions of this work.\n","authors":["Jonathan Erskine","Matt Clifford","Alexander Hepburn","Raúl Santos-Rodríguez"],"pdf_url":"https://arxiv.org/pdf/2403.19339v1.pdf","comment":"4 pages, 2 figures, Submitted to IJCAI 2024 Demonstration Track"},{"id":"http://arxiv.org/abs/2403.04260v2","updated":"2024-03-28T11:55:32Z","published":"2024-03-07T06:49:37Z","title":"Can Small Language Models be Good Reasoners for Sequential\n  Recommendation?","summary":"  Large language models (LLMs) open up new horizons for sequential\nrecommendations, owing to their remarkable language comprehension and\ngeneration capabilities. However, there are still numerous challenges that\nshould be addressed to successfully implement sequential recommendations\nempowered by LLMs. Firstly, user behavior patterns are often complex, and\nrelying solely on one-step reasoning from LLMs may lead to incorrect or\ntask-irrelevant responses. Secondly, the prohibitively resource requirements of\nLLM (e.g., ChatGPT-175B) are overwhelmingly high and impractical for real\nsequential recommender systems. In this paper, we propose a novel Step-by-step\nknowLedge dIstillation fraMework for recommendation (SLIM), paving a promising\npath for sequential recommenders to enjoy the exceptional reasoning\ncapabilities of LLMs in a \"slim\" (i.e., resource-efficient) manner. We\nintroduce CoT prompting based on user behavior sequences for the larger teacher\nmodel. The rationales generated by the teacher model are then utilized as\nlabels to distill the downstream smaller student model (e.g., LLaMA2-7B). In\nthis way, the student model acquires the step-by-step reasoning capabilities in\nrecommendation tasks. We encode the generated rationales from the student model\ninto a dense vector, which empowers recommendation in both ID-based and\nID-agnostic scenarios. Extensive experiments demonstrate the effectiveness of\nSLIM over state-of-the-art baselines, and further analysis showcasing its\nability to generate meaningful recommendation reasoning at affordable costs.\n","authors":["Yuling Wang","Changxin Tian","Binbin Hu","Yanhua Yu","Ziqi Liu","Zhiqiang Zhang","Jun Zhou","Liang Pang","Xiao Wang"],"pdf_url":"https://arxiv.org/pdf/2403.04260v2.pdf","comment":"Accepted by TheWebConf (WWW) 2024"},{"id":"http://arxiv.org/abs/2302.03357v3","updated":"2024-03-28T11:54:32Z","published":"2023-02-07T10:02:05Z","title":"Towards Enhancing Time Series Contrastive Learning: A Dynamic Bad Pair\n  Mining Approach","summary":"  Not all positive pairs are beneficial to time series contrastive learning. In\nthis paper, we study two types of bad positive pairs that can impair the\nquality of time series representation learned through contrastive learning: the\nnoisy positive pair and the faulty positive pair. We observe that, with the\npresence of noisy positive pairs, the model tends to simply learn the pattern\nof noise (Noisy Alignment). Meanwhile, when faulty positive pairs arise, the\nmodel wastes considerable amount of effort aligning non-representative patterns\n(Faulty Alignment). To address this problem, we propose a Dynamic Bad Pair\nMining (DBPM) algorithm, which reliably identifies and suppresses bad positive\npairs in time series contrastive learning. Specifically, DBPM utilizes a memory\nmodule to dynamically track the training behavior of each positive pair along\ntraining process. This allows us to identify potential bad positive pairs at\neach epoch based on their historical training behaviors. The identified bad\npairs are subsequently down-weighted through a transformation module, thereby\nmitigating their negative impact on the representation learning process. DBPM\nis a simple algorithm designed as a lightweight plug-in without learnable\nparameters to enhance the performance of existing state-of-the-art methods.\nThrough extensive experiments conducted on four large-scale, real-world time\nseries datasets, we demonstrate DBPM's efficacy in mitigating the adverse\neffects of bad positive pairs.\n","authors":["Xiang Lan","Hanshu Yan","Shenda Hong","Mengling Feng"],"pdf_url":"https://arxiv.org/pdf/2302.03357v3.pdf","comment":"ICLR 2024 Camera Ready (https://openreview.net/pdf?id=K2c04ulKXn)"},{"id":"http://arxiv.org/abs/2403.12847v3","updated":"2024-03-28T11:46:02Z","published":"2024-03-19T15:54:38Z","title":"Policy Bifurcation in Safe Reinforcement Learning","summary":"  Safe reinforcement learning (RL) offers advanced solutions to constrained\noptimal control problems. Existing studies in safe RL implicitly assume\ncontinuity in policy functions, where policies map states to actions in a\nsmooth, uninterrupted manner; however, our research finds that in some\nscenarios, the feasible policy should be discontinuous or multi-valued,\ninterpolating between discontinuous local optima can inevitably lead to\nconstraint violations. We are the first to identify the generating mechanism of\nsuch a phenomenon, and employ topological analysis to rigorously prove the\nexistence of policy bifurcation in safe RL, which corresponds to the\ncontractibility of the reachable tuple. Our theorem reveals that in scenarios\nwhere the obstacle-free state space is non-simply connected, a feasible policy\nis required to be bifurcated, meaning its output action needs to change\nabruptly in response to the varying state. To train such a bifurcated policy,\nwe propose a safe RL algorithm called multimodal policy optimization (MUPO),\nwhich utilizes a Gaussian mixture distribution as the policy output. The\nbifurcated behavior can be achieved by selecting the Gaussian component with\nthe highest mixing coefficient. Besides, MUPO also integrates spectral\nnormalization and forward KL divergence to enhance the policy's capability of\nexploring different modes. Experiments with vehicle control tasks show that our\nalgorithm successfully learns the bifurcated policy and ensures satisfying\nsafety, while a continuous policy suffers from inevitable constraint\nviolations.\n","authors":["Wenjun Zou","Yao Lyu","Jie Li","Yujie Yang","Shengbo Eben Li","Jingliang Duan","Xianyuan Zhan","Jingjing Liu","Yaqin Zhang","Keqiang Li"],"pdf_url":"https://arxiv.org/pdf/2403.12847v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16451v4","updated":"2024-03-28T11:36:06Z","published":"2024-03-25T06:30:54Z","title":"DeepMachining: Online Prediction of Machining Errors of Lathe Machines","summary":"  We describe DeepMachining, a deep learning-based AI system for online\nprediction of machining errors of lathe machine operations. We have built and\nevaluated DeepMachining based on manufacturing data from factories.\nSpecifically, we first pretrain a deep learning model for a given lathe\nmachine's operations to learn the salient features of machining states. Then,\nwe fine-tune the pretrained model to adapt to specific machining tasks. We\ndemonstrate that DeepMachining achieves high prediction accuracy for multiple\ntasks that involve different workpieces and cutting tools. To the best of our\nknowledge, this work is one of the first factory experiments using pre-trained\ndeep-learning models to predict machining errors of lathe machines.\n","authors":["Xiang-Li Lu","Hwai-Jung Hsu","Che-Wei Chou","H. T. Kung","Chen-Hsin Lee","Sheng-Mao Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.16451v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19326v1","updated":"2024-03-28T11:33:02Z","published":"2024-03-28T11:33:02Z","title":"MedBN: Robust Test-Time Adaptation against Malicious Test Samples","summary":"  Test-time adaptation (TTA) has emerged as a promising solution to address\nperformance decay due to unforeseen distribution shifts between training and\ntest data. While recent TTA methods excel in adapting to test data variations,\nsuch adaptability exposes a model to vulnerability against malicious examples,\nan aspect that has received limited attention. Previous studies have uncovered\nsecurity vulnerabilities within TTA even when a small proportion of the test\nbatch is maliciously manipulated. In response to the emerging threat, we\npropose median batch normalization (MedBN), leveraging the robustness of the\nmedian for statistics estimation within the batch normalization layer during\ntest-time inference. Our method is algorithm-agnostic, thus allowing seamless\nintegration with existing TTA frameworks. Our experimental results on benchmark\ndatasets, including CIFAR10-C, CIFAR100-C and ImageNet-C, consistently\ndemonstrate that MedBN outperforms existing approaches in maintaining robust\nperformance across different attack scenarios, encompassing both instant and\ncumulative attacks. Through extensive experiments, we show that our approach\nsustains the performance even in the absence of attacks, achieving a practical\nbalance between robustness and performance.\n","authors":["Hyejin Park","Jeongyeon Hwang","Sunung Mun","Sangdon Park","Jungseul Ok"],"pdf_url":"https://arxiv.org/pdf/2403.19326v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.19316v1","updated":"2024-03-28T11:17:00Z","published":"2024-03-28T11:17:00Z","title":"Hypergraph-based Multi-View Action Recognition using Event Cameras","summary":"  Action recognition from video data forms a cornerstone with wide-ranging\napplications. Single-view action recognition faces limitations due to its\nreliance on a single viewpoint. In contrast, multi-view approaches capture\ncomplementary information from various viewpoints for improved accuracy.\nRecently, event cameras have emerged as innovative bio-inspired sensors,\nleading to advancements in event-based action recognition. However, existing\nworks predominantly focus on single-view scenarios, leaving a gap in multi-view\nevent data exploitation, particularly in challenges like information deficit\nand semantic misalignment. To bridge this gap, we introduce HyperMV, a\nmulti-view event-based action recognition framework. HyperMV converts discrete\nevent data into frame-like representations and extracts view-related features\nusing a shared convolutional network. By treating segments as vertices and\nconstructing hyperedges using rule-based and KNN-based strategies, a multi-view\nhypergraph neural network that captures relationships across viewpoint and\ntemporal features is established. The vertex attention hypergraph propagation\nis also introduced for enhanced feature fusion. To prompt research in this\narea, we present the largest multi-view event-based action dataset\n$\\text{THU}^{\\text{MV-EACT}}\\text{-50}$, comprising 50 actions from 6\nviewpoints, which surpasses existing datasets by over tenfold. Experimental\nresults show that HyperMV significantly outperforms baselines in both\ncross-subject and cross-view scenarios, and also exceeds the state-of-the-arts\nin frame-based multi-view action recognition.\n","authors":["Yue Gao","Jiaxuan Lu","Siqi Li","Yipeng Li","Shaoyi Du"],"pdf_url":"https://arxiv.org/pdf/2403.19316v1.pdf","comment":"Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI 2024)"},{"id":"http://arxiv.org/abs/2311.06958v2","updated":"2024-03-28T11:13:33Z","published":"2023-11-12T20:52:14Z","title":"Towards probabilistic Weather Forecasting with Conditioned\n  Spatio-Temporal Normalizing Flows","summary":"  Generative normalizing flows are able to model multimodal spatial\ndistributions, and they have been shown to model temporal correlations\nsuccessfully as well. These models provide several benefits over other types of\ngenerative models due to their training stability, invertibility and efficiency\nin sampling and inference. This makes them a suitable candidate for stochastic\nspatio-temporal prediction problems, which are omnipresent in many fields of\nsciences, such as earth sciences, astrophysics or molecular sciences. In this\npaper, we present conditional normalizing flows for stochastic spatio-temporal\nmodelling. The method is evaluated on the task of daily temperature and hourly\ngeopotential map prediction from ERA5 datasets. Experiments show that our\nmethod is able to capture spatio-temporal correlations and extrapolates well\nbeyond the time horizon used during training.\n","authors":["Christina Winkler"],"pdf_url":"https://arxiv.org/pdf/2311.06958v2.pdf","comment":"Wrong version, will upload a new one"},{"id":"http://arxiv.org/abs/2403.18025v2","updated":"2024-03-28T11:01:21Z","published":"2024-03-26T18:23:16Z","title":"Improving Pre-trained Language Model Sensitivity via Mask Specific\n  losses: A case study on Biomedical NER","summary":"  Adapting language models (LMs) to novel domains is often achieved through\nfine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning\nintroduces new knowledge into an LM, enabling it to comprehend and efficiently\nperform a target domain task. Fine-tuning can however be inadvertently\ninsensitive if it ignores the wide array of disparities (e.g in word meaning)\nbetween source and target domains. For instance, words such as chronic and\npressure may be treated lightly in social conversations, however, clinically,\nthese words are usually an expression of concern. To address insensitive\nfine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach\nthat efficiently acquires target domain knowledge by appropriately weighting\nthe importance of domain-specific terms (DS-terms) during fine-tuning. MSLM\njointly masks DS-terms and generic words, then learns mask-specific losses by\nensuring LMs incur larger penalties for inaccurately predicting DS-terms\ncompared to generic words. Results of our analysis show that MSLM improves LMs\nsensitivity and detection of DS-terms. We empirically show that an optimal\nmasking rate not only depends on the LM, but also on the dataset and the length\nof sequences. Our proposed masking strategy outperforms advanced masking\nstrategies such as span- and PMI-based masking.\n","authors":["Micheal Abaho","Danushka Bollegala","Gary Leeming","Dan Joyce","Iain E Buchan"],"pdf_url":"https://arxiv.org/pdf/2403.18025v2.pdf","comment":"Paper alrerady accepted for publishing by the NAACL 2024 conference\n  (main conference paper)"},{"id":"http://arxiv.org/abs/2309.17053v3","updated":"2024-03-28T11:00:52Z","published":"2023-09-29T08:26:44Z","title":"On the Power of the Weisfeiler-Leman Test for Graph Motif Parameters","summary":"  Seminal research in the field of graph neural networks (GNNs) has revealed a\ndirect correspondence between the expressive capabilities of GNNs and the\n$k$-dimensional Weisfeiler-Leman ($k$WL) test, a widely-recognized method for\nverifying graph isomorphism. This connection has reignited interest in\ncomprehending the specific graph properties effectively distinguishable by the\n$k$WL test. A central focus of research in this field revolves around\ndetermining the least dimensionality $k$, for which $k$WL can discern graphs\nwith different number of occurrences of a pattern graph $P$. We refer to such a\nleast $k$ as the WL-dimension of this pattern counting problem. This inquiry\ntraditionally delves into two distinct counting problems related to patterns:\nsubgraph counting and induced subgraph counting. Intriguingly, despite their\ninitial appearance as separate challenges with seemingly divergent approaches,\nboth of these problems are interconnected components of a more comprehensive\nproblem: \"graph motif parameters\". In this paper, we provide a precise\ncharacterization of the WL-dimension of labeled graph motif parameters. As\nspecific instances of this result, we obtain characterizations of the\nWL-dimension of the subgraph counting and induced subgraph counting problem for\nevery labeled pattern $P$. We additionally demonstrate that in cases where the\n$k$WL test distinguishes between graphs with varying occurrences of a pattern\n$P$, the exact number of occurrences of $P$ can be computed uniformly using\nonly local information of the last layer of a corresponding GNN. We finally\ndelve into the challenge of recognizing the WL-dimension of various graph\nparameters. We give a polynomial time algorithm for determining the\nWL-dimension of the subgraph counting problem for given pattern $P$, answering\nan open question from previous work.\n","authors":["Matthias Lanzinger","Pablo Barceló"],"pdf_url":"https://arxiv.org/pdf/2309.17053v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.01579v3","updated":"2024-03-28T10:53:54Z","published":"2022-11-03T04:19:27Z","title":"Data-free Defense of Black Box Models Against Adversarial Attacks","summary":"  Several companies often safeguard their trained deep models (i.e., details of\narchitecture, learnt weights, training details etc.) from third-party users by\nexposing them only as black boxes through APIs. Moreover, they may not even\nprovide access to the training data due to proprietary reasons or sensitivity\nconcerns. In this work, we propose a novel defense mechanism for black box\nmodels against adversarial attacks in a data-free set up. We construct\nsynthetic data via generative model and train surrogate network using model\nstealing techniques. To minimize adversarial contamination on perturbed\nsamples, we propose 'wavelet noise remover' (WNR) that performs discrete\nwavelet decomposition on input images and carefully select only a few important\ncoefficients determined by our 'wavelet coefficient selection module' (WCSM).\nTo recover the high-frequency content of the image after noise removal via WNR,\nwe further train a 'regenerator' network with an objective to retrieve the\ncoefficients such that the reconstructed image yields similar to original\npredictions on the surrogate model. At test time, WNR combined with trained\nregenerator network is prepended to the black box network, resulting in a high\nboost in adversarial accuracy. Our method improves the adversarial accuracy on\nCIFAR-10 by 38.98% and 32.01% on state-of-the-art Auto Attack compared to\nbaseline, even when the attacker uses surrogate architecture (Alexnet-half and\nAlexnet) similar to the black box architecture (Alexnet) with same model\nstealing strategy as defender. The code is available at\nhttps://github.com/vcl-iisc/data-free-black-box-defense\n","authors":["Gaurav Kumar Nayak","Inder Khatri","Ruchit Rawal","Anirban Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2211.01579v3.pdf","comment":"CVPR Workshop (Under Review)"},{"id":"http://arxiv.org/abs/2403.19294v1","updated":"2024-03-28T10:31:23Z","published":"2024-03-28T10:31:23Z","title":"FlowDepth: Decoupling Optical Flow for Self-Supervised Monocular Depth\n  Estimation","summary":"  Self-supervised multi-frame methods have currently achieved promising results\nin depth estimation. However, these methods often suffer from mismatch problems\ndue to the moving objects, which break the static assumption. Additionally,\nunfairness can occur when calculating photometric errors in high-freq or\nlow-texture regions of the images. To address these issues, existing approaches\nuse additional semantic priori black-box networks to separate moving objects\nand improve the model only at the loss level. Therefore, we propose FlowDepth,\nwhere a Dynamic Motion Flow Module (DMFM) decouples the optical flow by a\nmechanism-based approach and warps the dynamic regions thus solving the\nmismatch problem. For the unfairness of photometric errors caused by high-freq\nand low-texture regions, we use Depth-Cue-Aware Blur (DCABlur) and Cost-Volume\nsparsity loss respectively at the input and the loss level to solve the\nproblem. Experimental results on the KITTI and Cityscapes datasets show that\nour method outperforms the state-of-the-art methods.\n","authors":["Yiyang Sun","Zhiyuan Xu","Xiaonian Wang","Jing Yao"],"pdf_url":"https://arxiv.org/pdf/2403.19294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04830v2","updated":"2024-03-28T10:30:57Z","published":"2023-11-08T16:56:16Z","title":"Real-Time Recurrent Reinforcement Learning","summary":"  In this paper we propose real-time recurrent reinforcement learning (RTRRL),\na biologically plausible approach to solving discrete and continuous control\ntasks in partially-observable markov decision processes (POMDPs). RTRRL\nconsists of three parts: (1) a Meta-RL RNN architecture, implementing on its\nown an actor-critic algorithm; (2) an outer reinforcement learning algorithm,\nexploiting temporal difference learning and dutch eligibility traces to train\nthe Meta-RL network; and (3) random-feedback local-online (RFLO) learning, an\nonline automatic differentiation algorithm for computing the gradients with\nrespect to parameters of the network.Our experimental results show that by\nreplacing the optimization algorithm in RTRRL with the biologically implausible\nback propagation through time (BPTT), or real-time recurrent learning (RTRL),\none does not improve returns, while matching the computational complexity for\nBPTT, and even increasing complexity for RTRL. RTRRL thus serves as a model of\nlearning in biological neural networks, mimicking reward pathways in the basal\nganglia.\n","authors":["Julian Lemmel","Radu Grosu"],"pdf_url":"https://arxiv.org/pdf/2311.04830v2.pdf","comment":"14 pages, 9 figures, includes Appendix"},{"id":"http://arxiv.org/abs/2403.18018v2","updated":"2024-03-28T10:19:46Z","published":"2024-03-26T18:07:10Z","title":"DORE: A Dataset For Portuguese Definition Generation","summary":"  Definition modelling (DM) is the task of automatically generating a\ndictionary definition for a specific word. Computational systems that are\ncapable of DM can have numerous applications benefiting a wide range of\naudiences. As DM is considered a supervised natural language generation\nproblem, these systems require large annotated datasets to train the machine\nlearning (ML) models. Several DM datasets have been released for English and\nother high-resource languages. While Portuguese is considered a\nmid/high-resource language in most natural language processing tasks and is\nspoken by more than 200 million native speakers, there is no DM dataset\navailable for Portuguese. In this research, we fill this gap by introducing\nDORE; the first dataset for Definition MOdelling for PoRtuguEse containing more\nthan 100,000 definitions. We also evaluate several deep learning based DM\nmodels on DORE and report the results. The dataset and the findings of this\npaper will facilitate research and study of Portuguese in wider contexts.\n","authors":["Anna Beatriz Dimas Furtado","Tharindu Ranasinghe","Frédéric Blain","Ruslan Mitkov"],"pdf_url":"https://arxiv.org/pdf/2403.18018v2.pdf","comment":"Accepted to LREC-COLING 2024 (The 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation)"},{"id":"http://arxiv.org/abs/2403.19289v1","updated":"2024-03-28T10:19:36Z","published":"2024-03-28T10:19:36Z","title":"Graph Neural Networks for Treatment Effect Prediction","summary":"  Estimating causal effects in e-commerce tends to involve costly treatment\nassignments which can be impractical in large-scale settings. Leveraging\nmachine learning to predict such treatment effects without actual intervention\nis a standard practice to diminish the risk. However, existing methods for\ntreatment effect prediction tend to rely on training sets of substantial size,\nwhich are built from real experiments and are thus inherently risky to create.\nIn this work we propose a graph neural network to diminish the required\ntraining set size, relying on graphs that are common in e-commerce data.\nSpecifically, we view the problem as node regression with a restricted number\nof labeled instances, develop a two-model neural architecture akin to previous\ncausal effect estimators, and test varying message-passing layers for encoding.\nFurthermore, as an extra step, we combine the model with an acquisition\nfunction to guide the creation of the training set in settings with extremely\nlow experimental budget. The framework is flexible since each step can be used\nseparately with other models or policies. The experiments on real large-scale\nnetworks indicate a clear advantage of our methodology over the state of the\nart, which in many cases performs close to random underlining the need for\nmodels that can generalize with limited labeled samples to reduce experimental\nrisks.\n","authors":["George Panagopoulos","Daniele Malitesta","Fragkiskos D. Malliaros","Jun Pang"],"pdf_url":"https://arxiv.org/pdf/2403.19289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19279v1","updated":"2024-03-28T10:02:10Z","published":"2024-03-28T10:02:10Z","title":"Fine-Tuning Language Models with Reward Learning on Policy","summary":"  Reinforcement learning from human feedback (RLHF) has emerged as an effective\napproach to aligning large language models (LLMs) to human preferences. RLHF\ncontains three steps, i.e., human preference collecting, reward learning, and\npolicy optimization, which are usually performed serially. Despite its\npopularity, however, (fixed) reward models may suffer from inaccurate\noff-distribution, since policy optimization continuously shifts LLMs' data\ndistribution. Repeatedly collecting new preference data from the latest LLMs\nmay alleviate this issue, which unfortunately makes the resulting system more\ncomplicated and difficult to optimize. In this paper, we propose reward\nlearning on policy (RLP), an unsupervised framework that refines a reward model\nusing policy samples to keep it on-distribution. Specifically, an unsupervised\nmulti-view learning method is introduced to learn robust representations of\npolicy samples. Meanwhile, a synthetic preference generation approach is\ndeveloped to simulate high-quality preference data with policy outputs.\nExtensive experiments on three benchmark datasets show that RLP consistently\noutperforms the state-of-the-art. Our code is available at\n\\url{https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/rlp}.\n","authors":["Hao Lang","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2403.19279v1.pdf","comment":"NAACL2024 Main Track Long Paper"},{"id":"http://arxiv.org/abs/2403.19273v1","updated":"2024-03-28T09:57:50Z","published":"2024-03-28T09:57:50Z","title":"A Machine Learning Approach for Crop Yield and Disease Prediction\n  Integrating Soil Nutrition and Weather Factors","summary":"  The development of an intelligent agricultural decision-supporting system for\ncrop selection and disease forecasting in Bangladesh is the main objective of\nthis work. The economy of the nation depends heavily on agriculture. However,\nchoosing crops with better production rates and efficiently controlling crop\ndisease are obstacles that farmers have to face. These issues are addressed in\nthis research by utilizing machine learning methods and real-world datasets.\nThe recommended approach uses a variety of datasets on the production of crops,\nsoil conditions, agro-meteorological regions, crop disease, and meteorological\nfactors. These datasets offer insightful information on disease trends, soil\nnutrition demand of crops, and agricultural production history. By\nincorporating this knowledge, the model first recommends the list of primarily\nselected crops based on the soil nutrition of a particular user location. Then\nthe predictions of meteorological variables like temperature, rainfall, and\nhumidity are made using SARIMAX models. These weather predictions are then used\nto forecast the possibilities of diseases for the primary crops list by\nutilizing the support vector classifier. Finally, the developed model makes use\nof the decision tree regression model to forecast crop yield and provides a\nfinal crop list along with associated possible disease forecast. Utilizing the\noutcome of the model, farmers may choose the best productive crops as well as\nprevent crop diseases and reduce output losses by taking preventive actions.\nConsequently, planning and decision-making processes are supported and farmers\ncan predict possible crop yields. Overall, by offering a detailed decision\nsupport system for crop selection and disease prediction, this work can play a\nvital role in advancing agricultural practices in Bangladesh.\n","authors":["Forkan Uddin Ahmed","Annesha Das","Md Zubair"],"pdf_url":"https://arxiv.org/pdf/2403.19273v1.pdf","comment":"This paper was presented to the IEEE conference, \"2024 International\n  Conference on Advances in Computing, Communication, Electrical, and Smart\n  Systems (iCACCESS), 8-9 March, Dhaka, Bangladesh\""},{"id":"http://arxiv.org/abs/2403.18703v2","updated":"2024-03-28T09:44:06Z","published":"2024-03-27T15:52:54Z","title":"FPGA-Based Neural Thrust Controller for UAVs","summary":"  The advent of unmanned aerial vehicles (UAVs) has improved a variety of\nfields by providing a versatile, cost-effective and accessible platform for\nimplementing state-of-the-art algorithms. To accomplish a broader range of\ntasks, there is a growing need for enhanced on-board computing to cope with\nincreasing complexity and dynamic environmental conditions. Recent advances\nhave seen the application of Deep Neural Networks (DNNs), particularly in\ncombination with Reinforcement Learning (RL), to improve the adaptability and\nperformance of UAVs, especially in unknown environments. However, the\ncomputational requirements of DNNs pose a challenge to the limited computing\nresources available on many UAVs. This work explores the use of Field\nProgrammable Gate Arrays (FPGAs) as a viable solution to this challenge,\noffering flexibility, high performance, energy and time efficiency. We propose\na novel hardware board equipped with an Artix-7 FPGA for a popular open-source\nmicro-UAV platform. We successfully validate its functionality by implementing\nan RL-based low-level controller using real-world experiments.\n","authors":["Sharif Azem","David Scheunert","Mengguang Li","Jonas Gehrunger","Kai Cui","Christian Hochberger","Heinz Koeppl"],"pdf_url":"https://arxiv.org/pdf/2403.18703v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19262v1","updated":"2024-03-28T09:36:55Z","published":"2024-03-28T09:36:55Z","title":"Removing the need for ground truth UWB data collection: self-supervised\n  ranging error correction using deep reinforcement learning","summary":"  Indoor positioning using UWB technology has gained interest due to its\ncentimeter-level accuracy potential. However, multipath effects and\nnon-line-of-sight conditions cause ranging errors between anchors and tags.\nExisting approaches for mitigating these ranging errors rely on collecting\nlarge labeled datasets, making them impractical for real-world deployments.\nThis paper proposes a novel self-supervised deep reinforcement learning\napproach that does not require labeled ground truth data. A reinforcement\nlearning agent uses the channel impulse response as a state and predicts\ncorrections to minimize the error between corrected and estimated ranges. The\nagent learns, self-supervised, by iteratively improving corrections that are\ngenerated by combining the predictability of trajectories with filtering and\nsmoothening. Experiments on real-world UWB measurements demonstrate comparable\nperformance to state-of-the-art supervised methods, overcoming data dependency\nand lack of generalizability limitations. This makes self-supervised deep\nreinforcement learning a promising solution for practical and scalable\nUWB-ranging error correction.\n","authors":["Dieter Coppens","Ben Van Herbruggen","Adnan Shahid","Eli De Poorter"],"pdf_url":"https://arxiv.org/pdf/2403.19262v1.pdf","comment":"11 pages, 8 figures and 4 tables"},{"id":"http://arxiv.org/abs/2212.00851v2","updated":"2024-03-28T09:25:21Z","published":"2022-12-01T20:18:21Z","title":"SOLD: Sinhala Offensive Language Dataset","summary":"  The widespread of offensive content online, such as hate speech and\ncyber-bullying, is a global phenomenon. This has sparked interest in the\nartificial intelligence (AI) and natural language processing (NLP) communities,\nmotivating the development of various systems trained to detect potentially\nharmful content automatically. These systems require annotated datasets to\ntrain the machine learning (ML) models. However, with a few notable exceptions,\nmost datasets on this topic have dealt with English and a few other\nhigh-resource languages. As a result, the research in offensive language\nidentification has been limited to these languages. This paper addresses this\ngap by tackling offensive language identification in Sinhala, a low-resource\nIndo-Aryan language spoken by over 17 million people in Sri Lanka. We introduce\nthe Sinhala Offensive Language Dataset (SOLD) and present multiple experiments\non this dataset. SOLD is a manually annotated dataset containing 10,000 posts\nfrom Twitter annotated as offensive and not offensive at both sentence-level\nand token-level, improving the explainability of the ML models. SOLD is the\nfirst large publicly available offensive language dataset compiled for Sinhala.\nWe also introduce SemiSOLD, a larger dataset containing more than 145,000\nSinhala tweets, annotated following a semi-supervised approach.\n","authors":["Tharindu Ranasinghe","Isuri Anuradha","Damith Premasiri","Kanishka Silva","Hansi Hettiarachchi","Lasitha Uyangodage","Marcos Zampieri"],"pdf_url":"https://arxiv.org/pdf/2212.00851v2.pdf","comment":"Accepted to Language Resources and Evaluation, Springer"},{"id":"http://arxiv.org/abs/2403.19253v1","updated":"2024-03-28T09:20:15Z","published":"2024-03-28T09:20:15Z","title":"Inferring Latent Temporal Sparse Coordination Graph for Multi-Agent\n  Reinforcement Learning","summary":"  Effective agent coordination is crucial in cooperative Multi-Agent\nReinforcement Learning (MARL). While agent cooperation can be represented by\ngraph structures, prevailing graph learning methods in MARL are limited. They\nrely solely on one-step observations, neglecting crucial historical\nexperiences, leading to deficient graphs that foster redundant or detrimental\ninformation exchanges. Additionally, high computational demands for action-pair\ncalculations in dense graphs impede scalability. To address these challenges,\nwe propose inferring a Latent Temporal Sparse Coordination Graph (LTS-CG) for\nMARL. The LTS-CG leverages agents' historical observations to calculate an\nagent-pair probability matrix, where a sparse graph is sampled from and used\nfor knowledge exchange between agents, thereby simultaneously capturing agent\ndependencies and relation uncertainty. The computational complexity of this\nprocedure is only related to the number of agents. This graph learning process\nis further augmented by two innovative characteristics: Predict-Future, which\nenables agents to foresee upcoming observations, and Infer-Present, ensuring a\nthorough grasp of the environmental context from limited data. These features\nallow LTS-CG to construct temporal graphs from historical and real-time\ninformation, promoting knowledge exchange during policy learning and effective\ncollaboration. Graph learning and agent training occur simultaneously in an\nend-to-end manner. Our demonstrated results on the StarCraft II benchmark\nunderscore LTS-CG's superior performance.\n","authors":["Wei Duan","Jie Lu","Junyu Xuan"],"pdf_url":"https://arxiv.org/pdf/2403.19253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16105v2","updated":"2024-03-28T09:16:03Z","published":"2024-02-25T15:08:37Z","title":"Informed Meta-Learning","summary":"  In noisy and low-data regimes prevalent in real-world applications, an\noutstanding challenge of machine learning lies in effectively incorporating\ninductive biases that promote data efficiency and robustness. Meta-learning and\ninformed ML stand out as two approaches for incorporating prior knowledge into\nthe ML pipeline. While the former relies on a purely data-driven source of\npriors, the latter is guided by a formal representation of expert knowledge.\nThis paper introduces a novel hybrid paradigm, informed meta-learning, seeking\ncomplementarity in cross-task knowledge sharing of humans and machines. We\nestablish the foundational components of informed meta-learning and present a\nconcrete instantiation of this framework--the Informed Neural Process. Through\na series of illustrative and larger-scale experiments, we demonstrate the\npotential benefits of informed meta-learning in improving data efficiency and\nrobustness to observational noise, task distribution shifts, and heterogeneity.\n","authors":["Katarzyna Kobalczyk","Mihaela van der Schaar"],"pdf_url":"https://arxiv.org/pdf/2402.16105v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10370v2","updated":"2024-03-28T09:12:21Z","published":"2023-12-16T08:08:36Z","title":"Do Similar Entities have Similar Embeddings?","summary":"  Knowledge graph embedding models (KGEMs) developed for link prediction learn\nvector representations for entities in a knowledge graph, known as embeddings.\nA common tacit assumption is the KGE entity similarity assumption, which states\nthat these KGEMs retain the graph's structure within their embedding space,\n\\textit{i.e.}, position similar entities within the graph close to one another.\nThis desirable property make KGEMs widely used in downstream tasks such as\nrecommender systems or drug repurposing. Yet, the relation of entity similarity\nand similarity in the embedding space has rarely been formally evaluated.\nTypically, KGEMs are assessed based on their sole link prediction capabilities,\nusing ranked-based metrics such as Hits@K or Mean Rank. This paper challenges\nthe prevailing assumption that entity similarity in the graph is inherently\nmirrored in the embedding space. Therefore, we conduct extensive experiments to\nmeasure the capability of KGEMs to cluster similar entities together, and\ninvestigate the nature of the underlying factors. Moreover, we study if\ndifferent KGEMs expose a different notion of similarity. Datasets, pre-trained\nembeddings and code are available at:\nhttps://github.com/nicolas-hbt/similar-embeddings/.\n","authors":["Nicolas Hubert","Heiko Paulheim","Armelle Brun","Davy Monticolo"],"pdf_url":"https://arxiv.org/pdf/2312.10370v2.pdf","comment":"Accepted at ESWC 2024"},{"id":"http://arxiv.org/abs/2403.01121v2","updated":"2024-03-28T09:11:27Z","published":"2024-03-02T08:05:03Z","title":"OpenGraph: Towards Open Graph Foundation Models","summary":"  Graph learning has become indispensable for interpreting and harnessing\nrelational data in diverse fields, ranging from recommendation systems to\nsocial network analysis. In this context, a variety of GNNs have emerged as\npromising methodologies for encoding the structural information of graphs. By\neffectively capturing the graph's underlying structure, these GNNs have shown\ngreat potential in enhancing performance in graph learning tasks, such as link\nprediction and node classification. However, despite their successes, a\nsignificant challenge persists: these advanced methods often face difficulties\nin generalizing to unseen graph data that significantly differs from the\ntraining instances. In this work, our aim is to advance the graph learning\nparadigm by developing a general graph foundation model. This model is designed\nto understand the complex topological patterns present in diverse graph data,\nenabling it to excel in zero-shot graph learning tasks across different\ndownstream datasets. To achieve this goal, we address several key technical\nchallenges in our OpenGraph model. Firstly, we propose a unified graph\ntokenizer to adapt our graph model to generalize well on unseen graph data,\neven when the underlying graph properties differ significantly from those\nencountered during training. Secondly, we develop a scalable graph transformer\nas the foundational encoder, which effectively captures node-wise dependencies\nwithin the global topological context. Thirdly, we introduce a data\naugmentation mechanism enhanced by a LLM to alleviate the limitations of data\nscarcity in real-world scenarios. Extensive experiments validate the\neffectiveness of our framework. By adapting our OpenGraph to new graph\ncharacteristics and comprehending the nuances of diverse graphs, our approach\nachieves remarkable zero-shot graph learning performance across various\nsettings and domains.\n","authors":["Lianghao Xia","Ben Kao","Chao Huang"],"pdf_url":"https://arxiv.org/pdf/2403.01121v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19246v1","updated":"2024-03-28T09:06:23Z","published":"2024-03-28T09:06:23Z","title":"MPXGAT: An Attention based Deep Learning Model for Multiplex Graphs\n  Embedding","summary":"  Graph representation learning has rapidly emerged as a pivotal field of\nstudy. Despite its growing popularity, the majority of research has been\nconfined to embedding single-layer graphs, which fall short in representing\ncomplex systems with multifaceted relationships. To bridge this gap, we\nintroduce MPXGAT, an innovative attention-based deep learning model tailored to\nmultiplex graph embedding. Leveraging the robustness of Graph Attention\nNetworks (GATs), MPXGAT captures the structure of multiplex networks by\nharnessing both intra-layer and inter-layer connections. This exploitation\nfacilitates accurate link prediction within and across the network's multiple\nlayers. Our comprehensive experimental evaluation, conducted on various\nbenchmark datasets, confirms that MPXGAT consistently outperforms\nstate-of-the-art competing algorithms.\n","authors":["Marco Bongiovanni","Luca Gallo","Roberto Grasso","Alfredo Pulvirenti"],"pdf_url":"https://arxiv.org/pdf/2403.19246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17458v3","updated":"2024-03-28T09:02:35Z","published":"2024-03-26T07:46:27Z","title":"Expectations Versus Reality: Evaluating Intrusion Detection Systems in\n  Practice","summary":"  Our paper provides empirical comparisons between recent IDSs to provide an\nobjective comparison between them to help users choose the most appropriate\nsolution based on their requirements. Our results show that no one solution is\nthe best, but is dependent on external variables such as the types of attacks,\ncomplexity, and network environment in the dataset. For example, BoT_IoT and\nStratosphere IoT datasets both capture IoT-related attacks, but the deep neural\nnetwork performed the best when tested using the BoT_IoT dataset while HELAD\nperformed the best when tested using the Stratosphere IoT dataset. So although\nwe found that a deep neural network solution had the highest average F1 scores\non tested datasets, it is not always the best-performing one. We further\ndiscuss difficulties in using IDS from literature and project repositories,\nwhich complicated drawing definitive conclusions regarding IDS selection.\n","authors":["Jake Hesford","Daniel Cheng","Alan Wan","Larry Huynh","Seungho Kim","Hyoungshick Kim","Jin B. Hong"],"pdf_url":"https://arxiv.org/pdf/2403.17458v3.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2403.19243v1","updated":"2024-03-28T08:58:20Z","published":"2024-03-28T08:58:20Z","title":"Sine Activated Low-Rank Matrices for Parameter Efficient Learning","summary":"  Low-rank decomposition has emerged as a vital tool for enhancing parameter\nefficiency in neural network architectures, gaining traction across diverse\napplications in machine learning. These techniques significantly lower the\nnumber of parameters, striking a balance between compactness and performance.\nHowever, a common challenge has been the compromise between parameter\nefficiency and the accuracy of the model, where reduced parameters often lead\nto diminished accuracy compared to their full-rank counterparts. In this work,\nwe propose a novel theoretical framework that integrates a sinusoidal function\nwithin the low-rank decomposition process. This approach not only preserves the\nbenefits of the parameter efficiency characteristic of low-rank methods but\nalso increases the decomposition's rank, thereby enhancing model accuracy. Our\nmethod proves to be an adaptable enhancement for existing low-rank models, as\nevidenced by its successful application in Vision Transformers (ViT), Large\nLanguage Models (LLMs), Neural Radiance Fields (NeRF), and 3D shape modeling.\nThis demonstrates the wide-ranging potential and efficiency of our proposed\ntechnique.\n","authors":["Yiping Ji","Hemanth Saratchandran","Cameron Gordon","Zeyu Zhang","Simon Lucey"],"pdf_url":"https://arxiv.org/pdf/2403.19243v1.pdf","comment":"The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2403.19232v1","updated":"2024-03-28T08:44:36Z","published":"2024-03-28T08:44:36Z","title":"AZ-NAS: Assembling Zero-Cost Proxies for Network Architecture Search","summary":"  Training-free network architecture search (NAS) aims to discover\nhigh-performing networks with zero-cost proxies, capturing network\ncharacteristics related to the final performance. However, network rankings\nestimated by previous training-free NAS methods have shown weak correlations\nwith the performance. To address this issue, we propose AZ-NAS, a novel\napproach that leverages the ensemble of various zero-cost proxies to enhance\nthe correlation between a predicted ranking of networks and the ground truth\nsubstantially in terms of the performance. To achieve this, we introduce four\nnovel zero-cost proxies that are complementary to each other, analyzing\ndistinct traits of architectures in the views of expressivity, progressivity,\ntrainability, and complexity. The proxy scores can be obtained simultaneously\nwithin a single forward and backward pass, making an overall NAS process highly\nefficient. In order to integrate the rankings predicted by our proxies\neffectively, we introduce a non-linear ranking aggregation method that\nhighlights the networks highly-ranked consistently across all the proxies.\nExperimental results conclusively demonstrate the efficacy and efficiency of\nAZ-NAS, outperforming state-of-the-art methods on standard benchmarks, all\nwhile maintaining a reasonable runtime cost.\n","authors":["Junghyup Lee","Bumsub Ham"],"pdf_url":"https://arxiv.org/pdf/2403.19232v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2402.14490v2","updated":"2024-03-28T08:36:27Z","published":"2024-02-22T12:27:38Z","title":"Imbalanced Data Clustering using Equilibrium K-Means","summary":"  Traditional centroid-based clustering algorithms, such as hard K-means (HKM,\nor Lloyd's algorithm) and fuzzy K-means (FKM, or Bezdek's algorithm), display\ndegraded performance when true underlying groups of data have varying sizes\n(i.e., imbalanced data). This paper introduces equilibrium K-means (EKM), a\nnovel fuzzy clustering algorithm that has the robustness to imbalanced data by\npreventing centroids from crowding together in the center of large clusters.\nEKM is simple, alternating between two steps; fast, with the same time and\nspace complexity as FKM; and scalable to large datasets. We evaluate the\nperformance of EKM on two synthetic and ten real datasets, comparing it to\nother centroid-based algorithms, including HKM, FKM, maximum-entropy fuzzy\nclustering (MEFC), two FKM variations designed for imbalanced data, and the\nGaussian mixture model. The results show that EKM performs competitively on\nbalanced data and significantly outperforms other algorithms on imbalanced\ndata. Deep clustering experiments on the MNIST dataset demonstrate the\nsignificance of making representation have an EKM-friendly structure when\ndealing with imbalanced data; In comparison to deep clustering with HKM, deep\nclustering with EKM obtains a more discriminative representation and a 35%\nimprovement in clustering accuracy. Additionally, we reformulate HKM, FKM,\nMEFC, and EKM in a general form of gradient descent, where fuzziness is\nintroduced differently and more simply than in Bezdek's work, and demonstrate\nhow the general form facilitates a uniform study of KM algorithms.\n","authors":["Yudong He"],"pdf_url":"https://arxiv.org/pdf/2402.14490v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.10757v2","updated":"2024-03-28T08:26:50Z","published":"2023-08-21T14:43:42Z","title":"To Whom are You Talking? A Deep Learning Model to Endow Social Robots\n  with Addressee Estimation Skills","summary":"  Communicating shapes our social word. For a robot to be considered social and\nbeing consequently integrated in our social environment it is fundamental to\nunderstand some of the dynamics that rule human-human communication. In this\nwork, we tackle the problem of Addressee Estimation, the ability to understand\nan utterance's addressee, by interpreting and exploiting non-verbal bodily cues\nfrom the speaker. We do so by implementing an hybrid deep learning model\ncomposed of convolutional layers and LSTM cells taking as input images\nportraying the face of the speaker and 2D vectors of the speaker's body\nposture. Our implementation choices were guided by the aim to develop a model\nthat could be deployed on social robots and be efficient in ecological\nscenarios. We demonstrate that our model is able to solve the Addressee\nEstimation problem in terms of addressee localisation in space, from a robot\nego-centric point of view.\n","authors":["Carlo Mazzola","Marta Romeo","Francesco Rea","Alessandra Sciutti","Angelo Cangelosi"],"pdf_url":"https://arxiv.org/pdf/2308.10757v2.pdf","comment":"Accepted v. of IJCNN 2023 publication. Funded by the Horizon Europe\n  project TERAIS (G.A. 101079338), the UKRI Node on Trust (EP/V026682/1), the\n  EU projects TRAINCREASE and MUSAE, and the US project THRIVE++. Cite:\n  https://doi.org/10.1109/IJCNN54540.2023.10191452 Code:\n  https://zenodo.org/doi/10.5281/zenodo.10709857 Data:\n  https://zenodo.org/doi/10.5281/zenodo.10711587 10 pages, 8 Figures, 3 Tables"},{"id":"http://arxiv.org/abs/2308.12532v6","updated":"2024-03-28T08:23:02Z","published":"2023-08-24T03:43:02Z","title":"FedSOL: Stabilized Orthogonal Learning with Proximal Restrictions in\n  Federated Learning","summary":"  Federated Learning (FL) aggregates locally trained models from individual\nclients to construct a global model. While FL enables learning a model with\ndata privacy, it often suffers from significant performance degradation when\nclients have heterogeneous data distributions. This data heterogeneity causes\nthe model to forget the global knowledge acquired from previously sampled\nclients after being trained on local datasets. Although the introduction of\nproximal objectives in local updates helps to preserve global knowledge, it can\nalso hinder local learning by interfering with local objectives. To address\nthis problem, we propose a novel method, Federated Stabilized Orthogonal\nLearning (FedSOL), which adopts an orthogonal learning strategy to balance the\ntwo conflicting objectives. FedSOL is designed to identify gradients of local\nobjectives that are inherently orthogonal to directions affecting the proximal\nobjective. Specifically, FedSOL targets parameter regions where learning on the\nlocal objective is minimally influenced by proximal weight perturbations. Our\nexperiments demonstrate that FedSOL consistently achieves state-of-the-art\nperformance across various scenarios.\n","authors":["Gihun Lee","Minchan Jeong","Sangmook Kim","Jaehoon Oh","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2308.12532v6.pdf","comment":"The IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  2024 (CVPR 2024)"},{"id":"http://arxiv.org/abs/2403.18159v2","updated":"2024-03-28T08:22:31Z","published":"2024-03-26T23:51:44Z","title":"Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal\n  Propagation Analysis for Large Language Models","summary":"  Large generative models such as large language models (LLMs) and diffusion\nmodels have revolutionized the fields of NLP and computer vision respectively.\nHowever, their slow inference, high computation and memory requirement makes it\nchallenging to deploy them on edge devices. In this study, we propose a\nlight-weight quantization aware fine tuning technique using knowledge\ndistillation (KD-QAT) to improve the performance of 4-bit weight quantized LLMs\nusing commonly available datasets to realize a popular language use case, on\ndevice chat applications. To improve this paradigm of finetuning, as main\ncontributions, we provide insights into stability of KD-QAT by empirically\nstudying the gradient propagation during training to better understand the\nvulnerabilities of KD-QAT based approaches to low-bit quantization errors.\nBased on our insights, we propose ov-freeze, a simple technique to stabilize\nthe KD-QAT process. Finally, we experiment with the popular 7B LLaMAv2-Chat\nmodel at 4-bit quantization level and demonstrate that ov-freeze results in\nnear floating point precision performance, i.e., less than 0.7% loss of\naccuracy on Commonsense Reasoning benchmarks.\n","authors":["Kartikeya Bhardwaj","Nilesh Prasad Pandey","Sweta Priyadarshi","Kyunggeun Lee","Jun Ma","Harris Teague"],"pdf_url":"https://arxiv.org/pdf/2403.18159v2.pdf","comment":"Accepted at Practical ML for Low Resource Settings Workshop at ICLR\n  2024"},{"id":"http://arxiv.org/abs/2403.19211v1","updated":"2024-03-28T08:19:33Z","published":"2024-03-28T08:19:33Z","title":"Dual-Personalizing Adapter for Federated Foundation Models","summary":"  Recently, foundation models, particularly large language models (LLMs), have\ndemonstrated an impressive ability to adapt to various tasks by fine-tuning\nlarge amounts of instruction data. Notably, federated foundation models emerge\nas a privacy preservation method to fine-tune models collaboratively under\nfederated learning (FL) settings by leveraging many distributed datasets with\nnon-IID data. To alleviate communication and computation overhead,\nparameter-efficient methods are introduced for efficiency, and some research\nadapted personalization methods to federated foundation models for better user\npreferences alignment. However, a critical gap in existing research is the\nneglect of test-time distribution shifts in real-world applications. Therefore,\nto bridge this gap, we propose a new setting, termed test-time personalization,\nwhich not only concentrates on the targeted local task but also extends to\nother tasks that exhibit test-time distribution shifts. To address challenges\nin this new setting, we explore a simple yet effective solution to learn a\ncomprehensive foundation model. Specifically, a dual-personalizing adapter\narchitecture (FedDPA) is proposed, comprising a global adapter and a local\nadapter for addressing test-time distribution shifts and personalization,\nrespectively. Additionally, we introduce an instance-wise dynamic weighting\nmechanism to optimize the balance between the global and local adapters,\nenhancing overall performance. The effectiveness of the proposed method has\nbeen evaluated on benchmark datasets across different NLP tasks.\n","authors":["Yiyuan Yang","Guodong Long","Tao Shen","Jing Jiang","Michael Blumenstein"],"pdf_url":"https://arxiv.org/pdf/2403.19211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19205v1","updated":"2024-03-28T08:06:48Z","published":"2024-03-28T08:06:48Z","title":"From Activation to Initialization: Scaling Insights for Optimizing\n  Neural Fields","summary":"  In the realm of computer vision, Neural Fields have gained prominence as a\ncontemporary tool harnessing neural networks for signal representation. Despite\nthe remarkable progress in adapting these networks to solve a variety of\nproblems, the field still lacks a comprehensive theoretical framework. This\narticle aims to address this gap by delving into the intricate interplay\nbetween initialization and activation, providing a foundational basis for the\nrobust optimization of Neural Fields. Our theoretical insights reveal a\ndeep-seated connection among network initialization, architectural choices, and\nthe optimization process, emphasizing the need for a holistic approach when\ndesigning cutting-edge Neural Fields.\n","authors":["Hemanth Saratchandran","Sameera Ramasinghe","Simon Lucey"],"pdf_url":"https://arxiv.org/pdf/2403.19205v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18807v2","updated":"2024-03-28T08:01:34Z","published":"2024-03-27T17:53:30Z","title":"ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth\n  Estimation","summary":"  In the absence of parallax cues, a learning-based single image depth\nestimation (SIDE) model relies heavily on shading and contextual cues in the\nimage. While this simplicity is attractive, it is necessary to train such\nmodels on large and varied datasets, which are difficult to capture. It has\nbeen shown that using embeddings from pre-trained foundational models, such as\nCLIP, improves zero shot transfer in several applications. Taking inspiration\nfrom this, in our paper we explore the use of global image priors generated\nfrom a pre-trained ViT model to provide more detailed contextual information.\nWe argue that the embedding vector from a ViT model, pre-trained on a large\ndataset, captures greater relevant information for SIDE than the usual route of\ngenerating pseudo image captions, followed by CLIP based text embeddings. Based\non this idea, we propose a new SIDE model using a diffusion backbone which is\nconditioned on ViT embeddings. Our proposed design establishes a new\nstate-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of\n0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on\nKITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to\n0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model\ntrained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%)\nover NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%,\n18%, 45%, 9%) by ZoeDepth. The code is available at\nhttps://ecodepth-iitd.github.io\n","authors":["Suraj Patni","Aradhye Agarwal","Chetan Arora"],"pdf_url":"https://arxiv.org/pdf/2403.18807v2.pdf","comment":"IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n  2024"},{"id":"http://arxiv.org/abs/2403.19181v1","updated":"2024-03-28T07:22:16Z","published":"2024-03-28T07:22:16Z","title":"Make Large Language Model a Better Ranker","summary":"  The evolution of Large Language Models (LLMs) has significantly enhanced\ncapabilities across various fields, leading to a paradigm shift in how\nRecommender Systems (RSs) are conceptualized and developed. However, existing\nresearch primarily focuses on point-wise and pair-wise recommendation\nparadigms. These approaches prove inefficient in LLM-based recommenders due to\nthe high computational cost of utilizing Large Language Models. While some\nstudies have delved into list-wise approaches, they fall short in ranking\ntasks. This shortfall is attributed to the misalignment between the objectives\nof ranking and language generation. To this end, this paper introduces the\nLanguage Model Framework with Aligned Listwise Ranking Objectives (ALRO). ALRO\nis designed to bridge the gap between the capabilities of LLMs and the nuanced\nrequirements of ranking tasks within recommender systems. A key feature of ALRO\nis the introduction of soft lambda loss, an adaptation of lambda loss tailored\nto suit language generation tasks. Additionally, ALRO incorporates a\npermutation-sensitive learning mechanism that addresses position bias, a\nprevalent issue in generative models, without imposing additional computational\nburdens during inference. Our evaluative studies reveal that ALRO outperforms\nexisting embedding-based recommendation methods and the existing LLM-based\nrecommendation baselines, highlighting its efficacy.\n","authors":["Wenshuo Chao","Zhi Zheng","Hengshu Zhu","Hao Liu"],"pdf_url":"https://arxiv.org/pdf/2403.19181v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.19178v1","updated":"2024-03-28T07:08:26Z","published":"2024-03-28T07:08:26Z","title":"Enhancing Trust and Privacy in Distributed Networks: A Comprehensive\n  Survey on Blockchain-based Federated Learning","summary":"  While centralized servers pose a risk of being a single point of failure,\ndecentralized approaches like blockchain offer a compelling solution by\nimplementing a consensus mechanism among multiple entities. Merging distributed\ncomputing with cryptographic techniques, decentralized technologies introduce a\nnovel computing paradigm. Blockchain ensures secure, transparent, and\ntamper-proof data management by validating and recording transactions via\nconsensus across network nodes. Federated Learning (FL), as a distributed\nmachine learning framework, enables participants to collaboratively train\nmodels while safeguarding data privacy by avoiding direct raw data exchange.\nDespite the growing interest in decentralized methods, their application in FL\nremains underexplored. This paper presents a thorough investigation into\nBlockchain-based FL (BCFL), spotlighting the synergy between blockchain's\nsecurity features and FL's privacy-preserving model training capabilities.\nFirst, we present the taxonomy of BCFL from three aspects, including\ndecentralized, separate networks, and reputation-based architectures. Then, we\nsummarize the general architecture of BCFL systems, providing a comprehensive\nperspective on FL architectures informed by blockchain. Afterward, we analyze\nthe application of BCFL in healthcare, IoT, and other privacy-sensitive areas.\nFinally, we identify future research directions of BCFL.\n","authors":["Ji Liu","Chunlu Chen","Yu Li","Lin Sun","Yulun Song","Jingbo Zhou","Bo Jing","Dejing Dou"],"pdf_url":"https://arxiv.org/pdf/2403.19178v1.pdf","comment":"25 pages, accepted by KAIS 2024"},{"id":"http://arxiv.org/abs/2401.00365v2","updated":"2024-03-28T06:38:55Z","published":"2023-12-31T01:39:38Z","title":"HQ-VAE: Hierarchical Discrete Representation Learning with Variational\n  Bayes","summary":"  Vector quantization (VQ) is a technique to deterministically learn features\nwith discrete codebook representations. It is commonly performed with a\nvariational autoencoding model, VQ-VAE, which can be further extended to\nhierarchical structures for making high-fidelity reconstructions. However, such\nhierarchical extensions of VQ-VAE often suffer from the codebook/layer collapse\nissue, where the codebook is not efficiently used to express the data, and\nhence degrades reconstruction accuracy. To mitigate this problem, we propose a\nnovel unified framework to stochastically learn hierarchical discrete\nrepresentation on the basis of the variational Bayes framework, called\nhierarchically quantized variational autoencoder (HQ-VAE). HQ-VAE naturally\ngeneralizes the hierarchical variants of VQ-VAE, such as VQ-VAE-2 and\nresidual-quantized VAE (RQ-VAE), and provides them with a Bayesian training\nscheme. Our comprehensive experiments on image datasets show that HQ-VAE\nenhances codebook usage and improves reconstruction performance. We also\nvalidated HQ-VAE in terms of its applicability to a different modality with an\naudio dataset.\n","authors":["Yuhta Takida","Yukara Ikemiya","Takashi Shibuya","Kazuki Shimada","Woosung Choi","Chieh-Hsin Lai","Naoki Murata","Toshimitsu Uesaka","Kengo Uchida","Wei-Hsiang Liao","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2401.00365v2.pdf","comment":"34 pages with 17 figures, accepted for TMLR"},{"id":"http://arxiv.org/abs/2403.19165v1","updated":"2024-03-28T06:24:04Z","published":"2024-03-28T06:24:04Z","title":"Evaluating Fair Feature Selection in Machine Learning for Healthcare","summary":"  With the universal adoption of machine learning in healthcare, the potential\nfor the automation of societal biases to further exacerbate health disparities\nposes a significant risk. We explore algorithmic fairness from the perspective\nof feature selection. Traditional feature selection methods identify features\nfor better decision making by removing resource-intensive, correlated, or\nnon-relevant features but overlook how these factors may differ across\nsubgroups. To counter these issues, we evaluate a fair feature selection method\nthat considers equal importance to all demographic groups. We jointly\nconsidered a fairness metric and an error metric within the feature selection\nprocess to ensure a balance between minimizing both bias and global\nclassification error. We tested our approach on three publicly available\nhealthcare datasets. On all three datasets, we observed improvements in\nfairness metrics coupled with a minimal degradation of balanced accuracy. Our\napproach addresses both distributive and procedural fairness within the fair\nmachine learning context.\n","authors":["Md Rahat Shahriar Zawad","Peter Washington"],"pdf_url":"https://arxiv.org/pdf/2403.19165v1.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.19163v1","updated":"2024-03-28T06:18:12Z","published":"2024-03-28T06:18:12Z","title":"D'OH: Decoder-Only random Hypernetworks for Implicit Neural\n  Representations","summary":"  Deep implicit functions have been found to be an effective tool for\nefficiently encoding all manner of natural signals. Their attractiveness stems\nfrom their ability to compactly represent signals with little to no off-line\ntraining data. Instead, they leverage the implicit bias of deep networks to\ndecouple hidden redundancies within the signal. In this paper, we explore the\nhypothesis that additional compression can be achieved by leveraging the\nredundancies that exist between layers. We propose to use a novel run-time\ndecoder-only hypernetwork - that uses no offline training data - to better\nmodel this cross-layer parameter redundancy. Previous applications of\nhyper-networks with deep implicit functions have applied feed-forward\nencoder/decoder frameworks that rely on large offline datasets that do not\ngeneralize beyond the signals they were trained on. We instead present a\nstrategy for the initialization of run-time deep implicit functions for\nsingle-instance signals through a Decoder-Only randomly projected Hypernetwork\n(D'OH). By directly changing the dimension of a latent code to approximate a\ntarget implicit neural architecture, we provide a natural way to vary the\nmemory footprint of neural representations without the costly need for neural\narchitecture search on a space of alternative low-rate structures.\n","authors":["Cameron Gordon","Lachlan Ewen MacDonald","Hemanth Saratchandran","Simon Lucey"],"pdf_url":"https://arxiv.org/pdf/2403.19163v1.pdf","comment":"29 pages, 17 figures"},{"id":"http://arxiv.org/abs/2403.19159v1","updated":"2024-03-28T06:03:47Z","published":"2024-03-28T06:03:47Z","title":"Disentangling Length from Quality in Direct Preference Optimization","summary":"  Reinforcement Learning from Human Feedback (RLHF) has been a crucial\ncomponent in the recent success of Large Language Models. However, RLHF is know\nto exploit biases in human preferences, such as verbosity. A well-formatted and\neloquent answer is often more highly rated by users, even when it is less\nhelpful and objective. A number of approaches have been developed to control\nthose biases in the classical RLHF literature, but the problem remains\nrelatively under-explored for Direct Alignment Algorithms such as Direct\nPreference Optimization (DPO). Unlike classical RLHF, DPO does not train a\nseparate reward model or use reinforcement learning directly, so previous\napproaches developed to control verbosity cannot be directly applied to this\nsetting. Our work makes several contributions. For the first time, we study the\nlength problem in the DPO setting, showing significant exploitation in DPO and\nlinking it to out-of-distribution bootstrapping. We then develop a principled\nbut simple regularization strategy that prevents length exploitation, while\nstill maintaining improvements in model quality. We demonstrate these effects\nacross datasets on summarization and dialogue, where we achieve up to 20\\%\nimprovement in win rates when controlling for length, despite the GPT4 judge's\nwell-known verbosity bias.\n","authors":["Ryan Park","Rafael Rafailov","Stefano Ermon","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2403.19159v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14302v2","updated":"2024-03-28T05:13:43Z","published":"2024-03-21T11:16:42Z","title":"SpikingResformer: Bridging ResNet and Vision Transformer in Spiking\n  Neural Networks","summary":"  The remarkable success of Vision Transformers in Artificial Neural Networks\n(ANNs) has led to a growing interest in incorporating the self-attention\nmechanism and transformer-based architecture into Spiking Neural Networks\n(SNNs). While existing methods propose spiking self-attention mechanisms that\nare compatible with SNNs, they lack reasonable scaling methods, and the overall\narchitectures proposed by these methods suffer from a bottleneck in effectively\nextracting local features. To address these challenges, we propose a novel\nspiking self-attention mechanism named Dual Spike Self-Attention (DSSA) with a\nreasonable scaling method. Based on DSSA, we propose a novel spiking Vision\nTransformer architecture called SpikingResformer, which combines the\nResNet-based multi-stage architecture with our proposed DSSA to improve both\nperformance and energy efficiency while reducing parameters. Experimental\nresults show that SpikingResformer achieves higher accuracy with fewer\nparameters and lower energy consumption than other spiking Vision Transformer\ncounterparts. Notably, our SpikingResformer-L achieves 79.40% top-1 accuracy on\nImageNet with 4 time-steps, which is the state-of-the-art result in the SNN\nfield.\n","authors":["Xinyu Shi","Zecheng Hao","Zhaofei Yu"],"pdf_url":"https://arxiv.org/pdf/2403.14302v2.pdf","comment":"To be published in the 2024 IEEE/CVF Conference on Computer Vision\n  and Pattern Recognition (CVPR)"},{"id":"http://arxiv.org/abs/2403.19150v1","updated":"2024-03-28T05:08:25Z","published":"2024-03-28T05:08:25Z","title":"Towards Understanding Dual BN In Hybrid Adversarial Training","summary":"  There is a growing concern about applying batch normalization (BN) in\nadversarial training (AT), especially when the model is trained on both\nadversarial samples and clean samples (termed Hybrid-AT). With the assumption\nthat adversarial and clean samples are from two different domains, a common\npractice in prior works is to adopt Dual BN, where BN and BN are used for\nadversarial and clean branches, respectively. A popular belief for motivating\nDual BN is that estimating normalization statistics of this mixture\ndistribution is challenging and thus disentangling it for normalization\nachieves stronger robustness. In contrast to this belief, we reveal that\ndisentangling statistics plays a less role than disentangling affine parameters\nin model training. This finding aligns with prior work (Rebuffi et al., 2023),\nand we build upon their research for further investigations. We demonstrate\nthat the domain gap between adversarial and clean samples is not very large,\nwhich is counter-intuitive considering the significant influence of adversarial\nperturbation on the model accuracy. We further propose a two-task hypothesis\nwhich serves as the empirical foundation and a unified framework for Hybrid-AT\nimprovement. We also investigate Dual BN in test-time and reveal that affine\nparameters characterize the robustness during inference. Overall, our work\nsheds new light on understanding the mechanism of Dual BN in Hybrid-AT and its\nunderlying justification.\n","authors":["Chenshuang Zhang","Chaoning Zhang","Kang Zhang","Axi Niu","Junmo Kim","In So Kweon"],"pdf_url":"https://arxiv.org/pdf/2403.19150v1.pdf","comment":"Accepted at TMLR"},{"id":"http://arxiv.org/abs/2403.19149v1","updated":"2024-03-28T05:07:41Z","published":"2024-03-28T05:07:41Z","title":"Topological Cycle Graph Attention Network for Brain Functional\n  Connectivity","summary":"  This study, we introduce a novel Topological Cycle Graph Attention Network\n(CycGAT), designed to delineate a functional backbone within brain functional\ngraph--key pathways essential for signal transmissio--from non-essential,\nredundant connections that form cycles around this core structure. We first\nintroduce a cycle incidence matrix that establishes an independent cycle basis\nwithin a graph, mapping its relationship with edges. We propose a cycle graph\nconvolution that leverages a cycle adjacency matrix, derived from the cycle\nincidence matrix, to specifically filter edge signals in a domain of cycles.\nAdditionally, we strengthen the representation power of the cycle graph\nconvolution by adding an attention mechanism, which is further augmented by the\nintroduction of edge positional encodings in cycles, to enhance the topological\nawareness of CycGAT. We demonstrate CycGAT's localization through simulation\nand its efficacy on an ABCD study's fMRI data (n=8765), comparing it with\nbaseline models. CycGAT outperforms these models, identifying a functional\nbackbone with significantly fewer cycles, crucial for understanding neural\ncircuits related to general intelligence. Our code will be released once\naccepted.\n","authors":["Jinghan Huang","Nanguang Chen","Anqi Qiu"],"pdf_url":"https://arxiv.org/pdf/2403.19149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11034v3","updated":"2024-03-28T04:46:19Z","published":"2023-12-18T09:09:52Z","title":"Appeal: Allow Mislabeled Samples the Chance to be Rectified in Partial\n  Label Learning","summary":"  In partial label learning (PLL), each instance is associated with a set of\ncandidate labels among which only one is ground-truth. The majority of the\nexisting works focuses on constructing robust classifiers to estimate the\nlabeling confidence of candidate labels in order to identify the correct one.\nHowever, these methods usually struggle to identify and rectify mislabeled\nsamples. To help these mislabeled samples \"appeal\" for themselves and help\nexisting PLL methods identify and rectify mislabeled samples, in this paper, we\npropose the first appeal-based PLL framework. Specifically, we introduce a\nnovel partner classifier and instantiate it predicated on the implicit fact\nthat non-candidate labels of a sample should not be assigned to it, which is\ninherently accurate and has not been fully investigated in PLL. Furthermore, a\nnovel collaborative term is formulated to link the base classifier and the\npartner one. During each stage of mutual supervision, both classifiers will\nblur each other's predictions through a blurring mechanism to prevent\noverconfidence in a specific label. Extensive experiments demonstrate that the\nappeal and disambiguation ability of several well-established stand-alone and\ndeep-learning based PLL approaches can be significantly improved by coupling\nwith this learning paradigm.\n","authors":["Chongjie Si","Xuehui Wang","Yan Wang","Xiaokang Yang","Wei Shen"],"pdf_url":"https://arxiv.org/pdf/2312.11034v3.pdf","comment":"Under review. An extended version of 2024 AAAI oral paper \"Partial\n  Label Learning with a Partner\""},{"id":"http://arxiv.org/abs/2403.19143v1","updated":"2024-03-28T04:35:27Z","published":"2024-03-28T04:35:27Z","title":"Tiny Graph Neural Networks for Radio Resource Management","summary":"  The surge in demand for efficient radio resource management has necessitated\nthe development of sophisticated yet compact neural network architectures. In\nthis paper, we introduce a novel approach to Graph Neural Networks (GNNs)\ntailored for radio resource management by presenting a new architecture: the\nLow Rank Message Passing Graph Neural Network (LR-MPGNN). The cornerstone of\nLR-MPGNN is the implementation of a low-rank approximation technique that\nsubstitutes the conventional linear layers with their low-rank counterparts.\nThis innovative design significantly reduces the model size and the number of\nparameters. We evaluate the performance of the proposed LR-MPGNN model based on\nseveral key metrics: model size, number of parameters, weighted sum rate of the\ncommunication system, and the distribution of eigenvalues of weight matrices.\nOur extensive evaluations demonstrate that the LR-MPGNN model achieves a\nsixtyfold decrease in model size, and the number of model parameters can be\nreduced by up to 98%. Performance-wise, the LR-MPGNN demonstrates robustness\nwith a marginal 2% reduction in the best-case scenario in the normalized\nweighted sum rate compared to the original MPGNN model. Additionally, the\ndistribution of eigenvalues of the weight matrices in the LR-MPGNN model is\nmore uniform and spans a wider range, suggesting a strategic redistribution of\nweights.\n","authors":["Ahmad Ghasemi","Hossein Pishro-Nik"],"pdf_url":"https://arxiv.org/pdf/2403.19143v1.pdf","comment":"Accepted as a full paper by the tinyML Research Symposium 2024"},{"id":"http://arxiv.org/abs/2403.11624v2","updated":"2024-03-28T04:11:28Z","published":"2024-03-18T09:56:00Z","title":"Dual-Channel Multiplex Graph Neural Networks for Recommendation","summary":"  Efficient recommender systems play a crucial role in accurately capturing\nuser and item attributes that mirror individual preferences. Some existing\nrecommendation techniques have started to shift their focus towards modeling\nvarious types of interaction relations between users and items in real-world\nrecommendation scenarios, such as clicks, marking favorites, and purchases on\nonline shopping platforms. Nevertheless, these approaches still grapple with\ntwo significant shortcomings: (1) Insufficient modeling and exploitation of the\nimpact of various behavior patterns formed by multiplex relations between users\nand items on representation learning, and (2) ignoring the effect of different\nrelations in the behavior patterns on the target relation in recommender system\nscenarios. In this study, we introduce a novel recommendation framework,\nDual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the\naforementioned challenges. It incorporates an explicit behavior pattern\nrepresentation learner to capture the behavior patterns composed of multiplex\nuser-item interaction relations, and includes a relation chain representation\nlearning and a relation chain-aware encoder to discover the impact of various\nauxiliary relations on the target relation, the dependencies between different\nrelations, and mine the appropriate order of relations in a behavior pattern.\nExtensive experiments on three real-world datasets demonstrate that our \\model\nsurpasses various state-of-the-art recommendation methods. It outperforms the\nbest baselines by 10.06\\% and 12.15\\% on average across all datasets in terms\nof R@10 and N@10 respectively.\n","authors":["Xiang Li","Chaofan Fu","Zhongying Zhao","Guanjie Zheng","Chao Huang","Junyu Dong","Yanwei Yu"],"pdf_url":"https://arxiv.org/pdf/2403.11624v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05699v4","updated":"2024-03-28T03:48:40Z","published":"2023-03-10T04:49:01Z","title":"Feature Unlearning for Pre-trained GANs and VAEs","summary":"  We tackle the problem of feature unlearning from a pre-trained image\ngenerative model: GANs and VAEs. Unlike a common unlearning task where an\nunlearning target is a subset of the training set, we aim to unlearn a specific\nfeature, such as hairstyle from facial images, from the pre-trained generative\nmodels. As the target feature is only presented in a local region of an image,\nunlearning the entire image from the pre-trained model may result in losing\nother details in the remaining region of the image. To specify which features\nto unlearn, we collect randomly generated images that contain the target\nfeatures. We then identify a latent representation corresponding to the target\nfeature and then use the representation to fine-tune the pre-trained model.\nThrough experiments on MNIST, CelebA, and FFHQ datasets, we show that target\nfeatures are successfully removed while keeping the fidelity of the original\nmodels. Further experiments with an adversarial attack show that the unlearned\nmodel is more robust under the presence of malicious parties.\n","authors":["Saemi Moon","Seunghyuk Cho","Dongwoo Kim"],"pdf_url":"https://arxiv.org/pdf/2303.05699v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02861v4","updated":"2024-03-28T03:29:34Z","published":"2023-10-04T14:47:27Z","title":"Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly\n  Detection","summary":"  Graph-level anomaly detection has gained significant attention as it finds\napplications in various domains, such as cancer diagnosis and enzyme\nprediction. However, existing methods fail to capture the spectral properties\nof graph anomalies, resulting in unexplainable framework design and\nunsatisfying performance. In this paper, we re-investigate the spectral\ndifferences between anomalous and normal graphs. Our main observation shows a\nsignificant disparity in the accumulated spectral energy between these two\nclasses. Moreover, we prove that the accumulated spectral energy of the graph\nsignal can be represented by its Rayleigh Quotient, indicating that the\nRayleigh Quotient is a driving factor behind the anomalous properties of\ngraphs. Motivated by this, we propose Rayleigh Quotient Graph Neural Network\n(RQGNN), the first spectral GNN that explores the inherent spectral features of\nanomalous graphs for graph-level anomaly detection. Specifically, we introduce\na novel framework with two components: the Rayleigh Quotient learning component\n(RQL) and Chebyshev Wavelet GNN with RQ-pooling (CWGNN-RQ). RQL explicitly\ncaptures the Rayleigh Quotient of graphs and CWGNN-RQ implicitly explores the\nspectral space of graphs. Extensive experiments on 10 real-world datasets show\nthat RQGNN outperforms the best rival by 6.74% in Macro-F1 score and 1.44% in\nAUC, demonstrating the effectiveness of our framework. Our code is available at\nhttps://github.com/xydong127/RQGNN.\n","authors":["Xiangyu Dong","Xingyi Zhang","Sibo Wang"],"pdf_url":"https://arxiv.org/pdf/2310.02861v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03596v2","updated":"2024-03-28T03:26:51Z","published":"2023-12-06T16:35:59Z","title":"MMM: Generative Masked Motion Model","summary":"  Recent advances in text-to-motion generation using diffusion and\nautoregressive models have shown promising results. However, these models often\nsuffer from a trade-off between real-time performance, high fidelity, and\nmotion editability. To address this gap, we introduce MMM, a novel yet simple\nmotion generation paradigm based on Masked Motion Model. MMM consists of two\nkey components: (1) a motion tokenizer that transforms 3D human motion into a\nsequence of discrete tokens in latent space, and (2) a conditional masked\nmotion transformer that learns to predict randomly masked motion tokens,\nconditioned on the pre-computed text tokens. By attending to motion and text\ntokens in all directions, MMM explicitly captures inherent dependency among\nmotion tokens and semantic mapping between motion and text tokens. During\ninference, this allows parallel and iterative decoding of multiple motion\ntokens that are highly consistent with fine-grained text descriptions,\ntherefore simultaneously achieving high-fidelity and high-speed motion\ngeneration. In addition, MMM has innate motion editability. By simply placing\nmask tokens in the place that needs editing, MMM automatically fills the gaps\nwhile guaranteeing smooth transitions between editing and non-editing parts.\nExtensive experiments on the HumanML3D and KIT-ML datasets demonstrate that MMM\nsurpasses current leading methods in generating high-quality motion (evidenced\nby superior FID scores of 0.08 and 0.429), while offering advanced editing\nfeatures such as body-part modification, motion in-betweening, and the\nsynthesis of long motion sequences. In addition, MMM is two orders of magnitude\nfaster on a single mid-range GPU than editable motion diffusion models. Our\nproject page is available at \\url{https://exitudio.github.io/MMM-page}.\n","authors":["Ekkasit Pinyoanuntapong","Pu Wang","Minwoo Lee","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2312.03596v2.pdf","comment":"accepted to CVPR"},{"id":"http://arxiv.org/abs/2403.19114v1","updated":"2024-03-28T03:10:39Z","published":"2024-03-28T03:10:39Z","title":"Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval:\n  Evolving Coding Benchmarks via LLM","summary":"  LLMs have become the go-to choice for code generation tasks, with an\nexponential increase in the training, development, and usage of LLMs\nspecifically for code generation. To evaluate the ability of LLMs on code, both\nacademic and industry practitioners rely on popular handcrafted benchmarks.\nHowever, prior benchmarks contain only a very limited set of problems, both in\nquantity and variety. Further, due to popularity and age, many benchmarks are\nprone to data leakage where example solutions can be readily found on the web\nand thus potentially in training data. Such limitations inevitably lead us to\ninquire: Is the leaderboard performance on existing benchmarks reliable and\ncomprehensive enough to measure the program synthesis ability of LLMs? To\naddress this, we introduce EvoEval -- a program synthesis benchmark suite\ncreated by evolving existing benchmarks into different targeted domains for a\ncomprehensive evaluation of LLM coding abilities. Our study on 51 LLMs shows\nthat compared to the high performance obtained on standard benchmarks like\nHumanEval, there is a significant drop in performance (on average 39.4%) when\nusing EvoEval. Additionally, the decrease in performance can range from 19.6%\nto 47.7%, leading to drastic ranking changes amongst LLMs and showing potential\noverfitting of existing benchmarks. Furthermore, we showcase various insights,\nincluding the brittleness of instruction-following models when encountering\nrewording or subtle changes as well as the importance of learning problem\ncomposition and decomposition. EvoEval not only provides comprehensive\nbenchmarks, but can be used to further evolve arbitrary problems to keep up\nwith advances and the ever-changing landscape of LLMs for code. We have\nopen-sourced our benchmarks, tools, and complete LLM generations at\nhttps://github.com/evo-eval/evoeval\n","authors":["Chunqiu Steven Xia","Yinlin Deng","Lingming Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.19114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04021v4","updated":"2024-03-28T03:01:45Z","published":"2023-12-07T03:37:39Z","title":"A Study on the Calibration of In-context Learning","summary":"  Accurate uncertainty quantification is crucial for the safe deployment of\nmachine learning models, and prior research has demonstrated improvements in\nthe calibration of modern language models (LMs). We study in-context learning\n(ICL), a prevalent method for adapting static LMs through tailored prompts, and\nexamine the balance between performance and calibration across a broad spectrum\nof natural language understanding and reasoning tasks. Through comprehensive\nexperiments, we observe that, with an increasing number of ICL examples, models\ninitially exhibit increased miscalibration before achieving better calibration\nand miscalibration tends to arise in low-shot settings. Moreover, we find that\nmethods aimed at improving usability, such as fine-tuning and chain-of-thought\n(CoT) prompting, can lead to miscalibration and unreliable natural language\nexplanations. Furthermore, we explore recalibration techniques and find that a\nscaling-binning calibrator can reduce calibration errors consistently.\n","authors":["Hanlin Zhang","Yi-Fan Zhang","Yaodong Yu","Dhruv Madeka","Dean Foster","Eric Xing","Himabindu Lakkaraju","Sham Kakade"],"pdf_url":"https://arxiv.org/pdf/2312.04021v4.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.19107v1","updated":"2024-03-28T02:51:33Z","published":"2024-03-28T02:51:33Z","title":"Synthetic Medical Imaging Generation with Generative Adversarial\n  Networks For Plain Radiographs","summary":"  In medical imaging, access to data is commonly limited due to patient privacy\nrestrictions and the issue that it can be difficult to acquire enough data in\nthe case of rare diseases.[1] The purpose of this investigation was to develop\na reusable open-source synthetic image generation pipeline, the GAN Image\nSynthesis Tool (GIST), that is easy to use as well as easy to deploy. The\npipeline helps to improve and standardize AI algorithms in the digital health\nspace by generating high quality synthetic image data that is not linked to\nspecific patients. Its image generation capabilities include the ability to\ngenerate imaging of pathologies or injuries with low incidence rates. This\nimprovement of digital health AI algorithms could improve diagnostic accuracy,\naid in patient care, decrease medicolegal claims, and ultimately decrease the\noverall cost of healthcare. The pipeline builds on existing Generative\nAdversarial Networks (GANs) algorithms, and preprocessing and evaluation steps\nwere included for completeness. For this work, we focused on ensuring the\npipeline supports radiography, with a focus on synthetic knee and elbow x-ray\nimages. In designing the pipeline, we evaluated the performance of current GAN\narchitectures, studying the performance on available x-ray data. We show that\nthe pipeline is capable of generating high quality and clinically relevant\nimages based on a lay person's evaluation and the Fr\\'echet Inception Distance\n(FID) metric.\n","authors":["John R. McNulty","Lee Kho","Alexandria L. Case","Charlie Fornaca","Drew Johnston","David Slater","Joshua M. Abzug","Sybil A. Russell"],"pdf_url":"https://arxiv.org/pdf/2403.19107v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19103v1","updated":"2024-03-28T02:35:53Z","published":"2024-03-28T02:35:53Z","title":"Automated Black-box Prompt Engineering for Personalized Text-to-Image\n  Generation","summary":"  Prompt engineering is effective for controlling the output of text-to-image\n(T2I) generative models, but it is also laborious due to the need for manually\ncrafted prompts. This challenge has spurred the development of algorithms for\nautomated prompt generation. However, these methods often struggle with\ntransferability across T2I models, require white-box access to the underlying\nmodel, and produce non-intuitive prompts. In this work, we introduce PRISM, an\nalgorithm that automatically identifies human-interpretable and transferable\nprompts that can effectively generate desired concepts given only black-box\naccess to T2I models. Inspired by large language model (LLM) jailbreaking,\nPRISM leverages the in-context learning ability of LLMs to iteratively refine\nthe candidate prompts distribution for given reference images. Our experiments\ndemonstrate the versatility and effectiveness of PRISM in generating accurate\nprompts for objects, styles and images across multiple T2I models, including\nStable Diffusion, DALL-E, and Midjourney.\n","authors":["Yutong He","Alexander Robey","Naoki Murata","Yiding Jiang","Joshua Williams","George J. Pappas","Hamed Hassani","Yuki Mitsufuji","Ruslan Salakhutdinov","J. Zico Kolter"],"pdf_url":"https://arxiv.org/pdf/2403.19103v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19099v1","updated":"2024-03-28T02:25:12Z","published":"2024-03-28T02:25:12Z","title":"Optimizing Quantum Convolutional Neural Network Architectures for\n  Arbitrary Data Dimension","summary":"  Quantum convolutional neural networks (QCNNs) represent a promising approach\nin quantum machine learning, paving new directions for both quantum and\nclassical data analysis. This approach is particularly attractive due to the\nabsence of the barren plateau problem, a fundamental challenge in training\nquantum neural networks (QNNs), and its feasibility. However, a limitation\narises when applying QCNNs to classical data. The network architecture is most\nnatural when the number of input qubits is a power of two, as this number is\nreduced by a factor of two in each pooling layer. The number of input qubits\ndetermines the dimensions (i.e. the number of features) of the input data that\ncan be processed, restricting the applicability of QCNN algorithms to\nreal-world data. To address this issue, we propose a QCNN architecture capable\nof handling arbitrary input data dimensions while optimizing the allocation of\nquantum resources such as ancillary qubits and quantum gates. This optimization\nis not only important for minimizing computational resources, but also\nessential in noisy intermediate-scale quantum (NISQ) computing, as the size of\nthe quantum circuits that can be executed reliably is limited. Through\nnumerical simulations, we benchmarked the classification performance of various\nQCNN architectures when handling arbitrary input data dimensions on the MNIST\nand Breast Cancer datasets. The results validate that the proposed QCNN\narchitecture achieves excellent classification performance while utilizing a\nminimal resource overhead, providing an optimal solution when reliable quantum\ncomputation is constrained by noise and imperfections.\n","authors":["Changwon Lee","Israel F. Araujo","Dongha Kim","Junghan Lee","Siheon Park","Ju-Young Ryu","Daniel K. Park"],"pdf_url":"https://arxiv.org/pdf/2403.19099v1.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2312.06153v2","updated":"2024-03-28T02:20:36Z","published":"2023-12-11T06:41:14Z","title":"Open Datasheets: Machine-readable Documentation for Open Datasets and\n  Responsible AI Assessments","summary":"  This paper introduces a no-code, machine-readable documentation framework for\nopen datasets, with a focus on responsible AI (RAI) considerations. The\nframework aims to improve comprehensibility, and usability of open datasets,\nfacilitating easier discovery and use, better understanding of content and\ncontext, and evaluation of dataset quality and accuracy. The proposed framework\nis designed to streamline the evaluation of datasets, helping researchers, data\nscientists, and other open data users quickly identify datasets that meet their\nneeds and organizational policies or regulations. The paper also discusses the\nimplementation of the framework and provides recommendations to maximize its\npotential. The framework is expected to enhance the quality and reliability of\ndata used in research and decision-making, fostering the development of more\nresponsible and trustworthy AI systems.\n","authors":["Anthony Cintron Roman","Jennifer Wortman Vaughan","Valerie See","Steph Ballard","Jehu Torres","Caleb Robinson","Juan M. Lavista Ferres"],"pdf_url":"https://arxiv.org/pdf/2312.06153v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.03788v5","updated":"2024-03-28T01:36:14Z","published":"2023-02-07T22:56:58Z","title":"Toward a Theory of Causation for Interpreting Neural Code Models","summary":"  Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly\nprogressing from research prototypes to commercial developer tools. As such,\nunderstanding the capabilities and limitations of such models is becoming\ncritical. However, the abilities of these models are typically measured using\nautomated metrics that often only reveal a portion of their real-world\nperformance. While, in general, the performance of NCMs appears promising,\ncurrently much is unknown about how such models arrive at decisions. To this\nend, this paper introduces $do_{code}$, a post hoc interpretability method\nspecific to NCMs that is capable of explaining model predictions. $do_{code}$\nis based upon causal inference to enable programming language-oriented\nexplanations. While the theoretical underpinnings of $do_{code}$ are extensible\nto exploring different model properties, we provide a concrete instantiation\nthat aims to mitigate the impact of spurious correlations by grounding\nexplanations of model behavior in properties of programming languages. To\ndemonstrate the practical benefit of $do_{code}$, we illustrate the insights\nthat our framework can provide by performing a case study on two popular deep\nlearning architectures and ten NCMs. The results of this case study illustrate\nthat our studied NCMs are sensitive to changes in code syntax. All our NCMs,\nexcept for the BERT-like model, statistically learn to predict tokens related\nto blocks of code (\\eg brackets, parenthesis, semicolon) with less confounding\nbias as compared to other programming language constructs. These insights\ndemonstrate the potential of $do_{code}$ as a useful method to detect and\nfacilitate the elimination of confounding bias in NCMs.\n","authors":["David N. Palacio","Alejandro Velasco","Nathan Cooper","Alvaro Rodriguez","Kevin Moran","Denys Poshyvanyk"],"pdf_url":"https://arxiv.org/pdf/2302.03788v5.pdf","comment":"Accepted to appear in IEEE Transactions on Software Engineering"},{"id":"http://arxiv.org/abs/2403.19083v1","updated":"2024-03-28T01:27:10Z","published":"2024-03-28T01:27:10Z","title":"Improving Cancer Imaging Diagnosis with Bayesian Networks and Deep\n  Learning: A Bayesian Deep Learning Approach","summary":"  With recent advancements in the development of artificial intelligence\napplications using theories and algorithms in machine learning, many accurate\nmodels can be created to train and predict on given datasets. With the\nrealization of the importance of imaging interpretation in cancer diagnosis,\nthis article aims to investigate the theory behind Deep Learning and Bayesian\nNetwork prediction models. Based on the advantages and drawbacks of each model,\ndifferent approaches will be used to construct a Bayesian Deep Learning Model,\ncombining the strengths while minimizing the weaknesses. Finally, the\napplications and accuracy of the resulting Bayesian Deep Learning approach in\nthe health industry in classifying images will be analyzed.\n","authors":["Pei Xi"," Lin"],"pdf_url":"https://arxiv.org/pdf/2403.19083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19082v1","updated":"2024-03-28T01:14:25Z","published":"2024-03-28T01:14:25Z","title":"Enhancing Conformal Prediction Using E-Test Statistics","summary":"  Conformal Prediction (CP) serves as a robust framework that quantifies\nuncertainty in predictions made by Machine Learning (ML) models. Unlike\ntraditional point predictors, CP generates statistically valid prediction\nregions, also known as prediction intervals, based on the assumption of data\nexchangeability. Typically, the construction of conformal predictions hinges on\np-values. This paper, however, ventures down an alternative path, harnessing\nthe power of e-test statistics to augment the efficacy of conformal predictions\nby introducing a BB-predictor (bounded from the below predictor).\n","authors":["A. A. Balinsky","A. D. Balinsky"],"pdf_url":"https://arxiv.org/pdf/2403.19082v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11076v2","updated":"2024-03-28T01:00:05Z","published":"2023-09-20T05:44:49Z","title":"Symbolic Regression on Sparse and Noisy Data with Gaussian Processes","summary":"  In this paper, we address the challenge of deriving dynamical models from\nsparse and noisy data. High-quality data is crucial for symbolic regression\nalgorithms; limited and noisy data can present modeling challenges. To overcome\nthis, we combine Gaussian process regression with a sparse identification of\nnonlinear dynamics (SINDy) method to denoise the data and identify nonlinear\ndynamical equations. Our simple approach offers improved robustness with\nsparse, noisy data compared to SINDy alone. We demonstrate its effectiveness on\na Lotka-Volterra model, a unicycle dynamic model in simulation, and hardware\ndata from an NVIDIA JetRacer system. We show superior performance over\nbaselines including 20.78% improvement over SINDy and 61.92% improvement over\nSSR in predicting future trajectories from discovered dynamics.\n","authors":["Junette Hsin","Shubhankar Agarwal","Adam Thorpe","Luis Sentis","David Fridovich-Keil"],"pdf_url":"https://arxiv.org/pdf/2309.11076v2.pdf","comment":"Submitted to CDC 2024"},{"id":"http://arxiv.org/abs/2403.08059v2","updated":"2024-03-28T00:59:37Z","published":"2024-03-12T20:11:38Z","title":"FluoroSAM: A Language-aligned Foundation Model for X-ray Image\n  Segmentation","summary":"  Automated X-ray image segmentation would accelerate research and development\nin diagnostic and interventional precision medicine. Prior efforts have\ncontributed task-specific models capable of solving specific image analysis\nproblems, but the utility of these models is restricted to their particular\ntask domain, and expanding to broader use requires additional data, labels, and\nretraining efforts. Recently, foundation models (FMs) -- machine learning\nmodels trained on large amounts of highly variable data thus enabling broad\napplicability -- have emerged as promising tools for automated image analysis.\nExisting FMs for medical image analysis focus on scenarios and modalities where\nobjects are clearly defined by visually apparent boundaries, such as surgical\ntool segmentation in endoscopy. X-ray imaging, by contrast, does not generally\noffer such clearly delineated boundaries or structure priors. During X-ray\nimage formation, complex 3D structures are projected in transmission onto the\nimaging plane, resulting in overlapping features of varying opacity and shape.\nTo pave the way toward an FM for comprehensive and automated analysis of\narbitrary medical X-ray images, we develop FluoroSAM, a language-aligned\nvariant of the Segment-Anything Model, trained from scratch on 1.6M synthetic\nX-ray images. FluoroSAM is trained on data including masks for 128 organ types\nand 464 non-anatomical objects, such as tools and implants. In real X-ray\nimages of cadaveric specimens, FluoroSAM is able to segment bony anatomical\nstructures based on text-only prompting with 0.51 and 0.79 DICE with\npoint-based refinement, outperforming competing SAM variants for all\nstructures. FluoroSAM is also capable of zero-shot generalization to segmenting\nclasses beyond the training set thanks to its language alignment, which we\ndemonstrate for full lung segmentation on real chest X-rays.\n","authors":["Benjamin D. Killeen","Liam J. Wang","Han Zhang","Mehran Armand","Russell H. Taylor","Dave Dreizin","Greg Osgood","Mathias Unberath"],"pdf_url":"https://arxiv.org/pdf/2403.08059v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12387v2","updated":"2024-03-28T00:52:59Z","published":"2023-10-18T23:58:54Z","title":"Learning to Optimise Climate Sensor Placement using a Transformer","summary":"  The optimal placement of sensors for environmental monitoring and disaster\nmanagement is a challenging problem due to its NP-hard nature. Traditional\nmethods for sensor placement involve exact, approximation, or heuristic\napproaches, with the latter being the most widely used. However, heuristic\nmethods are limited by expert intuition and experience. Deep learning (DL) has\nemerged as a promising approach for generating heuristic algorithms\nautomatically. In this paper, we introduce a novel sensor placement approach\nfocused on learning improvement heuristics using deep reinforcement learning\n(RL) methods. Our approach leverages an RL formulation for learning improvement\nheuristics, driven by an actor-critic algorithm for training the policy\nnetwork. We compare our method with several state-of-the-art approaches by\nconducting comprehensive experiments, demonstrating the effectiveness and\nsuperiority of our proposed approach in producing high-quality solutions. Our\nwork presents a promising direction for applying advanced DL and RL techniques\nto challenging climate sensor placement problems.\n","authors":["Chen Wang","Victoria Huang","Gang Chen","Hui Ma","Bryce Chen","Jochen Schmidt"],"pdf_url":"https://arxiv.org/pdf/2310.12387v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19076v1","updated":"2024-03-28T00:34:56Z","published":"2024-03-28T00:34:56Z","title":"Tiny Machine Learning: Progress and Futures","summary":"  Tiny Machine Learning (TinyML) is a new frontier of machine learning. By\nsqueezing deep learning models into billions of IoT devices and\nmicrocontrollers (MCUs), we expand the scope of AI applications and enable\nubiquitous intelligence. However, TinyML is challenging due to hardware\nconstraints: the tiny memory resource makes it difficult to hold deep learning\nmodels designed for cloud and mobile platforms. There is also limited compiler\nand inference engine support for bare-metal devices. Therefore, we need to\nco-design the algorithm and system stack to enable TinyML. In this review, we\nwill first discuss the definition, challenges, and applications of TinyML. We\nthen survey the recent progress in TinyML and deep learning on MCUs. Next, we\nwill introduce MCUNet, showing how we can achieve ImageNet-scale AI\napplications on IoT devices with system-algorithm co-design. We will further\nextend the solution from inference to training and introduce tiny on-device\ntraining techniques. Finally, we present future directions in this area.\nToday's large model might be tomorrow's tiny model. The scope of TinyML should\nevolve and adapt over time.\n","authors":["Ji Lin","Ligeng Zhu","Wei-Ming Chen","Wei-Chen Wang","Song Han"],"pdf_url":"https://arxiv.org/pdf/2403.19076v1.pdf","comment":"IEEE Circuits and Systems Magazine (2023). arXiv admin note: text\n  overlap with arXiv:2206.15472"},{"id":"http://arxiv.org/abs/2007.15776v3","updated":"2024-03-28T15:51:49Z","published":"2020-07-30T23:50:44Z","title":"Random Vector Functional Link Networks for Function Approximation on\n  Manifolds","summary":"  The learning speed of feed-forward neural networks is notoriously slow and\nhas presented a bottleneck in deep learning applications for several decades.\nFor instance, gradient-based learning algorithms, which are used extensively to\ntrain neural networks, tend to work slowly when all of the network parameters\nmust be iteratively tuned. To counter this, both researchers and practitioners\nhave tried introducing randomness to reduce the learning requirement. Based on\nthe original construction of Igelnik and Pao, single layer neural-networks with\nrandom input-to-hidden layer weights and biases have seen success in practice,\nbut the necessary theoretical justification is lacking. In this paper, we begin\nto fill this theoretical gap. We provide a (corrected) rigorous proof that the\nIgelnik and Pao construction is a universal approximator for continuous\nfunctions on compact domains, with approximation error decaying asymptotically\nlike $O(1/\\sqrt{n})$ for the number $n$ of network nodes. We then extend this\nresult to the non-asymptotic setting, proving that one can achieve any desired\napproximation error with high probability provided $n$ is sufficiently large.\nWe further adapt this randomized neural network architecture to approximate\nfunctions on smooth, compact submanifolds of Euclidean space, providing\ntheoretical guarantees in both the asymptotic and non-asymptotic forms.\nFinally, we illustrate our results on manifolds with numerical experiments.\n","authors":["Deanna Needell","Aaron A. Nelson","Rayan Saab","Palina Salanevich","Olov Schavemaker"],"pdf_url":"https://arxiv.org/pdf/2007.15776v3.pdf","comment":"37 pages, 1 figure"},{"id":"http://arxiv.org/abs/2101.10300v5","updated":"2024-03-28T03:47:39Z","published":"2021-01-25T18:33:54Z","title":"Channel Estimation via Successive Denoising in MIMO OFDM Systems: A\n  Reinforcement Learning Approach","summary":"  In general, reliable communication via multiple-input multiple-output (MIMO)\northogonal frequency division multiplexing (OFDM) requires accurate channel\nestimation at the receiver. The existing literature largely focuses on\ndenoising methods for channel estimation that depend on either (i) channel\nanalysis in the time-domain with prior channel knowledge or (ii) supervised\nlearning techniques which require large pre-labeled datasets for training. To\naddress these limitations, we present a frequency-domain denoising method based\non a reinforcement learning framework that does not need a priori channel\nknowledge and pre-labeled data. Our methodology includes a new successive\nchannel denoising process based on channel curvature computation, for which we\nobtain a channel curvature magnitude threshold to identify unreliable channel\nestimates. Based on this process, we formulate the denoising mechanism as a\nMarkov decision process, where we define the actions through a geometry-based\nchannel estimation update, and the reward function based on a policy that\nreduces mean squared error (MSE). We then resort to Q-learning to update the\nchannel estimates. Numerical results verify that our denoising algorithm can\nsuccessfully mitigate noise in channel estimates. In particular, our algorithm\nprovides a significant improvement over the practical least squares (LS)\nestimation method and provides performance that approaches that of the ideal\nlinear minimum mean square error (LMMSE) estimation with perfect knowledge of\nchannel statistics.\n","authors":["Myeung Suk Oh","Seyyedali Hosseinalipour","Taejoon Kim","Christopher G. Brinton","David J. Love"],"pdf_url":"https://arxiv.org/pdf/2101.10300v5.pdf","comment":"This paper has been published in the proceedings of 2021 IEEE\n  International Conference on Communications (ICC)"},{"id":"http://arxiv.org/abs/2403.19076v1","updated":"2024-03-28T00:34:56Z","published":"2024-03-28T00:34:56Z","title":"Tiny Machine Learning: Progress and Futures","summary":"  Tiny Machine Learning (TinyML) is a new frontier of machine learning. By\nsqueezing deep learning models into billions of IoT devices and\nmicrocontrollers (MCUs), we expand the scope of AI applications and enable\nubiquitous intelligence. However, TinyML is challenging due to hardware\nconstraints: the tiny memory resource makes it difficult to hold deep learning\nmodels designed for cloud and mobile platforms. There is also limited compiler\nand inference engine support for bare-metal devices. Therefore, we need to\nco-design the algorithm and system stack to enable TinyML. In this review, we\nwill first discuss the definition, challenges, and applications of TinyML. We\nthen survey the recent progress in TinyML and deep learning on MCUs. Next, we\nwill introduce MCUNet, showing how we can achieve ImageNet-scale AI\napplications on IoT devices with system-algorithm co-design. We will further\nextend the solution from inference to training and introduce tiny on-device\ntraining techniques. Finally, we present future directions in this area.\nToday's large model might be tomorrow's tiny model. The scope of TinyML should\nevolve and adapt over time.\n","authors":["Ji Lin","Ligeng Zhu","Wei-Ming Chen","Wei-Chen Wang","Song Han"],"pdf_url":"https://arxiv.org/pdf/2403.19076v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2206.15472"},{"id":"http://arxiv.org/abs/2403.19887v1","updated":"2024-03-28T23:55:06Z","published":"2024-03-28T23:55:06Z","title":"Jamba: A Hybrid Transformer-Mamba Language Model","summary":"  We present Jamba, a new base large language model based on a novel hybrid\nTransformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba\ninterleaves blocks of Transformer and Mamba layers, enjoying the benefits of\nboth model families. MoE is added in some of these layers to increase model\ncapacity while keeping active parameter usage manageable. This flexible\narchitecture allows resource- and objective-specific configurations. In the\nparticular configuration we have implemented, we end up with a powerful model\nthat fits in a single 80GB GPU. Built at large scale, Jamba provides high\nthroughput and small memory footprint compared to vanilla Transformers, and at\nthe same time state-of-the-art performance on standard language model\nbenchmarks and long-context evaluations. Remarkably, the model presents strong\nresults for up to 256K tokens context length. We study various architectural\ndecisions, such as how to combine Transformer and Mamba layers, and how to mix\nexperts, and show that some of them are crucial in large scale modeling. We\nalso describe several interesting properties of these architectures which the\ntraining and evaluation of Jamba have revealed, and plan to release checkpoints\nfrom various ablation runs, to encourage further exploration of this novel\narchitecture. We make the weights of our implementation of Jamba publicly\navailable under a permissive license.\n","authors":["Opher Lieber","Barak Lenz","Hofit Bata","Gal Cohen","Jhonathan Osin","Itay Dalmedigos","Erez Safahi","Shaked Meirom","Yonatan Belinkov","Shai Shalev-Shwartz","Omri Abend","Raz Alon","Tomer Asida","Amir Bergman","Roman Glozman","Michael Gokhman","Avashalom Manevich","Nir Ratner","Noam Rozen","Erez Shwartz","Mor Zusman","Yoav Shoham"],"pdf_url":"https://arxiv.org/pdf/2403.19887v1.pdf","comment":"Webpage: https://www.ai21.com/jamba"},{"id":"http://arxiv.org/abs/2403.19882v1","updated":"2024-03-28T23:31:59Z","published":"2024-03-28T23:31:59Z","title":"Enhancing Efficiency in Vision Transformer Networks: Design Techniques\n  and Insights","summary":"  Intrigued by the inherent ability of the human visual system to identify\nsalient regions in complex scenes, attention mechanisms have been seamlessly\nintegrated into various Computer Vision (CV) tasks. Building upon this\nparadigm, Vision Transformer (ViT) networks exploit attention mechanisms for\nimproved efficiency. This review navigates the landscape of redesigned\nattention mechanisms within ViTs, aiming to enhance their performance. This\npaper provides a comprehensive exploration of techniques and insights for\ndesigning attention mechanisms, systematically reviewing recent literature in\nthe field of CV. This survey begins with an introduction to the theoretical\nfoundations and fundamental concepts underlying attention mechanisms. We then\npresent a systematic taxonomy of various attention mechanisms within ViTs,\nemploying redesigned approaches. A multi-perspective categorization is proposed\nbased on their application, objectives, and the type of attention applied. The\nanalysis includes an exploration of the novelty, strengths, weaknesses, and an\nin-depth evaluation of the different proposed strategies. This culminates in\nthe development of taxonomies that highlight key properties and contributions.\nFinally, we gather the reviewed studies along with their available open-source\nimplementations at our\n\\href{https://github.com/mindflow-institue/Awesome-Attention-Mechanism-in-Medical-Imaging}{GitHub}\\footnote{\\url{https://github.com/xmindflow/Awesome-Attention-Mechanism-in-Medical-Imaging}}.\nWe aim to regularly update it with the most recent relevant papers.\n","authors":["Moein Heidari","Reza Azad","Sina Ghorbani Kolahi","René Arimond","Leon Niggemeier","Alaa Sulaiman","Afshin Bozorgpour","Ehsan Khodapanah Aghdam","Amirhossein Kazerouni","Ilker Hacihaliloglu","Dorit Merhof"],"pdf_url":"https://arxiv.org/pdf/2403.19882v1.pdf","comment":"Submitted to Computational Visual Media Journal"},{"id":"http://arxiv.org/abs/2206.01206v3","updated":"2024-03-28T23:25:14Z","published":"2022-06-01T20:16:32Z","title":"Positive Unlabeled Contrastive Learning","summary":"  Self-supervised pretraining on unlabeled data followed by supervised\nfine-tuning on labeled data is a popular paradigm for learning from limited\nlabeled examples. We extend this paradigm to the classical positive unlabeled\n(PU) setting, where the task is to learn a binary classifier given only a few\nlabeled positive samples, and (often) a large amount of unlabeled samples\n(which could be positive or negative).\n  We first propose a simple extension of standard infoNCE family of contrastive\nlosses, to the PU setting; and show that this learns superior representations,\nas compared to existing unsupervised and supervised approaches. We then develop\na simple methodology to pseudo-label the unlabeled samples using a new\nPU-specific clustering scheme; these pseudo-labels can then be used to train\nthe final (positive vs. negative) classifier. Our method handily outperforms\nstate-of-the-art PU methods over several standard PU benchmark datasets, while\nnot requiring a-priori knowledge of any class prior (which is a common\nassumption in other PU methods). We also provide a simple theoretical analysis\nthat motivates our methods.\n","authors":["Anish Acharya","Sujay Sanghavi","Li Jing","Bhargav Bhushanam","Dhruv Choudhary","Michael Rabbat","Inderjit Dhillon"],"pdf_url":"https://arxiv.org/pdf/2206.01206v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17076v2","updated":"2024-03-28T23:02:27Z","published":"2023-11-27T22:23:27Z","title":"Compositional Chain-of-Thought Prompting for Large Multimodal Models","summary":"  The combination of strong visual backbones and Large Language Model (LLM)\nreasoning has led to Large Multimodal Models (LMMs) becoming the current\nstandard for a wide range of vision and language (VL) tasks. However, recent\nresearch has shown that even the most advanced LMMs still struggle to capture\naspects of compositional visual reasoning, such as attributes and relationships\nbetween objects. One solution is to utilize scene graphs (SGs)--a formalization\nof objects and their relations and attributes that has been extensively used as\na bridge between the visual and textual domains. Yet, scene graph data requires\nscene graph annotations, which are expensive to collect and thus not easily\nscalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic\nforgetting of the pretraining objective. To overcome this, inspired by\nchain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a\nnovel zero-shot Chain-of-Thought prompting method that utilizes SG\nrepresentations in order to extract compositional knowledge from an LMM.\nSpecifically, we first generate an SG using the LMM, and then use that SG in\nthe prompt to produce a response. Through extensive experiments, we find that\nthe proposed CCoT approach not only improves LMM performance on several vision\nand language VL compositional benchmarks but also improves the performance of\nseveral popular LMMs on general multimodal benchmarks, without the need for\nfine-tuning or annotated ground-truth SGs. Code:\nhttps://github.com/chancharikmitra/CCoT\n","authors":["Chancharik Mitra","Brandon Huang","Trevor Darrell","Roei Herzig"],"pdf_url":"https://arxiv.org/pdf/2311.17076v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19871v1","updated":"2024-03-28T22:45:38Z","published":"2024-03-28T22:45:38Z","title":"Towards Stable Machine Learning Model Retraining via Slowly Varying\n  Sequences","summary":"  Retraining machine learning models remains an important task for real-world\nmachine learning model deployment. Existing methods focus largely on greedy\napproaches to find the best-performing model without considering the stability\nof trained model structures across different retraining evolutions. In this\nstudy, we develop a mixed integer optimization algorithm that holistically\nconsiders the problem of retraining machine learning models across different\ndata batch updates. Our method focuses on retaining consistent analytical\ninsights - which is important to model interpretability, ease of\nimplementation, and fostering trust with users - by using custom-defined\ndistance metrics that can be directly incorporated into the optimization\nproblem. Importantly, our method shows stronger stability than greedily trained\nmodels with a small, controllable sacrifice in model performance in a\nreal-world production case study. Finally, important analytical insights, as\ndemonstrated using SHAP feature importance, are shown to be consistent across\nretraining iterations.\n","authors":["Vassilis Digalakis Jr","Yu Ma","Phevos Paschalidis","Dimitris Bertsimas"],"pdf_url":"https://arxiv.org/pdf/2403.19871v1.pdf","comment":"For correspondence, contact Yu Ma, midsumer@mit.edu"},{"id":"http://arxiv.org/abs/2403.17410v2","updated":"2024-03-28T22:28:02Z","published":"2024-03-26T06:06:01Z","title":"On permutation-invariant neural networks","summary":"  Conventional machine learning algorithms have traditionally been designed\nunder the assumption that input data follows a vector-based format, with an\nemphasis on vector-centric paradigms. However, as the demand for tasks\ninvolving set-based inputs has grown, there has been a paradigm shift in the\nresearch community towards addressing these challenges. In recent years, the\nemergence of neural network architectures such as Deep Sets and Transformers\nhas presented a significant advancement in the treatment of set-based data.\nThese architectures are specifically engineered to naturally accommodate sets\nas input, enabling more effective representation and processing of set\nstructures. Consequently, there has been a surge of research endeavors\ndedicated to exploring and harnessing the capabilities of these architectures\nfor various tasks involving the approximation of set functions. This\ncomprehensive survey aims to provide an overview of the diverse problem\nsettings and ongoing research efforts pertaining to neural networks that\napproximate set functions. By delving into the intricacies of these approaches\nand elucidating the associated challenges, the survey aims to equip readers\nwith a comprehensive understanding of the field. Through this comprehensive\nperspective, we hope that researchers can gain valuable insights into the\npotential applications, inherent limitations, and future directions of\nset-based neural networks. Indeed, from this survey we gain two insights: i)\nDeep Sets and its variants can be generalized by differences in the aggregation\nfunction, and ii) the behavior of Deep Sets is sensitive to the choice of the\naggregation function. From these observations, we show that Deep Sets, one of\nthe well-known permutation-invariant neural networks, can be generalized in the\nsense of a quasi-arithmetic mean.\n","authors":["Masanari Kimura","Ryotaro Shimizu","Yuki Hirakawa","Ryosuke Goto","Yuki Saito"],"pdf_url":"https://arxiv.org/pdf/2403.17410v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19867v1","updated":"2024-03-28T22:26:38Z","published":"2024-03-28T22:26:38Z","title":"Finding Decision Tree Splits in Streaming and Massively Parallel Models","summary":"  In this work, we provide data stream algorithms that compute optimal splits\nin decision tree learning. In particular, given a data stream of observations\n$x_i$ and their labels $y_i$, the goal is to find the optimal split point $j$\nthat divides the data into two sets such that the mean squared error (for\nregression) or misclassification rate (for classification) is minimized. We\nprovide various fast streaming algorithms that use sublinear space and a small\nnumber of passes for these problems. These algorithms can also be extended to\nthe massively parallel computation model. Our work, while not directly\ncomparable, complements the seminal work of Domingos and Hulten (KDD 2000).\n","authors":["Huy Pham","Hoang Ta","Hoa T. Vu"],"pdf_url":"https://arxiv.org/pdf/2403.19867v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13460v2","updated":"2024-03-28T22:24:30Z","published":"2024-01-24T14:02:09Z","title":"Multi-Agent Diagnostics for Robustness via Illuminated Diversity","summary":"  In the rapidly advancing field of multi-agent systems, ensuring robustness in\nunfamiliar and adversarial settings is crucial. Notwithstanding their\noutstanding performance in familiar environments, these systems often falter in\nnew situations due to overfitting during the training phase. This is especially\npronounced in settings where both cooperative and competitive behaviours are\npresent, encapsulating a dual nature of overfitting and generalisation\nchallenges. To address this issue, we present Multi-Agent Diagnostics for\nRobustness via Illuminated Diversity (MADRID), a novel approach for generating\ndiverse adversarial scenarios that expose strategic vulnerabilities in\npre-trained multi-agent policies. Leveraging the concepts from open-ended\nlearning, MADRID navigates the vast space of adversarial settings, employing a\ntarget policy's regret to gauge the vulnerabilities of these settings. We\nevaluate the effectiveness of MADRID on the 11vs11 version of Google Research\nFootball, one of the most complex environments for multi-agent reinforcement\nlearning. Specifically, we employ MADRID for generating a diverse array of\nadversarial settings for TiZero, the state-of-the-art approach which \"masters\"\nthe game through 45 days of training on a large-scale distributed\ninfrastructure. We expose key shortcomings in TiZero's tactical\ndecision-making, underlining the crucial importance of rigorous evaluation in\nmulti-agent systems.\n","authors":["Mikayel Samvelyan","Davide Paglieri","Minqi Jiang","Jack Parker-Holder","Tim Rocktäschel"],"pdf_url":"https://arxiv.org/pdf/2401.13460v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19863v1","updated":"2024-03-28T22:17:19Z","published":"2024-03-28T22:17:19Z","title":"DeNetDM: Debiasing by Network Depth Modulation","summary":"  When neural networks are trained on biased datasets, they tend to\ninadvertently learn spurious correlations, leading to challenges in achieving\nstrong generalization and robustness. Current approaches to address such biases\ntypically involve utilizing bias annotations, reweighting based on pseudo-bias\nlabels, or enhancing diversity within bias-conflicting data points through\naugmentation techniques. We introduce DeNetDM, a novel debiasing method based\non the observation that shallow neural networks prioritize learning core\nattributes, while deeper ones emphasize biases when tasked with acquiring\ndistinct information. Using a training paradigm derived from Product of\nExperts, we create both biased and debiased branches with deep and shallow\narchitectures and then distill knowledge to produce the target debiased model.\nExtensive experiments and analyses demonstrate that our approach outperforms\ncurrent debiasing techniques, achieving a notable improvement of around 5% in\nthree datasets, encompassing both synthetic and real-world data. Remarkably,\nDeNetDM accomplishes this without requiring annotations pertaining to bias\nlabels or bias types, while still delivering performance on par with supervised\ncounterparts. Furthermore, our approach effectively harnesses the diversity of\nbias-conflicting points within the data, surpassing previous methods and\nobviating the need for explicit augmentation-based methods to enhance the\ndiversity of such bias-conflicting points. The source code will be available\nupon acceptance.\n","authors":["Silpa Vadakkeeveetil Sreelatha","Adarsh Kappiyath","Anjan Dutta"],"pdf_url":"https://arxiv.org/pdf/2403.19863v1.pdf","comment":"23 pages including supplementary"},{"id":"http://arxiv.org/abs/2403.19852v1","updated":"2024-03-28T21:54:48Z","published":"2024-03-28T21:54:48Z","title":"A Review of Graph Neural Networks in Epidemic Modeling","summary":"  Since the onset of the COVID-19 pandemic, there has been a growing interest\nin studying epidemiological models. Traditional mechanistic models\nmathematically describe the transmission mechanisms of infectious diseases.\nHowever, they often fall short when confronted with the growing challenges of\ntoday. Consequently, Graph Neural Networks (GNNs) have emerged as a\nprogressively popular tool in epidemic research. In this paper, we endeavor to\nfurnish a comprehensive review of GNNs in epidemic tasks and highlight\npotential future directions. To accomplish this objective, we introduce\nhierarchical taxonomies for both epidemic tasks and methodologies, offering a\ntrajectory of development within this domain. For epidemic tasks, we establish\na taxonomy akin to those typically employed within the epidemic domain. For\nmethodology, we categorize existing work into \\textit{Neural Models} and\n\\textit{Hybrid Models}. Following this, we perform an exhaustive and systematic\nexamination of the methodologies, encompassing both the tasks and their\ntechnical details. Furthermore, we discuss the limitations of existing methods\nfrom diverse perspectives and systematically propose future research\ndirections. This survey aims to bridge literature gaps and promote the\nprogression of this promising field. We hope that it will facilitate synergies\nbetween the communities of GNNs and epidemiology, and contribute to their\ncollective progress.\n","authors":["Zewen Liu","Guancheng Wan","B. Aditya Prakash","Max S. Y. Lau","Wei Jin"],"pdf_url":"https://arxiv.org/pdf/2403.19852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19851v1","updated":"2024-03-28T21:53:24Z","published":"2024-03-28T21:53:24Z","title":"Localizing Paragraph Memorization in Language Models","summary":"  Can we localize the weights and mechanisms used by a language model to\nmemorize and recite entire paragraphs of its training data? In this paper, we\nshow that while memorization is spread across multiple layers and model\ncomponents, gradients of memorized paragraphs have a distinguishable spatial\npattern, being larger in lower model layers than gradients of non-memorized\nexamples. Moreover, the memorized examples can be unlearned by fine-tuning only\nthe high-gradient weights. We localize a low-layer attention head that appears\nto be especially involved in paragraph memorization. This head is predominantly\nfocusing its attention on distinctive, rare tokens that are least frequent in a\ncorpus-level unigram distribution. Next, we study how localized memorization is\nacross the tokens in the prefix by perturbing tokens and measuring the caused\nchange in the decoding. A few distinctive tokens early in a prefix can often\ncorrupt the entire continuation. Overall, memorized continuations are not only\nharder to unlearn, but also to corrupt than non-memorized ones.\n","authors":["Niklas Stoehr","Mitchell Gordon","Chiyuan Zhang","Owen Lewis"],"pdf_url":"https://arxiv.org/pdf/2403.19851v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19849v1","updated":"2024-03-28T21:52:15Z","published":"2024-03-28T21:52:15Z","title":"Biased Over-the-Air Federated Learning under Wireless Heterogeneity","summary":"  Recently, Over-the-Air (OTA) computation has emerged as a promising federated\nlearning (FL) paradigm that leverages the waveform superposition properties of\nthe wireless channel to realize fast model updates. Prior work focused on the\nOTA device ``pre-scaler\" design under \\emph{homogeneous} wireless conditions,\nin which devices experience the same average path loss, resulting in zero-bias\nsolutions. Yet, zero-bias designs are limited by the device with the worst\naverage path loss and hence may perform poorly in \\emph{heterogeneous} wireless\nsettings. In this scenario, there may be a benefit in designing \\emph{biased}\nsolutions, in exchange for a lower variance in the model updates. To optimize\nthis trade-off, we study the design of OTA device pre-scalers by focusing on\nthe OTA-FL convergence. We derive an upper bound on the model ``optimality\nerror\", which explicitly captures the effect of bias and variance in terms of\nthe choice of the pre-scalers. Based on this bound, we identify two solutions\nof interest: minimum noise variance, and minimum noise variance zero-bias\nsolutions. Numerical evaluations show that using OTA device pre-scalers that\nminimize the variance of FL updates, while allowing a small bias, can provide\nhigh gains over existing schemes.\n","authors":["Muhammad Faraz Ul Abrar","Nicolò Michelusi"],"pdf_url":"https://arxiv.org/pdf/2403.19849v1.pdf","comment":"Accepted at IEEE International Conference on Communications (ICC),\n  2024"},{"id":"http://arxiv.org/abs/2403.19845v1","updated":"2024-03-28T21:37:57Z","published":"2024-03-28T21:37:57Z","title":"Generalized Gradient Descent is a Hypergraph Functor","summary":"  Cartesian reverse derivative categories (CRDCs) provide an axiomatic\ngeneralization of the reverse derivative, which allows generalized analogues of\nclassic optimization algorithms such as gradient descent to be applied to a\nbroad class of problems. In this paper, we show that generalized gradient\ndescent with respect to a given CRDC induces a hypergraph functor from a\nhypergraph category of optimization problems to a hypergraph category of\ndynamical systems. The domain of this functor consists of objective functions\nthat are 1) general in the sense that they are defined with respect to an\narbitrary CRDC, and 2) open in that they are decorated spans that can be\ncomposed with other such objective functions via variable sharing. The codomain\nis specified analogously as a category of general and open dynamical systems\nfor the underlying CRDC. We describe how the hypergraph functor induces a\ndistributed optimization algorithm for arbitrary composite problems specified\nin the domain. To illustrate the kinds of problems our framework can model, we\nshow that parameter sharing models in multitask learning, a prevalent machine\nlearning paradigm, yield a composite optimization problem for a given choice of\nCRDC. We then apply the gradient descent functor to this composite problem and\ndescribe the resulting distributed gradient descent algorithm for training\nparameter sharing models.\n","authors":["Tyler Hanks","Matthew Klawonn","James Fairbanks"],"pdf_url":"https://arxiv.org/pdf/2403.19845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06471v3","updated":"2024-03-28T21:36:56Z","published":"2023-03-11T17:52:03Z","title":"Multimodal Data Integration for Oncology in the Era of Deep Neural\n  Networks: A Review","summary":"  Cancer has relational information residing at varying scales, modalities, and\nresolutions of the acquired data, such as radiology, pathology, genomics,\nproteomics, and clinical records. Integrating diverse data types can improve\nthe accuracy and reliability of cancer diagnosis and treatment. There can be\ndisease-related information that is too subtle for humans or existing\ntechnological tools to discern visually. Traditional methods typically focus on\npartial or unimodal information about biological systems at individual scales\nand fail to encapsulate the complete spectrum of the heterogeneous nature of\ndata. Deep neural networks have facilitated the development of sophisticated\nmultimodal data fusion approaches that can extract and integrate relevant\ninformation from multiple sources. Recent deep learning frameworks such as\nGraph Neural Networks (GNNs) and Transformers have shown remarkable success in\nmultimodal learning. This review article provides an in-depth analysis of the\nstate-of-the-art in GNNs and Transformers for multimodal data fusion in\noncology settings, highlighting notable research studies and their findings. We\nalso discuss the foundations of multimodal learning, inherent challenges, and\nopportunities for integrative learning in oncology. By examining the current\nstate and potential future developments of multimodal data integration in\noncology, we aim to demonstrate the promising role that multimodal neural\nnetworks can play in cancer prevention, early detection, and treatment through\ninformed oncology practices in personalized settings.\n","authors":["Asim Waqas","Aakash Tripathi","Ravi P. Ramachandran","Paul Stewart","Ghulam Rasool"],"pdf_url":"https://arxiv.org/pdf/2303.06471v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19844v1","updated":"2024-03-28T21:36:07Z","published":"2024-03-28T21:36:07Z","title":"Expanding Chemical Representation with k-mers and Fragment-based\n  Fingerprints for Molecular Fingerprinting","summary":"  This study introduces a novel approach, combining substruct counting,\n$k$-mers, and Daylight-like fingerprints, to expand the representation of\nchemical structures in SMILES strings. The integrated method generates\ncomprehensive molecular embeddings that enhance discriminative power and\ninformation content. Experimental evaluations demonstrate its superiority over\ntraditional Morgan fingerprinting, MACCS, and Daylight fingerprint alone,\nimproving chemoinformatics tasks such as drug classification. The proposed\nmethod offers a more informative representation of chemical structures,\nadvancing molecular similarity analysis and facilitating applications in\nmolecular design and drug discovery. It presents a promising avenue for\nmolecular structure analysis and design, with significant potential for\npractical implementation.\n","authors":["Sarwan Ali","Prakash Chourasia","Murray Patterson"],"pdf_url":"https://arxiv.org/pdf/2403.19844v1.pdf","comment":"12 Pages, 3 tables, Accepted at SimBig2023"},{"id":"http://arxiv.org/abs/2312.10144v3","updated":"2024-03-28T21:32:10Z","published":"2023-12-15T19:00:07Z","title":"Data-Efficient Multimodal Fusion on a Single GPU","summary":"  The goal of multimodal alignment is to learn a single latent space that is\nshared between multimodal inputs. The most powerful models in this space have\nbeen trained using massive datasets of paired inputs and large-scale\ncomputational resources, making them prohibitively expensive to train in many\npractical scenarios. We surmise that existing unimodal encoders pre-trained on\nlarge amounts of unimodal data should provide an effective bootstrap to create\nmultimodal models from unimodal ones at much lower costs. We therefore propose\nFuseMix, a multimodal augmentation scheme that operates on the latent spaces of\narbitrary pre-trained unimodal encoders. Using FuseMix for multimodal\nalignment, we achieve competitive performance -- and in certain cases\noutperform state-of-the art methods -- in both image-text and audio-text\nretrieval, with orders of magnitude less compute and data: for example, we\noutperform CLIP on the Flickr30K text-to-image retrieval task with $\\sim \\!\n600\\times$ fewer GPU days and $\\sim \\! 80\\times$ fewer image-text pairs.\nAdditionally, we show how our method can be applied to convert pre-trained\ntext-to-image generative models into audio-to-image ones. Code is available at:\nhttps://github.com/layer6ai-labs/fusemix.\n","authors":["Noël Vouitsis","Zhaoyan Liu","Satya Krishna Gorti","Valentin Villecroze","Jesse C. Cresswell","Guangwei Yu","Gabriel Loaiza-Ganem","Maksims Volkovs"],"pdf_url":"https://arxiv.org/pdf/2312.10144v3.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17343v3","updated":"2024-03-28T21:28:00Z","published":"2024-03-26T03:05:20Z","title":"Residual-based Language Models are Free Boosters for Biomedical Imaging","summary":"  In this study, we uncover the unexpected efficacy of residual-based large\nlanguage models (LLMs) as part of encoders for biomedical imaging tasks, a\ndomain traditionally devoid of language or textual data. The approach diverges\nfrom established methodologies by utilizing a frozen transformer block,\nextracted from pre-trained LLMs, as an innovative encoder layer for the direct\nprocessing of visual tokens. This strategy represents a significant departure\nfrom the standard multi-modal vision-language frameworks, which typically hinge\non language-driven prompts and inputs. We found that these LLMs could boost\nperformance across a spectrum of biomedical imaging applications, including\nboth 2D and 3D visual classification tasks, serving as plug-and-play boosters.\nMore interestingly, as a byproduct, we found that the proposed framework\nachieved superior performance, setting new state-of-the-art results on\nextensive, standardized datasets in MedMNIST-2D and 3D. Through this work, we\naim to open new avenues for employing LLMs in biomedical imaging and enriching\nthe understanding of their potential in this specialized domain.\n","authors":["Zhixin Lai","Jing Wu","Suiyao Chen","Yucheng Zhou","Naira Hovakimyan"],"pdf_url":"https://arxiv.org/pdf/2403.17343v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18334v2","updated":"2024-03-28T21:27:18Z","published":"2023-05-25T09:27:01Z","title":"PQA: Exploring the Potential of Product Quantization in DNN Hardware\n  Acceleration","summary":"  Conventional multiply-accumulate (MAC) operations have long dominated\ncomputation time for deep neural networks (DNNs), espcially convolutional\nneural networks (CNNs). Recently, product quantization (PQ) has been applied to\nthese workloads, replacing MACs with memory lookups to pre-computed dot\nproducts. To better understand the efficiency tradeoffs of product-quantized\nDNNs (PQ-DNNs), we create a custom hardware accelerator to parallelize and\naccelerate nearest-neighbor search and dot-product lookups. Additionally, we\nperform an empirical study to investigate the efficiency--accuracy tradeoffs of\ndifferent PQ parameterizations and training methods. We identify PQ\nconfigurations that improve performance-per-area for ResNet20 by up to\n3.1$\\times$, even when compared to a highly optimized conventional DNN\naccelerator, with similar improvements on two additional compact DNNs. When\ncomparing to recent PQ solutions, we outperform prior work by $4\\times$ in\nterms of performance-per-area with a 0.6% accuracy degradation. Finally, we\nreduce the bitwidth of PQ operations to investigate the impact on both hardware\nefficiency and accuracy. With only 2-6-bit precision on three compact DNNs, we\nwere able to maintain DNN accuracy eliminating the need for DSPs.\n","authors":["Ahmed F. AbouElhamayed","Angela Cui","Javier Fernandez-Marques","Nicholas D. Lane","Mohamed S. Abdelfattah"],"pdf_url":"https://arxiv.org/pdf/2305.18334v2.pdf","comment":"ACM Transactions on Reconfigurable Technology and Systems (TRETS) -\n  FCCM 2024 Journal Track"},{"id":"http://arxiv.org/abs/2403.19839v1","updated":"2024-03-28T21:20:27Z","published":"2024-03-28T21:20:27Z","title":"The New Agronomists: Language Models are Experts in Crop Management","summary":"  Crop management plays a crucial role in determining crop yield, economic\nprofitability, and environmental sustainability. Despite the availability of\nmanagement guidelines, optimizing these practices remains a complex and\nmultifaceted challenge. In response, previous studies have explored using\nreinforcement learning with crop simulators, typically employing simple\nneural-network-based reinforcement learning (RL) agents. Building on this\nfoundation, this paper introduces a more advanced intelligent crop management\nsystem. This system uniquely combines RL, a language model (LM), and crop\nsimulations facilitated by the Decision Support System for Agrotechnology\nTransfer (DSSAT). We utilize deep RL, specifically a deep Q-network, to train\nmanagement policies that process numerous state variables from the simulator as\nobservations. A novel aspect of our approach is the conversion of these state\nvariables into more informative language, facilitating the language model's\ncapacity to understand states and explore optimal management practices. The\nempirical results reveal that the LM exhibits superior learning capabilities.\nThrough simulation experiments with maize crops in Florida (US) and Zaragoza\n(Spain), the LM not only achieves state-of-the-art performance under various\nevaluation metrics but also demonstrates a remarkable improvement of over 49\\%\nin economic profit, coupled with reduced environmental impact when compared to\nbaseline methods. Our code is available at\n\\url{https://github.com/jingwu6/LM_AG}.\n","authors":["Jing Wu","Zhixin Lai","Suiyao Chen","Ran Tao","Pan Zhao","Naira Hovakimyan"],"pdf_url":"https://arxiv.org/pdf/2403.19839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.01563v2","updated":"2024-03-28T21:16:15Z","published":"2023-03-02T20:28:19Z","title":"Data-efficient, Explainable and Safe Box Manipulation: Illustrating the\n  Advantages of Physical Priors in Model-Predictive Control","summary":"  Model-based RL/control have gained significant traction in robotics. Yet,\nthese approaches often remain data-inefficient and lack the explainability of\nhand-engineered solutions. This makes them difficult to debug/integrate in\nsafety-critical settings. However, in many systems, prior knowledge of\nenvironment kinematics/dynamics is available. Incorporating such priors can\nhelp address the aforementioned problems by reducing problem complexity and the\nneed for exploration, while also facilitating the expression of the decisions\ntaken by the agent in terms of physically meaningful entities. Our aim with\nthis paper is to illustrate and support this point of view via a case-study. We\nmodel a payload manipulation problem based on a real robotic system, and show\nthat leveraging prior knowledge about the dynamics of the environment in an MPC\nframework can lead to improvements in explainability, safety and\ndata-efficiency, leading to satisfying generalization properties with less\ndata.\n","authors":["Achkan Salehi","Stephane Doncieux"],"pdf_url":"https://arxiv.org/pdf/2303.01563v2.pdf","comment":"accepted for publication by l4dc 2024, 12 pages (with references), 4\n  figures, 2 tables"},{"id":"http://arxiv.org/abs/2403.19837v1","updated":"2024-03-28T21:15:38Z","published":"2024-03-28T21:15:38Z","title":"Concept-based Analysis of Neural Networks via Vision-Language Models","summary":"  Formal analysis of vision-based deep neural networks (DNNs) is highly\ndesirable but it is very challenging due to the difficulty of expressing formal\nspecifications for vision tasks and the lack of efficient verification\nprocedures. In this paper, we propose to leverage emerging multimodal,\nvision-language, foundation models (VLMs) as a lens through which we can reason\nabout vision models. VLMs have been trained on a large body of images\naccompanied by their textual description, and are thus implicitly aware of\nhigh-level, human-understandable concepts describing the images. We describe a\nlogical specification language $\\texttt{Con}_{\\texttt{spec}}$ designed to\nfacilitate writing specifications in terms of these concepts. To define and\nformally check $\\texttt{Con}_{\\texttt{spec}}$ specifications, we leverage a\nVLM, which provides a means to encode and efficiently check natural-language\nproperties of vision models. We demonstrate our techniques on a ResNet-based\nclassifier trained on the RIVAL-10 dataset leveraging CLIP as the multimodal\nmodel.\n","authors":["Ravi Mangal","Nina Narodytska","Divya Gopinath","Boyue Caroline Hu","Anirban Roy","Susmit Jha","Corina Pasareanu"],"pdf_url":"https://arxiv.org/pdf/2403.19837v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.10716v2","updated":"2024-03-28T20:48:00Z","published":"2022-06-21T20:32:19Z","title":"Meta Reinforcement Learning with Finite Training Tasks -- a Density\n  Estimation Approach","summary":"  In meta reinforcement learning (meta RL), an agent learns from a set of\ntraining tasks how to quickly solve a new task, drawn from the same task\ndistribution. The optimal meta RL policy, a.k.a. the Bayes-optimal behavior, is\nwell defined, and guarantees optimal reward in expectation, taken with respect\nto the task distribution. The question we explore in this work is how many\ntraining tasks are required to guarantee approximately optimal behavior with\nhigh probability. Recent work provided the first such PAC analysis for a\nmodel-free setting, where a history-dependent policy was learned from the\ntraining tasks. In this work, we propose a different approach: directly learn\nthe task distribution, using density estimation techniques, and then train a\npolicy on the learned task distribution. We show that our approach leads to\nbounds that depend on the dimension of the task distribution. In particular, in\nsettings where the task distribution lies in a low-dimensional manifold, we\nextend our analysis to use dimensionality reduction techniques and account for\nsuch structure, obtaining significantly better bounds than previous work, which\nstrictly depend on the number of states and actions. The key of our approach is\nthe regularization implied by the kernel density estimation method. We further\ndemonstrate that this regularization is useful in practice, when `plugged in'\nthe state-of-the-art VariBAD meta RL algorithm.\n","authors":["Zohar Rimon","Aviv Tamar","Gilad Adler"],"pdf_url":"https://arxiv.org/pdf/2206.10716v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.01818v3","updated":"2024-03-28T20:38:01Z","published":"2022-06-03T21:01:48Z","title":"QAGCN: Answering Multi-Relation Questions via Single-Step Implicit\n  Reasoning over Knowledge Graphs","summary":"  Multi-relation question answering (QA) is a challenging task, where given\nquestions usually require long reasoning chains in KGs that consist of multiple\nrelations. Recently, methods with explicit multi-step reasoning over KGs have\nbeen prominently used in this task and have demonstrated promising performance.\nExamples include methods that perform stepwise label propagation through KG\ntriples and methods that navigate over KG triples based on reinforcement\nlearning. A main weakness of these methods is that their reasoning mechanisms\nare usually complex and difficult to implement or train. In this paper, we\nargue that multi-relation QA can be achieved via end-to-end single-step\nimplicit reasoning, which is simpler, more efficient, and easier to adopt. We\npropose QAGCN -- a Question-Aware Graph Convolutional Network (GCN)-based\nmethod that includes a novel GCN architecture with controlled\nquestion-dependent message propagation for the implicit reasoning. Extensive\nexperiments have been conducted, where QAGCN achieved competitive and even\nsuperior performance compared to state-of-the-art explicit-reasoning methods.\nOur code and pre-trained models are available in the repository:\nhttps://github.com/ruijie-wang-uzh/QAGCN\n","authors":["Ruijie Wang","Luca Rossetto","Michael Cochez","Abraham Bernstein"],"pdf_url":"https://arxiv.org/pdf/2206.01818v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19820v1","updated":"2024-03-28T20:11:34Z","published":"2024-03-28T20:11:34Z","title":"Evaluating Explanatory Capabilities of Machine Learning Models in\n  Medical Diagnostics: A Human-in-the-Loop Approach","summary":"  This paper presents a comprehensive study on the evaluation of explanatory\ncapabilities of machine learning models, with a focus on Decision Trees, Random\nForest and XGBoost models using a pancreatic cancer dataset. We use\nHuman-in-the-Loop related techniques and medical guidelines as a source of\ndomain knowledge to establish the importance of the different features that are\nrelevant to establish a pancreatic cancer treatment. These features are not\nonly used as a dimensionality reduction approach for the machine learning\nmodels, but also as way to evaluate the explainability capabilities of the\ndifferent models using agnostic and non-agnostic explainability techniques. To\nfacilitate interpretation of explanatory results, we propose the use of\nsimilarity measures such as the Weighted Jaccard Similarity coefficient. The\ngoal is to not only select the best performing model but also the one that can\nbest explain its conclusions and aligns with human domain knowledge.\n","authors":["José Bobes-Bascarán","Eduardo Mosqueira-Rey","Ángel Fernández-Leal","Elena Hernández-Pereira","David Alonso-Ríos","Vicente Moret-Bonillo","Israel Figueirido-Arnoso","Yolanda Vidal-Ínsua"],"pdf_url":"https://arxiv.org/pdf/2403.19820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19816v1","updated":"2024-03-28T20:01:35Z","published":"2024-03-28T20:01:35Z","title":"The State of Lithium-Ion Battery Health Prognostics in the CPS Era","summary":"  Lithium-ion batteries (Li-ion) have revolutionized energy storage technology,\nbecoming integral to our daily lives by powering a diverse range of devices and\napplications. Their high energy density, fast power response, recyclability,\nand mobility advantages have made them the preferred choice for numerous\nsectors. This paper explores the seamless integration of Prognostics and Health\nManagement within batteries, presenting a multidisciplinary approach that\nenhances the reliability, safety, and performance of these powerhouses.\nRemaining useful life (RUL), a critical concept in prognostics, is examined in\ndepth, emphasizing its role in predicting component failure before it occurs.\nThe paper reviews various RUL prediction methods, from traditional models to\ncutting-edge data-driven techniques. Furthermore, it highlights the paradigm\nshift toward deep learning architectures within the field of Li-ion battery\nhealth prognostics, elucidating the pivotal role of deep learning in addressing\nbattery system complexities. Practical applications of PHM across industries\nare also explored, offering readers insights into real-world\nimplementations.This paper serves as a comprehensive guide, catering to both\nresearchers and practitioners in the field of Li-ion battery PHM.\n","authors":["Gaurav Shinde","Rohan Mohapatra","Pooja Krishan","Harish Garg","Srikanth Prabhu","Sanchari Das","Mohammad Masum","Saptarshi Sengupta"],"pdf_url":"https://arxiv.org/pdf/2403.19816v1.pdf","comment":"18 pages, 12 figures, 6 tables. arXiv admin note: text overlap with\n  arXiv:2310.00023"},{"id":"http://arxiv.org/abs/2311.02402v2","updated":"2024-03-28T20:01:02Z","published":"2023-11-04T13:28:06Z","title":"Hybrid quantum image classification and federated learning for hepatic\n  steatosis diagnosis","summary":"  In the realm of liver transplantation, accurately determining hepatic\nsteatosis levels is crucial. Recognizing the essential need for improved\ndiagnostic precision, particularly for optimizing diagnosis time by swiftly\nhandling easy-to-solve cases and allowing the expert time to focus on more\ncomplex cases, this study aims to develop cutting-edge algorithms that enhance\nthe classification of liver biopsy images. Additionally, the challenge of\nmaintaining data privacy arises when creating automated algorithmic solutions,\nas sharing patient data between hospitals is restricted, further complicating\nthe development and validation process. This research tackles diagnostic\naccuracy by leveraging novel techniques from the rapidly evolving field of\nquantum machine learning, known for their superior generalization abilities.\nConcurrently, it addresses privacy concerns through the implementation of\nprivacy-conscious collaborative machine learning with federated learning. We\nintroduce a hybrid quantum neural network model that leverages real-world\nclinical data to assess non-alcoholic liver steatosis accurately. This model\nachieves an image classification accuracy of 97%, surpassing traditional\nmethods by 1.8%. Moreover, by employing a federated learning approach that\nallows data from different clients to be shared while ensuring privacy, we\nmaintain an accuracy rate exceeding 90%. This initiative marks a significant\nstep towards a scalable, collaborative, efficient, and dependable computational\nframework that aids clinical pathologists in their daily diagnostic tasks.\n","authors":["Luca Lusnig","Asel Sagingalieva","Mikhail Surmach","Tatjana Protasevich","Ovidiu Michiu","Joseph McLoughlin","Christopher Mansell","Graziano de' Petris","Deborah Bonazza","Fabrizio Zanconati","Alexey Melnikov","Fabio Cavalli"],"pdf_url":"https://arxiv.org/pdf/2311.02402v2.pdf","comment":"13 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2310.05764v3","updated":"2024-03-28T19:53:36Z","published":"2023-10-09T14:45:33Z","title":"Harmonic Self-Conditioned Flow Matching for Multi-Ligand Docking and\n  Binding Site Design","summary":"  A significant amount of protein function requires binding small molecules,\nincluding enzymatic catalysis. As such, designing binding pockets for small\nmolecules has several impactful applications ranging from drug synthesis to\nenergy storage. Towards this goal, we first develop HarmonicFlow, an improved\ngenerative process over 3D protein-ligand binding structures based on our\nself-conditioned flow matching objective. FlowSite extends this flow model to\njointly generate a protein pocket's discrete residue types and the molecule's\nbinding 3D structure. We show that HarmonicFlow improves upon state-of-the-art\ngenerative processes for docking in simplicity, generality, and average sample\nquality in pocket-level docking. Enabled by this structure modeling, FlowSite\ndesigns binding sites substantially better than baseline approaches.\n","authors":["Hannes Stärk","Bowen Jing","Regina Barzilay","Tommi Jaakkola"],"pdf_url":"https://arxiv.org/pdf/2310.05764v3.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2305.03039v2","updated":"2024-03-28T19:51:55Z","published":"2023-05-04T17:57:54Z","title":"SuperNOVA: Design Strategies and Opportunities for Interactive\n  Visualization in Computational Notebooks","summary":"  Computational notebooks, such as Jupyter Notebook, have become data\nscientists' de facto programming environments. Many visualization researchers\nand practitioners have developed interactive visualization tools that support\nnotebooks, yet little is known about the appropriate design of these tools. To\naddress this critical research gap, we investigate the design strategies in\nthis space by analyzing 163 notebook visualization tools. Our analysis\nencompasses 64 systems from academic papers and 105 systems sourced from a pool\nof 55k notebooks containing interactive visualizations that we obtain via\nscraping 8.6 million notebooks on GitHub. Through this study, we identify key\ndesign implications and trade-offs, such as leveraging multimodal data in\nnotebooks as well as balancing the degree of visualization-notebook\nintegration. Furthermore, we provide empirical evidence that tools compatible\nwith more notebook platforms have a greater impact. Finally, we develop\nSuperNOVA, an open-source interactive browser to help researchers explore\nexisting notebook visualization tools. SuperNOVA is publicly accessible at:\nhttps://poloclub.github.io/supernova/.\n","authors":["Zijie J. Wang","David Munechika","Seongmin Lee","Duen Horng Chau"],"pdf_url":"https://arxiv.org/pdf/2305.03039v2.pdf","comment":"Accepted at CHI 2024 (Late-Breaking Work). 17 pages, 11 figures, 1\n  table. SuperNOVA is available at: http://poloclub.github.io/supernova/. The\n  code is available at: https://github.com/poloclub/supernova"},{"id":"http://arxiv.org/abs/2402.10189v2","updated":"2024-03-28T19:41:34Z","published":"2024-02-15T18:46:24Z","title":"Uncertainty Quantification for In-Context Learning of Large Language\n  Models","summary":"  In-context learning has emerged as a groundbreaking ability of Large Language\nModels (LLMs) and revolutionized various fields by providing a few\ntask-relevant demonstrations in the prompt. However, trustworthy issues with\nLLM's response, such as hallucination, have also been actively discussed.\nExisting works have been devoted to quantifying the uncertainty in LLM's\nresponse, but they often overlook the complex nature of LLMs and the uniqueness\nof in-context learning. In this work, we delve into the predictive uncertainty\nof LLMs associated with in-context learning, highlighting that such\nuncertainties may stem from both the provided demonstrations (aleatoric\nuncertainty) and ambiguities tied to the model's configurations (epistemic\nuncertainty). We propose a novel formulation and corresponding estimation\nmethod to quantify both types of uncertainties. The proposed method offers an\nunsupervised way to understand the prediction of in-context learning in a\nplug-and-play fashion. Extensive experiments are conducted to demonstrate the\neffectiveness of the decomposition. The code and data are available at:\nhttps://github.com/lingchen0331/UQ_ICL.\n","authors":["Chen Ling","Xujiang Zhao","Xuchao Zhang","Wei Cheng","Yanchi Liu","Yiyou Sun","Mika Oishi","Takao Osaki","Katsushi Matsuda","Jie Ji","Guangji Bai","Liang Zhao","Haifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2402.10189v2.pdf","comment":"Accepted to the main conference of NAACL 2024"},{"id":"http://arxiv.org/abs/2403.19806v1","updated":"2024-03-28T19:41:17Z","published":"2024-03-28T19:41:17Z","title":"Feature-Based Echo-State Networks: A Step Towards Interpretability and\n  Minimalism in Reservoir Computer","summary":"  This paper proposes a novel and interpretable recurrent neural-network\nstructure using the echo-state network (ESN) paradigm for time-series\nprediction. While the traditional ESNs perform well for dynamical systems\nprediction, it needs a large dynamic reservoir with increased computational\ncomplexity. It also lacks interpretability to discern contributions from\ndifferent input combinations to the output. Here, a systematic reservoir\narchitecture is developed using smaller parallel reservoirs driven by different\ninput combinations, known as features, and then they are nonlinearly combined\nto produce the output. The resultant feature-based ESN (Feat-ESN) outperforms\nthe traditional single-reservoir ESN with less reservoir nodes. The predictive\ncapability of the proposed architecture is demonstrated on three systems: two\nsynthetic datasets from chaotic dynamical systems and a set of real-time\ntraffic data.\n","authors":["Debdipta Goswami"],"pdf_url":"https://arxiv.org/pdf/2403.19806v1.pdf","comment":"6 pages, 12 figures, 1 table. arXiv admin note: substantial text\n  overlap with arXiv:2304.00198, arXiv:2211.05992"},{"id":"http://arxiv.org/abs/2310.14085v4","updated":"2024-03-28T19:37:02Z","published":"2023-10-21T18:38:13Z","title":"Adaptive, Doubly Optimal No-Regret Learning in Strongly Monotone and\n  Exp-Concave Games with Gradient Feedback","summary":"  Online gradient descent (OGD) is well known to be doubly optimal under strong\nconvexity or monotonicity assumptions: (1) in the single-agent setting, it\nachieves an optimal regret of $\\Theta(\\log T)$ for strongly convex cost\nfunctions; and (2) in the multi-agent setting of strongly monotone games, with\neach agent employing OGD, we obtain last-iterate convergence of the joint\naction to a unique Nash equilibrium at an optimal rate of\n$\\Theta(\\frac{1}{T})$. While these finite-time guarantees highlight its merits,\nOGD has the drawback that it requires knowing the strong convexity/monotonicity\nparameters. In this paper, we design a fully adaptive OGD algorithm,\n\\textsf{AdaOGD}, that does not require a priori knowledge of these parameters.\nIn the single-agent setting, our algorithm achieves $O(\\log^2(T))$ regret under\nstrong convexity, which is optimal up to a log factor. Further, if each agent\nemploys \\textsf{AdaOGD} in strongly monotone games, the joint action converges\nin a last-iterate sense to a unique Nash equilibrium at a rate of\n$O(\\frac{\\log^3 T}{T})$, again optimal up to log factors. We illustrate our\nalgorithms in a learning version of the classical newsvendor problem, where due\nto lost sales, only (noisy) gradient feedback can be observed. Our results\nimmediately yield the first feasible and near-optimal algorithm for both the\nsingle-retailer and multi-retailer settings. We also extend our results to the\nmore general setting of exp-concave cost functions and games, using the online\nNewton step (ONS) algorithm.\n","authors":["Michael I. Jordan","Tianyi Lin","Zhengyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.14085v4.pdf","comment":"Accepted by Operations Research; 47 pages"},{"id":"http://arxiv.org/abs/2302.04181v3","updated":"2024-03-28T19:35:01Z","published":"2023-02-08T16:40:11Z","title":"Attending to Graph Transformers","summary":"  Recently, transformer architectures for graphs emerged as an alternative to\nestablished techniques for machine learning with graphs, such as\n(message-passing) graph neural networks. So far, they have shown promising\nempirical results, e.g., on molecular prediction datasets, often attributed to\ntheir ability to circumvent graph neural networks' shortcomings, such as\nover-smoothing and over-squashing. Here, we derive a taxonomy of graph\ntransformer architectures, bringing some order to this emerging field. We\noverview their theoretical properties, survey structural and positional\nencodings, and discuss extensions for important graph classes, e.g., 3D\nmolecular graphs. Empirically, we probe how well graph transformers can recover\nvarious graph properties, how well they can deal with heterophilic graphs, and\nto what extent they prevent over-squashing. Further, we outline open challenges\nand research direction to stimulate future work. Our code is available at\nhttps://github.com/luis-mueller/probing-graph-transformers.\n","authors":["Luis Müller","Mikhail Galkin","Christopher Morris","Ladislav Rampášek"],"pdf_url":"https://arxiv.org/pdf/2302.04181v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19800v1","updated":"2024-03-28T19:29:17Z","published":"2024-03-28T19:29:17Z","title":"Gegenbauer Graph Neural Networks for Time-varying Signal Reconstruction","summary":"  Reconstructing time-varying graph signals (or graph time-series imputation)\nis a critical problem in machine learning and signal processing with broad\napplications, ranging from missing data imputation in sensor networks to\ntime-series forecasting. Accurately capturing the spatio-temporal information\ninherent in these signals is crucial for effectively addressing these tasks.\nHowever, existing approaches relying on smoothness assumptions of temporal\ndifferences and simple convex optimization techniques have inherent\nlimitations. To address these challenges, we propose a novel approach that\nincorporates a learning module to enhance the accuracy of the downstream task.\nTo this end, we introduce the Gegenbauer-based graph convolutional (GegenConv)\noperator, which is a generalization of the conventional Chebyshev graph\nconvolution by leveraging the theory of Gegenbauer polynomials. By deviating\nfrom traditional convex problems, we expand the complexity of the model and\noffer a more accurate solution for recovering time-varying graph signals.\nBuilding upon GegenConv, we design the Gegenbauer-based time Graph Neural\nNetwork (GegenGNN) architecture, which adopts an encoder-decoder structure.\nLikewise, our approach also utilizes a dedicated loss function that\nincorporates a mean squared error component alongside Sobolev smoothness\nregularization. This combination enables GegenGNN to capture both the fidelity\nto ground truth and the underlying smoothness properties of the signals,\nenhancing the reconstruction performance. We conduct extensive experiments on\nreal datasets to evaluate the effectiveness of our proposed approach. The\nexperimental results demonstrate that GegenGNN outperforms state-of-the-art\nmethods, showcasing its superior capability in recovering time-varying graph\nsignals.\n","authors":["Jhon A. Castro-Correa","Jhony H. Giraldo","Mohsen Badiey","Fragkiskos D. Malliaros"],"pdf_url":"https://arxiv.org/pdf/2403.19800v1.pdf","comment":"Accepted by IEEE Transactions on Neural Networks and Learning Systems\n  (TNNLS)"},{"id":"http://arxiv.org/abs/2403.19792v1","updated":"2024-03-28T19:17:54Z","published":"2024-03-28T19:17:54Z","title":"MAPL: Model Agnostic Peer-to-peer Learning","summary":"  Effective collaboration among heterogeneous clients in a decentralized\nsetting is a rather unexplored avenue in the literature. To structurally\naddress this, we introduce Model Agnostic Peer-to-peer Learning (coined as\nMAPL) a novel approach to simultaneously learn heterogeneous personalized\nmodels as well as a collaboration graph through peer-to-peer communication\namong neighboring clients. MAPL is comprised of two main modules: (i)\nlocal-level Personalized Model Learning (PML), leveraging a combination of\nintra- and inter-client contrastive losses; (ii) network-wide decentralized\nCollaborative Graph Learning (CGL) dynamically refining collaboration weights\nin a privacy-preserving manner based on local task similarities. Our extensive\nexperimentation demonstrates the efficacy of MAPL and its competitive (or, in\nmost cases, superior) performance compared to its centralized model-agnostic\ncounterparts, without relying on any central server. Our code is available and\ncan be accessed here: https://github.com/SayakMukherjee/MAPL\n","authors":["Sayak Mukherjee","Andrea Simonetto","Hadi Jamali-Rad"],"pdf_url":"https://arxiv.org/pdf/2403.19792v1.pdf","comment":"Our code is available and can be accessed here:\n  https://github.com/SayakMukherjee/MAPL"},{"id":"http://arxiv.org/abs/2403.19783v1","updated":"2024-03-28T19:09:46Z","published":"2024-03-28T19:09:46Z","title":"AlloyBERT: Alloy Property Prediction with Large Language Models","summary":"  The pursuit of novel alloys tailored to specific requirements poses\nsignificant challenges for researchers in the field. This underscores the\nimportance of developing predictive techniques for essential physical\nproperties of alloys based on their chemical composition and processing\nparameters. This study introduces AlloyBERT, a transformer encoder-based model\ndesigned to predict properties such as elastic modulus and yield strength of\nalloys using textual inputs. Leveraging the pre-trained RoBERTa encoder model\nas its foundation, AlloyBERT employs self-attention mechanisms to establish\nmeaningful relationships between words, enabling it to interpret human-readable\ninput and predict target alloy properties. By combining a tokenizer trained on\nour textual data and a RoBERTa encoder pre-trained and fine-tuned for this\nspecific task, we achieved a mean squared error (MSE) of 0.00015 on the Multi\nPrincipal Elemental Alloys (MPEA) data set and 0.00611 on the Refractory Alloy\nYield Strength (RAYS) dataset. This surpasses the performance of shallow\nmodels, which achieved a best-case MSE of 0.00025 and 0.0076 on the MPEA and\nRAYS datasets respectively. Our results highlight the potential of language\nmodels in material science and establish a foundational framework for\ntext-based prediction of alloy properties that does not rely on complex\nunderlying representations, calculations, or simulations.\n","authors":["Akshat Chaudhari","Chakradhar Guntuboina","Hongshuo Huang","Amir Barati Farimani"],"pdf_url":"https://arxiv.org/pdf/2403.19783v1.pdf","comment":"20 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.19781v1","updated":"2024-03-28T19:06:50Z","published":"2024-03-28T19:06:50Z","title":"Reinforcement Learning in Agent-Based Market Simulation: Unveiling\n  Realistic Stylized Facts and Behavior","summary":"  Investors and regulators can greatly benefit from a realistic market\nsimulator that enables them to anticipate the consequences of their decisions\nin real markets. However, traditional rule-based market simulators often fall\nshort in accurately capturing the dynamic behavior of market participants,\nparticularly in response to external market impact events or changes in the\nbehavior of other participants. In this study, we explore an agent-based\nsimulation framework employing reinforcement learning (RL) agents. We present\nthe implementation details of these RL agents and demonstrate that the\nsimulated market exhibits realistic stylized facts observed in real-world\nmarkets. Furthermore, we investigate the behavior of RL agents when confronted\nwith external market impacts, such as a flash crash. Our findings shed light on\nthe effectiveness and adaptability of RL-based agents within the simulation,\noffering insights into their response to significant market events.\n","authors":["Zhiyuan Yao","Zheng Li","Matthew Thomas","Ionut Florescu"],"pdf_url":"https://arxiv.org/pdf/2403.19781v1.pdf","comment":"Accpeted in IJCNN 2024"},{"id":"http://arxiv.org/abs/2403.19776v1","updated":"2024-03-28T18:58:43Z","published":"2024-03-28T18:58:43Z","title":"CLoRA: A Contrastive Approach to Compose Multiple LoRA Models","summary":"  Low-Rank Adaptations (LoRAs) have emerged as a powerful and popular technique\nin the field of image generation, offering a highly effective way to adapt and\nrefine pre-trained deep learning models for specific tasks without the need for\ncomprehensive retraining. By employing pre-trained LoRA models, such as those\nrepresenting a specific cat and a particular dog, the objective is to generate\nan image that faithfully embodies both animals as defined by the LoRAs.\nHowever, the task of seamlessly blending multiple concept LoRAs to capture a\nvariety of concepts in one image proves to be a significant challenge. Common\napproaches often fall short, primarily because the attention mechanisms within\ndifferent LoRA models overlap, leading to scenarios where one concept may be\ncompletely ignored (e.g., omitting the dog) or where concepts are incorrectly\ncombined (e.g., producing an image of two cats instead of one cat and one dog).\nTo overcome these issues, CLoRA addresses them by updating the attention maps\nof multiple LoRA models and leveraging them to create semantic masks that\nfacilitate the fusion of latent representations. Our method enables the\ncreation of composite images that truly reflect the characteristics of each\nLoRA, successfully merging multiple concepts or styles. Our comprehensive\nevaluations, both qualitative and quantitative, demonstrate that our approach\noutperforms existing methodologies, marking a significant advancement in the\nfield of image generation with LoRAs. Furthermore, we share our source code,\nbenchmark dataset, and trained LoRA models to promote further research on this\ntopic.\n","authors":["Tuna Han Salih Meral","Enis Simsar","Federico Tombari","Pinar Yanardag"],"pdf_url":"https://arxiv.org/pdf/2403.19776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.13821v3","updated":"2024-03-28T18:45:43Z","published":"2023-01-31T18:07:26Z","title":"Complete Neural Networks for Complete Euclidean Graphs","summary":"  Neural networks for point clouds, which respect their natural invariance to\npermutation and rigid motion, have enjoyed recent success in modeling geometric\nphenomena, from molecular dynamics to recommender systems. Yet, to date, no\nmodel with polynomial complexity is known to be complete, that is, able to\ndistinguish between any pair of non-isomorphic point clouds. We fill this\ntheoretical gap by showing that point clouds can be completely determined, up\nto permutation and rigid motion, by applying the 3-WL graph isomorphism test to\nthe point cloud's centralized Gram matrix. Moreover, we formulate an Euclidean\nvariant of the 2-WL test and show that it is also sufficient to achieve\ncompleteness. We then show how our complete Euclidean WL tests can be simulated\nby an Euclidean graph neural network of moderate size and demonstrate their\nseparation capability on highly symmetrical point clouds.\n","authors":["Snir Hordan","Tal Amir","Steven J. Gortler","Nadav Dym"],"pdf_url":"https://arxiv.org/pdf/2301.13821v3.pdf","comment":"The 38th AAAI Conference on Artificial Intelligence"},{"id":"http://arxiv.org/abs/2403.19770v1","updated":"2024-03-28T18:45:43Z","published":"2024-03-28T18:45:43Z","title":"Hierarchical Deep Learning for Intention Estimation of Teleoperation\n  Manipulation in Assembly Tasks","summary":"  In human-robot collaboration, shared control presents an opportunity to\nteleoperate robotic manipulation to improve the efficiency of manufacturing and\nassembly processes. Robots are expected to assist in executing the user's\nintentions. To this end, robust and prompt intention estimation is needed,\nrelying on behavioral observations. The framework presents an intention\nestimation technique at hierarchical levels i.e., low-level actions and\nhigh-level tasks, by incorporating multi-scale hierarchical information in\nneural networks. Technically, we employ hierarchical dependency loss to boost\noverall accuracy. Furthermore, we propose a multi-window method that assigns\nproper hierarchical prediction windows of input data. An analysis of the\npredictive power with various inputs demonstrates the predominance of the deep\nhierarchical model in the sense of prediction accuracy and early intention\nidentification. We implement the algorithm on a virtual reality (VR) setup to\nteleoperate robotic hands in a simulation with various assembly tasks to show\nthe effectiveness of online estimation.\n","authors":["Mingyu Cai","Karankumar Patel","Soshi Iba","Songpo Li"],"pdf_url":"https://arxiv.org/pdf/2403.19770v1.pdf","comment":"ICRA 2024"},{"id":"http://arxiv.org/abs/2203.13534v2","updated":"2024-03-28T18:38:18Z","published":"2022-03-25T09:33:49Z","title":"Generalization bounds for learning under graph-dependence: A survey","summary":"  Traditional statistical learning theory relies on the assumption that data\nare identically and independently distributed (i.i.d.). However, this\nassumption often does not hold in many real-life applications. In this survey,\nwe explore learning scenarios where examples are dependent and their dependence\nrelationship is described by a dependency graph, a commonly utilized model in\nprobability and combinatorics. We collect various graph-dependent concentration\nbounds, which are then used to derive Rademacher complexity and stability\ngeneralization bounds for learning from graph-dependent data. We illustrate\nthis paradigm through practical learning tasks and provide some research\ndirections for future work. To our knowledge, this survey is the first of this\nkind on this subject.\n","authors":["Rui-Ray Zhang","Massih-Reza Amini"],"pdf_url":"https://arxiv.org/pdf/2203.13534v2.pdf","comment":"To appear in Machine Learning Journal"},{"id":"http://arxiv.org/abs/2302.06089v5","updated":"2024-03-28T18:31:28Z","published":"2023-02-13T04:17:47Z","title":"Federated attention consistent learning models for prostate cancer\n  diagnosis and Gleason grading","summary":"  Artificial intelligence (AI) holds significant promise in transforming\nmedical imaging, enhancing diagnostics, and refining treatment strategies.\nHowever, the reliance on extensive multicenter datasets for training AI models\nposes challenges due to privacy concerns. Federated learning provides a\nsolution by facilitating collaborative model training across multiple centers\nwithout sharing raw data. This study introduces a federated\nattention-consistent learning (FACL) framework to address challenges associated\nwith large-scale pathological images and data heterogeneity. FACL enhances\nmodel generalization by maximizing attention consistency between local clients\nand the server model. To ensure privacy and validate robustness, we\nincorporated differential privacy by introducing noise during parameter\ntransfer. We assessed the effectiveness of FACL in cancer diagnosis and Gleason\ngrading tasks using 19,461 whole-slide images of prostate cancer from multiple\ncenters. In the diagnosis task, FACL achieved an area under the curve (AUC) of\n0.9718, outperforming seven centers with an average AUC of 0.9499 when\ncategories are relatively balanced. For the Gleason grading task, FACL attained\na Kappa score of 0.8463, surpassing the average Kappa score of 0.7379 from six\ncenters. In conclusion, FACL offers a robust, accurate, and cost-effective AI\ntraining model for prostate cancer pathology while maintaining effective data\nsafeguards.\n","authors":["Fei Kong","Xiyue Wang","Jinxi Xiang","Sen Yang","Xinran Wang","Meng Yue","Jun Zhang","Junhan Zhao","Xiao Han","Yuhan Dong","Biyue Zhu","Fang Wang","Yueping Liu"],"pdf_url":"https://arxiv.org/pdf/2302.06089v5.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2311.17693v2","updated":"2024-03-28T18:24:46Z","published":"2023-11-29T15:00:06Z","title":"Toward a Surgeon-in-the-Loop Ophthalmic Robotic Apprentice using\n  Reinforcement and Imitation Learning","summary":"  Robotic-assisted surgical systems have demonstrated significant potential in\nenhancing surgical precision and minimizing human errors. However, existing\nsystems lack the ability to accommodate the unique preferences and requirements\nof individual surgeons. Additionally, they primarily focus on general surgeries\n(e.g., laparoscopy) and are not suitable for highly precise microsurgeries,\nsuch as ophthalmic procedures. Thus, we propose a simulation-based image-guided\napproach for surgeon-centered autonomous agents that can adapt to the\nindividual surgeon's skill level and preferred surgical techniques during\nophthalmic cataract surgery. Our approach utilizes a simulated environment to\ntrain reinforcement and imitation learning agents guided by image data to\nperform all tasks of the incision phase of cataract surgery. By integrating the\nsurgeon's actions and preferences into the training process with the\nsurgeon-in-the-loop, our approach enables the robot to implicitly learn and\nadapt to the individual surgeon's unique approach through demonstrations. This\nresults in a more intuitive and personalized surgical experience for the\nsurgeon. Simultaneously, it ensures consistent performance for the autonomous\nrobotic apprentice. We define and evaluate the effectiveness of our approach\nusing our proposed metrics; and highlight the trade-off between a generic agent\nand a surgeon-centered adapted agent. Moreover, our approach has the potential\nto extend to other ophthalmic surgical procedures, opening the door to a new\ngeneration of surgeon-in-the-loop autonomous surgical robots. We provide an\nopen-source simulation framework for future development and reproducibility.\n","authors":["Amr Gomaa","Bilal Mahdy","Niko Kleer","Antonio Krüger"],"pdf_url":"https://arxiv.org/pdf/2311.17693v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2403.19651v1","updated":"2024-03-28T17:59:20Z","published":"2024-03-28T17:59:20Z","title":"MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions","summary":"  Image retrieval, i.e., finding desired images given a reference image,\ninherently encompasses rich, multi-faceted search intents that are difficult to\ncapture solely using image-based measures. Recent work leverages text\ninstructions to allow users to more freely express their search intents.\nHowever, existing work primarily focuses on image pairs that are visually\nsimilar and/or can be characterized by a small set of pre-defined relations.\nThe core thesis of this paper is that text instructions can enable retrieving\nimages with richer relations beyond visual similarity. To show this, we\nintroduce MagicLens, a series of self-supervised image retrieval models that\nsupport open-ended instructions. MagicLens is built on a key novel insight:\nimage pairs that naturally occur on the same web pages contain a wide range of\nimplicit relations (e.g., inside view of), and we can bring those implicit\nrelations explicit by synthesizing instructions via large multimodal models\n(LMMs) and large language models (LLMs). Trained on 36.7M (query image,\ninstruction, target image) triplets with rich semantic relations mined from the\nweb, MagicLens achieves comparable or better results on eight benchmarks of\nvarious image retrieval tasks than prior state-of-the-art (SOTA) methods.\nRemarkably, it outperforms previous SOTA but with a 50X smaller model size on\nmultiple benchmarks. Additional human analyses on a 1.4M-image unseen corpus\nfurther demonstrate the diversity of search intents supported by MagicLens.\n","authors":["Kai Zhang","Yi Luan","Hexiang Hu","Kenton Lee","Siyuan Qiao","Wenhu Chen","Yu Su","Ming-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2403.19651v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2303.08737v2","updated":"2024-03-28T16:59:24Z","published":"2023-03-15T16:21:50Z","title":"Evaluating gesture generation in a large-scale open challenge: The GENEA\n  Challenge 2022","summary":"  This paper reports on the second GENEA Challenge to benchmark data-driven\nautomatic co-speech gesture generation. Participating teams used the same\nspeech and motion dataset to build gesture-generation systems. Motion generated\nby all these systems was rendered to video using a standardised visualisation\npipeline and evaluated in several large, crowdsourced user studies. Unlike when\ncomparing different research papers, differences in results are here only due\nto differences between methods, enabling direct comparison between systems. The\ndataset was based on 18 hours of full-body motion capture, including fingers,\nof different persons engaging in a dyadic conversation. Ten teams participated\nin the challenge across two tiers: full-body and upper-body gesticulation. For\neach tier, we evaluated both the human-likeness of the gesture motion and its\nappropriateness for the specific speech signal. Our evaluations decouple\nhuman-likeness from gesture appropriateness, which has been a difficult problem\nin the field.\n  The evaluation results show some synthetic gesture conditions being rated as\nsignificantly more human-like than 3D human motion capture. To the best of our\nknowledge, this has not been demonstrated before. On the other hand, all\nsynthetic motion is found to be vastly less appropriate for the speech than the\noriginal motion-capture recordings. We also find that conventional objective\nmetrics do not correlate well with subjective human-likeness ratings in this\nlarge evaluation. The one exception is the Fr\\'echet gesture distance (FGD),\nwhich achieves a Kendall's tau rank correlation of around $-0.5$. Based on the\nchallenge results we formulate numerous recommendations for system building and\nevaluation.\n","authors":["Taras Kucherenko","Pieter Wolfert","Youngwoo Yoon","Carla Viegas","Teodor Nikolov","Mihail Tsakov","Gustav Eje Henter"],"pdf_url":"https://arxiv.org/pdf/2303.08737v2.pdf","comment":"The first three authors made equal contributions and share joint\n  first authorship. Accepted for publication in the ACM Transactions on\n  Graphics (TOG).Please see https://youngwoo-yoon.github.io/GENEAchallenge2022/\n  for all challenge materials. arXiv admin note: text overlap with\n  arXiv:2208.10441"},{"id":"http://arxiv.org/abs/2403.19456v1","updated":"2024-03-28T14:27:36Z","published":"2024-03-28T14:27:36Z","title":"Break-for-Make: Modular Low-Rank Adaptations for Composable\n  Content-Style Customization","summary":"  Personalized generation paradigms empower designers to customize visual\nintellectual properties with the help of textual descriptions by tuning or\nadapting pre-trained text-to-image models on a few images. Recent works explore\napproaches for concurrently customizing both content and detailed visual style\nappearance. However, these existing approaches often generate images where the\ncontent and style are entangled. In this study, we reconsider the customization\nof content and style concepts from the perspective of parameter space\nconstruction. Unlike existing methods that utilize a shared parameter space for\ncontent and style, we propose a learning framework that separates the parameter\nspace to facilitate individual learning of content and style, thereby enabling\ndisentangled content and style. To achieve this goal, we introduce \"partly\nlearnable projection\" (PLP) matrices to separate the original adapters into\ndivided sub-parameter spaces. We propose \"break-for-make\" customization\nlearning pipeline based on PLP, which is simple yet effective. We break the\noriginal adapters into \"up projection\" and \"down projection\", train content and\nstyle PLPs individually with the guidance of corresponding textual prompts in\nthe separate adapters, and maintain generalization by employing a\nmulti-correspondence projection learning strategy. Based on the adapters broken\napart for separate training content and style, we then make the entity\nparameter space by reconstructing the content and style PLPs matrices, followed\nby fine-tuning the combined adapter to generate the target object with the\ndesired appearance. Experiments on various styles, including textures,\nmaterials, and artistic style, show that our method outperforms\nstate-of-the-art single/multiple concept learning pipelines in terms of\ncontent-style-prompt alignment.\n","authors":["Yu Xu","Fan Tang","Juan Cao","Yuxin Zhang","Oliver Deussen","Weiming Dong","Jintao Li","Tong-Yee Lee"],"pdf_url":"https://arxiv.org/pdf/2403.19456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05608v3","updated":"2024-03-28T08:28:44Z","published":"2024-02-08T12:08:42Z","title":"Scalable Diffusion Models with State Space Backbone","summary":"  This paper presents a new exploration into a category of diffusion models\nbuilt upon state space architecture. We endeavor to train diffusion models for\nimage data, wherein the traditional U-Net backbone is supplanted by a state\nspace backbone, functioning on raw patches or latent space. Given its notable\nefficacy in accommodating long-range dependencies, Diffusion State Space Models\n(DiS) are distinguished by treating all inputs including time, condition, and\nnoisy image patches as tokens. Our assessment of DiS encompasses both\nunconditional and class-conditional image generation scenarios, revealing that\nDiS exhibits comparable, if not superior, performance to CNN-based or\nTransformer-based U-Net architectures of commensurate size. Furthermore, we\nanalyze the scalability of DiS, gauged by the forward pass complexity\nquantified in Gflops. DiS models with higher Gflops, achieved through\naugmentation of depth/width or augmentation of input tokens, consistently\ndemonstrate lower FID. In addition to demonstrating commendable scalability\ncharacteristics, DiS-H/2 models in latent space achieve performance levels akin\nto prior diffusion models on class-conditional ImageNet benchmarks at the\nresolution of 256$\\times$256 and 512$\\times$512, while significantly reducing\nthe computational burden. The code and models are available at:\nhttps://github.com/feizc/DiS.\n","authors":["Zhengcong Fei","Mingyuan Fan","Changqian Yu","Junshi Huang"],"pdf_url":"https://arxiv.org/pdf/2402.05608v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.09911v3","updated":"2024-03-28T07:16:11Z","published":"2023-08-19T05:34:13Z","title":"Noisy-Correspondence Learning for Text-to-Image Person Re-identification","summary":"  Text-to-image person re-identification (TIReID) is a compelling topic in the\ncross-modal community, which aims to retrieve the target person based on a\ntextual query. Although numerous TIReID methods have been proposed and achieved\npromising performance, they implicitly assume the training image-text pairs are\ncorrectly aligned, which is not always the case in real-world scenarios. In\npractice, the image-text pairs inevitably exist under-correlated or even\nfalse-correlated, a.k.a noisy correspondence (NC), due to the low quality of\nthe images and annotation errors. To address this problem, we propose a novel\nRobust Dual Embedding method (RDE) that can learn robust visual-semantic\nassociations even with NC. Specifically, RDE consists of two main components:\n1) A Confident Consensus Division (CCD) module that leverages the dual-grained\ndecisions of dual embedding modules to obtain a consensus set of clean training\ndata, which enables the model to learn correct and reliable visual-semantic\nassociations. 2) A Triplet Alignment Loss (TAL) relaxes the conventional\nTriplet Ranking loss with the hardest negative samples to a log-exponential\nupper bound over all negative ones, thus preventing the model collapse under NC\nand can also focus on hard-negative samples for promising performance. We\nconduct extensive experiments on three public benchmarks, namely CUHK-PEDES,\nICFG-PEDES, and RSTPReID, to evaluate the performance and robustness of our\nRDE. Our method achieves state-of-the-art results both with and without\nsynthetic noisy correspondences on all three datasets. Code is available at\nhttps://github.com/QinYang79/RDE.\n","authors":["Yang Qin","Yingke Chen","Dezhong Peng","Xi Peng","Joey Tianyi Zhou","Peng Hu"],"pdf_url":"https://arxiv.org/pdf/2308.09911v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19763v1","updated":"2024-03-28T18:24:09Z","published":"2024-03-28T18:24:09Z","title":"Creating Aesthetic Sonifications on the Web with SIREN","summary":"  SIREN is a flexible, extensible, and customizable web-based general-purpose\ninterface for auditory data display (sonification). Designed as a digital audio\nworkstation for sonification, synthesizers written in JavaScript using the Web\nAudio API facilitate intuitive mapping of data to auditory parameters for a\nwide range of purposes.\n  This paper explores the breadth of sound synthesis techniques supported by\nSIREN, and details the structure and definition of a SIREN synthesizer module.\nThe paper proposes further development that will increase SIREN's utility.\n","authors":["Tristan Peng","Hongchan Choi","Jonathan Berger"],"pdf_url":"https://arxiv.org/pdf/2403.19763v1.pdf","comment":"7 pages, 1 figure, 5 listings, submitted to the Web Audio Conference\n  2024"},{"id":"http://arxiv.org/abs/2403.19723v1","updated":"2024-03-28T03:20:54Z","published":"2024-03-28T03:20:54Z","title":"HGT: Leveraging Heterogeneous Graph-enhanced Large Language Models for\n  Few-shot Complex Table Understanding","summary":"  Table understanding (TU) has achieved promising advancements, but it faces\nthe challenges of the scarcity of manually labeled tables and the presence of\ncomplex table structures.To address these challenges, we propose HGT, a\nframework with a heterogeneous graph (HG)-enhanced large language model (LLM)\nto tackle few-shot TU tasks.It leverages the LLM by aligning the table\nsemantics with the LLM's parametric knowledge through soft prompts and\ninstruction turning and deals with complex tables by a multi-task pre-training\nscheme involving three novel multi-granularity self-supervised HG pre-training\nobjectives.We empirically demonstrate the effectiveness of HGT, showing that it\noutperforms the SOTA for few-shot complex TU on several benchmarks.\n","authors":["Rihui Jin","Yu Li","Guilin Qi","Nan Hu","Yuan-Fang Li","Jiaoyan Chen","Jianan Wang","Yongrui Chen","Dehai Min"],"pdf_url":"https://arxiv.org/pdf/2403.19723v1.pdf","comment":null}]},"2024-03-29T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2403.20331v1","updated":"2024-03-29T17:59:53Z","published":"2024-03-29T17:59:53Z","title":"Unsolvable Problem Detection: Evaluating Trustworthiness of Vision\n  Language Models","summary":"  This paper introduces a novel and significant challenge for Vision Language\nModels (VLMs), termed Unsolvable Problem Detection (UPD). UPD examines the\nVLM's ability to withhold answers when faced with unsolvable problems in the\ncontext of Visual Question Answering (VQA) tasks. UPD encompasses three\ndistinct settings: Absent Answer Detection (AAD), Incompatible Answer Set\nDetection (IASD), and Incompatible Visual Question Detection (IVQD). To deeply\ninvestigate the UPD problem, extensive experiments indicate that most VLMs,\nincluding GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying\nextents, highlighting significant room for the improvements. To address UPD, we\nexplore both training-free and training-based solutions, offering new insights\ninto their effectiveness and limitations. We hope our insights, together with\nfuture efforts within the proposed UPD settings, will enhance the broader\nunderstanding and development of more practical and reliable VLMs.\n","authors":["Atsuyuki Miyai","Jingkang Yang","Jingyang Zhang","Yifei Ming","Qing Yu","Go Irie","Yixuan Li","Hai Li","Ziwei Liu","Kiyoharu Aizawa"],"pdf_url":"https://arxiv.org/pdf/2403.20331v1.pdf","comment":"Code: https://github.com/AtsuMiyai/UPD"},{"id":"http://arxiv.org/abs/2403.07726v3","updated":"2024-03-29T17:59:07Z","published":"2024-03-12T15:06:22Z","title":"SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and\n  Related Observable Overgeneration Mistakes","summary":"  This paper presents the results of the SHROOM, a shared task focused on\ndetecting hallucinations: outputs from natural language generation (NLG)\nsystems that are fluent, yet inaccurate. Such cases of overgeneration put in\njeopardy many NLG applications, where correctness is often mission-critical.\nThe shared task was conducted with a newly constructed dataset of 4000 model\noutputs labeled by 5 annotators each, spanning 3 NLP tasks: machine\ntranslation, paraphrase generation and definition modeling.\n  The shared task was tackled by a total of 58 different users grouped in 42\nteams, out of which 27 elected to write a system description paper;\ncollectively, they submitted over 300 prediction sets on both tracks of the\nshared task. We observe a number of key trends in how this approach was tackled\n-- many participants rely on a handful of model, and often rely either on\nsynthetic data for fine-tuning or zero-shot prompting strategies. While a\nmajority of the teams did outperform our proposed baseline system, the\nperformances of top-scoring systems are still consistent with a random handling\nof the more challenging items.\n","authors":["Timothee Mickus","Elaine Zosa","Raúl Vázquez","Teemu Vahtola","Jörg Tiedemann","Vincent Segonne","Alessandro Raganato","Marianna Apidianaki"],"pdf_url":"https://arxiv.org/pdf/2403.07726v3.pdf","comment":"SemEval 2024 shared task. Pre-review version"},{"id":"http://arxiv.org/abs/2403.20329v1","updated":"2024-03-29T17:59:06Z","published":"2024-03-29T17:59:06Z","title":"ReALM: Reference Resolution As Language Modeling","summary":"  Reference resolution is an important problem, one that is essential to\nunderstand and successfully handle context of different kinds. This context\nincludes both previous turns and context that pertains to non-conversational\nentities, such as entities on the user's screen or those running in the\nbackground. While LLMs have been shown to be extremely powerful for a variety\nof tasks, their use in reference resolution, particularly for\nnon-conversational entities, remains underutilized. This paper demonstrates how\nLLMs can be used to create an extremely effective system to resolve references\nof various types, by showing how reference resolution can be converted into a\nlanguage modeling problem, despite involving forms of entities like those on\nscreen that are not traditionally conducive to being reduced to a text-only\nmodality. We demonstrate large improvements over an existing system with\nsimilar functionality across different types of references, with our smallest\nmodel obtaining absolute gains of over 5% for on-screen references. We also\nbenchmark against GPT-3.5 and GPT-4, with our smallest model achieving\nperformance comparable to that of GPT-4, and our larger models substantially\noutperforming it.\n","authors":["Joel Ruben Antony Moniz","Soundarya Krishnan","Melis Ozyildirim","Prathamesh Saraf","Halim Cagri Ates","Yuan Zhang","Hong Yu","Nidhi Rajshree"],"pdf_url":"https://arxiv.org/pdf/2403.20329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20327v1","updated":"2024-03-29T17:56:40Z","published":"2024-03-29T17:56:40Z","title":"Gecko: Versatile Text Embeddings Distilled from Large Language Models","summary":"  We present Gecko, a compact and versatile text embedding model. Gecko\nachieves strong retrieval performance by leveraging a key idea: distilling\nknowledge from large language models (LLMs) into a retriever. Our two-step\ndistillation process begins with generating diverse, synthetic paired data\nusing an LLM. Next, we further refine the data quality by retrieving a set of\ncandidate passages for each query, and relabeling the positive and hard\nnegative passages using the same LLM. The effectiveness of our approach is\ndemonstrated by the compactness of the Gecko. On the Massive Text Embedding\nBenchmark (MTEB), Gecko with 256 embedding dimensions outperforms all existing\nentries with 768 embedding size. Gecko with 768 embedding dimensions achieves\nan average score of 66.31, competing with 7x larger models and 5x higher\ndimensional embeddings.\n","authors":["Jinhyuk Lee","Zhuyun Dai","Xiaoqi Ren","Blair Chen","Daniel Cer","Jeremy R. Cole","Kai Hui","Michael Boratko","Rajvi Kapadia","Wen Ding","Yi Luan","Sai Meher Karthik Duddu","Gustavo Hernandez Abrego","Weiqiang Shi","Nithi Gupta","Aditya Kusupati","Prateek Jain","Siddhartha Reddy Jonnalagadda","Ming-Wei Chang","Iftekhar Naim"],"pdf_url":"https://arxiv.org/pdf/2403.20327v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2402.16139v2","updated":"2024-03-29T17:51:32Z","published":"2024-02-25T16:36:51Z","title":"What Generative Artificial Intelligence Means for Terminological\n  Definitions","summary":"  This paper examines the impact of Generative Artificial Intelligence (GenAI)\ntools like ChatGPT on the creation and consumption of terminological\ndefinitions. From the terminologist's point of view, the strategic use of GenAI\ntools can streamline the process of crafting definitions, reducing both time\nand effort, while potentially enhancing quality. GenAI tools enable AI-assisted\nterminography, notably post-editing terminography, where the machine produces a\ndefinition that the terminologist then corrects or refines. However, the\npotential of GenAI tools to fulfill all the terminological needs of a user,\nincluding term definitions, challenges the very existence of terminological\ndefinitions and resources as we know them. Unlike terminological definitions,\nGenAI tools can describe the knowledge activated by a term in a specific\ncontext. However, a main drawback of these tools is that their output can\ncontain errors. For this reason, users requiring reliability will likely still\nresort to terminological resources for definitions. Nevertheless, with the\ninevitable integration of AI into terminology work, the distinction between\nhuman-created and AI-created content will become increasingly blurred.\n","authors":["Antonio San Martín"],"pdf_url":"https://arxiv.org/pdf/2402.16139v2.pdf","comment":"37 pages, 1 figure"},{"id":"http://arxiv.org/abs/2403.20322v1","updated":"2024-03-29T17:50:28Z","published":"2024-03-29T17:50:28Z","title":"Towards a Framework for Evaluating Explanations in Automated Fact\n  Verification","summary":"  As deep neural models in NLP become more complex, and as a consequence\nopaque, the necessity to interpret them becomes greater. A burgeoning interest\nhas emerged in rationalizing explanations to provide short and coherent\njustifications for predictions. In this position paper, we advocate for a\nformal framework for key concepts and properties about rationalizing\nexplanations to support their evaluation systematically. We also outline one\nsuch formal framework, tailored to rationalizing explanations of increasingly\ncomplex structures, from free-form explanations to deductive explanations, to\nargumentative explanations (with the richest structure). Focusing on the\nautomated fact verification task, we provide illustrations of the use and\nusefulness of our formalization for evaluating explanations, tailored to their\nvarying structures.\n","authors":["Neema Kotonya","Francesca Toni"],"pdf_url":"https://arxiv.org/pdf/2403.20322v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.20308v1","updated":"2024-03-29T17:22:53Z","published":"2024-03-29T17:22:53Z","title":"ChainNet: Structured Metaphor and Metonymy in WordNet","summary":"  The senses of a word exhibit rich internal structure. In a typical lexicon,\nthis structure is overlooked: a word's senses are encoded as a list without\ninter-sense relations. We present ChainNet, a lexical resource which for the\nfirst time explicitly identifies these structures. ChainNet expresses how\nsenses in the Open English Wordnet are derived from one another: every nominal\nsense of a word is either connected to another sense by metaphor or metonymy,\nor is disconnected in the case of homonymy. Because WordNet senses are linked\nto resources which capture information about their meaning, ChainNet represents\nthe first dataset of grounded metaphor and metonymy.\n","authors":["Rowan Hall Maudslay","Simone Teufel","Francis Bond","James Pustejovsky"],"pdf_url":"https://arxiv.org/pdf/2403.20308v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19432v2","updated":"2024-03-29T17:21:02Z","published":"2024-03-28T14:03:12Z","title":"Uncovering Misattributed Suicide Causes through Annotation Inconsistency\n  Detection in Death Investigation Notes","summary":"  Data accuracy is essential for scientific research and policy development.\nThe National Violent Death Reporting System (NVDRS) data is widely used for\ndiscovering the patterns and causes of death. Recent studies suggested the\nannotation inconsistencies within the NVDRS and the potential impact on\nerroneous suicide-cause attributions. We present an empirical Natural Language\nProcessing (NLP) approach to detect annotation inconsistencies and adopt a\ncross-validation-like paradigm to identify problematic instances. We analyzed\n267,804 suicide death incidents between 2003 and 2020 from the NVDRS. Our\nresults showed that incorporating the target state's data into training the\nsuicide-crisis classifier brought an increase of 5.4% to the F-1 score on the\ntarget state's test set and a decrease of 1.1% on other states' test set. To\nconclude, we demonstrated the annotation inconsistencies in NVDRS's death\ninvestigation notes, identified problematic instances, evaluated the\neffectiveness of correcting problematic instances, and eventually proposed an\nNLP improvement solution.\n","authors":["Song Wang","Yiliang Zhou","Ziqiang Han","Cui Tao","Yunyu Xiao","Ying Ding","Joydeep Ghosh","Yifan Peng"],"pdf_url":"https://arxiv.org/pdf/2403.19432v2.pdf","comment":"19 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.20289v1","updated":"2024-03-29T17:00:55Z","published":"2024-03-29T17:00:55Z","title":"Emotion-Anchored Contrastive Learning Framework for Emotion Recognition\n  in Conversation","summary":"  Emotion Recognition in Conversation (ERC) involves detecting the underlying\nemotion behind each utterance within a conversation. Effectively generating\nrepresentations for utterances remains a significant challenge in this task.\nRecent works propose various models to address this issue, but they still\nstruggle with differentiating similar emotions such as excitement and\nhappiness. To alleviate this problem, We propose an Emotion-Anchored\nContrastive Learning (EACL) framework that can generate more distinguishable\nutterance representations for similar emotions. To achieve this, we utilize\nlabel encodings as anchors to guide the learning of utterance representations\nand design an auxiliary loss to ensure the effective separation of anchors for\nsimilar emotions. Moreover, an additional adaptation process is proposed to\nadapt anchors to serve as effective classifiers to improve classification\nperformance. Across extensive experiments, our proposed EACL achieves\nstate-of-the-art emotion recognition performance and exhibits superior\nperformance on similar emotions. Our code is available at\nhttps://github.com/Yu-Fangxu/EACL.\n","authors":["Fangxu Yu","Junjie Guo","Zhen Wu","Xinyu Dai"],"pdf_url":"https://arxiv.org/pdf/2403.20289v1.pdf","comment":"Accepted by Findings of NAACL 2024"},{"id":"http://arxiv.org/abs/2403.20288v1","updated":"2024-03-29T16:59:13Z","published":"2024-03-29T16:59:13Z","title":"Can LLMs Correct Physicians, Yet? Investigating Effective Interaction\n  Methods in the Medical Domain","summary":"  We explore the potential of Large Language Models (LLMs) to assist and\npotentially correct physicians in medical decision-making tasks. We evaluate\nseveral LLMs, including Meditron, Llama2, and Mistral, to analyze the ability\nof these models to interact effectively with physicians across different\nscenarios. We consider questions from PubMedQA and several tasks, ranging from\nbinary (yes/no) responses to long answer generation, where the answer of the\nmodel is produced after an interaction with a physician. Our findings suggest\nthat prompt design significantly influences the downstream accuracy of LLMs and\nthat LLMs can provide valuable feedback to physicians, challenging incorrect\ndiagnoses and contributing to more accurate decision-making. For example, when\nthe physician is accurate 38% of the time, Mistral can produce the correct\nanswer, improving accuracy up to 74% depending on the prompt being used, while\nLlama2 and Meditron models exhibit greater sensitivity to prompt choice. Our\nanalysis also uncovers the challenges of ensuring that LLM-generated\nsuggestions are pertinent and useful, emphasizing the need for further research\nin this area.\n","authors":["Burcu Sayin","Pasquale Minervini","Jacopo Staiano","Andrea Passerini"],"pdf_url":"https://arxiv.org/pdf/2403.20288v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20284v1","updated":"2024-03-29T16:53:11Z","published":"2024-03-29T16:53:11Z","title":"LayerNorm: A key component in parameter-efficient fine-tuning","summary":"  Fine-tuning a pre-trained model, such as Bidirectional Encoder\nRepresentations from Transformers (BERT), has been proven to be an effective\nmethod for solving many natural language processing (NLP) tasks. However, due\nto the large number of parameters in many state-of-the-art NLP models,\nincluding BERT, the process of fine-tuning is computationally expensive. One\nattractive solution to this issue is parameter-efficient fine-tuning, which\ninvolves modifying only a minimal segment of the model while keeping the\nremainder unchanged. Yet, it remains unclear which segment of the BERT model is\ncrucial for fine-tuning. In this paper, we first analyze different components\nin the BERT model to pinpoint which one undergoes the most significant changes\nafter fine-tuning. We find that output LayerNorm changes more than any other\ncomponents when fine-tuned for different General Language Understanding\nEvaluation (GLUE) tasks. Then we show that only fine-tuning the LayerNorm can\nreach comparable, or in some cases better, performance to full fine-tuning and\nother parameter-efficient fine-tuning methods. Moreover, we use Fisher\ninformation to determine the most critical subset of LayerNorm and demonstrate\nthat many NLP tasks in the GLUE benchmark can be solved by fine-tuning only a\nsmall portion of LayerNorm with negligible performance degradation.\n","authors":["Taha ValizadehAslani","Hualou Liang"],"pdf_url":"https://arxiv.org/pdf/2403.20284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05291v2","updated":"2024-03-29T16:49:59Z","published":"2023-12-08T18:14:21Z","title":"GlitchBench: Can large multimodal models detect video game glitches?","summary":"  Large multimodal models (LMMs) have evolved from large language models (LLMs)\nto integrate multiple input modalities, such as visual inputs. This integration\naugments the capacity of LLMs for tasks requiring visual comprehension and\nreasoning. However, the extent and limitations of their enhanced abilities are\nnot fully understood, especially when it comes to real-world tasks. To address\nthis gap, we introduce GlitchBench, a novel benchmark derived from video game\nquality assurance tasks, to test and evaluate the reasoning capabilities of\nLMMs. Our benchmark is curated from a variety of unusual and glitched scenarios\nfrom video games and aims to challenge both the visual and linguistic reasoning\npowers of LMMs in detecting and interpreting out-of-the-ordinary events. We\nevaluate multiple state-of-the-art LMMs, and we show that GlitchBench presents\na new challenge for these models. Code and data are available at:\nhttps://glitchbench.github.io/\n","authors":["Mohammad Reza Taesiri","Tianjun Feng","Anh Nguyen","Cor-Paul Bezemer"],"pdf_url":"https://arxiv.org/pdf/2312.05291v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.20279v1","updated":"2024-03-29T16:49:24Z","published":"2024-03-29T16:49:24Z","title":"LUQ: Long-text Uncertainty Quantification for LLMs","summary":"  Large Language Models (LLMs) have demonstrated remarkable capability in a\nvariety of NLP tasks. Despite their effectiveness, these models are prone to\ngenerate nonfactual content. Uncertainty Quantification (UQ) is pivotal in\nenhancing our understanding of a model's confidence in its generated content,\nthereby aiding in the mitigation of nonfactual outputs. Existing research on UQ\npredominantly targets short text generation, typically yielding brief,\nword-limited responses. However, real-world applications frequently necessitate\nmuch longer responses. Our study first highlights the limitations of current UQ\nmethods in handling long text generation. We then introduce \\textsc{Luq}, a\nnovel sampling-based UQ approach specifically designed for long text. Our\nfindings reveal that \\textsc{Luq} outperforms existing baseline methods in\ncorrelating with the model's factuality scores (negative coefficient of -0.85\nobserved for Gemini Pro). With \\textsc{Luq} as the tool for UQ, we investigate\nbehavior patterns of several popular LLMs' response confidence spectrum and how\nthat interplays with the response' factuality. We identify that LLMs lack\nconfidence in generating long text for rare facts and a factually strong model\n(i.e. GPT-4) tends to reject questions it is not sure about. To further improve\nthe factual accuracy of LLM responses, we propose a method called\n\\textsc{Luq-Ensemble} that ensembles responses from multiple models and selects\nthe response with the least uncertainty. The ensembling method greatly improves\nthe response factuality upon the best standalone LLM.\n","authors":["Caiqi Zhang","Fangyu Liu","Marco Basaldella","Nigel Collier"],"pdf_url":"https://arxiv.org/pdf/2403.20279v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14004v2","updated":"2024-03-29T16:42:53Z","published":"2023-05-23T12:32:24Z","title":"Sāmayik: A Benchmark and Dataset for English-Sanskrit Translation","summary":"  We release S\\={a}mayik, a dataset of around 53,000 parallel English-Sanskrit\nsentences, written in contemporary prose. Sanskrit is a classical language\nstill in sustenance and has a rich documented heritage. However, due to the\nlimited availability of digitized content, it still remains a low-resource\nlanguage. Existing Sanskrit corpora, whether monolingual or bilingual, have\npredominantly focused on poetry and offer limited coverage of contemporary\nwritten materials. S\\={a}mayik is curated from a diverse range of domains,\nincluding language instruction material, textual teaching pedagogy, and online\ntutorials, among others. It stands out as a unique resource that specifically\ncaters to the contemporary usage of Sanskrit, with a primary emphasis on prose\nwriting. Translation models trained on our dataset demonstrate statistically\nsignificant improvements when translating out-of-domain contemporary corpora,\noutperforming models trained on older classical-era poetry datasets. Finally,\nwe also release benchmark models by adapting four multilingual pre-trained\nmodels, three of them have not been previously exposed to Sanskrit for\ntranslating between English and Sanskrit while one of them is multi-lingual\npre-trained translation model including English and Sanskrit. The dataset and\nsource code is present at https://github.com/ayushbits/saamayik.\n","authors":["Ayush Maheshwari","Ashim Gupta","Amrith Krishna","Atul Kumar Singh","Ganesh Ramakrishnan","G. Anil Kumar","Jitin Singla"],"pdf_url":"https://arxiv.org/pdf/2305.14004v2.pdf","comment":"LREC-COLING, 2024"},{"id":"http://arxiv.org/abs/2403.20266v1","updated":"2024-03-29T16:16:48Z","published":"2024-03-29T16:16:48Z","title":"Latxa: An Open Language Model and Evaluation Suite for Basque","summary":"  We introduce Latxa, a family of large language models for Basque ranging from\n7 to 70 billion parameters. Latxa is based on Llama 2, which we continue\npretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens.\nAddressing the scarcity of high-quality benchmarks for Basque, we further\nintroduce 4 multiple choice evaluation datasets: EusProficiency, comprising\n5,169 questions from official language proficiency exams; EusReading,\ncomprising 352 reading comprehension questions; EusTrivia, comprising 1,715\ntrivia questions from 5 knowledge areas; and EusExams, comprising 16,774\nquestions from public examinations. In our extensive evaluation, Latxa\noutperforms all previous open models we compare to by a large margin. In\naddition, it is competitive with GPT-4 Turbo in language proficiency and\nunderstanding, despite lagging behind in reading comprehension and\nknowledge-intensive tasks. Both the Latxa family of models, as well as our new\npretraining corpora and evaluation datasets, are publicly available under open\nlicenses at https://github.com/hitz-zentroa/latxa. Our suite enables\nreproducible research on methods to build LLMs for low-resource languages.\n","authors":["Julen Etxaniz","Oscar Sainz","Naiara Perez","Itziar Aldabe","German Rigau","Eneko Agirre","Aitor Ormazabal","Mikel Artetxe","Aitor Soroa"],"pdf_url":"https://arxiv.org/pdf/2403.20266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20262v1","updated":"2024-03-29T16:13:31Z","published":"2024-03-29T16:13:31Z","title":"ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language\n  Models","summary":"  Research on Large Language Models (LLMs) has recently witnessed an increasing\ninterest in extending models' context size to better capture dependencies\nwithin long documents. While benchmarks have been proposed to assess long-range\nabilities, existing efforts primarily considered generic tasks that are not\nnecessarily aligned with real-world applications. In contrast, our work\nproposes a new benchmark for long-context LLMs focused on a practical meeting\nassistant scenario. In this scenario, the long contexts consist of transcripts\nobtained by automatic speech recognition, presenting unique challenges for LLMs\ndue to the inherent noisiness and oral nature of such data. Our benchmark,\nnamed ELITR-Bench, augments the existing ELITR corpus' transcripts with 271\nmanually crafted questions and their ground-truth answers. Our experiments with\nrecent long-context LLMs on ELITR-Bench highlight a gap between open-source and\nproprietary models, especially when questions are asked sequentially within a\nconversation. We also provide a thorough analysis of our GPT-4-based evaluation\nmethod, encompassing insights from a crowdsourcing study. Our findings suggest\nthat while GPT-4's evaluation scores are correlated with human judges', its\nability to differentiate among more than three score levels may be limited.\n","authors":["Thibaut Thonet","Jos Rozen","Laurent Besacier"],"pdf_url":"https://arxiv.org/pdf/2403.20262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20252v1","updated":"2024-03-29T15:58:46Z","published":"2024-03-29T15:58:46Z","title":"Using LLMs to Model the Beliefs and Preferences of Targeted Populations","summary":"  We consider the problem of aligning a large language model (LLM) to model the\npreferences of a human population. Modeling the beliefs, preferences, and\nbehaviors of a specific population can be useful for a variety of different\napplications, such as conducting simulated focus groups for new products,\nconducting virtual surveys, and testing behavioral interventions, especially\nfor interventions that are expensive, impractical, or unethical. Existing work\nhas had mixed success using LLMs to accurately model human behavior in\ndifferent contexts. We benchmark and evaluate two well-known fine-tuning\napproaches and evaluate the resulting populations on their ability to match the\npreferences of real human respondents on a survey of preferences for battery\nelectric vehicles (BEVs). We evaluate our models against their ability to match\npopulation-wide statistics as well as their ability to match individual\nresponses, and we investigate the role of temperature in controlling the\ntrade-offs between these two. Additionally, we propose and evaluate a novel\nloss term to improve model performance on responses that require a numeric\nresponse.\n","authors":["Keiichi Namikoshi","Alex Filipowicz","David A. Shamma","Rumen Iliev","Candice L. Hogan","Nikos Arechiga"],"pdf_url":"https://arxiv.org/pdf/2403.20252v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.04119v2","updated":"2024-03-29T15:27:47Z","published":"2022-12-08T07:29:07Z","title":"DialogCC: An Automated Pipeline for Creating High-Quality Multi-Modal\n  Dialogue Dataset","summary":"  As sharing images in an instant message is a crucial factor, there has been\nactive research on learning an image-text multi-modal dialogue models. However,\ntraining a well-generalized multi-modal dialogue model remains challenging due\nto the low quality and limited diversity of images per dialogue in existing\nmulti-modal dialogue datasets. In this paper, we propose an automated pipeline\nto construct a multi-modal dialogue dataset, ensuring both dialogue quality and\nimage diversity without requiring minimum human effort. In our pipeline, to\nguarantee the coherence between images and dialogue, we prompt GPT-4 to infer\npotential image-sharing moments - specifically, the utterance, speaker,\nrationale, and image description. Furthermore, we leverage CLIP similarity to\nmaintain consistency between aligned multiple images to the utterance. Through\nthis pipeline, we introduce DialogCC, a high-quality and diverse multi-modal\ndialogue dataset that surpasses existing datasets in terms of quality and\ndiversity in human evaluation. Our comprehensive experiments highlight that\nwhen multi-modal dialogue models are trained using our dataset, their\ngeneralization performance on unseen dialogue datasets is significantly\nenhanced. We make our source code and dataset publicly available.\n","authors":["Young-Jun Lee","Byungsoo Ko","Han-Gyu Kim","Jonghwan Hyeon","Ho-Jin Choi"],"pdf_url":"https://arxiv.org/pdf/2212.04119v2.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.20222v1","updated":"2024-03-29T15:07:21Z","published":"2024-03-29T15:07:21Z","title":"Shallow Cross-Encoders for Low-Latency Retrieval","summary":"  Transformer-based Cross-Encoders achieve state-of-the-art effectiveness in\ntext retrieval. However, Cross-Encoders based on large transformer models (such\nas BERT or T5) are computationally expensive and allow for scoring only a small\nnumber of documents within a reasonably small latency window. However, keeping\nsearch latencies low is important for user satisfaction and energy usage. In\nthis paper, we show that weaker shallow transformer models (i.e., transformers\nwith a limited number of layers) actually perform better than full-scale models\nwhen constrained to these practical low-latency settings since they can\nestimate the relevance of more documents in the same time budget. We further\nshow that shallow transformers may benefit from the generalized Binary\nCross-Entropy (gBCE) training scheme, which has recently demonstrated success\nfor recommendation tasks. Our experiments with TREC Deep Learning passage\nranking query sets demonstrate significant improvements in shallow and\nfull-scale models in low-latency scenarios. For example, when the latency limit\nis 25ms per query, MonoBERT-Large (a cross-encoder based on a full-scale BERT\nmodel) is only able to achieve NDCG@10 of 0.431 on TREC DL 2019, while\nTinyBERT-gBCE (a cross-encoder based on TinyBERT trained with gBCE) reaches\nNDCG@10 of 0.652, a +51% gain over MonoBERT-Large. We also show that shallow\nCross-Encoders are effective even when used without a GPU (e.g., with CPU\ninference, NDCG@10 decreases only by 3% compared to GPU inference with 50ms\nlatency), which makes Cross-Encoders practical to run even without specialized\nhardware acceleration.\n","authors":["Aleksandr V. Petrov","Sean MacAvaney","Craig Macdonald"],"pdf_url":"https://arxiv.org/pdf/2403.20222v1.pdf","comment":"Accepted by ECIR2024"},{"id":"http://arxiv.org/abs/2402.11676v2","updated":"2024-03-29T15:01:38Z","published":"2024-02-18T18:56:07Z","title":"A Multi-Aspect Framework for Counter Narrative Evaluation using Large\n  Language Models","summary":"  Counter narratives - informed responses to hate speech contexts designed to\nrefute hateful claims and de-escalate encounters - have emerged as an effective\nhate speech intervention strategy. While previous work has proposed automatic\ncounter narrative generation methods to aid manual interventions, the\nevaluation of these approaches remains underdeveloped. Previous automatic\nmetrics for counter narrative evaluation lack alignment with human judgment as\nthey rely on superficial reference comparisons instead of incorporating key\naspects of counter narrative quality as evaluation criteria. To address prior\nevaluation limitations, we propose a novel evaluation framework prompting LLMs\nto provide scores and feedback for generated counter narrative candidates using\n5 defined aspects derived from guidelines from counter narrative specialized\nNGOs. We found that LLM evaluators achieve strong alignment to human-annotated\nscores and feedback and outperform alternative metrics, indicating their\npotential as multi-aspect, reference-free and interpretable evaluators for\ncounter narrative evaluation.\n","authors":["Jaylen Jones","Lingbo Mo","Eric Fosler-Lussier","Huan Sun"],"pdf_url":"https://arxiv.org/pdf/2402.11676v2.pdf","comment":"22 pages, camera-ready version; references added, typos corrected,\n  methodology section expanded, additional table"},{"id":"http://arxiv.org/abs/2402.00786v4","updated":"2024-03-29T14:56:42Z","published":"2024-02-01T17:17:55Z","title":"CroissantLLM: A Truly Bilingual French-English Language Model","summary":"  We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T\nEnglish and French tokens, to bring to the research and industrial community a\nhigh-performance, fully open-sourced bilingual model that runs swiftly on\nconsumer-grade local hardware. To that end, we pioneer the approach of training\nan intrinsically bilingual model with a 1:1 English-to-French pretraining data\nratio, a custom tokenizer, and bilingual finetuning datasets. We release the\ntraining dataset, notably containing a French split with manually curated,\nhigh-quality, and varied data sources. To assess performance outside of\nEnglish, we craft a novel benchmark, FrenchBench, consisting of an array of\nclassification and generation tasks, covering various orthogonal aspects of\nmodel performance in the French Language. Additionally, rooted in transparency\nand to foster further Large Language Model research, we release codebases, and\ndozens of checkpoints across various model sizes, training data distributions,\nand training steps, as well as fine-tuned Chat models, and strong translation\nmodels. We evaluate our model through the FMTI framework, and validate 81 % of\nthe transparency criteria, far beyond the scores of even most open initiatives.\nThis work enriches the NLP landscape, breaking away from previous\nEnglish-centric work in order to strengthen our understanding of\nmultilinguality in language models.\n","authors":["Manuel Faysse","Patrick Fernandes","Nuno M. Guerreiro","António Loison","Duarte M. Alves","Caio Corro","Nicolas Boizard","João Alves","Ricardo Rei","Pedro H. Martins","Antoni Bigata Casademunt","François Yvon","André F. T. Martins","Gautier Viaud","Céline Hudelot","Pierre Colombo"],"pdf_url":"https://arxiv.org/pdf/2402.00786v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20215v1","updated":"2024-03-29T14:54:19Z","published":"2024-03-29T14:54:19Z","title":"Advancing the Arabic WordNet: Elevating Content Quality","summary":"  High-quality WordNets are crucial for achieving high-quality results in NLP\napplications that rely on such resources. However, the wordnets of most\nlanguages suffer from serious issues of correctness and completeness with\nrespect to the words and word meanings they define, such as incorrect lemmas,\nmissing glosses and example sentences, or an inadequate, Western-centric\nrepresentation of the morphology and the semantics of the language. Previous\nefforts have largely focused on increasing lexical coverage while ignoring\nother qualitative aspects. In this paper, we focus on the Arabic language and\nintroduce a major revision of the Arabic WordNet that addresses multiple\ndimensions of lexico-semantic resource quality. As a result, we updated more\nthan 58% of the synsets of the existing Arabic WordNet by adding missing\ninformation and correcting errors. In order to address issues of language\ndiversity and untranslatability, we also extended the wordnet structure by new\nelements: phrasets and lexical gaps.\n","authors":["Abed Alhakim Freihat","Hadi Khalilia","Gábor Bella","Fausto Giunchiglia"],"pdf_url":"https://arxiv.org/pdf/2403.20215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20196v1","updated":"2024-03-29T14:18:26Z","published":"2024-03-29T14:18:26Z","title":"Automatic Alignment of Discourse Relations of Different Discourse\n  Annotation Frameworks","summary":"  Existing discourse corpora are annotated based on different frameworks, which\nshow significant dissimilarities in definitions of arguments and relations and\nstructural constraints. Despite surface differences, these frameworks share\nbasic understandings of discourse relations. The relationship between these\nframeworks has been an open research question, especially the correlation\nbetween relation inventories utilized in different frameworks. Better\nunderstanding of this question is helpful for integrating discourse theories\nand enabling interoperability of discourse corpora annotated under different\nframeworks. However, studies that explore correlations between discourse\nrelation inventories are hindered by different criteria of discourse\nsegmentation, and expert knowledge and manual examination are typically needed.\nSome semi-automatic methods have been proposed, but they rely on corpora\nannotated in multiple frameworks in parallel. In this paper, we introduce a\nfully automatic approach to address the challenges. Specifically, we extend the\nlabel-anchored contrastive learning method introduced by Zhang et al. (2022b)\nto learn label embeddings during a classification task. These embeddings are\nthen utilized to map discourse relations from different frameworks. We show\nexperimental results on RST-DT (Carlson et al., 2001) and PDTB 3.0 (Prasad et\nal., 2018).\n","authors":["Yingxue Fu"],"pdf_url":"https://arxiv.org/pdf/2403.20196v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18703v7","updated":"2024-03-29T14:05:07Z","published":"2023-05-30T03:00:30Z","title":"Domain Specialization as the Key to Make Large Language Models\n  Disruptive: A Comprehensive Survey","summary":"  Large language models (LLMs) have significantly advanced the field of natural\nlanguage processing (NLP), providing a highly useful, task-agnostic foundation\nfor a wide range of applications. However, directly applying LLMs to solve\nsophisticated problems in specific domains meets many hurdles, caused by the\nheterogeneity of domain data, the sophistication of domain knowledge, the\nuniqueness of domain objectives, and the diversity of the constraints (e.g.,\nvarious social norms, cultural conformity, religious beliefs, and ethical\nstandards in the domain applications). Domain specification techniques are key\nto make large language models disruptive in many applications. Specifically, to\nsolve these hurdles, there has been a notable increase in research and\npractices conducted in recent years on the domain specialization of LLMs. This\nemerging field of study, with its substantial potential for impact,\nnecessitates a comprehensive and systematic review to better summarize and\nguide ongoing work in this area. In this article, we present a comprehensive\nsurvey on domain specification techniques for large language models, an\nemerging direction critical for large language model applications. First, we\npropose a systematic taxonomy that categorizes the LLM domain-specialization\ntechniques based on the accessibility to LLMs and summarizes the framework for\nall the subcategories as well as their relations and differences to each other.\nSecond, we present an extensive taxonomy of critical application domains that\ncan benefit dramatically from specialized LLMs, discussing their practical\nsignificance and open challenges. Last, we offer our insights into the current\nresearch status and future trends in this area.\n","authors":["Chen Ling","Xujiang Zhao","Jiaying Lu","Chengyuan Deng","Can Zheng","Junxiang Wang","Tanmoy Chowdhury","Yun Li","Hejie Cui","Xuchao Zhang","Tianjiao Zhao","Amit Panalkar","Dhagash Mehta","Stefano Pasquali","Wei Cheng","Haoyu Wang","Yanchi Liu","Zhengzhang Chen","Haifeng Chen","Chris White","Quanquan Gu","Jian Pei","Carl Yang","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2305.18703v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20184v1","updated":"2024-03-29T13:59:34Z","published":"2024-03-29T13:59:34Z","title":"Exploring Pathological Speech Quality Assessment with ASR-Powered\n  Wav2Vec2 in Data-Scarce Context","summary":"  Automatic speech quality assessment has raised more attention as an\nalternative or support to traditional perceptual clinical evaluation. However,\nmost research so far only gains good results on simple tasks such as binary\nclassification, largely due to data scarcity. To deal with this challenge,\ncurrent works tend to segment patients' audio files into many samples to\naugment the datasets. Nevertheless, this approach has limitations, as it\nindirectly relates overall audio scores to individual segments. This paper\nintroduces a novel approach where the system learns at the audio level instead\nof segments despite data scarcity. This paper proposes to use the pre-trained\nWav2Vec2 architecture for both SSL, and ASR as feature extractor in speech\nassessment. Carried out on the HNC dataset, our ASR-driven approach established\na new baseline compared with other approaches, obtaining average $MSE=0.73$ and\n$MSE=1.15$ for the prediction of intelligibility and severity scores\nrespectively, using only 95 training samples. It shows that the ASR based\nWav2Vec2 model brings the best results and may indicate a strong correlation\nbetween ASR and speech quality assessment. We also measure its ability on\nvariable segment durations and speech content, exploring factors influencing\nits decision.\n","authors":["Tuan Nguyen","Corinne Fredouille","Alain Ghio","Mathieu Balaguer","Virginie Woisard"],"pdf_url":"https://arxiv.org/pdf/2403.20184v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.20180v1","updated":"2024-03-29T13:56:21Z","published":"2024-03-29T13:56:21Z","title":"Measuring Taiwanese Mandarin Language Understanding","summary":"  The evaluation of large language models (LLMs) has drawn substantial\nattention in the field recently. This work focuses on evaluating LLMs in a\nChinese context, specifically, for Traditional Chinese which has been largely\nunderrepresented in existing benchmarks. We present TMLU, a holistic evaluation\nsuit tailored for assessing the advanced knowledge and reasoning capability in\nLLMs, under the context of Taiwanese Mandarin. TMLU consists of an array of 37\nsubjects across social science, STEM, humanities, Taiwan-specific content, and\nothers, ranging from middle school to professional levels. In addition, we\ncurate chain-of-thought-like few-shot explanations for each subject to\nfacilitate the evaluation of complex reasoning skills. To establish a\ncomprehensive baseline, we conduct extensive experiments and analysis on 24\nadvanced LLMs. The results suggest that Chinese open-weight models demonstrate\ninferior performance comparing to multilingual proprietary ones, and\nopen-weight models tailored for Taiwanese Mandarin lag behind the\nSimplified-Chinese counterparts. The findings indicate great headrooms for\nimprovement, and emphasize the goal of TMLU to foster the development of\nlocalized Taiwanese-Mandarin LLMs. We release the benchmark and evaluation\nscripts for the community to promote future research.\n","authors":["Po-Heng Chen","Sijia Cheng","Wei-Lin Chen","Yen-Ting Lin","Yun-Nung Chen"],"pdf_url":"https://arxiv.org/pdf/2403.20180v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2403.20158v1","updated":"2024-03-29T13:12:09Z","published":"2024-03-29T13:12:09Z","title":"ChatGPT v.s. Media Bias: A Comparative Study of GPT-3.5 and Fine-tuned\n  Language Models","summary":"  In our rapidly evolving digital sphere, the ability to discern media bias\nbecomes crucial as it can shape public sentiment and influence pivotal\ndecisions. The advent of large language models (LLMs), such as ChatGPT, noted\nfor their broad utility in various natural language processing (NLP) tasks,\ninvites exploration of their efficacy in media bias detection. Can ChatGPT\ndetect media bias? This study seeks to answer this question by leveraging the\nMedia Bias Identification Benchmark (MBIB) to assess ChatGPT's competency in\ndistinguishing six categories of media bias, juxtaposed against fine-tuned\nmodels such as BART, ConvBERT, and GPT-2. The findings present a dichotomy:\nChatGPT performs at par with fine-tuned models in detecting hate speech and\ntext-level context bias, yet faces difficulties with subtler elements of other\nbias detections, namely, fake news, racial, gender, and cognitive biases.\n","authors":["Zehao Wen","Rabih Younes"],"pdf_url":"https://arxiv.org/pdf/2403.20158v1.pdf","comment":"9 pages, 1 figure, published on Applied and Computational Engineering"},{"id":"http://arxiv.org/abs/2403.20157v1","updated":"2024-03-29T13:09:23Z","published":"2024-03-29T13:09:23Z","title":"A Systematic Analysis of Subwords and Cross-Lingual Transfer in\n  Multilingual Translation","summary":"  Multilingual modelling can improve machine translation for low-resource\nlanguages, partly through shared subword representations. This paper studies\nthe role of subword segmentation in cross-lingual transfer. We systematically\ncompare the efficacy of several subword methods in promoting synergy and\npreventing interference across different linguistic typologies. Our findings\nshow that subword regularisation boosts synergy in multilingual modelling,\nwhereas BPE more effectively facilitates transfer during cross-lingual\nfine-tuning. Notably, our results suggest that differences in orthographic word\nboundary conventions (the morphological granularity of written words) may\nimpede cross-lingual transfer more significantly than linguistic unrelatedness.\nOur study confirms that decisions around subword modelling can be key to\noptimising the benefits of multilingual modelling.\n","authors":["Francois Meyer","Jan Buys"],"pdf_url":"https://arxiv.org/pdf/2403.20157v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20147v1","updated":"2024-03-29T12:32:06Z","published":"2024-03-29T12:32:06Z","title":"IndiBias: A Benchmark Dataset to Measure Social Biases in Language\n  Models for Indian Context","summary":"  The pervasive influence of social biases in language data has sparked the\nneed for benchmark datasets that capture and evaluate these biases in Large\nLanguage Models (LLMs). Existing efforts predominantly focus on English\nlanguage and the Western context, leaving a void for a reliable dataset that\nencapsulates India's unique socio-cultural nuances. To bridge this gap, we\nintroduce IndiBias, a comprehensive benchmarking dataset designed specifically\nfor evaluating social biases in the Indian context. We filter and translate the\nexisting CrowS-Pairs dataset to create a benchmark dataset suited to the Indian\ncontext in Hindi language. Additionally, we leverage LLMs including ChatGPT and\nInstructGPT to augment our dataset with diverse societal biases and stereotypes\nprevalent in India. The included bias dimensions encompass gender, religion,\ncaste, age, region, physical appearance, and occupation. We also build a\nresource to address intersectional biases along three intersectional\ndimensions. Our dataset contains 800 filtered sentences from the CrowS-Pairs\ndataset and tuples for bias measurement across different demographics. It is\nmade available in English and Hindi languages, providing a size comparable to\nexisting benchmark datasets. Furthermore, using IndiBias we compare ten\ndifferent language models on multiple bias measurement metrics. We observed\nthat the language models exhibit more bias across a majority of the\nintersectional groups.\n","authors":["Nihar Ranjan Sahoo","Pranamya Prashant Kulkarni","Narjis Asad","Arif Ahmad","Tanu Goyal","Aparna Garimella","Pushpak Bhattacharyya"],"pdf_url":"https://arxiv.org/pdf/2403.20147v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20145v1","updated":"2024-03-29T12:25:37Z","published":"2024-03-29T12:25:37Z","title":"Fine-tuning Large Language Models for Automated Diagnostic Screening\n  Summaries","summary":"  Improving mental health support in developing countries is a pressing need.\nOne potential solution is the development of scalable, automated systems to\nconduct diagnostic screenings, which could help alleviate the burden on mental\nhealth professionals. In this work, we evaluate several state-of-the-art Large\nLanguage Models (LLMs), with and without fine-tuning, on our custom dataset for\ngenerating concise summaries from mental state examinations. We rigorously\nevaluate four different models for summary generation using established ROUGE\nmetrics and input from human evaluators. The results highlight that our\ntop-performing fine-tuned model outperforms existing models, achieving ROUGE-1\nand ROUGE-L values of 0.810 and 0.764, respectively. Furthermore, we assessed\nthe fine-tuned model's generalizability on a publicly available D4 dataset, and\nthe outcomes were promising, indicating its potential applicability beyond our\ncustom dataset.\n","authors":["Manjeet Yadav","Nilesh Kumar Sahu","Mudita Chaturvedi","Snehil Gupta","Haroon R Lone"],"pdf_url":"https://arxiv.org/pdf/2403.20145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12168v3","updated":"2024-03-29T12:12:30Z","published":"2024-02-19T14:22:54Z","title":"Defending Against Weight-Poisoning Backdoor Attacks for\n  Parameter-Efficient Fine-Tuning","summary":"  Recently, various parameter-efficient fine-tuning (PEFT) strategies for\napplication to language models have been proposed and successfully implemented.\nHowever, this raises the question of whether PEFT, which only updates a limited\nset of model parameters, constitutes security vulnerabilities when confronted\nwith weight-poisoning backdoor attacks. In this study, we show that PEFT is\nmore susceptible to weight-poisoning backdoor attacks compared to the\nfull-parameter fine-tuning method, with pre-defined triggers remaining\nexploitable and pre-defined targets maintaining high confidence, even after\nfine-tuning. Motivated by this insight, we developed a Poisoned Sample\nIdentification Module (PSIM) leveraging PEFT, which identifies poisoned samples\nthrough confidence, providing robust defense against weight-poisoning backdoor\nattacks. Specifically, we leverage PEFT to train the PSIM with randomly reset\nsample labels. During the inference process, extreme confidence serves as an\nindicator for poisoned samples, while others are clean. We conduct experiments\non text classification tasks, five fine-tuning strategies, and three\nweight-poisoning backdoor attack methods. Experiments show near 100% success\nrates for weight-poisoning backdoor attacks when utilizing PEFT. Furthermore,\nour defensive approach exhibits overall competitive performance in mitigating\nweight-poisoning backdoor attacks.\n","authors":["Shuai Zhao","Leilei Gan","Luu Anh Tuan","Jie Fu","Lingjuan Lyu","Meihuizi Jia","Jinming Wen"],"pdf_url":"https://arxiv.org/pdf/2402.12168v3.pdf","comment":"NAACL Findings 2024"},{"id":"http://arxiv.org/abs/2403.20134v1","updated":"2024-03-29T11:54:13Z","published":"2024-03-29T11:54:13Z","title":"User Modeling Challenges in Interactive AI Assistant Systems","summary":"  Interactive Artificial Intelligent(AI) assistant systems are designed to\noffer timely guidance to help human users to complete a variety tasks. One of\nthe remaining challenges is to understand user's mental states during the task\nfor more personalized guidance. In this work, we analyze users' mental states\nduring task executions and investigate the capabilities and challenges for\nlarge language models to interpret user profiles for more personalized user\nguidance.\n","authors":["Megan Su","Yuwei Bao"],"pdf_url":"https://arxiv.org/pdf/2403.20134v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01677v2","updated":"2024-03-29T11:35:30Z","published":"2023-11-03T02:59:56Z","title":"DialogBench: Evaluating LLMs as Human-like Dialogue Systems","summary":"  Large language models (LLMs) have achieved remarkable breakthroughs in new\ndialogue capabilities by leveraging instruction tuning, which refreshes human\nimpressions of dialogue systems. The long-standing goal of dialogue systems is\nto be human-like enough to establish long-term connections with users.\nTherefore, there has been an urgent need to evaluate LLMs as human-like\ndialogue systems. In this paper, we propose DialogBench, a dialogue evaluation\nbenchmark that contains 12 dialogue tasks to probe the capabilities of LLMs as\nhuman-like dialogue systems should have. Specifically, we prompt GPT-4 to\ngenerate evaluation instances for each task. We first design the basic prompt\nbased on widely used design principles and further mitigate the existing biases\nto generate higher-quality evaluation instances. Our extensive tests on English\nand Chinese DialogBench of 26 LLMs show that instruction tuning improves the\nhuman likeness of LLMs to a certain extent, but most LLMs still have much room\nfor improvement as human-like dialogue systems. Interestingly, results also\nshow that the positioning of assistant AI can make instruction tuning weaken\nthe human emotional perception of LLMs and their mastery of information about\nhuman daily life.\n","authors":["Jiao Ou","Junda Lu","Che Liu","Yihong Tang","Fuzheng Zhang","Di Zhang","Kun Gai"],"pdf_url":"https://arxiv.org/pdf/2311.01677v2.pdf","comment":"Accepted at NAACL 2024 (main conference)"},{"id":"http://arxiv.org/abs/2311.08590v3","updated":"2024-03-29T11:24:46Z","published":"2023-11-14T23:20:51Z","title":"PEMA: An Offsite-Tunable Plug-in External Memory Adaptation for Language\n  Models","summary":"  Pre-trained language models (PLMs) show impressive performance in various\ndownstream NLP tasks. However, pre-training large language models demands\nsubstantial memory and training compute. Furthermore, due to the substantial\nresources required, many PLM weights are confidential. Consequently, users are\ncompelled to share their data with model owners for fine-tuning specific tasks.\nTo overcome the limitations, we introduce Plug-in External Memory Adaptation\n(PEMA), a Parameter-Efficient Fine-Tuning (PEFT) method, enabling PLM\nfine-tuning without requiring access to all the weights. PEMA integrates with\ncontext representations from test data during inference to perform downstream\ntasks. It uses external memory to store PLM-generated context representations\nmapped with target tokens. Our method utilizes weight matrices of LoRA-like\nbottlenecked adapter in the PLM's final layer to enhance efficiency. Our\napproach also includes Gradual Unrolling, a novel interpolation strategy to\nimprove generation quality. We validate PEMA's effectiveness through\nexperiments on syntactic and real datasets for machine translation and style\ntransfer. Our findings show that PEMA outperforms other PEFT approaches in\nmemory and latency efficiency for training, and also excels in maintaining\nsentence meaning and generating appropriate language and styles.\n","authors":["HyunJin Kim","Young Jin Kim","JinYeong Bak"],"pdf_url":"https://arxiv.org/pdf/2311.08590v3.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2403.20103v1","updated":"2024-03-29T10:32:44Z","published":"2024-03-29T10:32:44Z","title":"NLP for Counterspeech against Hate: A Survey and How-To Guide","summary":"  In recent years, counterspeech has emerged as one of the most promising\nstrategies to fight online hate. These non-escalatory responses tackle online\nabuse while preserving the freedom of speech of the users, and can have a\ntangible impact in reducing online and offline violence. Recently, there has\nbeen growing interest from the Natural Language Processing (NLP) community in\naddressing the challenges of analysing, collecting, classifying, and\nautomatically generating counterspeech, to reduce the huge burden of manually\nproducing it. In particular, researchers have taken different directions in\naddressing these challenges, thus providing a variety of related tasks and\nresources. In this paper, we provide a guide for doing research on\ncounterspeech, by describing - with detailed examples - the steps to undertake,\nand providing best practices that can be learnt from the NLP studies on this\ntopic. Finally, we discuss open challenges and future directions of\ncounterspeech research in NLP.\n","authors":["Helena Bonaldi","Yi-Ling Chung","Gavin Abercrombie","Marco Guerini"],"pdf_url":"https://arxiv.org/pdf/2403.20103v1.pdf","comment":"To appear in Proceedings of the 2024 Conference of the North American\n  Chapter of the Association for Computational Linguistics (findings)"},{"id":"http://arxiv.org/abs/2403.20101v1","updated":"2024-03-29T10:31:32Z","published":"2024-03-29T10:31:32Z","title":"RealKIE: Five Novel Datasets for Enterprise Key Information Extraction","summary":"  We introduce RealKIE, a benchmark of five challenging datasets aimed at\nadvancing key information extraction methods, with an emphasis on enterprise\napplications. The datasets include a diverse range of documents including SEC\nS1 Filings, US Non-disclosure Agreements, UK Charity Reports, FCC Invoices, and\nResource Contracts. Each presents unique challenges: poor text serialization,\nsparse annotations in long documents, and complex tabular layouts. These\ndatasets provide a realistic testing ground for key information extraction\ntasks like investment analysis and legal data processing.\n  In addition to presenting these datasets, we offer an in-depth description of\nthe annotation process, document processing techniques, and baseline modeling\napproaches. This contribution facilitates the development of NLP models capable\nof handling practical challenges and supports further research into information\nextraction technologies applicable to industry-specific problems.\n  The annotated data and OCR outputs are available to download at\nhttps://indicodatasolutions.github.io/RealKIE/ code to reproduce the baselines\nwill be available shortly.\n","authors":["Benjamin Townsend","Madison May","Christopher Wells"],"pdf_url":"https://arxiv.org/pdf/2403.20101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20088v1","updated":"2024-03-29T09:52:18Z","published":"2024-03-29T09:52:18Z","title":"An Efficient Approach for Studying Cross-Lingual Transfer in\n  Multilingual Language Models","summary":"  The capacity and effectiveness of pre-trained multilingual models (MLMs) for\nzero-shot cross-lingual transfer is well established. However, phenomena of\npositive or negative transfer, and the effect of language choice still need to\nbe fully understood, especially in the complex setting of massively\nmultilingual LMs. We propose an \\textit{efficient} method to study transfer\nlanguage influence in zero-shot performance on another target language. Unlike\nprevious work, our approach disentangles downstream tasks from language, using\ndedicated adapter units. Our findings suggest that some languages do not\nlargely affect others, while some languages, especially ones unseen during\npre-training, can be extremely beneficial or detrimental for different target\nlanguages. We find that no transfer language is beneficial for all target\nlanguages. We do, curiously, observe languages previously unseen by MLMs\nconsistently benefit from transfer from almost any language. We additionally\nuse our modular approach to quantify negative interference efficiently and\ncategorize languages accordingly. Furthermore, we provide a list of promising\ntransfer-target language configurations that consistently lead to target\nlanguage performance improvements. Code and data are publicly available:\nhttps://github.com/ffaisal93/neg_inf\n","authors":["Fahim Faisal","Antonios Anastasopoulos"],"pdf_url":"https://arxiv.org/pdf/2403.20088v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20084v1","updated":"2024-03-29T09:33:34Z","published":"2024-03-29T09:33:34Z","title":"IPA Transcription of Bengali Texts","summary":"  The International Phonetic Alphabet (IPA) serves to systematize phonemes in\nlanguage, enabling precise textual representation of pronunciation. In Bengali\nphonology and phonetics, ongoing scholarly deliberations persist concerning the\nIPA standard and core Bengali phonemes. This work examines prior research,\nidentifies current and potential issues, and suggests a framework for a Bengali\nIPA standard, facilitating linguistic analysis and NLP resource creation and\ndownstream technology development. In this work, we present a comprehensive\nstudy of Bengali IPA transcription and introduce a novel IPA transcription\nframework incorporating a novel dataset with DL-based benchmarks.\n","authors":["Kanij Fatema","Fazle Dawood Haider","Nirzona Ferdousi Turpa","Tanveer Azmal","Sourav Ahmed","Navid Hasan","Mohammad Akhlaqur Rahman","Biplab Kumar Sarkar","Afrar Jahin","Md. Rezuwan Hassan","Md Foriduzzaman Zihad","Rubayet Sabbir Faruque","Asif Sushmit","Mashrur Imtiaz","Farig Sadeque","Syed Shahrier Rahman"],"pdf_url":"https://arxiv.org/pdf/2403.20084v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02330v2","updated":"2024-03-29T09:01:56Z","published":"2024-02-04T03:47:10Z","title":"Enhance Reasoning for Large Language Models in the Game Werewolf","summary":"  This paper presents an innovative framework that integrates Large Language\nModels (LLMs) with an external Thinker module to enhance the reasoning\ncapabilities of LLM-based agents. Unlike augmenting LLMs with prompt\nengineering, Thinker directly harnesses knowledge from databases and employs\nvarious optimization techniques. The framework forms a reasoning hierarchy\nwhere LLMs handle intuitive System-1 tasks such as natural language processing,\nwhile the Thinker focuses on cognitive System-2 tasks that require complex\nlogical analysis and domain-specific knowledge. Our framework is presented\nusing a 9-player Werewolf game that demands dual-system reasoning. We introduce\na communication protocol between LLMs and the Thinker, and train the Thinker\nusing data from 18800 human sessions and reinforcement learning. Experiments\ndemonstrate the framework's effectiveness in deductive reasoning, speech\ngeneration, and online game evaluation. Additionally, we fine-tune a 6B LLM to\nsurpass GPT4 when integrated with the Thinker. This paper also contributes the\nlargest dataset for social deduction games to date.\n","authors":["Shuang Wu","Liwen Zhu","Tao Yang","Shiwei Xu","Qiang Fu","Yang Wei","Haobo Fu"],"pdf_url":"https://arxiv.org/pdf/2402.02330v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20056v1","updated":"2024-03-29T08:47:15Z","published":"2024-03-29T08:47:15Z","title":"Cross-Lingual Transfer Robustness to Lower-Resource Languages on\n  Adversarial Datasets","summary":"  Multilingual Language Models (MLLMs) exhibit robust cross-lingual transfer\ncapabilities, or the ability to leverage information acquired in a source\nlanguage and apply it to a target language. These capabilities find practical\napplications in well-established Natural Language Processing (NLP) tasks such\nas Named Entity Recognition (NER). This study aims to investigate the\neffectiveness of a source language when applied to a target language,\nparticularly in the context of perturbing the input test set. We evaluate on 13\npairs of languages, each including one high-resource language (HRL) and one\nlow-resource language (LRL) with a geographic, genetic, or borrowing\nrelationship. We evaluate two well-known MLLMs--MBERT and XLM-R--on these\npairs, in native LRL and cross-lingual transfer settings, in two tasks, under a\nset of different perturbations. Our findings indicate that NER cross-lingual\ntransfer depends largely on the overlap of entity chunks. If a source and\ntarget language have more entities in common, the transfer ability is stronger.\nModels using cross-lingual transfer also appear to be somewhat more robust to\ncertain perturbations of the input, perhaps indicating an ability to leverage\nstronger representations derived from the HRL. Our research provides valuable\ninsights into cross-lingual transfer and its implications for NLP applications,\nand underscores the need to consider linguistic nuances and potential\nlimitations when employing MLLMs across distinct languages.\n","authors":["Shadi Manafi","Nikhil Krishnaswamy"],"pdf_url":"https://arxiv.org/pdf/2403.20056v1.pdf","comment":"accepted in LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.20046v1","updated":"2024-03-29T08:30:34Z","published":"2024-03-29T08:30:34Z","title":"Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to\n  Boost for Reasoning","summary":"  Recent works have shown the benefits to LLMs from fine-tuning golden-standard\nChain-of-Thought (CoT) rationales or using them as correct examples in few-shot\nprompting. While humans can indeed imitate correct examples, learning from our\nmistakes is another vital aspect of human cognition. Hence, a question\nnaturally arises: \\textit{can LLMs learn and benefit from their mistakes,\nespecially for their reasoning? } This study investigates this problem from\nboth the prompting and model-tuning perspectives. We begin by introducing\n\\textsc{CoTErrorSet}, a new benchmark with 609,432 questions, each designed\nwith both correct and error references, and demonstrating the types and reasons\nfor making such mistakes. To explore the effectiveness of those mistakes, we\ndesign two methods: (1) \\textbf{Self-rethinking} prompting guides LLMs to\nrethink whether they have made similar previous mistakes; and (2)\n\\textbf{Mistake tuning} involves finetuning models in both correct and\nincorrect reasoning domains, rather than only tuning models to learn ground\ntruth in traditional methodology. We conduct a series of experiments to prove\nLLMs can obtain benefits from mistakes in both directions. Our two methods\noffer potentially cost-effective strategies by leveraging errors to enhance\nreasoning capabilities, which costs significantly less than creating\nmeticulously hand-crafted golden references. We ultimately make a thorough\nanalysis of the reasons behind LLMs' errors, which provides directions that\nfuture research needs to overcome. \\textsc{CoTErrorSet} will be published soon\non \\texttt{Anonymity Link}.\n","authors":["Yongqi Tong","Dawei Li","Sizhe Wang","Yujia Wang","Fei Teng","Jingbo Shang"],"pdf_url":"https://arxiv.org/pdf/2403.20046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20041v1","updated":"2024-03-29T08:26:53Z","published":"2024-03-29T08:26:53Z","title":"Transformer-Lite: High-efficiency Deployment of Large Language Models on\n  Mobile Phone GPUs","summary":"  The Large Language Model (LLM) is widely employed for tasks such as\nintelligent assistants, text summarization, translation, and multi-modality on\nmobile phones. However, the current methods for on-device LLM deployment\nmaintain slow inference speed, which causes poor user experience. To facilitate\nhigh-efficiency LLM deployment on device GPUs, we propose four optimization\ntechniques: (a) a symbolic expression-based approach to support dynamic shape\nmodel inference; (b) operator optimizations and execution priority setting to\nenhance inference speed and reduce phone lagging; (c) an FP4 quantization\nmethod termed M0E4 to reduce dequantization overhead; (d) a sub-tensor-based\ntechnique to eliminate the need for copying KV cache after LLM inference.\nFurthermore, we implement these methods in our mobile inference engine,\nTransformer-Lite, which is compatible with both Qualcomm and MTK processors. We\nevaluated Transformer-Lite's performance using LLMs with varied architectures\nand parameters ranging from 2B to 14B. Specifically, we achieved prefill and\ndecoding speeds of 121 token/s and 14 token/s for ChatGLM2 6B, and 330 token/s\nand 30 token/s for smaller Gemma 2B, respectively. Compared with CPU-based\nFastLLM and GPU-based MLC-LLM, our engine attains over 10x speedup for the\nprefill speed and 2~3x speedup for the decoding speed.\n","authors":["Luchang Li","Sheng Qian","Jie Lu","Lunxi Yuan","Rui Wang","Qin Xie"],"pdf_url":"https://arxiv.org/pdf/2403.20041v1.pdf","comment":"21 pages, 6 figures"},{"id":"http://arxiv.org/abs/2304.14178v3","updated":"2024-03-29T08:13:38Z","published":"2023-04-27T13:27:01Z","title":"mPLUG-Owl: Modularization Empowers Large Language Models with\n  Multimodality","summary":"  Large language models (LLMs) have demonstrated impressive zero-shot abilities\non a variety of open-ended tasks, while recent research has also explored the\nuse of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl,\na novel training paradigm that equips LLMs with multi-modal abilities through\nmodularized learning of foundation LLM, a visual knowledge module, and a visual\nabstractor module. This approach can support multiple modalities and facilitate\ndiverse unimodal and multimodal abilities through modality collaboration. The\ntraining paradigm of mPLUG-Owl involves a two-stage method for aligning image\nand text, which learns visual knowledge with the assistance of LLM while\nmaintaining and even improving the generation abilities of LLM. In the first\nstage, the visual knowledge module and abstractor module are trained with a\nfrozen LLM module to align the image and text. In the second stage,\nlanguage-only and multi-modal supervised datasets are used to jointly fine-tune\na low-rank adaption (LoRA) module on LLM and the abstractor module by freezing\nthe visual knowledge module. We carefully build a visually-related instruction\nevaluation set OwlEval. Experimental results show that our model outperforms\nexisting multi-modal models, demonstrating mPLUG-Owl's impressive instruction\nand visual understanding ability, multi-turn conversation ability, and\nknowledge reasoning ability. Besides, we observe some unexpected and exciting\nabilities such as multi-image correlation and scene text understanding, which\nmakes it possible to leverage it for harder real scenarios, such as vision-only\ndocument comprehension. Our code, pre-trained model, instruction-tuned models,\nand evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The\nonline demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.\n","authors":["Qinghao Ye","Haiyang Xu","Guohai Xu","Jiabo Ye","Ming Yan","Yiyang Zhou","Junyang Wang","Anwen Hu","Pengcheng Shi","Yaya Shi","Chenliang Li","Yuanhong Xu","Hehong Chen","Junfeng Tian","Qi Qian","Ji Zhang","Fei Huang","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2304.14178v3.pdf","comment":"Working in Process"},{"id":"http://arxiv.org/abs/2402.18825v2","updated":"2024-03-29T08:08:41Z","published":"2024-02-29T03:20:45Z","title":"Utilizing Local Hierarchy with Adversarial Training for Hierarchical\n  Text Classification","summary":"  Hierarchical text classification (HTC) is a challenging subtask of\nmulti-label classification due to its complex taxonomic structure. Nearly all\nrecent HTC works focus on how the labels are structured but ignore the\nsub-structure of ground-truth labels according to each input text which\ncontains fruitful label co-occurrence information. In this work, we introduce\nthis local hierarchy with an adversarial framework. We propose a HiAdv\nframework that can fit in nearly all HTC models and optimize them with the\nlocal hierarchy as auxiliary information. We test on two typical HTC models and\nfind that HiAdv is effective in all scenarios and is adept at dealing with\ncomplex taxonomic hierarchies. Further experiments demonstrate that the\npromotion of our framework indeed comes from the local hierarchy and the local\nhierarchy is beneficial for rare classes which have insufficient training data.\n","authors":["Zihan Wang","Peiyi Wang","Houfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2402.18825v2.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.20026v1","updated":"2024-03-29T07:28:50Z","published":"2024-03-29T07:28:50Z","title":"FSMR: A Feature Swapping Multi-modal Reasoning Approach with Joint\n  Textual and Visual Clues","summary":"  Multi-modal reasoning plays a vital role in bridging the gap between textual\nand visual information, enabling a deeper understanding of the context. This\npaper presents the Feature Swapping Multi-modal Reasoning (FSMR) model,\ndesigned to enhance multi-modal reasoning through feature swapping. FSMR\nleverages a pre-trained visual-language model as an encoder, accommodating both\ntext and image inputs for effective feature representation from both\nmodalities. It introduces a unique feature swapping module, enabling the\nexchange of features between identified objects in images and corresponding\nvocabulary words in text, thereby enhancing the model's comprehension of the\ninterplay between images and text. To further bolster its multi-modal alignment\ncapabilities, FSMR incorporates a multi-modal cross-attention mechanism,\nfacilitating the joint modeling of textual and visual information. During\ntraining, we employ image-text matching and cross-entropy losses to ensure\nsemantic consistency between visual and language elements. Extensive\nexperiments on the PMR dataset demonstrate FSMR's superiority over\nstate-of-the-art baseline models across various performance metrics.\n","authors":["Shuang Li","Jiahua Wang","Lijie Wen"],"pdf_url":"https://arxiv.org/pdf/2403.20026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.20689v4","updated":"2024-03-29T07:17:39Z","published":"2023-10-31T17:52:22Z","title":"Learning From Mistakes Makes LLM Better Reasoner","summary":"  Large language models (LLMs) recently exhibited remarkable reasoning\ncapabilities on solving math problems. To further improve their reasoning\ncapabilities, this work explores whether LLMs can LEarn from MistAkes (LEMA),\nakin to the human learning process. Consider a human student who failed to\nsolve a math problem, he will learn from what mistake he has made and how to\ncorrect it. Mimicking this error-driven learning process, LEMA incorporates\nmistake-correction data pairs during fine-tuning LLMs. Specifically, we first\ncollect inaccurate reasoning paths from various LLMs, and then employ GPT-4 as\na ''corrector'' to identify the mistake step, explain the reason for the\nmistake, correct the mistake and generate the final answer. In addition, we\napply a correction-centric evolution strategy that effectively expands the\nquestion set for generating correction data. Experiments across various LLMs\nand reasoning tasks show that LEMA effectively improves CoT-alone fine-tuning.\nOur further ablations shed light on the non-homogeneous effectiveness between\nCoT data and correction data. These results suggest a significant potential for\nLLMs to improve through learning from their mistakes. Our code, models and\nprompts are publicly available at https://github.com/microsoft/LEMA.\n","authors":["Shengnan An","Zexiong Ma","Zeqi Lin","Nanning Zheng","Jian-Guang Lou","Weizhu Chen"],"pdf_url":"https://arxiv.org/pdf/2310.20689v4.pdf","comment":"23 pages, 13 figures, 6 tables"},{"id":"http://arxiv.org/abs/2403.20015v1","updated":"2024-03-29T07:01:39Z","published":"2024-03-29T07:01:39Z","title":"Adverb Is the Key: Simple Text Data Augmentation with Adverb Deletion","summary":"  In the field of text data augmentation, rule-based methods are widely adopted\nfor real-world applications owing to their cost-efficiency. However,\nconventional rule-based approaches suffer from the possibility of losing the\noriginal semantics of the given text. We propose a novel text data augmentation\nstrategy that avoids such phenomena through a straightforward deletion of\nadverbs, which play a subsidiary role in the sentence. Our comprehensive\nexperiments demonstrate the efficiency and effectiveness of our proposed\napproach for not just single text classification, but also natural language\ninference that requires semantic preservation. We publicly released our source\ncode for reproducibility.\n","authors":["Juhwan Choi","YoungBin Kim"],"pdf_url":"https://arxiv.org/pdf/2403.20015v1.pdf","comment":"ICLR 2024 Tiny Papers"},{"id":"http://arxiv.org/abs/2403.20014v1","updated":"2024-03-29T07:01:29Z","published":"2024-03-29T07:01:29Z","title":"PURPLE: Making a Large Language Model a Better SQL Writer","summary":"  Large Language Model (LLM) techniques play an increasingly important role in\nNatural Language to SQL (NL2SQL) translation. LLMs trained by extensive corpora\nhave strong natural language understanding and basic SQL generation abilities\nwithout additional tuning specific to NL2SQL tasks. Existing LLMs-based NL2SQL\napproaches try to improve the translation by enhancing the LLMs with an\nemphasis on user intention understanding. However, LLMs sometimes fail to\ngenerate appropriate SQL due to their lack of knowledge in organizing complex\nlogical operator composition. A promising method is to input the LLMs with\ndemonstrations, which include known NL2SQL translations from various databases.\nLLMs can learn to organize operator compositions from the input demonstrations\nfor the given task. In this paper, we propose PURPLE (Pre-trained models\nUtilized to Retrieve Prompts for Logical Enhancement), which improves accuracy\nby retrieving demonstrations containing the requisite logical operator\ncomposition for the NL2SQL task on hand, thereby guiding LLMs to produce better\nSQL translation. PURPLE achieves a new state-of-the-art performance of 80.5%\nexact-set match accuracy and 87.8% execution match accuracy on the validation\nset of the popular NL2SQL benchmark Spider. PURPLE maintains high accuracy\nacross diverse benchmarks, budgetary constraints, and various LLMs, showing\nrobustness and cost-effectiveness.\n","authors":["Tonghui Ren","Yuankai Fan","Zhenying He","Ren Huang","Jiaqi Dai","Can Huang","Yinan Jing","Kai Zhang","Yifan Yang","X. Sean Wang"],"pdf_url":"https://arxiv.org/pdf/2403.20014v1.pdf","comment":"12 pages, accepted by ICDE 2024 (40th IEEE International Conference\n  on Data Engineering)"},{"id":"http://arxiv.org/abs/2403.20009v1","updated":"2024-03-29T06:48:30Z","published":"2024-03-29T06:48:30Z","title":"On Large Language Models' Hallucination with Regard to Known Facts","summary":"  Large language models are successful in answering factoid questions but are\nalso prone to hallucination.We investigate the phenomenon of LLMs possessing\ncorrect answer knowledge yet still hallucinating from the perspective of\ninference dynamics, an area not previously covered in studies on\nhallucinations.We are able to conduct this analysis via two key ideas.First, we\nidentify the factual questions that query the same triplet knowledge but result\nin different answers. The difference between the model behaviors on the correct\nand incorrect outputs hence suggests the patterns when hallucinations happen.\nSecond, to measure the pattern, we utilize mappings from the residual streams\nto vocabulary space. We reveal the different dynamics of the output token\nprobabilities along the depths of layers between the correct and hallucinated\ncases. In hallucinated cases, the output token's information rarely\ndemonstrates abrupt increases and consistent superiority in the later stages of\nthe model. Leveraging the dynamic curve as a feature, we build a classifier\ncapable of accurately detecting hallucinatory predictions with an 88\\% success\nrate. Our study shed light on understanding the reasons for LLMs'\nhallucinations on their known facts, and more importantly, on accurately\npredicting when they are hallucinating.\n","authors":["Che Jiang","Biqing Qi","Xiangyu Hong","Dayuan Fu","Yang Cheng","Fandong Meng","Mo Yu","Bowen Zhou","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.20009v1.pdf","comment":"Accepted by NAACL 2024 MainConference"},{"id":"http://arxiv.org/abs/2403.20005v1","updated":"2024-03-29T06:43:55Z","published":"2024-03-29T06:43:55Z","title":"Large Language Model based Situational Dialogues for Second Language\n  Learning","summary":"  In second language learning, scenario-based conversation practice is\nimportant for language learners to achieve fluency in speaking, but students\noften lack sufficient opportunities to practice their conversational skills\nwith qualified instructors or native speakers. To bridge this gap, we propose\nsituational dialogue models for students to engage in conversational practice.\nOur situational dialogue models are fine-tuned on large language models (LLMs),\nwith the aim of combining the engaging nature of an open-ended conversation\nwith the focused practice of scenario-based tasks. Leveraging the\ngeneralization capabilities of LLMs, we demonstrate that our situational\ndialogue models perform effectively not only on training topics but also on\ntopics not encountered during training. This offers a promising solution to\nsupport a wide range of conversational topics without extensive manual work.\nAdditionally, research in the field of dialogue systems still lacks reliable\nautomatic evaluation metrics, leading to human evaluation as the gold standard\n(Smith et al., 2022), which is typically expensive. To address the limitations\nof existing evaluation methods, we present a novel automatic evaluation method\nthat employs fine-tuned LLMs to efficiently and effectively assess the\nperformance of situational dialogue models.\n","authors":["Shuyao Xu","Long Qin","Tianyang Chen","Zhenzhou Zha","Bingxue Qiu","Weizhi Wang"],"pdf_url":"https://arxiv.org/pdf/2403.20005v1.pdf","comment":"14 pages, 6 figures"},{"id":"http://arxiv.org/abs/2312.17269v2","updated":"2024-03-29T06:32:18Z","published":"2023-12-27T00:03:05Z","title":"Conversational Question Answering with Reformulations over Knowledge\n  Graph","summary":"  Conversational question answering (convQA) over knowledge graphs (KGs)\ninvolves answering multi-turn natural language questions about information\ncontained in a KG. State-of-the-art methods of ConvQA often struggle with\ninexplicit question-answer pairs. These inputs are easy for human beings to\nunderstand given a conversation history, but hard for a machine to interpret,\nwhich can degrade ConvQA performance. To address this problem, we propose a\nreinforcement learning (RL) based model, CornNet, which utilizes question\nreformulations generated by large language models (LLMs) to improve ConvQA\nperformance. CornNet adopts a teacher-student architecture where a teacher\nmodel learns question representations using human writing reformulations, and a\nstudent model to mimic the teacher model's output via reformulations generated\nby LLMs. The learned question representation is then used by an RL model to\nlocate the correct answer in a KG. Extensive experimental results show that\nCornNet outperforms state-of-the-art convQA models.\n","authors":["Lihui Liu","Blaine Hill","Boxin Du","Fei Wang","Hanghang Tong"],"pdf_url":"https://arxiv.org/pdf/2312.17269v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19995v1","updated":"2024-03-29T06:22:37Z","published":"2024-03-29T06:22:37Z","title":"Development of Compositionality and Generalization through Interactive\n  Learning of Language and Action of Robots","summary":"  Humans excel at applying learned behavior to unlearned situations. A crucial\ncomponent of this generalization behavior is our ability to compose/decompose a\nwhole into reusable parts, an attribute known as compositionality. One of the\nfundamental questions in robotics concerns this characteristic. \"How can\nlinguistic compositionality be developed concomitantly with sensorimotor skills\nthrough associative learning, particularly when individuals only learn partial\nlinguistic compositions and their corresponding sensorimotor patterns?\" To\naddress this question, we propose a brain-inspired neural network model that\nintegrates vision, proprioception, and language into a framework of predictive\ncoding and active inference, based on the free-energy principle. The\neffectiveness and capabilities of this model were assessed through various\nsimulation experiments conducted with a robot arm. Our results show that\ngeneralization in learning to unlearned verb-noun compositions, is\nsignificantly enhanced when training variations of task composition are\nincreased. We attribute this to self-organized compositional structures in\nlinguistic latent state space being influenced significantly by sensorimotor\nlearning. Ablation studies show that visual attention and working memory are\nessential to accurately generate visuo-motor sequences to achieve\nlinguistically represented goals. These insights advance our understanding of\nmechanisms underlying development of compositionality through interactions of\nlinguistic and sensorimotor experience.\n","authors":["Prasanna Vijayaraghavan","Jeffrey Frederic Queisser","Sergio Verduzco Flores","Jun Tani"],"pdf_url":"https://arxiv.org/pdf/2403.19995v1.pdf","comment":"59 pages, 6 figures, 10 supplementary figures"},{"id":"http://arxiv.org/abs/2310.13001v3","updated":"2024-03-29T05:51:53Z","published":"2023-10-06T12:31:05Z","title":"Conversational Financial Information Retrieval Model (ConFIRM)","summary":"  With the exponential growth in large language models (LLMs), leveraging their\nemergent properties for specialized domains like finance merits exploration.\nHowever, regulated fields such as finance pose unique constraints, requiring\ndomain-optimized frameworks. We present ConFIRM, an LLM-based conversational\nfinancial information retrieval model tailored for query intent classification\nand knowledge base labeling.\n  ConFIRM comprises two modules:\n  1) a method to synthesize finance domain-specific question-answer pairs, and\n  2) evaluation of parameter efficient fine-tuning approaches for the query\nclassification task. We generate a dataset of over 4000 samples, assessing\naccuracy on a separate test set.\n  ConFIRM achieved over 90% accuracy, essential for regulatory compliance.\nConFIRM provides a data-efficient solution to extract precise query intent for\nfinancial dialog systems.\n","authors":["Stephen Choi","William Gazeley","Siu Ho Wong","Tingting Li"],"pdf_url":"https://arxiv.org/pdf/2310.13001v3.pdf","comment":"10 pages, 2 figures, 2 tables, 2 appendices"},{"id":"http://arxiv.org/abs/2403.09488v2","updated":"2024-03-29T05:51:11Z","published":"2024-03-14T15:30:14Z","title":"Rectifying Demonstration Shortcut in In-Context Learning","summary":"  Large language models (LLMs) are able to solve various tasks with only a few\ndemonstrations utilizing their in-context learning (ICL) abilities. However,\nLLMs often rely on their pre-trained semantic priors of demonstrations rather\nthan on the input-label relationships to proceed with ICL prediction. In this\nwork, we term this phenomenon as the 'Demonstration Shortcut'. While previous\nworks have primarily focused on improving ICL prediction results for predefined\ntasks, we aim to rectify the Demonstration Shortcut, thereby enabling the LLM\nto effectively learn new input-label relationships from demonstrations. To\nachieve this, we introduce In-Context Calibration, a demonstration-aware\ncalibration method. We evaluate the effectiveness of the proposed method in two\nsettings: (1) the Original ICL Task using the standard label space and (2) the\nTask Learning setting, where the label space is replaced with semantically\nunrelated tokens. In both settings, In-Context Calibration demonstrates\nsubstantial improvements, with results generalized across three LLM families\n(OPT, GPT, and Llama2) under various configurations.\n","authors":["Joonwon Jang","Sanghwan Jang","Wonbin Kweon","Minjin Jeon","Hwanjo Yu"],"pdf_url":"https://arxiv.org/pdf/2403.09488v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00648v5","updated":"2024-03-29T05:22:15Z","published":"2023-10-01T12:07:44Z","title":"PETA: Parameter-Efficient Trojan Attacks","summary":"  Parameter-efficient fine-tuning (PEFT) enables efficient adaptation of\npre-trained language models (PLMs) to specific tasks. By tuning only a minimal\nset of (extra) parameters, PEFT achieves performance that is comparable to\nstandard fine-tuning. However, despite its prevalent use, the security\nimplications of PEFT remain largely unexplored. In this paper, we take the\ninitial steps and present PETA, a novel trojan attack that compromises the\nweights of PLMs by accounting for downstream adaptation through bilevel\noptimization: the upper-level objective embeds the backdoor into a model while\nthe lower-level objective simulates PEFT to both retain the PLM's task-specific\nperformance and ensure that the backdoor persists after fine-tuning. With\nextensive evaluation across a variety of downstream tasks and trigger designs,\nwe demonstrate PETA's effectiveness in terms of both attack success rate and\nclean accuracy, even when the attacker does not have full knowledge of the\nvictim user's training process.\n","authors":["Lauren Hong","Ting Wang"],"pdf_url":"https://arxiv.org/pdf/2310.00648v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19154v2","updated":"2024-03-29T05:15:12Z","published":"2024-03-28T05:35:22Z","title":"STaR-GATE: Teaching Language Models to Ask Clarifying Questions","summary":"  When prompting language models to complete a task, users often leave\nimportant aspects unsaid. While asking questions could resolve this ambiguity\n(GATE; Li et al., 2023), models often struggle to ask good questions. We\nexplore a language model's ability to self-improve (STaR; Zelikman et al.,\n2022) by rewarding the model for generating useful questions-a simple method we\ndub STaR-GATE. We generate a synthetic dataset of 25,500 unique persona-task\nprompts to simulate conversations between a pretrained language model-the\nQuestioner-and a Roleplayer whose preferences are unknown to the Questioner. By\nasking questions, the Questioner elicits preferences from the Roleplayer. The\nQuestioner is iteratively finetuned on questions that increase the probability\nof high-quality responses to the task, which are generated by an Oracle with\naccess to the Roleplayer's latent preferences. After two iterations of\nself-improvement, the Questioner asks better questions, allowing it to generate\nresponses that are preferred over responses from the initial model on 72% of\ntasks. Our results indicate that teaching a language model to ask better\nquestions leads to better personalized responses.\n","authors":["Chinmaya Andukuri","Jan-Philipp Fränken","Tobias Gerstenberg","Noah D. Goodman"],"pdf_url":"https://arxiv.org/pdf/2403.19154v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15456v3","updated":"2024-03-29T04:38:51Z","published":"2024-03-19T06:39:23Z","title":"WoLF: Wide-scope Large Language Model Framework for CXR Understanding","summary":"  Significant methodological strides have been made toward Chest X-ray (CXR)\nunderstanding via modern vision-language models (VLMs), demonstrating\nimpressive Visual Question Answering (VQA) and CXR report generation abilities.\nHowever, existing CXR understanding frameworks still possess several procedural\ncaveats. (1) Previous methods solely use CXR reports, which are insufficient\nfor comprehensive Visual Question Answering (VQA), especially when additional\nhealth-related data like medication history and prior diagnoses are needed. (2)\nPrevious methods use raw CXR reports, which are often arbitrarily structured.\nWhile modern language models can understand various text formats, restructuring\nreports for clearer, organized anatomy-based information could enhance their\nusefulness. (3) Current evaluation methods for CXR-VQA primarily emphasize\nlinguistic correctness, lacking the capability to offer nuanced assessments of\nthe generated answers. In this work, to address the aforementioned caveats, we\nintroduce WoLF, a Wide-scope Large Language Model Framework for CXR\nunderstanding. To resolve (1), we capture multi-faceted records of patients,\nwhich are utilized for accurate diagnoses in real-world clinical scenarios.\nSpecifically, we adopt the Electronic Health Records (EHR) to generate\ninstruction-following data suited for CXR understanding. Regarding (2), we\nenhance report generation performance by decoupling knowledge in CXR reports\nbased on anatomical structure even within the attention step via masked\nattention. To address (3), we introduce an AI-evaluation protocol optimized for\nassessing the capabilities of LLM. Through extensive experimental validations,\nWoLF demonstrates superior performance over other models on MIMIC-CXR in the\nAI-evaluation arena about VQA (up to +9.47%p mean score) and by metrics about\nreport generation (+7.3%p BLEU-1).\n","authors":["Seil Kang","Donghyun Kim","Junhyeok Kim","Hyo Kyung Lee","Seong Jae Hwang"],"pdf_url":"https://arxiv.org/pdf/2403.15456v3.pdf","comment":"11 pages main paper, 2 pages supplementary"},{"id":"http://arxiv.org/abs/2403.19962v1","updated":"2024-03-29T03:48:12Z","published":"2024-03-29T03:48:12Z","title":"Enhancing the General Agent Capabilities of Low-Parameter LLMs through\n  Tuning and Multi-Branch Reasoning","summary":"  Open-source pre-trained Large Language Models (LLMs) exhibit strong language\nunderstanding and generation capabilities, making them highly successful in a\nvariety of tasks. However, when used as agents for dealing with complex\nproblems in the real world, their performance is far inferior to large\ncommercial models such as ChatGPT and GPT-4. As intelligent agents, LLMs need\nto have the capabilities of task planning, long-term memory, and the ability to\nleverage external tools to achieve satisfactory performance. Various methods\nhave been proposed to enhance the agent capabilities of LLMs. On the one hand,\nmethods involve constructing agent-specific data and fine-tuning the models. On\nthe other hand, some methods focus on designing prompts that effectively\nactivate the reasoning abilities of the LLMs. We explore both strategies on the\n7B and 13B models. We propose a comprehensive method for constructing\nagent-specific data using GPT-4. Through supervised fine-tuning with\nconstructed data, we find that for these models with a relatively small number\nof parameters, supervised fine-tuning can significantly reduce hallucination\noutputs and formatting errors in agent tasks. Furthermore, techniques such as\nmulti-path reasoning and task decomposition can effectively decrease problem\ncomplexity and enhance the performance of LLMs as agents. We evaluate our\nmethod on five agent tasks of AgentBench and achieve satisfactory results.\n","authors":["Qinhao Zhou","Zihan Zhang","Xiang Xiang","Ke Wang","Yuchuan Wu","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2403.19962v1.pdf","comment":"To appear at NAACL 2024"},{"id":"http://arxiv.org/abs/2403.09732v3","updated":"2024-03-29T03:21:01Z","published":"2024-03-13T02:32:41Z","title":"PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with\n  Cross-consistency","summary":"  Recent advancements in Text-to-SQL (Text2SQL) emphasize stimulating the large\nlanguage models (LLM) on in-context learning, achieving significant results.\nNevertheless, they face challenges when dealing with verbose database\ninformation and complex user intentions. This paper presents a two-stage\nframework to enhance the performance of current LLM-based natural language to\nSQL systems. We first introduce a novel prompt representation, called\nreference-enhanced representation, which includes schema information and\nrandomly sampled cell values from tables to instruct LLMs in generating SQL\nqueries. Then, in the first stage, question-SQL pairs are retrieved as few-shot\ndemonstrations, prompting the LLM to generate a preliminary SQL (PreSQL). After\nthat, the mentioned entities in PreSQL are parsed to conduct schema linking,\nwhich can significantly compact the useful information. In the second stage,\nwith the linked schema, we simplify the prompt's schema information and\ninstruct the LLM to produce the final SQL. Finally, as the post-refinement\nmodule, we propose using cross-consistency across different LLMs rather than\nself-consistency within a particular LLM. Our methods achieve new SOTA results\non the Spider benchmark, with an execution accuracy of 87.6%.\n","authors":["Zhishuai Li","Xiang Wang","Jingjing Zhao","Sun Yang","Guoqing Du","Xiaoru Hu","Bin Zhang","Yuxiao Ye","Ziyue Li","Rui Zhao","Hangyu Mao"],"pdf_url":"https://arxiv.org/pdf/2403.09732v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19936v1","updated":"2024-03-29T02:42:39Z","published":"2024-03-29T02:42:39Z","title":"SLFNet: Generating Semantic Logic Forms from Natural Language Using\n  Semantic Probability Graphs","summary":"  Building natural language interfaces typically uses a semantic parser to\nparse the user's natural language and convert it into structured\n\\textbf{S}emantic \\textbf{L}ogic \\textbf{F}orms (SLFs). The mainstream approach\nis to adopt a sequence-to-sequence framework, which requires that natural\nlanguage commands and SLFs must be represented serially. Since a single natural\nlanguage may have multiple SLFs or multiple natural language commands may have\nthe same SLF, training a sequence-to-sequence model is sensitive to the choice\namong them, a phenomenon recorded as \"order matters\". To solve this problem, we\npropose a novel neural network, SLFNet, which firstly incorporates dependent\nsyntactic information as prior knowledge and can capture the long-range\ninteractions between contextual information and words. Secondly construct\nsemantic probability graphs to obtain local dependencies between predictor\nvariables. Finally we propose the Multi-Head SLF Attention mechanism to\nsynthesize SLFs from natural language commands based on Sequence-to-Slots.\nExperiments show that SLFNet achieves state-of-the-art performance on the\nChineseQCI-TS and Okapi datasets, and competitive performance on the ATIS\ndataset.\n","authors":["Hao Wu","Fan Xu"],"pdf_url":"https://arxiv.org/pdf/2403.19936v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19930v1","updated":"2024-03-29T02:36:54Z","published":"2024-03-29T02:36:54Z","title":"Are LLMs Effective Backbones for Fine-tuning? An Experimental\n  Investigation of Supervised LLMs on Chinese Short Text Matching","summary":"  The recent success of Large Language Models (LLMs) has garnered significant\nattention in both academia and industry. Prior research on LLMs has primarily\nfocused on enhancing or leveraging their generalization capabilities in zero-\nand few-shot settings. However, there has been limited investigation into\neffectively fine-tuning LLMs for a specific natural language understanding task\nin supervised settings. In this study, we conduct an experimental analysis by\nfine-tuning LLMs for the task of Chinese short text matching. We explore\nvarious factors that influence performance when fine-tuning LLMs, including\ntask modeling methods, prompt formats, and output formats.\n","authors":["Shulin Liu","Chengcheng Xu","Hao Liu","Tinghao Yu","Tao Yang"],"pdf_url":"https://arxiv.org/pdf/2403.19930v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19928v1","updated":"2024-03-29T02:32:15Z","published":"2024-03-29T02:32:15Z","title":"DiJiang: Efficient Large Language Models through Compact Kernelization","summary":"  In an effort to reduce the computational load of Transformers, research on\nlinear attention has gained significant momentum. However, the improvement\nstrategies for attention mechanisms typically necessitate extensive retraining,\nwhich is impractical for large language models with a vast array of parameters.\nIn this paper, we present DiJiang, a novel Frequency Domain Kernelization\napproach that enables the transformation of a pre-trained vanilla Transformer\ninto a linear complexity model with little training costs. By employing a\nweighted Quasi-Monte Carlo method for sampling, the proposed approach\ntheoretically offers superior approximation efficiency. To further reduce the\ntraining computational complexity, our kernelization is based on Discrete\nCosine Transform (DCT) operations. Extensive experiments demonstrate that the\nproposed method achieves comparable performance to the original Transformer,\nbut with significantly reduced training costs and much faster inference speeds.\nOur DiJiang-7B achieves comparable performance with LLaMA2-7B on various\nbenchmark while requires only about 1/50 training cost. Code is available at\nhttps://github.com/YuchuanTian/DiJiang.\n","authors":["Hanting Chen","Zhicheng Liu","Xutao Wang","Yuchuan Tian","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2403.19928v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19913v1","updated":"2024-03-29T01:53:24Z","published":"2024-03-29T01:53:24Z","title":"MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of\n  Large Language Models","summary":"  Large language models such as ChatGPT and GPT-4 have recently achieved\nastonishing performance on a variety of natural language processing tasks. In\nthis paper, we propose MANGO, a benchmark to evaluate their capabilities to\nperform text-based mapping and navigation. Our benchmark includes 53 mazes\ntaken from a suite of textgames: each maze is paired with a walkthrough that\nvisits every location but does not cover all possible paths. The task is\nquestion-answering: for each maze, a large language model reads the walkthrough\nand answers hundreds of mapping and navigation questions such as \"How should\nyou go to Attic from West of House?\" and \"Where are we if we go north and east\nfrom Cellar?\". Although these questions are easy to humans, it turns out that\neven GPT-4, the best-to-date language model, performs poorly at answering them.\nFurther, our experiments suggest that a strong mapping and navigation ability\nwould benefit large language models in performing relevant downstream tasks,\nsuch as playing textgames. Our MANGO benchmark will facilitate future research\non methods that improve the mapping and navigation capabilities of language\nmodels. We host our leaderboard, data, code, and evaluation program at\nhttps://mango.ttic.edu and https://github.com/oaklight/mango/.\n","authors":["Peng Ding","Jiading Fang","Peng Li","Kangrui Wang","Xiaochen Zhou","Mo Yu","Jing Li","Matthew R. Walter","Hongyuan Mei"],"pdf_url":"https://arxiv.org/pdf/2403.19913v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10814v2","updated":"2024-03-29T01:18:18Z","published":"2023-09-19T17:54:21Z","title":"Natural Language Embedded Programs for Hybrid Language Symbolic\n  Reasoning","summary":"  How can we perform computations over natural language representations to\nsolve tasks that require symbolic and numeric reasoning? We propose natural\nlanguage embedded programs (NLEP) as a unifying framework for addressing\nmath/symbolic reasoning, natural language understanding, and instruction\nfollowing tasks. Our approach prompts a language model to generate full Python\nprograms that define functions over data structures which contain natural\nlanguage representations of structured knowledge. A Python interpreter then\nexecutes the generated code and prints the output. Despite using a task-general\nprompt, we find that this approach can improve upon strong baselines across a\nrange of different tasks including math and symbolic reasoning, text\nclassification, question answering, and instruction following. We found that\nthe generated programs are interpretable since they outline the exact reasoning\nprocess followed by the program interpreter.\n","authors":["Tianhua Zhang","Jiaxin Ge","Hongyin Luo","Yung-Sung Chuang","Mingye Gao","Yuan Gong","Xixin Wu","Yoon Kim","Helen Meng","James Glass"],"pdf_url":"https://arxiv.org/pdf/2309.10814v2.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.19889v1","updated":"2024-03-29T00:14:46Z","published":"2024-03-29T00:14:46Z","title":"Towards a Robust Retrieval-Based Summarization System","summary":"  This paper describes an investigation of the robustness of large language\nmodels (LLMs) for retrieval augmented generation (RAG)-based summarization\ntasks. While LLMs provide summarization capabilities, their performance in\ncomplex, real-world scenarios remains under-explored. Our first contribution is\nLogicSumm, an innovative evaluation framework incorporating realistic scenarios\nto assess LLM robustness during RAG-based summarization. Based on limitations\nidentified by LogiSumm, we then developed SummRAG, a comprehensive system to\ncreate training dialogues and fine-tune a model to enhance robustness within\nLogicSumm's scenarios. SummRAG is an example of our goal of defining structured\nmethods to test the capabilities of an LLM, rather than addressing issues in a\none-off fashion. Experimental results confirm the power of SummRAG, showcasing\nimproved logical coherence and summarization quality. Data, corresponding model\nweights, and Python code are available online.\n","authors":["Shengjie Liu","Jing Wu","Jingyuan Bao","Wenyi Wang","Naira Hovakimyan","Christopher G Healey"],"pdf_url":"https://arxiv.org/pdf/2403.19889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06588v2","updated":"2024-03-29T23:53:28Z","published":"2023-10-10T12:53:48Z","title":"FTFT: Efficient and Robust Fine-Tuning by Transferring Training Dynamics","summary":"  Despite the massive success of fine-tuning Pre-trained Language Models\n(PLMs), they remain susceptible to out-of-distribution input. Dataset\ncartography is a simple yet effective dual-model approach that improves the\nrobustness of fine-tuned PLMs. It involves fine-tuning a model on the original\ntraining set (i.e. reference model), selecting a subset of important training\ninstances based on the training dynamics, and fine-tuning again only on these\nselected examples (i.e. main model). However, this approach requires\nfine-tuning the same model twice, which is computationally expensive for large\nPLMs. In this paper, we show that (1) training dynamics are highly transferable\nacross model sizes and pre-training methods, and that (2) fine-tuning main\nmodels using these selected training instances achieves higher training\nefficiency than empirical risk minimization (ERM). Building on these\nobservations, we propose a novel fine-tuning approach: Fine-Tuning by\ntransFerring Training dynamics (FTFT). Compared with dataset cartography, FTFT\nuses more efficient reference models and aggressive early stopping. FTFT\nachieves robustness improvements over ERM while lowering the training cost by\nup to $\\sim 50\\%$.\n","authors":["Yupei Du","Albert Gatt","Dong Nguyen"],"pdf_url":"https://arxiv.org/pdf/2310.06588v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15278v2","updated":"2024-03-29T22:43:37Z","published":"2024-03-22T15:21:07Z","title":"Specifying Genericity through Inclusiveness and Abstractness Continuous\n  Scales","summary":"  This paper introduces a novel annotation framework for the fine-grained\nmodeling of Noun Phrases' (NPs) genericity in natural language. The framework\nis designed to be simple and intuitive, making it accessible to non-expert\nannotators and suitable for crowd-sourced tasks. Drawing from theoretical and\ncognitive literature on genericity, this framework is grounded in established\nlinguistic theory. Through a pilot study, we created a small but crucial\nannotated dataset of 324 sentences, serving as a foundation for future\nresearch. To validate our approach, we conducted an evaluation comparing our\ncontinuous annotations with existing binary annotations on the same dataset,\ndemonstrating the framework's effectiveness in capturing nuanced aspects of\ngenericity. Our work offers a practical resource for linguists, providing a\nfirst annotated dataset and an annotation scheme designed to build\nreal-language datasets that can be used in studies on the semantics of\ngenericity, and NLP practitioners, contributing to the development of\ncommonsense knowledge repositories valuable in enhancing various NLP\napplications.\n","authors":["Claudia Collacciani","Andrea Amelio Ravelli","Marianna Marcella Bolognesi"],"pdf_url":"https://arxiv.org/pdf/2403.15278v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2311.08968v2","updated":"2024-03-29T22:14:30Z","published":"2023-11-15T14:01:41Z","title":"Identifying Linear Relational Concepts in Large Language Models","summary":"  Transformer language models (LMs) have been shown to represent concepts as\ndirections in the latent space of hidden activations. However, for any\nhuman-interpretable concept, how can we find its direction in the latent space?\nWe present a technique called linear relational concepts (LRC) for finding\nconcept directions corresponding to human-interpretable concepts by first\nmodeling the relation between subject and object as a linear relational\nembedding (LRE). We find that inverting the LRE and using earlier object layers\nresults in a powerful technique for finding concept directions that outperforms\nstandard black-box probing classifiers. We evaluate LRCs on their performance\nas concept classifiers as well as their ability to causally change model\noutput.\n","authors":["David Chanin","Anthony Hunter","Oana-Maria Camburu"],"pdf_url":"https://arxiv.org/pdf/2311.08968v2.pdf","comment":"To be published in NAACL 2024"},{"id":"http://arxiv.org/abs/2402.01858v2","updated":"2024-03-29T21:18:37Z","published":"2024-02-02T19:28:33Z","title":"Explaining latent representations of generative models with large\n  multimodal models","summary":"  Learning interpretable representations of data generative latent factors is\nan important topic for the development of artificial intelligence. With the\nrise of the large multimodal model, it can align images with text to generate\nanswers. In this work, we propose a framework to comprehensively explain each\nlatent variable in the generative models using a large multimodal model. We\nfurther measure the uncertainty of our generated explanations, quantitatively\nevaluate the performance of explanation generation among multiple large\nmultimodal models, and qualitatively visualize the variations of each latent\nvariable to learn the disentanglement effects of different generative models on\nexplanations. Finally, we discuss the explanatory capabilities and limitations\nof state-of-the-art large multimodal models.\n","authors":["Mengdan Zhu","Zhenke Liu","Bo Pan","Abhinav Angirekula","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.01858v2.pdf","comment":"ICLR 2024 Workshop Paper on Reliable and Responsible Foundation\n  Models"},{"id":"http://arxiv.org/abs/2402.09394v2","updated":"2024-03-29T21:17:23Z","published":"2024-02-14T18:45:14Z","title":"Long-form evaluation of model editing","summary":"  Evaluations of model editing currently only use the `next few token'\ncompletions after a prompt. As a result, the impact of these methods on longer\nnatural language generation is largely unknown. We introduce long-form\nevaluation of model editing (LEME) a novel evaluation protocol that measures\nthe efficacy and impact of model editing in long-form generative settings. Our\nprotocol consists of a machine-rated survey and a classifier which correlates\nwell with human ratings. Importantly, we find that our protocol has very little\nrelationship with previous short-form metrics (despite being designed to extend\nefficacy, generalization, locality, and portability into a long-form setting),\nindicating that our method introduces a novel set of dimensions for\nunderstanding model editing methods. Using this protocol, we benchmark a number\nof model editing techniques and present several findings including that, while\nsome methods (ROME and MEMIT) perform well in making consistent edits within a\nlimited scope, they suffer much more from factual drift than other methods.\nFinally, we present a qualitative analysis that illustrates common failure\nmodes in long-form generative settings including internal consistency, lexical\ncohesion, and locality issues.\n","authors":["Domenic Rosati","Robie Gonzales","Jinkun Chen","Xuemin Yu","Melis Erkan","Yahya Kayani","Satya Deepika Chavatapalli","Frank Rudzicz","Hassan Sajjad"],"pdf_url":"https://arxiv.org/pdf/2402.09394v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.15043v2","updated":"2024-03-29T19:01:27Z","published":"2024-01-26T18:13:57Z","title":"Health Text Simplification: An Annotated Corpus for Digestive Cancer\n  Education and Novel Strategies for Reinforcement Learning","summary":"  Objective: The reading level of health educational materials significantly\ninfluences the understandability and accessibility of the information,\nparticularly for minoritized populations. Many patient educational resources\nsurpass the reading level and complexity of widely accepted standards. There is\na critical need for high-performing text simplification models in health\ninformation to enhance dissemination and literacy. This need is particularly\nacute in cancer education, where effective prevention and screening education\ncan substantially reduce morbidity and mortality.\n  Methods: We introduce Simplified Digestive Cancer (SimpleDC), a parallel\ncorpus of cancer education materials tailored for health text simplification\nresearch, comprising educational content from the American Cancer Society,\nCenters for Disease Control and Prevention, and National Cancer Institute.\nUtilizing SimpleDC alongside the existing Med-EASi corpus, we explore Large\nLanguage Model (LLM)-based simplification methods, including fine-tuning,\nreinforcement learning (RL), reinforcement learning with human feedback (RLHF),\ndomain adaptation, and prompt-based approaches. Our experimentation encompasses\nLlama 2 and GPT-4. A novel RLHF reward function is introduced, featuring a\nlightweight model adept at distinguishing between original and simplified\ntexts, thereby enhancing the model's effectiveness with unlabeled data.\n  Results: Fine-tuned Llama 2 models demonstrated high performance across\nvarious metrics. Our innovative RLHF reward function surpassed existing RL text\nsimplification reward functions in effectiveness. The results underscore that\nRL/RLHF can augment fine-tuning, facilitating model training on unlabeled text\nand improving performance.\n","authors":["Md Mushfiqur Rahman","Mohammad Sabik Irbaz","Kai North","Michelle S. Williams","Marcos Zampieri","Kevin Lybarger"],"pdf_url":"https://arxiv.org/pdf/2401.15043v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18314v3","updated":"2024-03-29T18:48:35Z","published":"2024-03-27T07:34:44Z","title":"Chinese Offensive Language Detection:Current Status and Future\n  Directions","summary":"  Despite the considerable efforts being made to monitor and regulate\nuser-generated content on social media platforms, the pervasiveness of\noffensive language, such as hate speech or cyberbullying, in the digital space\nremains a significant challenge. Given the importance of maintaining a\ncivilized and respectful online environment, there is an urgent and growing\nneed for automatic systems capable of detecting offensive speech in real time.\nHowever, developing effective systems for processing languages such as Chinese\npresents a significant challenge, owing to the language's complex and nuanced\nnature, which makes it difficult to process automatically. This paper provides\na comprehensive overview of offensive language detection in Chinese, examining\ncurrent benchmarks and approaches and highlighting specific models and tools\nfor addressing the unique challenges of detecting offensive language in this\ncomplex language. The primary objective of this survey is to explore the\nexisting techniques and identify potential avenues for further research that\ncan address the cultural and linguistic complexities of Chinese.\n","authors":["Yunze Xiao","Houda Bouamor","Wajdi Zaghouani"],"pdf_url":"https://arxiv.org/pdf/2403.18314v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06877v3","updated":"2024-03-29T18:27:17Z","published":"2024-01-12T20:08:39Z","title":"Promptly Predicting Structures: The Return of Inference","summary":"  Prompt-based methods have been used extensively across NLP to build zero- and\nfew-shot label predictors. Many NLP tasks are naturally structured: that is,\ntheir outputs consist of multiple labels which constrain each other. Annotating\ndata for such tasks can be cumbersome. Can the promise of the prompt-based\nparadigm be extended to such structured outputs? In this paper, we present a\nframework for constructing zero- and few-shot linguistic structure predictors.\nOur key insight is that we can use structural constraints -- and combinatorial\ninference derived from them -- to filter out inconsistent structures predicted\nby large language models. We instantiated this framework on two structured\nprediction tasks, and five datasets. Across all cases, our results show that\nenforcing consistency not only constructs structurally valid outputs, but also\nimproves performance over the unconstrained variants.\n","authors":["Maitrey Mehta","Valentina Pyatkin","Vivek Srikumar"],"pdf_url":"https://arxiv.org/pdf/2401.06877v3.pdf","comment":"19 pages, 13 figures Accepted to NAACL'2024 (Main)"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2403.20331v1","updated":"2024-03-29T17:59:53Z","published":"2024-03-29T17:59:53Z","title":"Unsolvable Problem Detection: Evaluating Trustworthiness of Vision\n  Language Models","summary":"  This paper introduces a novel and significant challenge for Vision Language\nModels (VLMs), termed Unsolvable Problem Detection (UPD). UPD examines the\nVLM's ability to withhold answers when faced with unsolvable problems in the\ncontext of Visual Question Answering (VQA) tasks. UPD encompasses three\ndistinct settings: Absent Answer Detection (AAD), Incompatible Answer Set\nDetection (IASD), and Incompatible Visual Question Detection (IVQD). To deeply\ninvestigate the UPD problem, extensive experiments indicate that most VLMs,\nincluding GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying\nextents, highlighting significant room for the improvements. To address UPD, we\nexplore both training-free and training-based solutions, offering new insights\ninto their effectiveness and limitations. We hope our insights, together with\nfuture efforts within the proposed UPD settings, will enhance the broader\nunderstanding and development of more practical and reliable VLMs.\n","authors":["Atsuyuki Miyai","Jingkang Yang","Jingyang Zhang","Yifei Ming","Qing Yu","Go Irie","Yixuan Li","Hai Li","Ziwei Liu","Kiyoharu Aizawa"],"pdf_url":"https://arxiv.org/pdf/2403.20331v1.pdf","comment":"Code: https://github.com/AtsuMiyai/UPD"},{"id":"http://arxiv.org/abs/2403.20330v1","updated":"2024-03-29T17:59:34Z","published":"2024-03-29T17:59:34Z","title":"Are We on the Right Way for Evaluating Large Vision-Language Models?","summary":"  Large vision-language models (LVLMs) have recently achieved rapid progress,\nsparking numerous studies to evaluate their multi-modal capabilities. However,\nwe dig into current evaluation works and identify two primary issues: 1) Visual\ncontent is unnecessary for many samples. The answers can be directly inferred\nfrom the questions and options, or the world knowledge embedded in LLMs. This\nphenomenon is prevalent across current benchmarks. For instance, GeminiPro\nachieves 42.9% on the MMMU benchmark without any visual input, and outperforms\nthe random choice baseline across six benchmarks over 20% on average. 2)\nUnintentional data leakage exists in LLM and LVLM training. LLM and LVLM could\nstill answer some visual-necessary questions without visual content, indicating\nthe memorizing of these samples within large-scale training data. For example,\nSphinx-X-MoE gets 43.6% on MMMU without accessing images, surpassing its LLM\nbackbone with 17.9%. Both problems lead to misjudgments of actual multi-modal\ngains and potentially misguide the study of LVLM. To this end, we present\nMMStar, an elite vision-indispensable multi-modal benchmark comprising 1,500\nsamples meticulously selected by humans. MMStar benchmarks 6 core capabilities\nand 18 detailed axes, aiming to evaluate LVLMs' multi-modal capacities with\ncarefully balanced and purified samples. These samples are first roughly\nselected from current benchmarks with an automated pipeline, human review is\nthen involved to ensure each curated sample exhibits visual dependency, minimal\ndata leakage, and requires advanced multi-modal capabilities. Moreover, two\nmetrics are developed to measure data leakage and actual performance gain in\nmulti-modal training. We evaluate 16 leading LVLMs on MMStar to assess their\nmulti-modal capabilities, and on 7 benchmarks with the proposed metrics to\ninvestigate their data leakage and actual multi-modal gain.\n","authors":["Lin Chen","Jinsong Li","Xiaoyi Dong","Pan Zhang","Yuhang Zang","Zehui Chen","Haodong Duan","Jiaqi Wang","Yu Qiao","Dahua Lin","Feng Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.20330v1.pdf","comment":"Project page: https://mmstar-benchmark.github.io/"},{"id":"http://arxiv.org/abs/2311.17245v5","updated":"2024-03-29T17:58:34Z","published":"2023-11-28T21:39:20Z","title":"LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and\n  200+ FPS","summary":"  Recent advancements in real-time neural rendering using point-based\ntechniques have paved the way for the widespread adoption of 3D\nrepresentations. However, foundational approaches like 3D Gaussian Splatting\ncome with a substantial storage overhead caused by growing the SfM points to\nmillions, often demanding gigabyte-level disk space for a single unbounded\nscene, posing significant scalability challenges and hindering the splatting\nefficiency.\n  To address this challenge, we introduce LightGaussian, a novel method\ndesigned to transform 3D Gaussians into a more efficient and compact format.\nDrawing inspiration from the concept of Network Pruning, LightGaussian\nidentifies Gaussians that are insignificant in contributing to the scene\nreconstruction and adopts a pruning and recovery process, effectively reducing\nredundancy in Gaussian counts while preserving visual effects. Additionally,\nLightGaussian employs distillation and pseudo-view augmentation to distill\nspherical harmonics to a lower degree, allowing knowledge transfer to more\ncompact representations while maintaining reflectance. Furthermore, we propose\na hybrid scheme, VecTree Quantization, to quantize all attributes, resulting in\nlower bitwidth representations with minimal accuracy losses.\n  In summary, LightGaussian achieves an averaged compression rate over 15x\nwhile boosting the FPS from 139 to 215, enabling an efficient representation of\ncomplex scenes on Mip-NeRF 360, Tank and Temple datasets.\n  Project website: https://lightgaussian.github.io/\n","authors":["Zhiwen Fan","Kevin Wang","Kairun Wen","Zehao Zhu","Dejia Xu","Zhangyang Wang"],"pdf_url":"https://arxiv.org/pdf/2311.17245v5.pdf","comment":"16pages, 8figures"},{"id":"http://arxiv.org/abs/2310.11256v2","updated":"2024-03-29T17:50:17Z","published":"2023-10-17T13:22:36Z","title":"Gromov-Wassertein-like Distances in the Gaussian Mixture Models Space","summary":"  The Gromov-Wasserstein (GW) distance is frequently used in machine learning\nto compare distributions across distinct metric spaces. Despite its utility, it\nremains computationally intensive, especially for large-scale problems.\nRecently, a novel Wasserstein distance specifically tailored for Gaussian\nmixture models and known as MW (mixture Wasserstein) has been introduced by\nseveral authors. In scenarios where data exhibit clustering, this approach\nsimplifies to a small-scale discrete optimal transport problem, which\ncomplexity depends solely on the number of Gaussian components in the GMMs.\nThis paper aims to extend MW by introducing new Gromov-type distances. These\ndistances are designed to be isometry-invariant in Euclidean spaces and are\napplicable for comparing GMMs across different dimensional spaces. Our first\ncontribution is the Mixture Gromov Wasserstein distance (MGW), which can be\nviewed as a Gromovized version of MW. This new distance has a straightforward\ndiscrete formulation, making it highly efficient for estimating distances\nbetween GMMs in practical applications. To facilitate the derivation of a\ntransport plan between GMMs, we present a second distance, the Embedded\nWasserstein distance (EW). This distance turns out to be closely related to\nseveral recent alternatives to Gromov-Wasserstein. We show that EW can be\nadapted to derive a distance as well as optimal transportation plans between\nGMMs. We demonstrate the efficiency of these newly proposed distances on medium\nto large-scale problems, including shape matching and hyperspectral image color\ntransfer.\n","authors":["Antoine Salmona","Julie Delon","Agnès Desolneux"],"pdf_url":"https://arxiv.org/pdf/2310.11256v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2310.05737v3","updated":"2024-03-29T17:44:41Z","published":"2023-10-09T14:10:29Z","title":"Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation","summary":"  While Large Language Models (LLMs) are the dominant models for generative\ntasks in language, they do not perform as well as diffusion models on image and\nvideo generation. To effectively use LLMs for visual generation, one crucial\ncomponent is the visual tokenizer that maps pixel-space inputs to discrete\ntokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, a\nvideo tokenizer designed to generate concise and expressive tokens for both\nvideos and images using a common token vocabulary. Equipped with this new\ntokenizer, we show that LLMs outperform diffusion models on standard image and\nvideo generation benchmarks including ImageNet and Kinetics. In addition, we\ndemonstrate that our tokenizer surpasses the previously top-performing video\ntokenizer on two more tasks: (1) video compression comparable to the\nnext-generation video codec (VCC) according to human evaluations, and (2)\nlearning effective representations for action recognition tasks.\n","authors":["Lijun Yu","José Lezama","Nitesh B. Gundavarapu","Luca Versari","Kihyuk Sohn","David Minnen","Yong Cheng","Vighnesh Birodkar","Agrim Gupta","Xiuye Gu","Alexander G. Hauptmann","Boqing Gong","Ming-Hsuan Yang","Irfan Essa","David A. Ross","Lu Jiang"],"pdf_url":"https://arxiv.org/pdf/2310.05737v3.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.20320v1","updated":"2024-03-29T17:43:58Z","published":"2024-03-29T17:43:58Z","title":"MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning","summary":"  Adapting models pre-trained on large-scale datasets to a variety of\ndownstream tasks is a common strategy in deep learning. Consequently,\nparameter-efficient fine-tuning methods have emerged as a promising way to\nadapt pre-trained models to different tasks while training only a minimal\nnumber of parameters. While most of these methods are designed for single-task\nadaptation, parameter-efficient training in Multi-Task Learning (MTL)\narchitectures is still unexplored. In this paper, we introduce MTLoRA, a novel\nframework for parameter-efficient training of MTL models. MTLoRA employs\nTask-Agnostic and Task-Specific Low-Rank Adaptation modules, which effectively\ndisentangle the parameter space in MTL fine-tuning, thereby enabling the model\nto adeptly handle both task specialization and interaction within MTL contexts.\nWe applied MTLoRA to hierarchical-transformer-based MTL architectures, adapting\nthem to multiple downstream dense prediction tasks. Our extensive experiments\non the PASCAL dataset show that MTLoRA achieves higher accuracy on downstream\ntasks compared to fully fine-tuning the MTL model while reducing the number of\ntrainable parameters by 3.6x. Furthermore, MTLoRA establishes a Pareto-optimal\ntrade-off between the number of trainable parameters and the accuracy of the\ndownstream tasks, outperforming current state-of-the-art parameter-efficient\ntraining methods in both accuracy and efficiency. Our code is publicly\navailable.\n","authors":["Ahmed Agiza","Marina Neseem","Sherief Reda"],"pdf_url":"https://arxiv.org/pdf/2403.20320v1.pdf","comment":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2024"},{"id":"http://arxiv.org/abs/2401.15741v4","updated":"2024-03-29T17:42:21Z","published":"2024-01-28T19:58:19Z","title":"SERNet-Former: Semantic Segmentation by Efficient Residual Network with\n  Attention-Boosting Gates and Attention-Fusion Networks","summary":"  Improving the efficiency of state-of-the-art methods in semantic segmentation\nrequires overcoming the increasing computational cost as well as issues such as\nfusing semantic information from global and local contexts. Based on the recent\nsuccess and problems that convolutional neural networks (CNNs) encounter in\nsemantic segmentation, this research proposes an encoder-decoder architecture\nwith a unique efficient residual network, Efficient-ResNet. Attention-boosting\ngates (AbGs) and attention-boosting modules (AbMs) are deployed by aiming to\nfuse the equivariant and feature-based semantic information with the equivalent\nsizes of the output of global context of the efficient residual network in the\nencoder. Respectively, the decoder network is developed with the additional\nattention-fusion networks (AfNs) inspired by AbM. AfNs are designed to improve\nthe efficiency in the one-to-one conversion of the semantic information by\ndeploying additional convolution layers in the decoder part. Our network is\ntested on the challenging CamVid and Cityscapes datasets, and the proposed\nmethods reveal significant improvements on the residual networks. To the best\nof our knowledge, the developed network, SERNet-Former, achieves\nstate-of-the-art results (84.62 % mean IoU) on CamVid dataset and challenging\nresults (87.35 % mean IoU) on Cityscapes validation dataset.\n","authors":["Serdar Erisen"],"pdf_url":"https://arxiv.org/pdf/2401.15741v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20318v1","updated":"2024-03-29T17:41:57Z","published":"2024-03-29T17:41:57Z","title":"SeaBird: Segmentation in Bird's View with Dice Loss Improves Monocular\n  3D Detection of Large Objects","summary":"  Monocular 3D detectors achieve remarkable performance on cars and smaller\nobjects. However, their performance drops on larger objects, leading to fatal\naccidents. Some attribute the failures to training data scarcity or their\nreceptive field requirements of large objects. In this paper, we highlight this\nunderstudied problem of generalization to large objects. We find that modern\nfrontal detectors struggle to generalize to large objects even on nearly\nbalanced datasets. We argue that the cause of failure is the sensitivity of\ndepth regression losses to noise of larger objects. To bridge this gap, we\ncomprehensively investigate regression and dice losses, examining their\nrobustness under varying error levels and object sizes. We mathematically prove\nthat the dice loss leads to superior noise-robustness and model convergence for\nlarge objects compared to regression losses for a simplified case. Leveraging\nour theoretical insights, we propose SeaBird (Segmentation in Bird's View) as\nthe first step towards generalizing to large objects. SeaBird effectively\nintegrates BEV segmentation on foreground objects for 3D detection, with the\nsegmentation head trained with the dice loss. SeaBird achieves SoTA results on\nthe KITTI-360 leaderboard and improves existing detectors on the nuScenes\nleaderboard, particularly for large objects. Code and models at\nhttps://github.com/abhi1kumar/SeaBird\n","authors":["Abhinav Kumar","Yuliang Guo","Xinyu Huang","Liu Ren","Xiaoming Liu"],"pdf_url":"https://arxiv.org/pdf/2403.20318v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.20317v1","updated":"2024-03-29T17:40:37Z","published":"2024-03-29T17:40:37Z","title":"Convolutional Prompting meets Language Models for Continual Learning","summary":"  Continual Learning (CL) enables machine learning models to learn from\ncontinuously shifting new training data in absence of data from old tasks.\nRecently, pretrained vision transformers combined with prompt tuning have shown\npromise for overcoming catastrophic forgetting in CL. These approaches rely on\na pool of learnable prompts which can be inefficient in sharing knowledge\nacross tasks leading to inferior performance. In addition, the lack of\nfine-grained layer specific prompts does not allow these to fully express the\nstrength of the prompts for CL. We address these limitations by proposing\nConvPrompt, a novel convolutional prompt creation mechanism that maintains\nlayer-wise shared embeddings, enabling both layer-specific learning and better\nconcept transfer across tasks. The intelligent use of convolution enables us to\nmaintain a low parameter overhead without compromising performance. We further\nleverage Large Language Models to generate fine-grained text descriptions of\neach category which are used to get task similarity and dynamically decide the\nnumber of prompts to be learned. Extensive experiments demonstrate the\nsuperiority of ConvPrompt and improves SOTA by ~3% with significantly less\nparameter overhead. We also perform strong ablation over various modules to\ndisentangle the importance of different components.\n","authors":["Anurag Roy","Riddhiman Moulick","Vinay K. Verma","Saptarshi Ghosh","Abir Das"],"pdf_url":"https://arxiv.org/pdf/2403.20317v1.pdf","comment":"CVPR 2024 Camera Ready"},{"id":"http://arxiv.org/abs/2307.08727v2","updated":"2024-03-29T17:38:00Z","published":"2023-07-17T17:48:06Z","title":"Learning to Count without Annotations","summary":"  While recent supervised methods for reference-based object counting continue\nto improve the performance on benchmark datasets, they have to rely on small\ndatasets due to the cost associated with manually annotating dozens of objects\nin images. We propose UnCounTR, a model that can learn this task without\nrequiring any manual annotations. To this end, we construct \"Self-Collages\",\nimages with various pasted objects as training samples, that provide a rich\nlearning signal covering arbitrary object types and counts. Our method builds\non existing unsupervised representations and segmentation techniques to\nsuccessfully demonstrate for the first time the ability of reference-based\ncounting without manual supervision. Our experiments show that our method not\nonly outperforms simple baselines and generic models such as FasterRCNN and\nDETR, but also matches the performance of supervised counting models in some\ndomains.\n","authors":["Lukas Knobel","Tengda Han","Yuki M. Asano"],"pdf_url":"https://arxiv.org/pdf/2307.08727v2.pdf","comment":"Accepted at CVPR'24. Code available at\n  https://github.com/lukasknobel/SelfCollages"},{"id":"http://arxiv.org/abs/2310.18274v2","updated":"2024-03-29T17:34:40Z","published":"2023-10-27T16:59:51Z","title":"LipSim: A Provably Robust Perceptual Similarity Metric","summary":"  Recent years have seen growing interest in developing and applying perceptual\nsimilarity metrics. Research has shown the superiority of perceptual metrics\nover pixel-wise metrics in aligning with human perception and serving as a\nproxy for the human visual system. On the other hand, as perceptual metrics\nrely on neural networks, there is a growing concern regarding their resilience,\ngiven the established vulnerability of neural networks to adversarial attacks.\nIt is indeed logical to infer that perceptual metrics may inherit both the\nstrengths and shortcomings of neural networks. In this work, we demonstrate the\nvulnerability of state-of-the-art perceptual similarity metrics based on an\nensemble of ViT-based feature extractors to adversarial attacks. We then\npropose a framework to train a robust perceptual similarity metric called\nLipSim (Lipschitz Similarity Metric) with provable guarantees. By leveraging\n1-Lipschitz neural networks as the backbone, LipSim provides guarded areas\naround each data point and certificates for all perturbations within an\n$\\ell_2$ ball. Finally, a comprehensive set of experiments shows the\nperformance of LipSim in terms of natural and certified scores and on the image\nretrieval application. The code is available at\nhttps://github.com/SaraGhazanfari/LipSim.\n","authors":["Sara Ghazanfari","Alexandre Araujo","Prashanth Krishnamurthy","Farshad Khorrami","Siddharth Garg"],"pdf_url":"https://arxiv.org/pdf/2310.18274v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20312v1","updated":"2024-03-29T17:33:42Z","published":"2024-03-29T17:33:42Z","title":"Learn \"No\" to Say \"Yes\" Better: Improving Vision-Language Models via\n  Negations","summary":"  Existing vision-language models (VLMs) treat text descriptions as a unit,\nconfusing individual concepts in a prompt and impairing visual semantic\nmatching and reasoning. An important aspect of reasoning in logic and language\nis negations. This paper highlights the limitations of popular VLMs such as\nCLIP, at understanding the implications of negations, i.e., the effect of the\nword \"not\" in a given prompt. To enable evaluation of VLMs on fluent prompts\nwith negations, we present CC-Neg, a dataset containing 228,246 images, true\ncaptions and their corresponding negated captions. Using CC-Neg along with\nmodifications to the contrastive loss of CLIP, our proposed CoN-CLIP framework,\nhas an improved understanding of negations. This training paradigm improves\nCoN-CLIP's ability to encode semantics reliably, resulting in 3.85% average\ngain in top-1 accuracy for zero-shot image classification across 8 datasets.\nFurther, CoN-CLIP outperforms CLIP on challenging compositionality benchmarks\nsuch as SugarCREPE by 4.4%, showcasing emergent compositional understanding of\nobjects, relations, and attributes in text. Overall, our work addresses a\ncrucial limitation of VLMs by introducing a dataset and framework that\nstrengthens semantic associations between images and text, demonstrating\nimproved large-scale foundation models with significantly reduced computational\ncost, promoting efficiency and accessibility.\n","authors":["Jaisidh Singh","Ishaan Shrivastava","Mayank Vatsa","Richa Singh","Aparna Bharati"],"pdf_url":"https://arxiv.org/pdf/2403.20312v1.pdf","comment":"14 pages + 6 figures in main manuscript (excluding references)"},{"id":"http://arxiv.org/abs/2312.01215v2","updated":"2024-03-29T17:30:58Z","published":"2023-12-02T19:49:27Z","title":"RNb-NeuS: Reflectance and Normal-based Multi-View 3D Reconstruction","summary":"  This paper introduces a versatile paradigm for integrating multi-view\nreflectance (optional) and normal maps acquired through photometric stereo. Our\napproach employs a pixel-wise joint re-parameterization of reflectance and\nnormal, considering them as a vector of radiances rendered under simulated,\nvarying illumination. This re-parameterization enables the seamless integration\nof reflectance and normal maps as input data in neural volume rendering-based\n3D reconstruction while preserving a single optimization objective. In\ncontrast, recent multi-view photometric stereo (MVPS) methods depend on\nmultiple, potentially conflicting objectives. Despite its apparent simplicity,\nour proposed approach outperforms state-of-the-art approaches in MVPS\nbenchmarks across F-score, Chamfer distance, and mean angular error metrics.\nNotably, it significantly improves the detailed 3D reconstruction of areas with\nhigh curvature or low visibility.\n","authors":["Baptiste Brument","Robin Bruneau","Yvain Quéau","Jean Mélou","François Bernard Lauze"," Jean-Denis","Jean-Denis Durou","Lilian Calvet"],"pdf_url":"https://arxiv.org/pdf/2312.01215v2.pdf","comment":"14 pages, 13 figures, 7 tables. Accepted to CVPR 2024. The project\n  page can be accessed via\n  https://robinbruneau.github.io/publications/rnb_neus.html. The source code is\n  available at https://github.com/bbrument/RNb-NeuS"},{"id":"http://arxiv.org/abs/2403.20309v1","updated":"2024-03-29T17:29:58Z","published":"2024-03-29T17:29:58Z","title":"InstantSplat: Unbounded Sparse-view Pose-free Gaussian Splatting in 40\n  Seconds","summary":"  While novel view synthesis (NVS) has made substantial progress in 3D computer\nvision, it typically requires an initial estimation of camera intrinsics and\nextrinsics from dense viewpoints. This pre-processing is usually conducted via\na Structure-from-Motion (SfM) pipeline, a procedure that can be slow and\nunreliable, particularly in sparse-view scenarios with insufficient matched\nfeatures for accurate reconstruction. In this work, we integrate the strengths\nof point-based representations (e.g., 3D Gaussian Splatting, 3D-GS) with\nend-to-end dense stereo models (DUSt3R) to tackle the complex yet unresolved\nissues in NVS under unconstrained settings, which encompasses pose-free and\nsparse view challenges. Our framework, InstantSplat, unifies dense stereo\npriors with 3D-GS to build 3D Gaussians of large-scale scenes from sparseview &\npose-free images in less than 1 minute. Specifically, InstantSplat comprises a\nCoarse Geometric Initialization (CGI) module that swiftly establishes a\npreliminary scene structure and camera parameters across all training views,\nutilizing globally-aligned 3D point maps derived from a pre-trained dense\nstereo pipeline. This is followed by the Fast 3D-Gaussian Optimization (F-3DGO)\nmodule, which jointly optimizes the 3D Gaussian attributes and the initialized\nposes with pose regularization. Experiments conducted on the large-scale\noutdoor Tanks & Temples datasets demonstrate that InstantSplat significantly\nimproves SSIM (by 32%) while concurrently reducing Absolute Trajectory Error\n(ATE) by 80%. These establish InstantSplat as a viable solution for scenarios\ninvolving posefree and sparse-view conditions. Project page:\ninstantsplat.github.io.\n","authors":["Zhiwen Fan","Wenyan Cong","Kairun Wen","Kevin Wang","Jian Zhang","Xinghao Ding","Danfei Xu","Boris Ivanovic","Marco Pavone","Georgios Pavlakos","Zhangyang Wang","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2403.20309v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.01362v4","updated":"2024-03-29T17:11:38Z","published":"2023-07-03T21:33:40Z","title":"A Strong Baseline for Point Cloud Registration via Direct Superpoints\n  Matching","summary":"  Deep neural networks endow the downsampled superpoints with highly\ndiscriminative feature representations. Previous dominant point cloud\nregistration approaches match these feature representations as the first step,\ne.g., using the Sinkhorn algorithm. A RANSAC-like method is then usually\nadopted as a post-processing refinement to filter the outliers. Other dominant\nmethod is to directly predict the superpoint matchings using learned MLP\nlayers. Both of them have drawbacks: RANSAC-based methods are computationally\nintensive and prediction-based methods suffer from outputing non-existing\npoints in the point cloud. In this paper, we propose a straightforward and\neffective baseline to find correspondences of superpoints in a global matching\nmanner. We employ the normalized matching scores as weights for each\ncorrespondence, allowing us to reject the outliers and further weigh the rest\ninliers when fitting the transformation matrix without relying on the\ncumbersome RANSAC. Moreover, the entire model can be trained in an end-to-end\nfashion, leading to better accuracy. Our simple yet effective baseline shows\ncomparable or even better results than state-of-the-art methods on three\ndatasets including ModelNet, 3DMatch, and KITTI. We do not advocate our\napproach to be \\emph{the} solution for point cloud registration but use the\nresults to emphasize the role of matching strategy for point cloud\nregistration. The code and models are available at\nhttps://github.com/neu-vi/Superpoints_Registration.\n","authors":["Aniket Gupta","Yiming Xie","Hanumant Singh","Huaizu Jiang"],"pdf_url":"https://arxiv.org/pdf/2307.01362v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20287v1","updated":"2024-03-29T16:58:13Z","published":"2024-03-29T16:58:13Z","title":"Benchmarking Counterfactual Image Generation","summary":"  Counterfactual image generation is pivotal for understanding the causal\nrelations of variables, with applications in interpretability and generation of\nunbiased synthetic data. However, evaluating image generation is a\nlong-standing challenge in itself. The need to evaluate counterfactual\ngeneration compounds on this challenge, precisely because counterfactuals, by\ndefinition, are hypothetical scenarios without observable ground truths. In\nthis paper, we present a novel comprehensive framework aimed at benchmarking\ncounterfactual image generation methods. We incorporate metrics that focus on\nevaluating diverse aspects of counterfactuals, such as composition,\neffectiveness, minimality of interventions, and image realism. We assess the\nperformance of three distinct conditional image generation model types, based\non the Structural Causal Model paradigm. Our work is accompanied by a\nuser-friendly Python package which allows to further evaluate and benchmark\nexisting and future counterfactual image generation methods. Our framework is\nextendable to additional SCM and other causal methods, generative models, and\ndatasets.\n","authors":["Thomas Melistas","Nikos Spyrou","Nefeli Gkouti","Pedro Sanchez","Athanasios Vlontzos","Giorgos Papanastasiou","Sotirios A. Tsaftaris"],"pdf_url":"https://arxiv.org/pdf/2403.20287v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02560v2","updated":"2024-03-29T16:56:33Z","published":"2023-04-05T16:30:36Z","title":"VicTR: Video-conditioned Text Representations for Activity Recognition","summary":"  Vision-Language models (VLMs) have excelled in the image-domain -- especially\nin zero-shot settings -- thanks to the availability of vast pretraining data\n(i.e., paired image-text samples). However for videos, such paired data is not\nas abundant. Therefore, video-VLMs are usually designed by adapting pretrained\nimage-VLMs to the video-domain, instead of training from scratch. All such\nrecipes rely on augmenting visual embeddings with temporal information (i.e.,\nimage $\\rightarrow$ video), often keeping text embeddings unchanged or even\nbeing discarded. In this paper, we argue the contrary, that better video-VLMs\ncan be designed by focusing more on augmenting text, rather than visual\ninformation. More specifically, we introduce Video-conditioned Text\nRepresentations (VicTR): a form of text embeddings optimized w.r.t. visual\nembeddings, creating a more-flexible contrastive latent space. Our model can\nfurther make use of freely-available semantic information, in the form of\nvisually-grounded auxiliary text (e.g. object or scene information). We\nevaluate our model on few-shot, zero-shot (HMDB-51, UCF-101), short-form\n(Kinetics-400) and long-form (Charades) activity recognition benchmarks,\nshowing strong performance among video-VLMs.\n","authors":["Kumara Kahatapitiya","Anurag Arnab","Arsha Nagrani","Michael S. Ryoo"],"pdf_url":"https://arxiv.org/pdf/2304.02560v2.pdf","comment":"To appear at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.15905v4","updated":"2024-03-29T16:53:58Z","published":"2024-03-23T18:19:02Z","title":"Towards Low-Energy Adaptive Personalization for Resource-Constrained\n  Devices","summary":"  The personalization of machine learning (ML) models to address data drift is\na significant challenge in the context of Internet of Things (IoT)\napplications. Presently, most approaches focus on fine-tuning either the full\nbase model or its last few layers to adapt to new data, while often neglecting\nenergy costs. However, various types of data drift exist, and fine-tuning the\nfull base model or the last few layers may not result in optimal performance in\ncertain scenarios. We propose Target Block Fine-Tuning (TBFT), a low-energy\nadaptive personalization framework designed for resource-constrained devices.\nWe categorize data drift and personalization into three types: input-level,\nfeature-level, and output-level. For each type, we fine-tune different blocks\nof the model to achieve optimal performance with reduced energy costs.\nSpecifically, input-, feature-, and output-level correspond to fine-tuning the\nfront, middle, and rear blocks of the model. We evaluate TBFT on a ResNet\nmodel, three datasets, three different training sizes, and a Raspberry Pi.\nCompared with the $Block Avg$, where each block is fine-tuned individually and\ntheir performance improvements are averaged, TBFT exhibits an improvement in\nmodel accuracy by an average of 15.30% whilst saving 41.57% energy consumption\non average compared with full fine-tuning.\n","authors":["Yushan Huang","Josh Millar","Yuxuan Long","Yuchen Zhao","Hamed Haddadi"],"pdf_url":"https://arxiv.org/pdf/2403.15905v4.pdf","comment":"Accepetd to The 4th Workshop on Machine Learning and Systems\n  (EuroMLSys '24)"},{"id":"http://arxiv.org/abs/2312.05291v2","updated":"2024-03-29T16:49:59Z","published":"2023-12-08T18:14:21Z","title":"GlitchBench: Can large multimodal models detect video game glitches?","summary":"  Large multimodal models (LMMs) have evolved from large language models (LLMs)\nto integrate multiple input modalities, such as visual inputs. This integration\naugments the capacity of LLMs for tasks requiring visual comprehension and\nreasoning. However, the extent and limitations of their enhanced abilities are\nnot fully understood, especially when it comes to real-world tasks. To address\nthis gap, we introduce GlitchBench, a novel benchmark derived from video game\nquality assurance tasks, to test and evaluate the reasoning capabilities of\nLMMs. Our benchmark is curated from a variety of unusual and glitched scenarios\nfrom video games and aims to challenge both the visual and linguistic reasoning\npowers of LMMs in detecting and interpreting out-of-the-ordinary events. We\nevaluate multiple state-of-the-art LMMs, and we show that GlitchBench presents\na new challenge for these models. Code and data are available at:\nhttps://glitchbench.github.io/\n","authors":["Mohammad Reza Taesiri","Tianjun Feng","Anh Nguyen","Cor-Paul Bezemer"],"pdf_url":"https://arxiv.org/pdf/2312.05291v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2312.04670v2","updated":"2024-03-29T16:39:28Z","published":"2023-12-07T20:11:03Z","title":"Rapid Motor Adaptation for Robotic Manipulator Arms","summary":"  Developing generalizable manipulation skills is a core challenge in embodied\nAI. This includes generalization across diverse task configurations,\nencompassing variations in object shape, density, friction coefficient, and\nexternal disturbances such as forces applied to the robot. Rapid Motor\nAdaptation (RMA) offers a promising solution to this challenge. It posits that\nessential hidden variables influencing an agent's task performance, such as\nobject mass and shape, can be effectively inferred from the agent's action and\nproprioceptive history. Drawing inspiration from RMA in locomotion and in-hand\nrotation, we use depth perception to develop agents tailored for rapid motor\nadaptation in a variety of manipulation tasks. We evaluated our agents on four\nchallenging tasks from the Maniskill2 benchmark, namely pick-and-place\noperations with hundreds of objects from the YCB and EGAD datasets, peg\ninsertion with precise position and orientation, and operating a variety of\nfaucets and handles, with customized environment variations. Empirical results\ndemonstrate that our agents surpass state-of-the-art methods like automatic\ndomain randomization and vision-based policies, obtaining better generalization\nperformance and sample efficiency.\n","authors":["Yichao Liang","Kevin Ellis","João Henriques"],"pdf_url":"https://arxiv.org/pdf/2312.04670v2.pdf","comment":"Accepted at CVPR 2024. 12 pages"},{"id":"http://arxiv.org/abs/2312.02214v2","updated":"2024-03-29T16:31:44Z","published":"2023-12-03T07:23:53Z","title":"FlashAvatar: High-fidelity Head Avatar with Efficient Gaussian Embedding","summary":"  We propose FlashAvatar, a novel and lightweight 3D animatable avatar\nrepresentation that could reconstruct a digital avatar from a short monocular\nvideo sequence in minutes and render high-fidelity photo-realistic images at\n300FPS on a consumer-grade GPU. To achieve this, we maintain a uniform 3D\nGaussian field embedded in the surface of a parametric face model and learn\nextra spatial offset to model non-surface regions and subtle facial details.\nWhile full use of geometric priors can capture high-frequency facial details\nand preserve exaggerated expressions, proper initialization can help reduce the\nnumber of Gaussians, thus enabling super-fast rendering speed. Extensive\nexperimental results demonstrate that FlashAvatar outperforms existing works\nregarding visual quality and personalized details and is almost an order of\nmagnitude faster in rendering speed. Project page:\nhttps://ustc3dv.github.io/FlashAvatar/\n","authors":["Jun Xiang","Xuan Gao","Yudong Guo","Juyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.02214v2.pdf","comment":"Project page: https://ustc3dv.github.io/FlashAvatar/"},{"id":"http://arxiv.org/abs/2403.20275v1","updated":"2024-03-29T16:30:17Z","published":"2024-03-29T16:30:17Z","title":"Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for\n  Reconstructing Challenging Surfaces","summary":"  Touch and vision go hand in hand, mutually enhancing our ability to\nunderstand the world. From a research perspective, the problem of mixing touch\nand vision is underexplored and presents interesting challenges. To this end,\nwe propose Tactile-Informed 3DGS, a novel approach that incorporates touch data\n(local depth maps) with multi-view vision data to achieve surface\nreconstruction and novel view synthesis. Our method optimises 3D Gaussian\nprimitives to accurately model the object's geometry at points of contact. By\ncreating a framework that decreases the transmittance at touch locations, we\nachieve a refined surface reconstruction, ensuring a uniformly smooth depth\nmap. Touch is particularly useful when considering non-Lambertian objects (e.g.\nshiny or reflective surfaces) since contemporary methods tend to fail to\nreconstruct with fidelity specular highlights. By combining vision and tactile\nsensing, we achieve more accurate geometry reconstructions with fewer images\nthan prior methods. We conduct evaluation on objects with glossy and reflective\nsurfaces and demonstrate the effectiveness of our approach, offering\nsignificant improvements in reconstruction quality.\n","authors":["Mauro Comi","Alessio Tonioni","Max Yang","Jonathan Tremblay","Valts Blukis","Yijiong Lin","Nathan F. Lepora","Laurence Aitchison"],"pdf_url":"https://arxiv.org/pdf/2403.20275v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2403.20273v1","updated":"2024-03-29T16:27:40Z","published":"2024-03-29T16:27:40Z","title":"CATSNet: a context-aware network for Height Estimation in a Forested\n  Area based on Pol-TomoSAR data","summary":"  Tropical forests are a key component of the global carbon cycle. With plans\nfor upcoming space-borne missions like BIOMASS to monitor forestry, several\nairborne missions, including TropiSAR and AfriSAR campaigns, have been\nsuccessfully launched and experimented. Typical Synthetic Aperture Radar\nTomography (TomoSAR) methods involve complex models with low accuracy and high\ncomputation costs. In recent years, deep learning methods have also gained\nattention in the TomoSAR framework, showing interesting performance. Recently,\na solution based on a fully connected Tomographic Neural Network (TSNN) has\ndemonstrated its effectiveness in accurately estimating forest and ground\nheights by exploiting the pixel-wise elements of the covariance matrix derived\nfrom TomoSAR data. This work instead goes beyond the pixel-wise approach to\ndefine a context-aware deep learning-based solution named CATSNet. A\nconvolutional neural network is considered to leverage patch-based information\nand extract features from a neighborhood rather than focus on a single pixel.\nThe training is conducted by considering TomoSAR data as the input and Light\nDetection and Ranging (LiDAR) values as the ground truth. The experimental\nresults show striking advantages in both performance and generalization ability\nby leveraging context information within Multiple Baselines (MB) TomoSAR data\nacross different polarimetric modalities, surpassing existing techniques.\n","authors":["Wenyu Yang","Sergio Vitale","Hossein Aghababaei","Giampaolo Ferraioli","Vito Pascazio","Gilda Schirinzi"],"pdf_url":"https://arxiv.org/pdf/2403.20273v1.pdf","comment":"Submitted to IEEE TGRS, under review"},{"id":"http://arxiv.org/abs/2403.20271v1","updated":"2024-03-29T16:26:20Z","published":"2024-03-29T16:26:20Z","title":"Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to\n  Comprehend What You Want","summary":"  The interaction between humans and artificial intelligence (AI) is a crucial\nfactor that reflects the effectiveness of multimodal large language models\n(MLLMs). However, current MLLMs primarily focus on image-level comprehension\nand limit interaction to textual instructions, thereby constraining their\nflexibility in usage and depth of response. In this paper, we introduce the\nDraw-and-Understand project: a new model, a multi-domain dataset, and a\nchallenging benchmark for visual prompting. Specifically, we propose SPHINX-V,\na new end-to-end trained Multimodal Large Language Model (MLLM) that connects a\nvision encoder, a visual prompt encoder and an LLM for various visual prompts\n(points, bounding boxes, and free-form shape) and language understanding. To\nadvance visual prompting research for MLLMs, we introduce MDVP-Data and\nMDVP-Bench. MDVP-Data features a multi-domain dataset containing 1.6M unique\nimage-visual prompt-text instruction-following samples, including natural\nimages, document images, OCR images, mobile screenshots, web screenshots, and\nmulti-panel images. Furthermore, we present MDVP-Bench, a comprehensive and\nchallenging benchmark to assess a model's capability in understanding visual\nprompting instructions. Our experiments demonstrate SPHINX-V's impressive\nmultimodal interaction capabilities through visual prompting, revealing\nsignificant improvements in detailed pixel-level description and\nquestion-answering abilities.\n","authors":["Weifeng Lin","Xinyu Wei","Ruichuan An","Peng Gao","Bocheng Zou","Yulin Luo","Siyuan Huang","Shanghang Zhang","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2403.20271v1.pdf","comment":"16 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.16970v2","updated":"2024-03-29T16:14:41Z","published":"2024-03-25T17:31:12Z","title":"Joint chest X-ray diagnosis and clinical visual attention prediction\n  with multi-stage cooperative learning: enhancing interpretability","summary":"  As deep learning has become the state-of-the-art for computer-assisted\ndiagnosis, interpretability of the automatic decisions is crucial for clinical\ndeployment. While various methods were proposed in this domain, visual\nattention maps of clinicians during radiological screening offer a unique asset\nto provide important insights and can potentially enhance the quality of\ncomputer-assisted diagnosis. With this paper, we introduce a novel\ndeep-learning framework for joint disease diagnosis and prediction of\ncorresponding visual saliency maps for chest X-ray scans. Specifically, we\ndesigned a novel dual-encoder multi-task UNet, which leverages both a\nDenseNet201 backbone and a Residual and Squeeze-and-Excitation block-based\nencoder to extract diverse features for saliency map prediction, and a\nmulti-scale feature-fusion classifier to perform disease classification. To\ntackle the issue of asynchronous training schedules of individual tasks in\nmulti-task learning, we proposed a multi-stage cooperative learning strategy,\nwith contrastive learning for feature encoder pretraining to boost performance.\nExperiments show that our proposed method outperformed existing techniques for\nchest X-ray diagnosis and the quality of visual saliency map prediction.\n","authors":["Zirui Qiu","Hassan Rivaz","Yiming Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.16970v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01482v3","updated":"2024-03-29T16:13:13Z","published":"2024-03-03T11:24:16Z","title":"EAGLE: Eigen Aggregation Learning for Object-Centric Unsupervised\n  Semantic Segmentation","summary":"  Semantic segmentation has innately relied on extensive pixel-level annotated\ndata, leading to the emergence of unsupervised methodologies. Among them,\nleveraging self-supervised Vision Transformers for unsupervised semantic\nsegmentation (USS) has been making steady progress with expressive deep\nfeatures. Yet, for semantically segmenting images with complex objects, a\npredominant challenge remains: the lack of explicit object-level semantic\nencoding in patch-level features. This technical limitation often leads to\ninadequate segmentation of complex objects with diverse structures. To address\nthis gap, we present a novel approach, EAGLE, which emphasizes object-centric\nrepresentation learning for unsupervised semantic segmentation. Specifically,\nwe introduce EiCue, a spectral technique providing semantic and structural cues\nthrough an eigenbasis derived from the semantic similarity matrix of deep image\nfeatures and color affinity from an image. Further, by incorporating our\nobject-centric contrastive loss with EiCue, we guide our model to learn\nobject-level representations with intra- and inter-image object-feature\nconsistency, thereby enhancing semantic accuracy. Extensive experiments on\nCOCO-Stuff, Cityscapes, and Potsdam-3 datasets demonstrate the state-of-the-art\nUSS results of EAGLE with accurate and consistent semantic segmentation across\ncomplex scenes.\n","authors":["Chanyoung Kim","Woojung Han","Dayun Ju","Seong Jae Hwang"],"pdf_url":"https://arxiv.org/pdf/2403.01482v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20260v1","updated":"2024-03-29T16:08:59Z","published":"2024-03-29T16:08:59Z","title":"Prototype-based Interpretable Breast Cancer Prediction Models: Analysis\n  and Challenges","summary":"  Deep learning models have achieved high performance in medical applications,\nhowever, their adoption in clinical practice is hindered due to their black-box\nnature. Self-explainable models, like prototype-based models, can be especially\nbeneficial as they are interpretable by design. However, if the learnt\nprototypes are of low quality then the prototype-based models are as good as\nblack-box. Having high quality prototypes is a pre-requisite for a truly\ninterpretable model. In this work, we propose a prototype evaluation framework\nfor coherence (PEF-C) for quantitatively evaluating the quality of the\nprototypes based on domain knowledge. We show the use of PEF-C in the context\nof breast cancer prediction using mammography. Existing works on\nprototype-based models on breast cancer prediction using mammography have\nfocused on improving the classification performance of prototype-based models\ncompared to black-box models and have evaluated prototype quality through\nanecdotal evidence. We are the first to go beyond anecdotal evidence and\nevaluate the quality of the mammography prototypes systematically using our\nPEF-C. Specifically, we apply three state-of-the-art prototype-based models,\nProtoPNet, BRAIxProtoPNet++ and PIP-Net on mammography images for breast cancer\nprediction and evaluate these models w.r.t. i) classification performance, and\nii) quality of the prototypes, on three public datasets. Our results show that\nprototype-based models are competitive with black-box models in terms of\nclassification performance, and achieve a higher score in detecting ROIs.\nHowever, the quality of the prototypes are not yet sufficient and can be\nimproved in aspects of relevance, purity and learning a variety of prototypes.\nWe call the XAI community to systematically evaluate the quality of the\nprototypes to check their true usability in high stake decisions and improve\nsuch models further.\n","authors":["Shreyasi Pathak","Jörg Schlötterer","Jeroen Veltman","Jeroen Geerdink","Maurice van Keulen","Christin Seifert"],"pdf_url":"https://arxiv.org/pdf/2403.20260v1.pdf","comment":"21 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.20254v1","updated":"2024-03-29T16:01:00Z","published":"2024-03-29T16:01:00Z","title":"Benchmarking the Robustness of Temporal Action Detection Models Against\n  Temporal Corruptions","summary":"  Temporal action detection (TAD) aims to locate action positions and recognize\naction categories in long-term untrimmed videos. Although many methods have\nachieved promising results, their robustness has not been thoroughly studied.\nIn practice, we observe that temporal information in videos can be occasionally\ncorrupted, such as missing or blurred frames. Interestingly, existing methods\noften incur a significant performance drop even if only one frame is affected.\nTo formally evaluate the robustness, we establish two temporal corruption\nrobustness benchmarks, namely THUMOS14-C and ActivityNet-v1.3-C. In this paper,\nwe extensively analyze the robustness of seven leading TAD methods and obtain\nsome interesting findings: 1) Existing methods are particularly vulnerable to\ntemporal corruptions, and end-to-end methods are often more susceptible than\nthose with a pre-trained feature extractor; 2) Vulnerability mainly comes from\nlocalization error rather than classification error; 3) When corruptions occur\nin the middle of an action instance, TAD models tend to yield the largest\nperformance drop. Besides building a benchmark, we further develop a simple but\neffective robust training method to defend against temporal corruptions,\nthrough the FrameDrop augmentation and Temporal-Robust Consistency loss.\nRemarkably, our approach not only improves robustness but also yields promising\nimprovements on clean data. We believe that this study will serve as a\nbenchmark for future research in robust video analysis. Source code and models\nare available at https://github.com/Alvin-Zeng/temporal-robustness-benchmark.\n","authors":["Runhao Zeng","Xiaoyong Chen","Jiaming Liang","Huisi Wu","Guangzhong Cao","Yong Guo"],"pdf_url":"https://arxiv.org/pdf/2403.20254v1.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.20253v1","updated":"2024-03-29T15:59:11Z","published":"2024-03-29T15:59:11Z","title":"MedCLIP-SAM: Bridging Text and Image Towards Universal Medical Image\n  Segmentation","summary":"  Medical image segmentation of anatomical structures and pathology is crucial\nin modern clinical diagnosis, disease study, and treatment planning. To date,\ngreat progress has been made in deep learning-based segmentation techniques,\nbut most methods still lack data efficiency, generalizability, and\ninteractability. Consequently, the development of new, precise segmentation\nmethods that demand fewer labeled datasets is of utmost importance in medical\nimage analysis. Recently, the emergence of foundation models, such as CLIP and\nSegment-Anything-Model (SAM), with comprehensive cross-domain representation\nopened the door for interactive and universal image segmentation. However,\nexploration of these models for data-efficient medical image segmentation is\nstill limited, but is highly necessary. In this paper, we propose a novel\nframework, called MedCLIP-SAM that combines CLIP and SAM models to generate\nsegmentation of clinical scans using text prompts in both zero-shot and weakly\nsupervised settings. To achieve this, we employed a new Decoupled Hard Negative\nNoise Contrastive Estimation (DHN-NCE) loss to fine-tune the BiomedCLIP model\nand the recent gScoreCAM to generate prompts to obtain segmentation masks from\nSAM in a zero-shot setting. Additionally, we explored the use of zero-shot\nsegmentation labels in a weakly supervised paradigm to improve the segmentation\nquality further. By extensively testing three diverse segmentation tasks and\nmedical image modalities (breast tumor ultrasound, brain tumor MRI, and lung\nX-ray), our proposed framework has demonstrated excellent accuracy.\n","authors":["Taha Koleilat","Hojat Asgariandehkordi","Hassan Rivaz","Yiming Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.20253v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.20251v1","updated":"2024-03-29T15:57:38Z","published":"2024-03-29T15:57:38Z","title":"Latent Embedding Clustering for Occlusion Robust Head Pose Estimation","summary":"  Head pose estimation has become a crucial area of research in computer vision\ngiven its usefulness in a wide range of applications, including robotics,\nsurveillance, or driver attention monitoring. One of the most difficult\nchallenges in this field is managing head occlusions that frequently take place\nin real-world scenarios. In this paper, we propose a novel and efficient\nframework that is robust in real world head occlusion scenarios. In particular,\nwe propose an unsupervised latent embedding clustering with regression and\nclassification components for each pose angle. The model optimizes latent\nfeature representations for occluded and non-occluded images through a\nclustering term while improving fine-grained angle predictions. Experimental\nevaluation on in-the-wild head pose benchmark datasets reveal competitive\nperformance in comparison to state-of-the-art methodologies with the advantage\nof having a significant data reduction. We observe a substantial improvement in\noccluded head pose estimation. Also, an ablation study is conducted to\nascertain the impact of the clustering term within our proposed framework.\n","authors":["José Celestino","Manuel Marques","Jacinto C. Nascimento"],"pdf_url":"https://arxiv.org/pdf/2403.20251v1.pdf","comment":"Accepted at 18th IEEE International Conference on Automatic Face and\n  Gesture Recognition (FG'24)"},{"id":"http://arxiv.org/abs/2311.13612v2","updated":"2024-03-29T15:55:48Z","published":"2023-11-21T23:30:01Z","title":"Descriptor and Word Soups: Overcoming the Parameter Efficiency Accuracy\n  Tradeoff for Out-of-Distribution Few-shot Learning","summary":"  Over the past year, a large body of multimodal research has emerged around\nzero-shot evaluation using GPT descriptors. These studies boost the zero-shot\naccuracy of pretrained VL models with an ensemble of label-specific text\ngenerated by GPT. A recent study, WaffleCLIP, demonstrated that similar\nzero-shot accuracy can be achieved with an ensemble of random descriptors.\nHowever, both zero-shot methods are un-trainable and consequently sub-optimal\nwhen some few-shot out-of-distribution (OOD) training data is available.\nInspired by these prior works, we present two more flexible methods called\ndescriptor and word soups, which do not require an LLM at test time and can\nleverage training data to increase OOD target accuracy. Descriptor soup\ngreedily selects a small set of textual descriptors using generic few-shot\ntraining data, then calculates robust class embeddings using the selected\ndescriptors. Word soup greedily assembles a chain of words in a similar manner.\nCompared to existing few-shot soft prompt tuning methods, word soup requires\nfewer parameters by construction and less GPU memory, since it does not require\nbackpropagation. Both soups outperform current published few-shot methods, even\nwhen combined with SoTA zero-shot methods, on cross-dataset and domain\ngeneralization benchmarks. Compared with SoTA prompt and descriptor ensembling\nmethods, such as ProDA and WaffleCLIP, word soup achieves higher OOD accuracy\nwith fewer ensemble members. Please checkout our code:\ngithub.com/Chris210634/word_soups\n","authors":["Christopher Liao","Theodoros Tsiligkaridis","Brian Kulis"],"pdf_url":"https://arxiv.org/pdf/2311.13612v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20249v1","updated":"2024-03-29T15:54:36Z","published":"2024-03-29T15:54:36Z","title":"Relation Rectification in Diffusion Model","summary":"  Despite their exceptional generative abilities, large text-to-image diffusion\nmodels, much like skilled but careless artists, often struggle with accurately\ndepicting visual relationships between objects. This issue, as we uncover\nthrough careful analysis, arises from a misaligned text encoder that struggles\nto interpret specific relationships and differentiate the logical order of\nassociated objects. To resolve this, we introduce a novel task termed Relation\nRectification, aiming to refine the model to accurately represent a given\nrelationship it initially fails to generate. To address this, we propose an\ninnovative solution utilizing a Heterogeneous Graph Convolutional Network\n(HGCN). It models the directional relationships between relation terms and\ncorresponding objects within the input prompts. Specifically, we optimize the\nHGCN on a pair of prompts with identical relational words but reversed object\norders, supplemented by a few reference images. The lightweight HGCN adjusts\nthe text embeddings generated by the text encoder, ensuring the accurate\nreflection of the textual relation in the embedding space. Crucially, our\nmethod retains the parameters of the text encoder and diffusion model,\npreserving the model's robust performance on unrelated descriptions. We\nvalidated our approach on a newly curated dataset of diverse relational data,\ndemonstrating both quantitative and qualitative enhancements in generating\nimages with precise visual relations. Project page:\nhttps://wuyinwei-hah.github.io/rrnet.github.io/.\n","authors":["Yinwei Wu","Xingyi Yang","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2403.20249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05269v2","updated":"2024-03-29T15:44:05Z","published":"2023-12-07T19:19:25Z","title":"LifelongMemory: Leveraging LLMs for Answering Queries in Long-form\n  Egocentric Videos","summary":"  In this paper we introduce LifelongMemory, a new framework for accessing\nlong-form egocentric videographic memory through natural language question\nanswering and retrieval. LifelongMemory generates concise video activity\ndescriptions of the camera wearer and leverages the zero-shot capabilities of\npretrained large language models to perform reasoning over long-form video\ncontext. Furthermore, Lifelong Memory uses a confidence and explanation module\nto produce confident, high-quality, and interpretable answers. Our approach\nachieves state-of-the-art performance on the EgoSchema benchmark for question\nanswering and is highly competitive on the natural language query (NLQ)\nchallenge of Ego4D. Code is available at\nhttps://github.com/Agentic-Learning-AI-Lab/lifelong-memory.\n","authors":["Ying Wang","Yanlai Yang","Mengye Ren"],"pdf_url":"https://arxiv.org/pdf/2312.05269v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.04119v2","updated":"2024-03-29T15:27:47Z","published":"2022-12-08T07:29:07Z","title":"DialogCC: An Automated Pipeline for Creating High-Quality Multi-Modal\n  Dialogue Dataset","summary":"  As sharing images in an instant message is a crucial factor, there has been\nactive research on learning an image-text multi-modal dialogue models. However,\ntraining a well-generalized multi-modal dialogue model remains challenging due\nto the low quality and limited diversity of images per dialogue in existing\nmulti-modal dialogue datasets. In this paper, we propose an automated pipeline\nto construct a multi-modal dialogue dataset, ensuring both dialogue quality and\nimage diversity without requiring minimum human effort. In our pipeline, to\nguarantee the coherence between images and dialogue, we prompt GPT-4 to infer\npotential image-sharing moments - specifically, the utterance, speaker,\nrationale, and image description. Furthermore, we leverage CLIP similarity to\nmaintain consistency between aligned multiple images to the utterance. Through\nthis pipeline, we introduce DialogCC, a high-quality and diverse multi-modal\ndialogue dataset that surpasses existing datasets in terms of quality and\ndiversity in human evaluation. Our comprehensive experiments highlight that\nwhen multi-modal dialogue models are trained using our dataset, their\ngeneralization performance on unseen dialogue datasets is significantly\nenhanced. We make our source code and dataset publicly available.\n","authors":["Young-Jun Lee","Byungsoo Ko","Han-Gyu Kim","Jonghwan Hyeon","Ho-Jin Choi"],"pdf_url":"https://arxiv.org/pdf/2212.04119v2.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.20236v1","updated":"2024-03-29T15:26:44Z","published":"2024-03-29T15:26:44Z","title":"Long-Tailed Anomaly Detection with Learnable Class Names","summary":"  Anomaly detection (AD) aims to identify defective images and localize their\ndefects (if any). Ideally, AD models should be able to detect defects over many\nimage classes; without relying on hard-coded class names that can be\nuninformative or inconsistent across datasets; learn without anomaly\nsupervision; and be robust to the long-tailed distributions of real-world\napplications. To address these challenges, we formulate the problem of\nlong-tailed AD by introducing several datasets with different levels of class\nimbalance and metrics for performance evaluation. We then propose a novel\nmethod, LTAD, to detect defects from multiple and long-tailed classes, without\nrelying on dataset class names. LTAD combines AD by reconstruction and semantic\nAD modules. AD by reconstruction is implemented with a transformer-based\nreconstruction module. Semantic AD is implemented with a binary classifier,\nwhich relies on learned pseudo class names and a pretrained foundation model.\nThese modules are learned over two phases. Phase 1 learns the pseudo-class\nnames and a variational autoencoder (VAE) for feature synthesis that augments\nthe training data to combat long-tails. Phase 2 then learns the parameters of\nthe reconstruction and classification modules of LTAD. Extensive experiments\nusing the proposed long-tailed datasets show that LTAD substantially\noutperforms the state-of-the-art methods for most forms of dataset imbalance.\nThe long-tailed dataset split is available at\nhttps://zenodo.org/records/10854201 .\n","authors":["Chih-Hui Ho","Kuan-Chuan Peng","Nuno Vasconcelos"],"pdf_url":"https://arxiv.org/pdf/2403.20236v1.pdf","comment":"This paper is accepted to CVPR 2024. The supplementary material is\n  included. The long-tailed dataset split is available at\n  https://zenodo.org/records/10854201"},{"id":"http://arxiv.org/abs/2403.20231v1","updated":"2024-03-29T15:20:34Z","published":"2024-03-29T15:20:34Z","title":"U-VAP: User-specified Visual Appearance Personalization via Decoupled\n  Self Augmentation","summary":"  Concept personalization methods enable large text-to-image models to learn\nspecific subjects (e.g., objects/poses/3D models) and synthesize renditions in\nnew contexts. Given that the image references are highly biased towards visual\nattributes, state-of-the-art personalization models tend to overfit the whole\nsubject and cannot disentangle visual characteristics in pixel space. In this\nstudy, we proposed a more challenging setting, namely fine-grained visual\nappearance personalization. Different from existing methods, we allow users to\nprovide a sentence describing the desired attributes. A novel decoupled\nself-augmentation strategy is proposed to generate target-related and\nnon-target samples to learn user-specified visual attributes. These augmented\ndata allow for refining the model's understanding of the target attribute while\nmitigating the impact of unrelated attributes. At the inference stage,\nadjustments are conducted on semantic space through the learned target and\nnon-target embeddings to further enhance the disentanglement of target\nattributes. Extensive experiments on various kinds of visual attributes with\nSOTA personalization methods show the ability of the proposed method to mimic\ntarget visual appearance in novel contexts, thus improving the controllability\nand flexibility of personalization.\n","authors":["You Wu","Kean Liu","Xiaoyue Mi","Fan Tang","Juan Cao","Jintao Li"],"pdf_url":"https://arxiv.org/pdf/2403.20231v1.pdf","comment":"14 pages, 13 figures, 2 tables"},{"id":"http://arxiv.org/abs/2303.06346v2","updated":"2024-03-29T15:10:29Z","published":"2023-03-11T08:42:54Z","title":"3DInAction: Understanding Human Actions in 3D Point Clouds","summary":"  We propose a novel method for 3D point cloud action recognition.\nUnderstanding human actions in RGB videos has been widely studied in recent\nyears, however, its 3D point cloud counterpart remains under-explored. This is\nmostly due to the inherent limitation of the point cloud data modality -- lack\nof structure, permutation invariance, and varying number of points -- which\nmakes it difficult to learn a spatio-temporal representation. To address this\nlimitation, we propose the 3DinAction pipeline that first estimates patches\nmoving in time (t-patches) as a key building block, alongside a hierarchical\narchitecture that learns an informative spatio-temporal representation. We show\nthat our method achieves improved performance on existing datasets, including\nDFAUST and IKEA ASM. Code is publicly available at\nhttps://github.com/sitzikbs/3dincaction.\n","authors":["Yizhak Ben-Shabat","Oren Shrout","Stephen Gould"],"pdf_url":"https://arxiv.org/pdf/2303.06346v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20225v1","updated":"2024-03-29T15:08:37Z","published":"2024-03-29T15:08:37Z","title":"MTMMC: A Large-Scale Real-World Multi-Modal Camera Tracking Benchmark","summary":"  Multi-target multi-camera tracking is a crucial task that involves\nidentifying and tracking individuals over time using video streams from\nmultiple cameras. This task has practical applications in various fields, such\nas visual surveillance, crowd behavior analysis, and anomaly detection.\nHowever, due to the difficulty and cost of collecting and labeling data,\nexisting datasets for this task are either synthetically generated or\nartificially constructed within a controlled camera network setting, which\nlimits their ability to model real-world dynamics and generalize to diverse\ncamera configurations. To address this issue, we present MTMMC, a real-world,\nlarge-scale dataset that includes long video sequences captured by 16\nmulti-modal cameras in two different environments - campus and factory - across\nvarious time, weather, and season conditions. This dataset provides a\nchallenging test-bed for studying multi-camera tracking under diverse\nreal-world complexities and includes an additional input modality of spatially\naligned and temporally synchronized RGB and thermal cameras, which enhances the\naccuracy of multi-camera tracking. MTMMC is a super-set of existing datasets,\nbenefiting independent fields such as person detection, re-identification, and\nmultiple object tracking. We provide baselines and new learning setups on this\ndataset and set the reference scores for future studies. The datasets, models,\nand test server will be made publicly available.\n","authors":["Sanghyun Woo","Kwanyong Park","Inkyu Shin","Myungchul Kim","In So Kweon"],"pdf_url":"https://arxiv.org/pdf/2403.20225v1.pdf","comment":"Accepted on CVPR 2024"},{"id":"http://arxiv.org/abs/2312.02216v2","updated":"2024-03-29T14:59:13Z","published":"2023-12-03T10:41:06Z","title":"DragVideo: Interactive Drag-style Video Editing","summary":"  Video generation models have shown their superior ability to generate\nphoto-realistic video. However, how to accurately control (or edit) the video\nremains a formidable challenge. The main issues are: 1) how to perform direct\nand accurate user control in editing; 2) how to execute editings like changing\nshape, expression, and layout without unsightly distortion and artifacts to the\nedited content; and 3) how to maintain spatio-temporal consistency of video\nafter editing. To address the above issues, we propose DragVideo, a general\ndrag-style video editing framework. Inspired by DragGAN, DragVideo addresses\nissues 1) and 2) by proposing the drag-style video latent optimization method\nwhich gives desired control by updating noisy video latent according to drag\ninstructions through video-level drag objective function. We amend issue 3) by\nintegrating the video diffusion model with sample-specific LoRA and Mutual\nSelf-Attention in DragVideo to ensure the edited result is spatio-temporally\nconsistent. We also present a series of testing examples for drag-style video\nediting and conduct extensive experiments across a wide array of challenging\nediting tasks, such as motion, skeleton editing, etc, underscoring DragVideo\ncan edit video in an intuitive, faithful to the user's intention manner, with\nnearly unnoticeable distortion and artifacts, while maintaining spatio-temporal\nconsistency. While traditional prompt-based video editing fails to do the\nformer two and directly applying image drag editing fails in the last,\nDragVideo's versatility and generality are emphasized. Github link:\nhttps://github.com/RickySkywalker/DragVideo-Official.\n","authors":["Yufan Deng","Ruida Wang","Yuhao Zhang","Yu-Wing Tai","Chi-Keung Tang"],"pdf_url":"https://arxiv.org/pdf/2312.02216v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12440v2","updated":"2024-03-29T14:55:50Z","published":"2024-03-19T04:54:59Z","title":"Self-learning Canonical Space for Multi-view 3D Human Pose Estimation","summary":"  Multi-view 3D human pose estimation is naturally superior to single view one,\nbenefiting from more comprehensive information provided by images of multiple\nviews. The information includes camera poses, 2D/3D human poses, and 3D\ngeometry. However, the accurate annotation of these information is hard to\nobtain, making it challenging to predict accurate 3D human pose from multi-view\nimages. To deal with this issue, we propose a fully self-supervised framework,\nnamed cascaded multi-view aggregating network (CMANet), to construct a\ncanonical parameter space to holistically integrate and exploit multi-view\ninformation. In our framework, the multi-view information is grouped into two\ncategories: 1) intra-view information , 2) inter-view information. Accordingly,\nCMANet consists of two components: intra-view module (IRV) and inter-view\nmodule (IEV). IRV is used for extracting initial camera pose and 3D human pose\nof each view; IEV is to fuse complementary pose information and cross-view 3D\ngeometry for a final 3D human pose. To facilitate the aggregation of the intra-\nand inter-view, we define a canonical parameter space, depicted by per-view\ncamera pose and human pose and shape parameters ($\\theta$ and $\\beta$) of SMPL\nmodel, and propose a two-stage learning procedure. At first stage, IRV learns\nto estimate camera pose and view-dependent 3D human pose supervised by\nconfident output of an off-the-shelf 2D keypoint detector. At second stage, IRV\nis frozen and IEV further refines the camera pose and optimizes the 3D human\npose by implicitly encoding the cross-view complement and 3D geometry\nconstraint, achieved by jointly fitting predicted multi-view 2D keypoints. The\nproposed framework, modules, and learning strategy are demonstrated to be\neffective by comprehensive experiments and CMANet is superior to\nstate-of-the-art methods in extensive quantitative and qualitative analysis.\n","authors":["Xiaoben Li","Mancheng Meng","Ziyan Wu","Terrence Chen","Fan Yang","Dinggang Shen"],"pdf_url":"https://arxiv.org/pdf/2403.12440v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20213v1","updated":"2024-03-29T14:50:43Z","published":"2024-03-29T14:50:43Z","title":"H2RSVLM: Towards Helpful and Honest Remote Sensing Large Vision Language\n  Model","summary":"  The generic large Vision-Language Models (VLMs) is rapidly developing, but\nstill perform poorly in Remote Sensing (RS) domain, which is due to the unique\nand specialized nature of RS imagery and the comparatively limited spatial\nperception of current VLMs. Existing Remote Sensing specific Vision Language\nModels (RSVLMs) still have considerable potential for improvement, primarily\nowing to the lack of large-scale, high-quality RS vision-language datasets. We\nconstructed HqDC-1.4M, the large scale High quality and Detailed Captions for\nRS images, containing 1.4 million image-caption pairs, which not only enhance\nthe RSVLM's understanding of RS images but also significantly improve the\nmodel's spatial perception abilities, such as localization and counting,\nthereby increasing the helpfulness of the RSVLM. Moreover, to address the\ninevitable \"hallucination\" problem in RSVLM, we developed RSSA, the first\ndataset aimed at enhancing the Self-Awareness capability of RSVLMs. By\nincorporating a variety of unanswerable questions into typical RS visual\nquestion-answering tasks, RSSA effectively improves the truthfulness and\nreduces the hallucinations of the model's outputs, thereby enhancing the\nhonesty of the RSVLM. Based on these datasets, we proposed the H2RSVLM, the\nHelpful and Honest Remote Sensing Vision Language Model. H2RSVLM has achieved\noutstanding performance on multiple RS public datasets and is capable of\nrecognizing and refusing to answer the unanswerable questions, effectively\nmitigating the incorrect generations. We will release the code, data and model\nweights at https://github.com/opendatalab/H2RSVLM .\n","authors":["Chao Pang","Jiang Wu","Jiayu Li","Yi Liu","Jiaxing Sun","Weijia Li","Xingxing Weng","Shuai Wang","Litong Feng","Gui-Song Xia","Conghui He"],"pdf_url":"https://arxiv.org/pdf/2403.20213v1.pdf","comment":"Equal contribution: Chao Pang, Jiang Wu; Corresponding author:\n  Gui-Song Xia, Conghui He"},{"id":"http://arxiv.org/abs/2403.10897v2","updated":"2024-03-29T14:49:11Z","published":"2024-03-16T11:21:24Z","title":"Rethinking Multi-view Representation Learning via Distilled\n  Disentangling","summary":"  Multi-view representation learning aims to derive robust representations that\nare both view-consistent and view-specific from diverse data sources. This\npaper presents an in-depth analysis of existing approaches in this domain,\nhighlighting a commonly overlooked aspect: the redundancy between\nview-consistent and view-specific representations. To this end, we propose an\ninnovative framework for multi-view representation learning, which incorporates\na technique we term 'distilled disentangling'. Our method introduces the\nconcept of masked cross-view prediction, enabling the extraction of compact,\nhigh-quality view-consistent representations from various sources without\nincurring extra computational overhead. Additionally, we develop a distilled\ndisentangling module that efficiently filters out consistency-related\ninformation from multi-view representations, resulting in purer view-specific\nrepresentations. This approach significantly reduces redundancy between\nview-consistent and view-specific representations, enhancing the overall\nefficiency of the learning process. Our empirical evaluations reveal that\nhigher mask ratios substantially improve the quality of view-consistent\nrepresentations. Moreover, we find that reducing the dimensionality of\nview-consistent representations relative to that of view-specific\nrepresentations further refines the quality of the combined representations.\nOur code is accessible at: https://github.com/Guanzhou-Ke/MRDD.\n","authors":["Guanzhou Ke","Bo Wang","Xiaoli Wang","Shengfeng He"],"pdf_url":"https://arxiv.org/pdf/2403.10897v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.12931v2","updated":"2024-03-29T14:48:42Z","published":"2024-03-19T17:34:27Z","title":"You Only Sample Once: Taming One-Step Text-To-Image Synthesis by\n  Self-Cooperative Diffusion GANs","summary":"  We introduce YOSO, a novel generative model designed for rapid, scalable, and\nhigh-fidelity one-step image synthesis. This is achieved by integrating the\ndiffusion process with GANs. Specifically, we smooth the distribution by the\ndenoising generator itself, performing self-cooperative learning. We show that\nour method can serve as a one-step generation model training from scratch with\ncompetitive performance. Moreover, we show that our method can be extended to\nfinetune pre-trained text-to-image diffusion for high-quality one-step\ntext-to-image synthesis even with LoRA fine-tuning. In particular, we provide\nthe first diffusion transformer that can generate images in one step trained on\n512 resolution, with the capability of adapting to 1024 resolution without\nexplicit training. Our code is provided at https://github.com/Luo-Yihong/YOSO.\n","authors":["Yihong Luo","Xiaolong Chen","Jing Tang"],"pdf_url":"https://arxiv.org/pdf/2403.12931v2.pdf","comment":"Early version"},{"id":"http://arxiv.org/abs/2403.11371v4","updated":"2024-03-29T14:19:56Z","published":"2024-03-17T23:29:41Z","title":"V2X-DGW: Domain Generalization for Multi-agent Perception under Adverse\n  Weather Conditions","summary":"  Current LiDAR-based Vehicle-to-Everything (V2X) multi-agent perception\nsystems have shown the significant success on 3D object detection. While these\nmodels perform well in the trained clean weather, they struggle in unseen\nadverse weather conditions with the real-world domain gap. In this paper, we\npropose a domain generalization approach, named V2X-DGW, for LiDAR-based 3D\nobject detection on multi-agent perception system under adverse weather\nconditions. Not only in the clean weather does our research aim to ensure\nfavorable multi-agent performance, but also in the unseen adverse weather\nconditions by learning only on the clean weather data. To advance research in\nthis area, we have simulated the impact of three prevalent adverse weather\nconditions on two widely-used multi-agent datasets, resulting in the creation\nof two novel benchmark datasets: OPV2V-w and V2XSet-w.\n  To this end, we first introduce the Adaptive Weather Augmentation (AWA) to\nmimic the unseen adverse weather conditions, and then propose two alignments\nfor generalizable representation learning: Trust-region Weather-invariant\nAlignment (TWA) and Agent-aware Contrastive Alignment (ACA). Extensive\nexperimental results demonstrate that our V2X-DGW achieved improvements in the\nunseen adverse weather conditions.\n","authors":["Baolu Li","Jinlong Li","Xinyu Liu","Runsheng Xu","Zhengzhong Tu","Jiacheng Guo","Xiaopeng Li","Hongkai Yu"],"pdf_url":"https://arxiv.org/pdf/2403.11371v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20195v1","updated":"2024-03-29T14:17:30Z","published":"2024-03-29T14:17:30Z","title":"Enhancing Lithological Mapping with Spatially Constrained Bayesian\n  Network (SCB-Net): An Approach for Field Data-Constrained Predictions with\n  Uncertainty Evaluation","summary":"  Geological maps are an extremely valuable source of information for the Earth\nsciences. They provide insights into mineral exploration, vulnerability to\nnatural hazards, and many other applications. These maps are created using\nnumerical or conceptual models that use geological observations to extrapolate\ndata. Geostatistical techniques have traditionally been used to generate\nreliable predictions that take into account the spatial patterns inherent in\nthe data. However, as the number of auxiliary variables increases, these\nmethods become more labor-intensive. Additionally, traditional machine learning\nmethods often struggle with spatially correlated data and extracting valuable\nnon-linear information from geoscientific datasets. To address these\nlimitations, a new architecture called the Spatially Constrained Bayesian\nNetwork (SCB-Net) has been developed. The SCB-Net aims to effectively exploit\nthe information from auxiliary variables while producing spatially constrained\npredictions. It is made up of two parts, the first part focuses on learning\nunderlying patterns in the auxiliary variables while the second part integrates\nground-truth data and the learned embeddings from the first part. Moreover, to\nassess model uncertainty, a technique called Monte Carlo dropout is used as a\nBayesian approximation. The SCB-Net has been applied to two selected areas in\nnorthern Quebec, Canada, and has demonstrated its potential in generating\nfield-data-constrained lithological maps while allowing assessment of\nprediction uncertainty for decision-making. This study highlights the promising\nadvancements of deep neural networks in geostatistics, particularly in handling\ncomplex spatial feature learning tasks, leading to improved spatial information\ntechniques.\n","authors":["Victor Silva dos Santos","Erwan Gloaguen","Shiva Tirdad"],"pdf_url":"https://arxiv.org/pdf/2403.20195v1.pdf","comment":"17 pages, 3559 words, 14 figures"},{"id":"http://arxiv.org/abs/2403.20193v1","updated":"2024-03-29T14:14:22Z","published":"2024-03-29T14:14:22Z","title":"Motion Inversion for Video Customization","summary":"  In this research, we present a novel approach to motion customization in\nvideo generation, addressing the widespread gap in the thorough exploration of\nmotion representation within video generative models. Recognizing the unique\nchallenges posed by video's spatiotemporal nature, our method introduces Motion\nEmbeddings, a set of explicit, temporally coherent one-dimensional embeddings\nderived from a given video. These embeddings are designed to integrate\nseamlessly with the temporal transformer modules of video diffusion models,\nmodulating self-attention computations across frames without compromising\nspatial integrity. Our approach offers a compact and efficient solution to\nmotion representation and enables complex manipulations of motion\ncharacteristics through vector arithmetic in the embedding space. Furthermore,\nwe identify the Temporal Discrepancy in video generative models, which refers\nto variations in how different motion modules process temporal relationships\nbetween frames. We leverage this understanding to optimize the integration of\nour motion embeddings. Our contributions include the introduction of a tailored\nmotion embedding for customization tasks, insights into the temporal processing\ndifferences in video models, and a demonstration of the practical advantages\nand effectiveness of our method through extensive experiments.\n","authors":["Luozhou Wang","Guibao Shen","Yixun Liang","Xin Tao","Pengfei Wan","Di Zhang","Yijun Li","Yingcong Chen"],"pdf_url":"https://arxiv.org/pdf/2403.20193v1.pdf","comment":"Project Page:\n  \\href{https://wileewang.github.io/MotionInversion/}{https://wileewang.github.io/MotionInversion/}"},{"id":"http://arxiv.org/abs/2403.20186v1","updated":"2024-03-29T14:04:45Z","published":"2024-03-29T14:04:45Z","title":"Sketch-to-Architecture: Generative AI-aided Architectural Design","summary":"  Recently, the development of large-scale models has paved the way for various\ninterdisciplinary research, including architecture. By using generative AI, we\npresent a novel workflow that utilizes AI models to generate conceptual\nfloorplans and 3D models from simple sketches, enabling rapid ideation and\ncontrolled generation of architectural renderings based on textual\ndescriptions. Our work demonstrates the potential of generative AI in the\narchitectural design process, pointing towards a new direction of\ncomputer-aided architectural design. Our project website is available at:\nhttps://zrealli.github.io/sketch2arc\n","authors":["Pengzhi Li","Baijuan Li","Zhiheng Li"],"pdf_url":"https://arxiv.org/pdf/2403.20186v1.pdf","comment":"Pacific Graphics 2023, accepted as Poster"},{"id":"http://arxiv.org/abs/2403.20183v1","updated":"2024-03-29T13:57:46Z","published":"2024-03-29T13:57:46Z","title":"HARMamba: Efficient Wearable Sensor Human Activity Recognition Based on\n  Bidirectional Selective SSM","summary":"  Wearable sensor human activity recognition (HAR) is a crucial area of\nresearch in activity sensing. While transformer-based temporal deep learning\nmodels have been extensively studied and implemented, their large number of\nparameters present significant challenges in terms of system computing load and\nmemory usage, rendering them unsuitable for real-time mobile activity\nrecognition applications. Recently, an efficient hardware-aware state space\nmodel (SSM) called Mamba has emerged as a promising alternative. Mamba\ndemonstrates strong potential in long sequence modeling, boasts a simpler\nnetwork architecture, and offers an efficient hardware-aware design. Leveraging\nSSM for activity recognition represents an appealing avenue for exploration. In\nthis study, we introduce HARMamba, which employs a more lightweight selective\nSSM as the foundational model architecture for activity recognition. The goal\nis to address the computational resource constraints encountered in real-time\nactivity recognition scenarios. Our approach involves processing sensor data\nflow by independently learning each channel and segmenting the data into\n\"patches\". The marked sensor sequence's position embedding serves as the input\ntoken for the bidirectional state space model, ultimately leading to activity\ncategorization through the classification head. Compared to established\nactivity recognition frameworks like Transformer-based models, HARMamba\nachieves superior performance while also reducing computational and memory\noverhead. Furthermore, our proposed method has been extensively tested on four\npublic activity datasets: PAMAP2, WISDM, UNIMIB, and UCI, demonstrating\nimpressive performance in activity recognition tasks.\n","authors":["Shuangjian Li","Tao Zhu","Furong Duan","Liming Chen","Huansheng Ning","Yaping Wan"],"pdf_url":"https://arxiv.org/pdf/2403.20183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01705v2","updated":"2024-03-29T13:53:33Z","published":"2023-04-04T11:01:46Z","title":"Cross-modal tumor segmentation using generative blending augmentation\n  and self training","summary":"  \\textit{Objectives}: Data scarcity and domain shifts lead to biased training\nsets that do not accurately represent deployment conditions. A related\npractical problem is cross-modal image segmentation, where the objective is to\nsegment unlabelled images using previously labelled datasets from other imaging\nmodalities. \\textit{Methods}: We propose a cross-modal segmentation method\nbased on conventional image synthesis boosted by a new data augmentation\ntechnique called Generative Blending Augmentation (GBA). GBA leverages a SinGAN\nmodel to learn representative generative features from a single training image\nto diversify realistically tumor appearances. This way, we compensate for image\nsynthesis errors, subsequently improving the generalization power of a\ndownstream segmentation model. The proposed augmentation is further combined to\nan iterative self-training procedure leveraging pseudo labels at each pass.\n\\textit{Results}: The proposed solution ranked first for vestibular schwannoma\n(VS) segmentation during the validation and test phases of the MICCAI CrossMoDA\n2022 challenge, with best mean Dice similarity and average symmetric surface\ndistance measures. \\textit{Conclusion and significance}: Local contrast\nalteration of tumor appearances and iterative self-training with pseudo labels\nare likely to lead to performance improvements in a variety of segmentation\ncontexts.\n","authors":["Guillaume Sallé","Pierre-Henri Conze","Julien Bert","Nicolas Boussion","Dimitris Visvikis","Vincent Jaouen"],"pdf_url":"https://arxiv.org/pdf/2304.01705v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20173v1","updated":"2024-03-29T13:40:44Z","published":"2024-03-29T13:40:44Z","title":"MCNet: A crowd denstity estimation network based on integrating\n  multiscale attention module","summary":"  Aiming at the metro video surveillance system has not been able to\neffectively solve the metro crowd density estimation problem, a Metro Crowd\ndensity estimation Network (called MCNet) is proposed to automatically classify\ncrowd density level of passengers. Firstly, an Integrating Multi-scale\nAttention (IMA) module is proposed to enhance the ability of the plain\nclassifiers to extract semantic crowd texture features to accommodate to the\ncharacteristics of the crowd texture feature. The innovation of the IMA module\nis to fuse the dilation convolution, multiscale feature extraction and\nattention mechanism to obtain multi-scale crowd feature activation from a\nlarger receptive field with lower computational cost, and to strengthen the\ncrowds activation state of convolutional features in top layers. Secondly, a\nnovel lightweight crowd texture feature extraction network is proposed, which\ncan directly process video frames and automatically extract texture features\nfor crowd density estimation, while its faster image processing speed and fewer\nnetwork parameters make it flexible to be deployed on embedded platforms with\nlimited hardware resources. Finally, this paper integrates IMA module and the\nlightweight crowd texture feature extraction network to construct the MCNet,\nand validate the feasibility of this network on image classification dataset:\nCifar10 and four crowd density datasets: PETS2009, Mall, QUT and SH_METRO to\nvalidate the MCNet whether can be a suitable solution for crowd density\nestimation in metro video surveillance where there are image processing\nchallenges such as high density, high occlusion, perspective distortion and\nlimited hardware resources.\n","authors":["Qiang Guo","Rubo Zhang","Di Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.20173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20168v1","updated":"2024-03-29T13:35:37Z","published":"2024-03-29T13:35:37Z","title":"Unsupervised Tumor-Aware Distillation for Multi-Modal Brain Image\n  Translation","summary":"  Multi-modal brain images from MRI scans are widely used in clinical diagnosis\nto provide complementary information from different modalities. However,\nobtaining fully paired multi-modal images in practice is challenging due to\nvarious factors, such as time, cost, and artifacts, resulting in\nmodality-missing brain images. To address this problem, unsupervised\nmulti-modal brain image translation has been extensively studied. Existing\nmethods suffer from the problem of brain tumor deformation during translation,\nas they fail to focus on the tumor areas when translating the whole images. In\nthis paper, we propose an unsupervised tumor-aware distillation teacher-student\nnetwork called UTAD-Net, which is capable of perceiving and translating tumor\nareas precisely. Specifically, our model consists of two parts: a teacher\nnetwork and a student network. The teacher network learns an end-to-end mapping\nfrom source to target modality using unpaired images and corresponding tumor\nmasks first. Then, the translation knowledge is distilled into the student\nnetwork, enabling it to generate more realistic tumor areas and whole images\nwithout masks. Experiments show that our model achieves competitive performance\non both quantitative and qualitative evaluations of image quality compared with\nstate-of-the-art methods. Furthermore, we demonstrate the effectiveness of the\ngenerated images on downstream segmentation tasks. Our code is available at\nhttps://github.com/scut-HC/UTAD-Net.\n","authors":["Chuan Huang","Jia Wei","Rui Li"],"pdf_url":"https://arxiv.org/pdf/2403.20168v1.pdf","comment":"8 pages, 5 figures. It has been provisionally accepted for IJCNN 2024"},{"id":"http://arxiv.org/abs/2304.00746v4","updated":"2024-03-29T13:32:53Z","published":"2023-04-03T06:40:52Z","title":"VGTS: Visually Guided Text Spotting for Novel Categories in Historical\n  Manuscripts","summary":"  In the field of historical manuscript research, scholars frequently encounter\nnovel symbols in ancient texts, investing considerable effort in their\nidentification and documentation. Although existing object detection methods\nachieve impressive performance on known categories, they struggle to recognize\nnovel symbols without retraining. To address this limitation, we propose a\nVisually Guided Text Spotting (VGTS) approach that accurately spots novel\ncharacters using just one annotated support sample. The core of VGTS is a\nspatial alignment module consisting of a Dual Spatial Attention (DSA) block and\na Geometric Matching (GM) block. The DSA block aims to identify, focus on, and\nlearn discriminative spatial regions in the support and query images, mimicking\nthe human visual spotting process. It first refines the support image by\nanalyzing inter-channel relationships to identify critical areas, and then\nrefines the query image by focusing on informative key points. The GM block, on\nthe other hand, establishes the spatial correspondence between the two images,\nenabling accurate localization of the target character in the query image. To\ntackle the example imbalance problem in low-resource spotting tasks, we develop\na novel torus loss function that enhances the discriminative power of the\nembedding space for distance metric learning. To further validate our approach,\nwe introduce a new dataset featuring ancient Dongba hieroglyphics (DBH)\nassociated with the Naxi minority of China. Extensive experiments on the DBH\ndataset and other public datasets, including EGY, VML-HD, TKH, and NC, show\nthat VGTS consistently surpasses state-of-the-art methods. The proposed\nframework exhibits great potential for application in historical manuscript\ntext spotting, enabling scholars to efficiently identify and document novel\nsymbols with minimal annotation effort.\n","authors":["Wenbo Hu","Hongjian Zhan","Xinchen Ma","Cong Liu","Bing Yin","Yue Lu"],"pdf_url":"https://arxiv.org/pdf/2304.00746v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20159v1","updated":"2024-03-29T13:16:05Z","published":"2024-03-29T13:16:05Z","title":"HGS-Mapping: Online Dense Mapping Using Hybrid Gaussian Representation\n  in Urban Scenes","summary":"  Online dense mapping of urban scenes forms a fundamental cornerstone for\nscene understanding and navigation of autonomous vehicles. Recent advancements\nin mapping methods are mainly based on NeRF, whose rendering speed is too slow\nto meet online requirements. 3D Gaussian Splatting (3DGS), with its rendering\nspeed hundreds of times faster than NeRF, holds greater potential in online\ndense mapping. However, integrating 3DGS into a street-view dense mapping\nframework still faces two challenges, including incomplete reconstruction due\nto the absence of geometric information beyond the LiDAR coverage area and\nextensive computation for reconstruction in large urban scenes. To this end, we\npropose HGS-Mapping, an online dense mapping framework in unbounded large-scale\nscenes. To attain complete construction, our framework introduces Hybrid\nGaussian Representation, which models different parts of the entire scene using\nGaussians with distinct properties. Furthermore, we employ a hybrid Gaussian\ninitialization mechanism and an adaptive update method to achieve high-fidelity\nand rapid reconstruction. To the best of our knowledge, we are the first to\nintegrate Gaussian representation into online dense mapping of urban scenes.\nOur approach achieves SOTA reconstruction accuracy while only employing 66%\nnumber of Gaussians, leading to 20% faster reconstruction speed.\n","authors":["Ke Wu","Kaizhao Zhang","Zhiwei Zhang","Shanshuai Yuan","Muer Tie","Julong Wei","Zijun Xu","Jieru Zhao","Zhongxue Gan","Wenchao Ding"],"pdf_url":"https://arxiv.org/pdf/2403.20159v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00274v2","updated":"2024-03-29T13:14:59Z","published":"2024-03-01T04:31:56Z","title":"CustomListener: Text-guided Responsive Interaction for User-friendly\n  Listening Head Generation","summary":"  Listening head generation aims to synthesize a non-verbal responsive listener\nhead by modeling the correlation between the speaker and the listener in\ndynamic conversion.The applications of listener agent generation in virtual\ninteraction have promoted many works achieving the diverse and fine-grained\nmotion generation. However, they can only manipulate motions through simple\nemotional labels, but cannot freely control the listener's motions. Since\nlistener agents should have human-like attributes (e.g. identity, personality)\nwhich can be freely customized by users, this limits their realism. In this\npaper, we propose a user-friendly framework called CustomListener to realize\nthe free-form text prior guided listener generation. To achieve\nspeaker-listener coordination, we design a Static to Dynamic Portrait module\n(SDP), which interacts with speaker information to transform static text into\ndynamic portrait token with completion rhythm and amplitude information. To\nachieve coherence between segments, we design a Past Guided Generation Module\n(PGG) to maintain the consistency of customized listener attributes through the\nmotion prior, and utilize a diffusion-based structure conditioned on the\nportrait token and the motion prior to realize the controllable generation. To\ntrain and evaluate our model, we have constructed two text-annotated listening\nhead datasets based on ViCo and RealTalk, which provide text-video paired\nlabels. Extensive experiments have verified the effectiveness of our model.\n","authors":["Xi Liu","Ying Guo","Cheng Zhen","Tong Li","Yingying Ao","Pengfei Yan"],"pdf_url":"https://arxiv.org/pdf/2403.00274v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2401.06312v4","updated":"2024-03-29T13:10:56Z","published":"2024-01-12T00:49:49Z","title":"Video Super-Resolution Transformer with Masked Inter&Intra-Frame\n  Attention","summary":"  Recently, Vision Transformer has achieved great success in recovering missing\ndetails in low-resolution sequences, i.e., the video super-resolution (VSR)\ntask. Despite its superiority in VSR accuracy, the heavy computational burden\nas well as the large memory footprint hinder the deployment of\nTransformer-based VSR models on constrained devices. In this paper, we address\nthe above issue by proposing a novel feature-level masked processing framework:\nVSR with Masked Intra and inter frame Attention (MIA-VSR). The core of MIA-VSR\nis leveraging feature-level temporal continuity between adjacent frames to\nreduce redundant computations and make more rational use of previously enhanced\nSR features. Concretely, we propose an intra-frame and inter-frame attention\nblock which takes the respective roles of past features and input features into\nconsideration and only exploits previously enhanced features to provide\nsupplementary information. In addition, an adaptive block-wise mask prediction\nmodule is developed to skip unimportant computations according to feature\nsimilarity between adjacent frames. We conduct detailed ablation studies to\nvalidate our contributions and compare the proposed method with recent\nstate-of-the-art VSR approaches. The experimental results demonstrate that\nMIA-VSR improves the memory and computation efficiency over state-of-the-art\nmethods, without trading off PSNR accuracy. The code is available at\nhttps://github.com/LabShuHangGU/MIA-VSR.\n","authors":["Xingyu Zhou","Leheng Zhang","Xiaorui Zhao","Keze Wang","Leida Li","Shuhang Gu"],"pdf_url":"https://arxiv.org/pdf/2401.06312v4.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2307.09591v2","updated":"2024-03-29T13:04:03Z","published":"2023-07-18T19:56:20Z","title":"Gradient strikes back: How filtering out high frequencies improves\n  explanations","summary":"  Attribution methods correspond to a class of explainability methods (XAI)\nthat aim to assess how individual inputs contribute to a model's\ndecision-making process. We have identified a significant limitation in one\ntype of attribution methods, known as \"white-box\" methods. Although highly\nefficient, these methods rely on a gradient signal that is often contaminated\nby high-frequency noise. To overcome this limitation, we introduce a new\napproach called \"FORGrad\". This simple method effectively filters out noise\nartifacts by using optimal cut-off frequencies tailored to the unique\ncharacteristics of each model architecture. Our findings show that FORGrad\nconsistently enhances the performance of already existing white-box methods,\nenabling them to compete effectively with more accurate yet computationally\ndemanding \"black-box\" methods. We anticipate that our research will foster\nbroader adoption of simpler and more efficient white-box methods for\nexplainability, offering a better balance between faithfulness and\ncomputational efficiency.\n","authors":["Sabine Muzellec","Thomas Fel","Victor Boutin","Léo andéol","Rufin VanRullen","Thomas Serre"],"pdf_url":"https://arxiv.org/pdf/2307.09591v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.08396v5","updated":"2024-03-29T12:50:38Z","published":"2023-05-15T07:23:54Z","title":"MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation","summary":"  Since their emergence, Convolutional Neural Networks (CNNs) have made\nsignificant strides in medical image analysis. However, the local nature of the\nconvolution operator may pose a limitation for capturing global and long-range\ninteractions in CNNs. Recently, Transformers have gained popularity in the\ncomputer vision community and also in medical image segmentation due to their\nability to process global features effectively. The scalability issues of the\nself-attention mechanism and lack of the CNN-like inductive bias may have\nlimited their adoption. Therefore, hybrid Vision transformers\n(CNN-Transformer), exploiting the advantages of both Convolution and\nSelf-attention Mechanisms, have gained importance. In this work, we present\nMaxViT-UNet, a new Encoder-Decoder based UNet type hybrid vision transformer\n(CNN-Transformer) for medical image segmentation. The proposed Hybrid Decoder\nis designed to harness the power of both the convolution and self-attention\nmechanisms at each decoding stage with a nominal memory and computational\nburden. The inclusion of multi-axis self-attention, within each decoder stage,\nsignificantly enhances the discriminating capacity between the object and\nbackground regions, thereby helping in improving the segmentation efficiency.\nIn the Hybrid Decoder, a new block is also proposed. The fusion process\ncommences by integrating the upsampled lower-level decoder features, obtained\nthrough transpose convolution, with the skip-connection features derived from\nthe hybrid encoder. Subsequently, the fused features undergo refinement through\nthe utilization of a multi-axis attention mechanism. The proposed decoder block\nis repeated multiple times to segment the nuclei regions progressively.\nExperimental results on MoNuSeg18 and MoNuSAC20 datasets demonstrate the\neffectiveness of the proposed technique.\n","authors":["Abdul Rehman Khan","Asifullah Khan"],"pdf_url":"https://arxiv.org/pdf/2305.08396v5.pdf","comment":"19 pages, 6 figures, 5 tables"},{"id":"http://arxiv.org/abs/2403.20153v1","updated":"2024-03-29T12:49:40Z","published":"2024-03-29T12:49:40Z","title":"Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D\n  Generative Prior","summary":"  Recent methods for audio-driven talking head synthesis often optimize neural\nradiance fields (NeRF) on a monocular talking portrait video, leveraging its\ncapability to render high-fidelity and 3D-consistent novel-view frames.\nHowever, they often struggle to reconstruct complete face geometry due to the\nabsence of comprehensive 3D information in the input monocular videos. In this\npaper, we introduce a novel audio-driven talking head synthesis framework,\ncalled Talk3D, that can faithfully reconstruct its plausible facial geometries\nby effectively adopting the pre-trained 3D-aware generative prior. Given the\npersonalized 3D generative model, we present a novel audio-guided attention\nU-Net architecture that predicts the dynamic face variations in the NeRF space\ndriven by audio. Furthermore, our model is further modulated by audio-unrelated\nconditioning tokens which effectively disentangle variations unrelated to audio\nfeatures. Compared to existing methods, our method excels in generating\nrealistic facial geometries even under extreme head poses. We also conduct\nextensive experiments showing our approach surpasses state-of-the-art\nbenchmarks in terms of both quantitative and qualitative evaluations.\n","authors":["Jaehoon Ko","Kyusun Cho","Joungbin Lee","Heeji Yoon","Sangmin Lee","Sangjun Ahn","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2403.20153v1.pdf","comment":"Project page: https://ku-cvlab.github.io/Talk3D/"},{"id":"http://arxiv.org/abs/2403.12687v2","updated":"2024-03-29T12:45:27Z","published":"2024-03-19T12:45:52Z","title":"Audio-Visual Compound Expression Recognition Method based on Late\n  Modality Fusion and Rule-based Decision","summary":"  This paper presents the results of the SUN team for the Compound Expressions\nRecognition Challenge of the 6th ABAW Competition. We propose a novel\naudio-visual method for compound expression recognition. Our method relies on\nemotion recognition models that fuse modalities at the emotion probability\nlevel, while decisions regarding the prediction of compound expressions are\nbased on predefined rules. Notably, our method does not use any training data\nspecific to the target task. Thus, the problem is a zero-shot classification\ntask. The method is evaluated in multi-corpus training and cross-corpus\nvalidation setups. Using our proposed method is achieved an F1-score value\nequals to 22.01% on the C-EXPR-DB test subset. Our findings from the challenge\ndemonstrate that the proposed method can potentially form a basis for\ndeveloping intelligent tools for annotating audio-visual data in the context of\nhuman's basic and compound emotions.\n","authors":["Elena Ryumina","Maxim Markitantov","Dmitry Ryumin","Heysem Kaya","Alexey Karpov"],"pdf_url":"https://arxiv.org/pdf/2403.12687v2.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2402.11677v2","updated":"2024-03-29T12:34:34Z","published":"2024-02-18T18:56:13Z","title":"MultiCorrupt: A Multi-Modal Robustness Dataset and Benchmark of\n  LiDAR-Camera Fusion for 3D Object Detection","summary":"  Multi-modal 3D object detection models for automated driving have\ndemonstrated exceptional performance on computer vision benchmarks like\nnuScenes. However, their reliance on densely sampled LiDAR point clouds and\nmeticulously calibrated sensor arrays poses challenges for real-world\napplications. Issues such as sensor misalignment, miscalibration, and disparate\nsampling frequencies lead to spatial and temporal misalignment in data from\nLiDAR and cameras. Additionally, the integrity of LiDAR and camera data is\noften compromised by adverse environmental conditions such as inclement\nweather, leading to occlusions and noise interference. To address this\nchallenge, we introduce MultiCorrupt, a comprehensive benchmark designed to\nevaluate the robustness of multi-modal 3D object detectors against ten distinct\ntypes of corruptions. We evaluate five state-of-the-art multi-modal detectors\non MultiCorrupt and analyze their performance in terms of their resistance\nability. Our results show that existing methods exhibit varying degrees of\nrobustness depending on the type of corruption and their fusion strategy. We\nprovide insights into which multi-modal design choices make such models robust\nagainst certain perturbations. The dataset generation code and benchmark are\nopen-sourced at https://github.com/ika-rwth-aachen/MultiCorrupt.\n","authors":["Till Beemelmanns","Quan Zhang","Lutz Eckstein"],"pdf_url":"https://arxiv.org/pdf/2402.11677v2.pdf","comment":"Code: https://github.com/ika-rwth-aachen/MultiCorrupt"},{"id":"http://arxiv.org/abs/2311.15851v3","updated":"2024-03-29T12:25:45Z","published":"2023-11-27T14:17:41Z","title":"Single-Model and Any-Modality for Video Object Tracking","summary":"  In the realm of video object tracking, auxiliary modalities such as depth,\nthermal, or event data have emerged as valuable assets to complement the RGB\ntrackers. In practice, most existing RGB trackers learn a single set of\nparameters to use them across datasets and applications. However, a similar\nsingle-model unification for multi-modality tracking presents several\nchallenges. These challenges stem from the inherent heterogeneity of inputs --\neach with modality-specific representations, the scarcity of multi-modal\ndatasets, and the absence of all the modalities at all times. In this work, we\nintroduce Un-Track, a Unified Tracker of a single set of parameters for any\nmodality. To handle any modality, our method learns their common latent space\nthrough low-rank factorization and reconstruction techniques. More importantly,\nwe use only the RGB-X pairs to learn the common latent space. This unique\nshared representation seamlessly binds all modalities together, enabling\neffective unification and accommodating any missing modality, all within a\nsingle transformer-based architecture. Our Un-Track achieves +8.1 absolute\nF-score gain, on the DepthTrack dataset, by introducing only +2.14 (over 21.50)\nGFLOPs with +6.6M (over 93M) parameters, through a simple yet efficient\nprompting strategy. Extensive comparisons on five benchmark datasets with\ndifferent modalities show that Un-Track surpasses both SOTA unified trackers\nand modality-specific counterparts, validating our effectiveness and\npracticality. The source code is publicly available at\nhttps://github.com/Zongwei97/UnTrack.\n","authors":["Zongwei Wu","Jilai Zheng","Xiangxuan Ren","Florin-Alexandru Vasluianu","Chao Ma","Danda Pani Paudel","Luc Van Gool","Radu Timofte"],"pdf_url":"https://arxiv.org/pdf/2311.15851v3.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.20142v1","updated":"2024-03-29T12:23:58Z","published":"2024-03-29T12:23:58Z","title":"StegoGAN: Leveraging Steganography for Non-Bijective Image-to-Image\n  Translation","summary":"  Most image-to-image translation models postulate that a unique correspondence\nexists between the semantic classes of the source and target domains. However,\nthis assumption does not always hold in real-world scenarios due to divergent\ndistributions, different class sets, and asymmetrical information\nrepresentation. As conventional GANs attempt to generate images that match the\ndistribution of the target domain, they may hallucinate spurious instances of\nclasses absent from the source domain, thereby diminishing the usefulness and\nreliability of translated images. CycleGAN-based methods are also known to hide\nthe mismatched information in the generated images to bypass cycle consistency\nobjectives, a process known as steganography. In response to the challenge of\nnon-bijective image translation, we introduce StegoGAN, a novel model that\nleverages steganography to prevent spurious features in generated images. Our\napproach enhances the semantic consistency of the translated images without\nrequiring additional postprocessing or supervision. Our experimental\nevaluations demonstrate that StegoGAN outperforms existing GAN-based models\nacross various non-bijective image-to-image translation tasks, both\nqualitatively and quantitatively. Our code and pretrained models are accessible\nat https://github.com/sian-wusidi/StegoGAN.\n","authors":["Sidi Wu","Yizi Chen","Samuel Mermet","Lorenz Hurni","Konrad Schindler","Nicolas Gonthier","Loic Landrieu"],"pdf_url":"https://arxiv.org/pdf/2403.20142v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00320v2","updated":"2024-03-29T12:04:52Z","published":"2023-12-30T20:52:20Z","title":"DXAI: Explaining Classification by Image Decomposition","summary":"  We propose a new way to explain and to visualize neural network\nclassification through a decomposition-based explainable AI (DXAI). Instead of\nproviding an explanation heatmap, our method yields a decomposition of the\nimage into class-agnostic and class-distinct parts, with respect to the data\nand chosen classifier. Following a fundamental signal processing paradigm of\nanalysis and synthesis, the original image is the sum of the decomposed parts.\nWe thus obtain a radically different way of explaining classification. The\nclass-agnostic part ideally is composed of all image features which do not\nposses class information, where the class-distinct part is its complementary.\nThis new visualization can be more helpful and informative in certain\nscenarios, especially when the attributes are dense, global and additive in\nnature, for instance, when colors or textures are essential for class\ndistinction. Code is available at https://github.com/dxai2024/dxai.\n","authors":["Elnatan Kadar","Guy Gilboa"],"pdf_url":"https://arxiv.org/pdf/2401.00320v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20126v1","updated":"2024-03-29T11:31:12Z","published":"2024-03-29T11:31:12Z","title":"ECLIPSE: Efficient Continual Learning in Panoptic Segmentation with\n  Visual Prompt Tuning","summary":"  Panoptic segmentation, combining semantic and instance segmentation, stands\nas a cutting-edge computer vision task. Despite recent progress with deep\nlearning models, the dynamic nature of real-world applications necessitates\ncontinual learning, where models adapt to new classes (plasticity) over time\nwithout forgetting old ones (catastrophic forgetting). Current continual\nsegmentation methods often rely on distillation strategies like knowledge\ndistillation and pseudo-labeling, which are effective but result in increased\ntraining complexity and computational overhead. In this paper, we introduce a\nnovel and efficient method for continual panoptic segmentation based on Visual\nPrompt Tuning, dubbed ECLIPSE. Our approach involves freezing the base model\nparameters and fine-tuning only a small set of prompt embeddings, addressing\nboth catastrophic forgetting and plasticity and significantly reducing the\ntrainable parameters. To mitigate inherent challenges such as error propagation\nand semantic drift in continual segmentation, we propose logit manipulation to\neffectively leverage common knowledge across the classes. Experiments on ADE20K\ncontinual panoptic segmentation benchmark demonstrate the superiority of\nECLIPSE, notably its robustness against catastrophic forgetting and its\nreasonable plasticity, achieving a new state-of-the-art. The code is available\nat https://github.com/clovaai/ECLIPSE.\n","authors":["Beomyoung Kim","Joonsang Yu","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2403.20126v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2312.00648v2","updated":"2024-03-29T11:30:20Z","published":"2023-12-01T15:20:58Z","title":"SPOT: Self-Training with Patch-Order Permutation for Object-Centric\n  Learning with Autoregressive Transformers","summary":"  Unsupervised object-centric learning aims to decompose scenes into\ninterpretable object entities, termed slots. Slot-based auto-encoders stand out\nas a prominent method for this task. Within them, crucial aspects include\nguiding the encoder to generate object-specific slots and ensuring the decoder\nutilizes them during reconstruction. This work introduces two novel techniques,\n(i) an attention-based self-training approach, which distills superior\nslot-based attention masks from the decoder to the encoder, enhancing object\nsegmentation, and (ii) an innovative patch-order permutation strategy for\nautoregressive transformers that strengthens the role of slot vectors in\nreconstruction. The effectiveness of these strategies is showcased\nexperimentally. The combined approach significantly surpasses prior slot-based\nautoencoder methods in unsupervised object segmentation, especially with\ncomplex real-world images. We provide the implementation code at\nhttps://github.com/gkakogeorgiou/spot .\n","authors":["Ioannis Kakogeorgiou","Spyros Gidaris","Konstantinos Karantzalos","Nikos Komodakis"],"pdf_url":"https://arxiv.org/pdf/2312.00648v2.pdf","comment":"CVPR 2024. Code: https://github.com/gkakogeorgiou/spot"},{"id":"http://arxiv.org/abs/2401.00616v3","updated":"2024-03-29T11:27:32Z","published":"2024-01-01T00:08:39Z","title":"GD^2-NeRF: Generative Detail Compensation via GAN and Diffusion for\n  One-shot Generalizable Neural Radiance Fields","summary":"  In this paper, we focus on the One-shot Novel View Synthesis (O-NVS) task\nwhich targets synthesizing photo-realistic novel views given only one reference\nimage per scene. Previous One-shot Generalizable Neural Radiance Fields\n(OG-NeRF) methods solve this task in an inference-time finetuning-free manner,\nyet suffer the blurry issue due to the encoder-only architecture that highly\nrelies on the limited reference image. On the other hand, recent\ndiffusion-based image-to-3d methods show vivid plausible results via distilling\npre-trained 2D diffusion models into a 3D representation, yet require tedious\nper-scene optimization. Targeting these issues, we propose the GD$^2$-NeRF, a\nGenerative Detail compensation framework via GAN and Diffusion that is both\ninference-time finetuning-free and with vivid plausible details. In detail,\nfollowing a coarse-to-fine strategy, GD$^2$-NeRF is mainly composed of a\nOne-stage Parallel Pipeline (OPP) and a 3D-consistent Detail Enhancer\n(Diff3DE). At the coarse stage, OPP first efficiently inserts the GAN model\ninto the existing OG-NeRF pipeline for primarily relieving the blurry issue\nwith in-distribution priors captured from the training dataset, achieving a\ngood balance between sharpness (LPIPS, FID) and fidelity (PSNR, SSIM). Then, at\nthe fine stage, Diff3DE further leverages the pre-trained image diffusion\nmodels to complement rich out-distribution details while maintaining decent 3D\nconsistency. Extensive experiments on both the synthetic and real-world\ndatasets show that GD$^2$-NeRF noticeably improves the details while without\nper-scene finetuning.\n","authors":["Xiao Pan","Zongxin Yang","Shuai Bai","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2401.00616v3.pdf","comment":"Submitted to Journal"},{"id":"http://arxiv.org/abs/2403.19435v2","updated":"2024-03-29T11:15:04Z","published":"2024-03-28T14:04:17Z","title":"BAMM: Bidirectional Autoregressive Motion Model","summary":"  Generating human motion from text has been dominated by denoising motion\nmodels either through diffusion or generative masking process. However, these\nmodels face great limitations in usability by requiring prior knowledge of the\nmotion length. Conversely, autoregressive motion models address this limitation\nby adaptively predicting motion endpoints, at the cost of degraded generation\nquality and editing capabilities. To address these challenges, we propose\nBidirectional Autoregressive Motion Model (BAMM), a novel text-to-motion\ngeneration framework. BAMM consists of two key components: (1) a motion\ntokenizer that transforms 3D human motion into discrete tokens in latent space,\nand (2) a masked self-attention transformer that autoregressively predicts\nrandomly masked tokens via a hybrid attention masking strategy. By unifying\ngenerative masked modeling and autoregressive modeling, BAMM captures rich and\nbidirectional dependencies among motion tokens, while learning the\nprobabilistic mapping from textual inputs to motion outputs with\ndynamically-adjusted motion sequence length. This feature enables BAMM to\nsimultaneously achieving high-quality motion generation with enhanced usability\nand built-in motion editability. Extensive experiments on HumanML3D and KIT-ML\ndatasets demonstrate that BAMM surpasses current state-of-the-art methods in\nboth qualitative and quantitative measures. Our project page is available at\nhttps://github.com/exitudio/BAMM-page.\n","authors":["Ekkasit Pinyoanuntapong","Muhammad Usama Saleem","Pu Wang","Minwoo Lee","Srijan Das","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2403.19435v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20112v1","updated":"2024-03-29T10:49:02Z","published":"2024-03-29T10:49:02Z","title":"Segmentation, Classification and Interpretation of Breast Cancer Medical\n  Images using Human-in-the-Loop Machine Learning","summary":"  This paper explores the application of Human-in-the-Loop (HITL) strategies in\ntraining machine learning models in the medical domain. In this case a\ndoctor-in-the-loop approach is proposed to leverage human expertise in dealing\nwith large and complex data. Specifically, the paper deals with the integration\nof genomic data and Whole Slide Imaging (WSI) analysis of breast cancer. Three\ndifferent tasks were developed: segmentation of histopathological images,\nclassification of this images regarding the genomic subtype of the cancer and,\nfinally, interpretation of the machine learning results. The involvement of a\npathologist helped us to develop a better segmentation model and to enhance the\nexplainatory capabilities of the models, but the classification results were\nsuboptimal, highlighting the limitations of this approach: despite involving\nhuman experts, complex domains can still pose challenges, and a HITL approach\nmay not always be effective.\n","authors":["David Vázquez-Lema","Eduardo Mosqueira-Rey","Elena Hernández-Pereira","Carlos Fernández-Lozano","Fernando Seara-Romera","Jorge Pombo-Otero"],"pdf_url":"https://arxiv.org/pdf/2403.20112v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20106v1","updated":"2024-03-29T10:40:41Z","published":"2024-03-29T10:40:41Z","title":"Aggregating Local and Global Features via Selective State Spaces Model\n  for Efficient Image Deblurring","summary":"  Image deblurring is a process of restoring a high quality image from the\ncorresponding blurred image. Significant progress in this field has been made\npossible by the emergence of various effective deep learning models, including\nCNNs and Transformers. However, these methods often face the dilemma between\neliminating long-range blur degradation perturbations and maintaining\ncomputational efficiency, which hinders their practical application. To address\nthis issue, we propose an efficient image deblurring network that leverages\nselective structured state spaces model to aggregate enriched and accurate\nfeatures. Specifically, we design an aggregate local and global block\n(ALGBlock) to capture and fuse both local invariant properties and non-local\ninformation. The ALGBlock consists of two blocks: (1) The local block models\nlocal connectivity using simplified channel attention. (2) The global block\ncaptures long-range dependency features with linear complexity through\nselective structured state spaces. Nevertheless, we note that the image details\nare local features of images, we accentuate the local part for restoration by\nrecalibrating the weight when aggregating the two branches for recovery.\nExperimental results demonstrate that the proposed method outperforms\nstate-of-the-art approaches on widely used benchmarks, highlighting its\nsuperior performance.\n","authors":["Hu Gao","Depeng Dang"],"pdf_url":"https://arxiv.org/pdf/2403.20106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20105v1","updated":"2024-03-29T10:38:25Z","published":"2024-03-29T10:38:25Z","title":"FreeSeg-Diff: Training-Free Open-Vocabulary Segmentation with Diffusion\n  Models","summary":"  Foundation models have exhibited unprecedented capabilities in tackling many\ndomains and tasks. Models such as CLIP are currently widely used to bridge\ncross-modal representations, and text-to-image diffusion models are arguably\nthe leading models in terms of realistic image generation. Image generative\nmodels are trained on massive datasets that provide them with powerful internal\nspatial representations. In this work, we explore the potential benefits of\nsuch representations, beyond image generation, in particular, for dense visual\nprediction tasks. We focus on the task of image segmentation, which is\ntraditionally solved by training models on closed-vocabulary datasets, with\npixel-level annotations. To avoid the annotation cost or training large\ndiffusion models, we constraint our setup to be zero-shot and training-free. In\na nutshell, our pipeline leverages different and relatively small-sized,\nopen-source foundation models for zero-shot open-vocabulary segmentation. The\npipeline is as follows: the image is passed to both a captioner model (i.e.\nBLIP) and a diffusion model (i.e., Stable Diffusion Model) to generate a text\ndescription and visual representation, respectively. The features are clustered\nand binarized to obtain class agnostic masks for each object. These masks are\nthen mapped to a textual class, using the CLIP model to support\nopen-vocabulary. Finally, we add a refinement step that allows to obtain a more\nprecise segmentation mask. Our approach (dubbed FreeSeg-Diff), which does not\nrely on any training, outperforms many training-based approaches on both Pascal\nVOC and COCO datasets. In addition, we show very competitive results compared\nto the recent weakly-supervised segmentation approaches. We provide\ncomprehensive experiments showing the superiority of diffusion model features\ncompared to other pretrained models. Project page:\nhttps://bcorrad.github.io/freesegdiff/\n","authors":["Barbara Toniella Corradini","Mustafa Shukor","Paul Couairon","Guillaume Couairon","Franco Scarselli","Matthieu Cord"],"pdf_url":"https://arxiv.org/pdf/2403.20105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20101v1","updated":"2024-03-29T10:31:32Z","published":"2024-03-29T10:31:32Z","title":"RealKIE: Five Novel Datasets for Enterprise Key Information Extraction","summary":"  We introduce RealKIE, a benchmark of five challenging datasets aimed at\nadvancing key information extraction methods, with an emphasis on enterprise\napplications. The datasets include a diverse range of documents including SEC\nS1 Filings, US Non-disclosure Agreements, UK Charity Reports, FCC Invoices, and\nResource Contracts. Each presents unique challenges: poor text serialization,\nsparse annotations in long documents, and complex tabular layouts. These\ndatasets provide a realistic testing ground for key information extraction\ntasks like investment analysis and legal data processing.\n  In addition to presenting these datasets, we offer an in-depth description of\nthe annotation process, document processing techniques, and baseline modeling\napproaches. This contribution facilitates the development of NLP models capable\nof handling practical challenges and supports further research into information\nextraction technologies applicable to industry-specific problems.\n  The annotated data and OCR outputs are available to download at\nhttps://indicodatasolutions.github.io/RealKIE/ code to reproduce the baselines\nwill be available shortly.\n","authors":["Benjamin Townsend","Madison May","Christopher Wells"],"pdf_url":"https://arxiv.org/pdf/2403.20101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11600v2","updated":"2024-03-29T10:18:02Z","published":"2023-11-20T08:27:56Z","title":"Deep Equilibrium Diffusion Restoration with Parallel Sampling","summary":"  Diffusion model-based image restoration (IR) aims to use diffusion models to\nrecover high-quality (HQ) images from degraded images, achieving promising\nperformance. Due to the inherent property of diffusion models, most existing\nmethods need long serial sampling chains to restore HQ images step-by-step,\nresulting in expensive sampling time and high computation costs. Moreover, such\nlong sampling chains hinder understanding the relationship between inputs and\nrestoration results since it is hard to compute the gradients in the whole\nchains. In this work, we aim to rethink the diffusion model-based IR models\nthrough a different perspective, i.e., a deep equilibrium (DEQ) fixed point\nsystem, called DeqIR. Specifically, we derive an analytical solution by\nmodeling the entire sampling chain in these IR models as a joint multivariate\nfixed point system. Based on the analytical solution, we can conduct parallel\nsampling and restore HQ images without training. Furthermore, we compute fast\ngradients via DEQ inversion and found that initialization optimization can\nboost image quality and control the generation direction. Extensive experiments\non benchmarks demonstrate the effectiveness of our method on typical IR tasks\nand real-world settings.\n","authors":["Jiezhang Cao","Yue Shi","Kai Zhang","Yulun Zhang","Radu Timofte","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2311.11600v2.pdf","comment":"CVPR'2024"},{"id":"http://arxiv.org/abs/2311.01025v2","updated":"2024-03-29T10:09:33Z","published":"2023-11-02T06:38:19Z","title":"Integrating Language-Derived Appearance Elements with Visual Cues in\n  Pedestrian Detection","summary":"  Large language models (LLMs) have shown their capabilities in understanding\ncontextual and semantic information regarding knowledge of instance\nappearances. In this paper, we introduce a novel approach to utilize the\nstrengths of LLMs in understanding contextual appearance variations and to\nleverage this knowledge into a vision model (here, pedestrian detection). While\npedestrian detection is considered one of the crucial tasks directly related to\nour safety (e.g., intelligent driving systems), it is challenging because of\nvarying appearances and poses in diverse scenes. Therefore, we propose to\nformulate language-derived appearance elements and incorporate them with visual\ncues in pedestrian detection. To this end, we establish a description corpus\nthat includes numerous narratives describing various appearances of pedestrians\nand other instances. By feeding them through an LLM, we extract appearance\nknowledge sets that contain the representations of appearance variations.\nSubsequently, we perform a task-prompting process to obtain appearance elements\nwhich are guided representative appearance knowledge relevant to a downstream\npedestrian detection task. The obtained knowledge elements are adaptable to\nvarious detection frameworks, so that we can provide plentiful appearance\ninformation by integrating the language-derived appearance elements with visual\ncues within a detector. Through comprehensive experiments with various\npedestrian detectors, we verify the adaptability and effectiveness of our\nmethod showing noticeable performance gains and achieving state-of-the-art\ndetection performance on two public pedestrian detection benchmarks (i.e.,\nCrowdHuman and WiderPedestrian).\n","authors":["Sungjune Park","Hyunjun Kim","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2311.01025v2.pdf","comment":"11 pages, 5 figures, 5 tables"},{"id":"http://arxiv.org/abs/2312.03203v2","updated":"2024-03-29T10:09:30Z","published":"2023-12-06T00:46:30Z","title":"Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled\n  Feature Fields","summary":"  3D scene representations have gained immense popularity in recent years.\nMethods that use Neural Radiance fields are versatile for traditional tasks\nsuch as novel view synthesis. In recent times, some work has emerged that aims\nto extend the functionality of NeRF beyond view synthesis, for semantically\naware tasks such as editing and segmentation using 3D feature field\ndistillation from 2D foundation models. However, these methods have two major\nlimitations: (a) they are limited by the rendering speed of NeRF pipelines, and\n(b) implicitly represented feature fields suffer from continuity artifacts\nreducing feature quality. Recently, 3D Gaussian Splatting has shown\nstate-of-the-art performance on real-time radiance field rendering. In this\nwork, we go one step further: in addition to radiance field rendering, we\nenable 3D Gaussian splatting on arbitrary-dimension semantic features via 2D\nfoundation model distillation. This translation is not straightforward: naively\nincorporating feature fields in the 3DGS framework encounters significant\nchallenges, notably the disparities in spatial resolution and channel\nconsistency between RGB images and feature maps. We propose architectural and\ntraining changes to efficiently avert this problem. Our proposed method is\ngeneral, and our experiments showcase novel view semantic segmentation,\nlanguage-guided editing and segment anything through learning feature fields\nfrom state-of-the-art 2D foundation models such as SAM and CLIP-LSeg. Across\nexperiments, our distillation method is able to provide comparable or better\nresults, while being significantly faster to both train and render.\nAdditionally, to the best of our knowledge, we are the first method to enable\npoint and bounding-box prompting for radiance field manipulation, by leveraging\nthe SAM model. Project website at: https://feature-3dgs.github.io/\n","authors":["Shijie Zhou","Haoran Chang","Sicheng Jiang","Zhiwen Fan","Zehao Zhu","Dejia Xu","Pradyumna Chari","Suya You","Zhangyang Wang","Achuta Kadambi"],"pdf_url":"https://arxiv.org/pdf/2312.03203v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20092v1","updated":"2024-03-29T10:05:29Z","published":"2024-03-29T10:05:29Z","title":"Modeling Weather Uncertainty for Multi-weather Co-Presence Estimation","summary":"  Images from outdoor scenes may be taken under various weather conditions. It\nis well studied that weather impacts the performance of computer vision\nalgorithms and needs to be handled properly. However, existing algorithms model\nweather condition as a discrete status and estimate it using multi-label\nclassification. The fact is that, physically, specifically in meteorology,\nweather are modeled as a continuous and transitional status. Instead of\ndirectly implementing hard classification as existing multi-weather\nclassification methods do, we consider the physical formulation of\nmulti-weather conditions and model the impact of physical-related parameter on\nlearning from the image appearance. In this paper, we start with solid revisit\nof the physics definition of weather and how it can be described as a\ncontinuous machine learning and computer vision task. Namely, we propose to\nmodel the weather uncertainty, where the level of probability and co-existence\nof multiple weather conditions are both considered. A Gaussian mixture model is\nused to encapsulate the weather uncertainty and a uncertainty-aware\nmulti-weather learning scheme is proposed based on prior-posterior learning. A\nnovel multi-weather co-presence estimation transformer (MeFormer) is proposed.\nIn addition, a new multi-weather co-presence estimation (MePe) dataset, along\nwith 14 fine-grained weather categories and 16,078 samples, is proposed to\nbenchmark both conventional multi-label weather classification task and\nmulti-weather co-presence estimation task. Large scale experiments show that\nthe proposed method achieves state-of-the-art performance and substantial\ngeneralization capabilities on both the conventional multi-label weather\nclassification task and the proposed multi-weather co-presence estimation task.\nBesides, modeling weather uncertainty also benefits adverse-weather semantic\nsegmentation.\n","authors":["Qi Bi","Shaodi You","Theo Gevers"],"pdf_url":"https://arxiv.org/pdf/2403.20092v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2309.13604v2","updated":"2024-03-29T09:59:34Z","published":"2023-09-24T10:48:20Z","title":"Distribution-Aware Continual Test-Time Adaptation for Semantic\n  Segmentation","summary":"  Since autonomous driving systems usually face dynamic and ever-changing\nenvironments, continual test-time adaptation (CTTA) has been proposed as a\nstrategy for transferring deployed models to continually changing target\ndomains. However, the pursuit of long-term adaptation often introduces\ncatastrophic forgetting and error accumulation problems, which impede the\npractical implementation of CTTA in the real world. Recently, existing CTTA\nmethods mainly focus on utilizing a majority of parameters to fit target domain\nknowledge through self-training. Unfortunately, these approaches often amplify\nthe challenge of error accumulation due to noisy pseudo-labels, and pose\npractical limitations stemming from the heavy computational costs associated\nwith entire model updates. In this paper, we propose a distribution-aware\ntuning (DAT) method to make the semantic segmentation CTTA efficient and\npractical in real-world applications. DAT adaptively selects and updates two\nsmall groups of trainable parameters based on data distribution during the\ncontinual adaptation process, including domain-specific parameters (DSP) and\ntask-relevant parameters (TRP). Specifically, DSP exhibits sensitivity to\noutputs with substantial distribution shifts, effectively mitigating the\nproblem of error accumulation. In contrast, TRP are allocated to positions that\nare responsive to outputs with minor distribution shifts, which are fine-tuned\nto avoid the catastrophic forgetting problem. In addition, since CTTA is a\ntemporal task, we introduce the Parameter Accumulation Update (PAU) strategy to\ncollect the updated DSP and TRP in target domain sequences. We conduct\nextensive experiments on two widely-used semantic segmentation CTTA benchmarks,\nachieving promising performance compared to previous state-of-the-art methods.\n","authors":["Jiayi Ni","Senqiao Yang","Ran Xu","Jiaming Liu","Xiaoqi Li","Wenyu Jiao","Zehui Chen","Yi Liu","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2309.13604v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20086v1","updated":"2024-03-29T09:46:14Z","published":"2024-03-29T09:46:14Z","title":"Selective Attention-based Modulation for Continual Learning","summary":"  We present SAM, a biologically-plausible selective attention-driven\nmodulation approach to enhance classification models in a continual learning\nsetting. Inspired by neurophysiological evidence that the primary visual cortex\ndoes not contribute to object manifold untangling for categorization and that\nprimordial attention biases are still embedded in the modern brain, we propose\nto employ auxiliary saliency prediction features as a modulation signal to\ndrive and stabilize the learning of a sequence of non-i.i.d. classification\ntasks. Experimental results confirm that SAM effectively enhances the\nperformance (in some cases up to about twenty percent points) of\nstate-of-the-art continual learning methods, both in class-incremental and\ntask-incremental settings. Moreover, we show that attention-based modulation\nsuccessfully encourages the learning of features that are more robust to the\npresence of spurious features and to adversarial attacks than baseline methods.\nCode is available at: https://github.com/perceivelab/SAM.\n","authors":["Giovanni Bellitto","Federica Proietto Salanitri","Matteo Pennisi","Matteo Boschini","Angelo Porrello","Simone Calderara","Simone Palazzo","Concetto Spampinato"],"pdf_url":"https://arxiv.org/pdf/2403.20086v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20080v1","updated":"2024-03-29T09:22:44Z","published":"2024-03-29T09:22:44Z","title":"Mixed-precision Supernet Training from Vision Foundation Models using\n  Low Rank Adapter","summary":"  Compression of large and performant vision foundation models (VFMs) into\narbitrary bit-wise operations (BitOPs) allows their deployment on various\nhardware. We propose to fine-tune a VFM to a mixed-precision quantized\nsupernet. The supernet-based neural architecture search (NAS) can be adopted\nfor this purpose, which trains a supernet, and then subnets within arbitrary\nhardware budgets can be extracted. However, existing methods face difficulties\nin optimizing the mixed-precision search space and incurring large memory costs\nduring training. To tackle these challenges, first, we study the effective\nsearch space design for fine-tuning a VFM by comparing different operators\n(such as resolution, feature size, width, depth, and bit-widths) in terms of\nperformance and BitOPs reduction. Second, we propose memory-efficient supernet\ntraining using a low-rank adapter (LoRA) and a progressive training strategy.\nThe proposed method is evaluated for the recently proposed VFM, Segment\nAnything Model, fine-tuned on segmentation tasks. The searched model yields\nabout a 95% reduction in BitOPs without incurring performance degradation.\n","authors":["Yuiko Sakuma","Masakazu Yoshimura","Junji Otsuka","Atsushi Irie","Takeshi Ohashi"],"pdf_url":"https://arxiv.org/pdf/2403.20080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20079v1","updated":"2024-03-29T09:20:29Z","published":"2024-03-29T09:20:29Z","title":"SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior","summary":"  Novel View Synthesis (NVS) for street scenes play a critical role in the\nautonomous driving simulation. The current mainstream technique to achieve it\nis neural rendering, such as Neural Radiance Fields (NeRF) and 3D Gaussian\nSplatting (3DGS). Although thrilling progress has been made, when handling\nstreet scenes, current methods struggle to maintain rendering quality at the\nviewpoint that deviates significantly from the training viewpoints. This issue\nstems from the sparse training views captured by a fixed camera on a moving\nvehicle. To tackle this problem, we propose a novel approach that enhances the\ncapacity of 3DGS by leveraging prior from a Diffusion Model along with\ncomplementary multi-modal data. Specifically, we first fine-tune a Diffusion\nModel by adding images from adjacent frames as condition, meanwhile exploiting\ndepth data from LiDAR point clouds to supply additional spatial information.\nThen we apply the Diffusion Model to regularize the 3DGS at unseen views during\ntraining. Experimental results validate the effectiveness of our method\ncompared with current state-of-the-art models, and demonstrate its advance in\nrendering images from broader views.\n","authors":["Zhongrui Yu","Haoran Wang","Jinze Yang","Hanzhang Wang","Zeke Xie","Yunfeng Cai","Jiale Cao","Zhong Ji","Mingming Sun"],"pdf_url":"https://arxiv.org/pdf/2403.20079v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20078v1","updated":"2024-03-29T09:19:52Z","published":"2024-03-29T09:19:52Z","title":"Negative Label Guided OOD Detection with Pretrained Vision-Language\n  Models","summary":"  Out-of-distribution (OOD) detection aims at identifying samples from unknown\nclasses, playing a crucial role in trustworthy models against errors on\nunexpected inputs. Extensive research has been dedicated to exploring OOD\ndetection in the vision modality. Vision-language models (VLMs) can leverage\nboth textual and visual information for various multi-modal applications,\nwhereas few OOD detection methods take into account information from the text\nmodality. In this paper, we propose a novel post hoc OOD detection method,\ncalled NegLabel, which takes a vast number of negative labels from extensive\ncorpus databases. We design a novel scheme for the OOD score collaborated with\nnegative labels. Theoretical analysis helps to understand the mechanism of\nnegative labels. Extensive experiments demonstrate that our method NegLabel\nachieves state-of-the-art performance on various OOD detection benchmarks and\ngeneralizes well on multiple VLM architectures. Furthermore, our method\nNegLabel exhibits remarkable robustness against diverse domain shifts. The\ncodes are available at https://github.com/tmlr-group/NegLabel.\n","authors":["Xue Jiang","Feng Liu","Zhen Fang","Hong Chen","Tongliang Liu","Feng Zheng","Bo Han"],"pdf_url":"https://arxiv.org/pdf/2403.20078v1.pdf","comment":"ICLR 2024 Spotlight"},{"id":"http://arxiv.org/abs/2403.20058v1","updated":"2024-03-29T08:47:49Z","published":"2024-03-29T08:47:49Z","title":"Revolutionizing Disease Diagnosis with simultaneous functional PET/MR\n  and Deeply Integrated Brain Metabolic, Hemodynamic, and Perfusion Networks","summary":"  Simultaneous functional PET/MR (sf-PET/MR) presents a cutting-edge multimodal\nneuroimaging technique. It provides an unprecedented opportunity for\nconcurrently monitoring and integrating multifaceted brain networks built by\nspatiotemporally covaried metabolic activity, neural activity, and cerebral\nblood flow (perfusion). Albeit high scientific/clinical values, short in\nhardware accessibility of PET/MR hinders its applications, let alone modern\nAI-based PET/MR fusion models. Our objective is to develop a clinically\nfeasible AI-based disease diagnosis model trained on comprehensive sf-PET/MR\ndata with the power of, during inferencing, allowing single modality input\n(e.g., PET only) as well as enforcing multimodal-based accuracy. To this end,\nwe propose MX-ARM, a multimodal MiXture-of-experts Alignment and Reconstruction\nModel. It is modality detachable and exchangeable, allocating different\nmulti-layer perceptrons dynamically (\"mixture of experts\") through learnable\nweights to learn respective representations from different modalities. Such\ndesign will not sacrifice model performance in uni-modal situation. To fully\nexploit the inherent complex and nonlinear relation among modalities while\nproducing fine-grained representations for uni-modal inference, we subsequently\nadd a modal alignment module to line up a dominant modality (e.g., PET) with\nrepresentations of auxiliary modalities (MR). We further adopt multimodal\nreconstruction to promote the quality of learned features. Experiments on\nprecious multimodal sf-PET/MR data for Mild Cognitive Impairment diagnosis\nshowcase the efficacy of our model toward clinically feasible precision\nmedicine.\n","authors":["Luoyu Wang","Yitian Tao","Qing Yang","Yan Liang","Siwei Liu","Hongcheng Shi","Dinggang Shen","Han Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.20058v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2209.11964v2","updated":"2024-03-29T08:46:46Z","published":"2022-09-24T08:57:10Z","title":"Strong Transferable Adversarial Attacks via Ensembled Asymptotically\n  Normal Distribution Learning","summary":"  Strong adversarial examples are crucial for evaluating and enhancing the\nrobustness of deep neural networks. However, the performance of popular attacks\nis usually sensitive, for instance, to minor image transformations, stemming\nfrom limited information -- typically only one input example, a handful of\nwhite-box source models, and undefined defense strategies. Hence, the crafted\nadversarial examples are prone to overfit the source model, which hampers their\ntransferability to unknown architectures. In this paper, we propose an approach\nnamed Multiple Asymptotically Normal Distribution Attacks (MultiANDA) which\nexplicitly characterize adversarial perturbations from a learned distribution.\nSpecifically, we approximate the posterior distribution over the perturbations\nby taking advantage of the asymptotic normality property of stochastic gradient\nascent (SGA), then employ the deep ensemble strategy as an effective proxy for\nBayesian marginalization in this process, aiming to estimate a mixture of\nGaussians that facilitates a more thorough exploration of the potential\noptimization space. The approximated posterior essentially describes the\nstationary distribution of SGA iterations, which captures the geometric\ninformation around the local optimum. Thus, MultiANDA allows drawing an\nunlimited number of adversarial perturbations for each input and reliably\nmaintains the transferability. Our proposed method outperforms ten\nstate-of-the-art black-box attacks on deep learning models with or without\ndefenses through extensive experiments on seven normally trained and seven\ndefense models.\n","authors":["Zhengwei Fang","Rui Wang","Tao Huang","Liping Jing"],"pdf_url":"https://arxiv.org/pdf/2209.11964v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16653v2","updated":"2024-03-29T08:39:23Z","published":"2023-09-28T17:55:05Z","title":"DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content\n  Creation","summary":"  Recent advances in 3D content creation mostly leverage optimization-based 3D\ngeneration via score distillation sampling (SDS). Though promising results have\nbeen exhibited, these methods often suffer from slow per-sample optimization,\nlimiting their practical usage. In this paper, we propose DreamGaussian, a\nnovel 3D content generation framework that achieves both efficiency and quality\nsimultaneously. Our key insight is to design a generative 3D Gaussian Splatting\nmodel with companioned mesh extraction and texture refinement in UV space. In\ncontrast to the occupancy pruning used in Neural Radiance Fields, we\ndemonstrate that the progressive densification of 3D Gaussians converges\nsignificantly faster for 3D generative tasks. To further enhance the texture\nquality and facilitate downstream applications, we introduce an efficient\nalgorithm to convert 3D Gaussians into textured meshes and apply a fine-tuning\nstage to refine the details. Extensive experiments demonstrate the superior\nefficiency and competitive generation quality of our proposed approach.\nNotably, DreamGaussian produces high-quality textured meshes in just 2 minutes\nfrom a single-view image, achieving approximately 10 times acceleration\ncompared to existing methods.\n","authors":["Jiaxiang Tang","Jiawei Ren","Hang Zhou","Ziwei Liu","Gang Zeng"],"pdf_url":"https://arxiv.org/pdf/2309.16653v2.pdf","comment":"Camera-ready version. Project page: https://dreamgaussian.github.io/"},{"id":"http://arxiv.org/abs/2311.14671v2","updated":"2024-03-29T08:36:41Z","published":"2023-11-24T18:59:42Z","title":"SEGIC: Unleashing the Emergent Correspondence for In-Context\n  Segmentation","summary":"  In-context segmentation aims at segmenting novel images using a few labeled\nexample images, termed as \"in-context examples\", exploring content similarities\nbetween examples and the target. The resulting models can be generalized\nseamlessly to novel segmentation tasks, significantly reducing the labeling and\ntraining costs compared with conventional pipelines. However, in-context\nsegmentation is more challenging than classic ones requiring the model to learn\nsegmentation rules conditioned on a few samples. Unlike previous work with\nad-hoc or non-end-to-end designs, we propose SEGIC, an end-to-end\nsegment-in-context framework built upon a single vision foundation model (VFM).\nIn particular, SEGIC leverages the emergent correspondence within VFM to\ncapture dense relationships between target images and in-context samples. As\nsuch, information from in-context samples is then extracted into three types of\ninstructions, i.e. geometric, visual, and meta instructions, serving as\nexplicit conditions for the final mask prediction. SEGIC is a straightforward\nyet effective approach that yields state-of-the-art performance on one-shot\nsegmentation benchmarks. Notably, SEGIC can be easily generalized to diverse\ntasks, including video object segmentation and open-vocabulary segmentation.\nCode will be available at https://github.com/MengLcool/SEGIC.\n","authors":["Lingchen Meng","Shiyi Lan","Hengduo Li","Jose M. Alvarez","Zuxuan Wu","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2311.14671v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20047v1","updated":"2024-03-29T08:33:05Z","published":"2024-03-29T08:33:05Z","title":"Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real\n  World","summary":"  Sparse training has emerged as a promising method for resource-efficient deep\nneural networks (DNNs) in real-world applications. However, the reliability of\nsparse models remains a crucial concern, particularly in detecting unknown\nout-of-distribution (OOD) data. This study addresses the knowledge gap by\ninvestigating the reliability of sparse training from an OOD perspective and\nreveals that sparse training exacerbates OOD unreliability. The lack of unknown\ninformation and the sparse constraints hinder the effective exploration of\nweight space and accurate differentiation between known and unknown knowledge.\nTo tackle these challenges, we propose a new unknown-aware sparse training\nmethod, which incorporates a loss modification, auto-tuning strategy, and a\nvoting scheme to guide weight space exploration and mitigate confusion between\nknown and unknown information without incurring significant additional costs or\nrequiring access to additional OOD data. Theoretical insights demonstrate how\nour method reduces model confidence when faced with OOD samples. Empirical\nexperiments across multiple datasets, model architectures, and sparsity levels\nvalidate the effectiveness of our method, with improvements of up to\n\\textbf{8.4\\%} in AUROC while maintaining comparable or higher accuracy and\ncalibration. This research enhances the understanding and readiness of sparse\nDNNs for deployment in resource-limited applications. Our code is available on:\n\\url{https://github.com/StevenBoys/MOON}.\n","authors":["Bowen Lei","Dongkuan Xu","Ruqi Zhang","Bani Mallick"],"pdf_url":"https://arxiv.org/pdf/2403.20047v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09866v2","updated":"2024-03-29T08:25:36Z","published":"2023-12-15T15:09:30Z","title":"PLGSLAM: Progressive Neural Scene Represenation with Local to Global\n  Bundle Adjustment","summary":"  Neural implicit scene representations have recently shown encouraging results\nin dense visual SLAM. However, existing methods produce low-quality scene\nreconstruction and low-accuracy localization performance when scaling up to\nlarge indoor scenes and long sequences. These limitations are mainly due to\ntheir single, global radiance field with finite capacity, which does not adapt\nto large scenarios. Their end-to-end pose networks are also not robust enough\nwith the growth of cumulative errors in large scenes. To this end, we introduce\nPLGSLAM, a neural visual SLAM system capable of high-fidelity surface\nreconstruction and robust camera tracking in real-time. To handle large-scale\nindoor scenes, PLGSLAM proposes a progressive scene representation method which\ndynamically allocates new local scene representation trained with frames within\na local sliding window. This allows us to scale up to larger indoor scenes and\nimproves robustness (even under pose drifts). In local scene representation,\nPLGSLAM utilizes tri-planes for local high-frequency features with multi-layer\nperceptron (MLP) networks for the low-frequency feature, achieving smoothness\nand scene completion in unobserved areas. Moreover, we propose local-to-global\nbundle adjustment method with a global keyframe database to address the\nincreased pose drifts on long sequences. Experimental results demonstrate that\nPLGSLAM achieves state-of-the-art scene reconstruction results and tracking\nperformance across various datasets and scenarios (both in small and\nlarge-scale indoor environments).\n","authors":["Tianchen Deng","Guole Shen","Tong Qin","Jianyu Wang","Wentao Zhao","Jingchuan Wang","Danwei Wang","Weidong Chen"],"pdf_url":"https://arxiv.org/pdf/2312.09866v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2304.14178v3","updated":"2024-03-29T08:13:38Z","published":"2023-04-27T13:27:01Z","title":"mPLUG-Owl: Modularization Empowers Large Language Models with\n  Multimodality","summary":"  Large language models (LLMs) have demonstrated impressive zero-shot abilities\non a variety of open-ended tasks, while recent research has also explored the\nuse of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl,\na novel training paradigm that equips LLMs with multi-modal abilities through\nmodularized learning of foundation LLM, a visual knowledge module, and a visual\nabstractor module. This approach can support multiple modalities and facilitate\ndiverse unimodal and multimodal abilities through modality collaboration. The\ntraining paradigm of mPLUG-Owl involves a two-stage method for aligning image\nand text, which learns visual knowledge with the assistance of LLM while\nmaintaining and even improving the generation abilities of LLM. In the first\nstage, the visual knowledge module and abstractor module are trained with a\nfrozen LLM module to align the image and text. In the second stage,\nlanguage-only and multi-modal supervised datasets are used to jointly fine-tune\na low-rank adaption (LoRA) module on LLM and the abstractor module by freezing\nthe visual knowledge module. We carefully build a visually-related instruction\nevaluation set OwlEval. Experimental results show that our model outperforms\nexisting multi-modal models, demonstrating mPLUG-Owl's impressive instruction\nand visual understanding ability, multi-turn conversation ability, and\nknowledge reasoning ability. Besides, we observe some unexpected and exciting\nabilities such as multi-image correlation and scene text understanding, which\nmakes it possible to leverage it for harder real scenarios, such as vision-only\ndocument comprehension. Our code, pre-trained model, instruction-tuned models,\nand evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The\nonline demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.\n","authors":["Qinghao Ye","Haiyang Xu","Guohai Xu","Jiabo Ye","Ming Yan","Yiyang Zhou","Junyang Wang","Anwen Hu","Pengcheng Shi","Yaya Shi","Chenliang Li","Yuanhong Xu","Hehong Chen","Junfeng Tian","Qi Qian","Ji Zhang","Fei Huang","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2304.14178v3.pdf","comment":"Working in Process"},{"id":"http://arxiv.org/abs/2402.18918v2","updated":"2024-03-29T08:06:38Z","published":"2024-02-29T07:20:02Z","title":"SNE-RoadSegV2: Advancing Heterogeneous Feature Fusion and Fallibility\n  Awareness for Freespace Detection","summary":"  Feature-fusion networks with duplex encoders have proven to be an effective\ntechnique to solve the freespace detection problem. However, despite the\ncompelling results achieved by previous research efforts, the exploration of\nadequate and discriminative heterogeneous feature fusion, as well as the\ndevelopment of fallibility-aware loss functions remains relatively scarce. This\npaper makes several significant contributions to address these limitations: (1)\nIt presents a novel heterogeneous feature fusion block, comprising a holistic\nattention module, a heterogeneous feature contrast descriptor, and an\naffinity-weighted feature recalibrator, enabling a more in-depth exploitation\nof the inherent characteristics of the extracted features, (2) it incorporates\nboth inter-scale and intra-scale skip connections into the decoder architecture\nwhile eliminating redundant ones, leading to both improved accuracy and\ncomputational efficiency, and (3) it introduces two fallibility-aware loss\nfunctions that separately focus on semantic-transition and depth-inconsistent\nregions, collectively contributing to greater supervision during model\ntraining. Our proposed heterogeneous feature fusion network (SNE-RoadSegV2),\nwhich incorporates all these innovative components, demonstrates superior\nperformance in comparison to all other freespace detection algorithms across\nmultiple public datasets. Notably, it ranks the 1st on the official KITTI Road\nbenchmark.\n","authors":["Yi Feng","Yu Ma","Qijun Chen","Ioannis Pitas","Rui Fan"],"pdf_url":"https://arxiv.org/pdf/2402.18918v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20035v1","updated":"2024-03-29T08:03:42Z","published":"2024-03-29T08:03:42Z","title":"UltraLight VM-UNet: Parallel Vision Mamba Significantly Reduces\n  Parameters for Skin Lesion Segmentation","summary":"  Traditionally for improving the segmentation performance of models, most\napproaches prefer to use adding more complex modules. And this is not suitable\nfor the medical field, especially for mobile medical devices, where\ncomputationally loaded models are not suitable for real clinical environments\ndue to computational resource constraints. Recently, state-space models (SSMs),\nrepresented by Mamba, have become a strong competitor to traditional CNNs and\nTransformers. In this paper, we deeply explore the key elements of parameter\ninfluence in Mamba and propose an UltraLight Vision Mamba UNet (UltraLight\nVM-UNet) based on this. Specifically, we propose a method for processing\nfeatures in parallel Vision Mamba, named PVM Layer, which achieves excellent\nperformance with the lowest computational load while keeping the overall number\nof processing channels constant. We conducted comparisons and ablation\nexperiments with several state-of-the-art lightweight models on three skin\nlesion public datasets and demonstrated that the UltraLight VM-UNet exhibits\nthe same strong performance competitiveness with parameters of only 0.049M and\nGFLOPs of 0.060. In addition, this study deeply explores the key elements of\nparameter influence in Mamba, which will lay a theoretical foundation for Mamba\nto possibly become a new mainstream module for lightweighting in the future.\nThe code is available from https://github.com/wurenkai/UltraLight-VM-UNet .\n","authors":["Renkai Wu","Yinghao Liu","Pengchen Liang","Qing Chang"],"pdf_url":"https://arxiv.org/pdf/2403.20035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18795v2","updated":"2024-03-29T08:02:14Z","published":"2024-03-27T17:40:14Z","title":"Gamba: Marry Gaussian Splatting with Mamba for single view 3D\n  reconstruction","summary":"  We tackle the challenge of efficiently reconstructing a 3D asset from a\nsingle image with growing demands for automated 3D content creation pipelines.\nPrevious methods primarily rely on Score Distillation Sampling (SDS) and Neural\nRadiance Fields (NeRF). Despite their significant success, these approaches\nencounter practical limitations due to lengthy optimization and considerable\nmemory usage. In this report, we introduce Gamba, an end-to-end amortized 3D\nreconstruction model from single-view images, emphasizing two main insights:\n(1) 3D representation: leveraging a large number of 3D Gaussians for an\nefficient 3D Gaussian splatting process; (2) Backbone design: introducing a\nMamba-based sequential network that facilitates context-dependent reasoning and\nlinear scalability with the sequence (token) length, accommodating a\nsubstantial number of Gaussians. Gamba incorporates significant advancements in\ndata preprocessing, regularization design, and training methodologies. We\nassessed Gamba against existing optimization-based and feed-forward 3D\ngeneration approaches using the real-world scanned OmniObject3D dataset. Here,\nGamba demonstrates competitive generation capabilities, both qualitatively and\nquantitatively, while achieving remarkable speed, approximately 0.6 second on a\nsingle NVIDIA A100 GPU.\n","authors":["Qiuhong Shen","Xuanyu Yi","Zike Wu","Pan Zhou","Hanwang Zhang","Shuicheng Yan","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18795v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20034v1","updated":"2024-03-29T07:59:37Z","published":"2024-03-29T07:59:37Z","title":"NeSLAM: Neural Implicit Mapping and Self-Supervised Feature Tracking\n  With Depth Completion and Denoising","summary":"  In recent years, there have been significant advancements in 3D\nreconstruction and dense RGB-D SLAM systems. One notable development is the\napplication of Neural Radiance Fields (NeRF) in these systems, which utilizes\nimplicit neural representation to encode 3D scenes. This extension of NeRF to\nSLAM has shown promising results. However, the depth images obtained from\nconsumer-grade RGB-D sensors are often sparse and noisy, which poses\nsignificant challenges for 3D reconstruction and affects the accuracy of the\nrepresentation of the scene geometry. Moreover, the original hierarchical\nfeature grid with occupancy value is inaccurate for scene geometry\nrepresentation. Furthermore, the existing methods select random pixels for\ncamera tracking, which leads to inaccurate localization and is not robust in\nreal-world indoor environments. To this end, we present NeSLAM, an advanced\nframework that achieves accurate and dense depth estimation, robust camera\ntracking, and realistic synthesis of novel views. First, a depth completion and\ndenoising network is designed to provide dense geometry prior and guide the\nneural implicit representation optimization. Second, the occupancy scene\nrepresentation is replaced with Signed Distance Field (SDF) hierarchical scene\nrepresentation for high-quality reconstruction and view synthesis. Furthermore,\nwe also propose a NeRF-based self-supervised feature tracking algorithm for\nrobust real-time tracking. Experiments on various indoor datasets demonstrate\nthe effectiveness and accuracy of the system in reconstruction, tracking\nquality, and novel view synthesis.\n","authors":["Tianchen Deng","Yanbo Wang","Hongle Xie","Hesheng Wang","Jingchuan Wang","Danwei Wang","Weidong Chen"],"pdf_url":"https://arxiv.org/pdf/2403.20034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20032v1","updated":"2024-03-29T07:58:21Z","published":"2024-03-29T07:58:21Z","title":"HO-Gaussian: Hybrid Optimization of 3D Gaussian Splatting for Urban\n  Scenes","summary":"  The rapid growth of 3D Gaussian Splatting (3DGS) has revolutionized neural\nrendering, enabling real-time production of high-quality renderings. However,\nthe previous 3DGS-based methods have limitations in urban scenes due to\nreliance on initial Structure-from-Motion(SfM) points and difficulties in\nrendering distant, sky and low-texture areas. To overcome these challenges, we\npropose a hybrid optimization method named HO-Gaussian, which combines a\ngrid-based volume with the 3DGS pipeline. HO-Gaussian eliminates the dependency\non SfM point initialization, allowing for rendering of urban scenes, and\nincorporates the Point Densitification to enhance rendering quality in\nproblematic regions during training. Furthermore, we introduce Gaussian\nDirection Encoding as an alternative for spherical harmonics in the rendering\npipeline, which enables view-dependent color representation. To account for\nmulti-camera systems, we introduce neural warping to enhance object consistency\nacross different cameras. Experimental results on widely used autonomous\ndriving datasets demonstrate that HO-Gaussian achieves photo-realistic\nrendering in real-time on multi-camera urban datasets.\n","authors":["Zhuopeng Li","Yilin Zhang","Chenming Wu","Jianke Zhu","Liangjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.20032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20031v1","updated":"2024-03-29T07:53:06Z","published":"2024-03-29T07:53:06Z","title":"A Unified Framework for Human-centric Point Cloud Video Understanding","summary":"  Human-centric Point Cloud Video Understanding (PVU) is an emerging field\nfocused on extracting and interpreting human-related features from sequences of\nhuman point clouds, further advancing downstream human-centric tasks and\napplications. Previous works usually focus on tackling one specific task and\nrely on huge labeled data, which has poor generalization capability.\nConsidering that human has specific characteristics, including the structural\nsemantics of human body and the dynamics of human motions, we propose a unified\nframework to make full use of the prior knowledge and explore the inherent\nfeatures in the data itself for generalized human-centric point cloud video\nunderstanding. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performance on various human-related tasks, including action\nrecognition and 3D pose estimation. All datasets and code will be released\nsoon.\n","authors":["Yiteng Xu","Kecheng Ye","Xiao Han","Yiming Ren","Xinge Zhu","Yuexin Ma"],"pdf_url":"https://arxiv.org/pdf/2403.20031v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17496v2","updated":"2024-03-29T07:38:21Z","published":"2024-03-26T08:53:25Z","title":"Dr.Hair: Reconstructing Scalp-Connected Hair Strands without\n  Pre-training via Differentiable Rendering of Line Segments","summary":"  In the film and gaming industries, achieving a realistic hair appearance\ntypically involves the use of strands originating from the scalp. However,\nreconstructing these strands from observed surface images of hair presents\nsignificant challenges. The difficulty in acquiring Ground Truth (GT) data has\nled state-of-the-art learning-based methods to rely on pre-training with\nmanually prepared synthetic CG data. This process is not only labor-intensive\nand costly but also introduces complications due to the domain gap when\ncompared to real-world data. In this study, we propose an optimization-based\napproach that eliminates the need for pre-training. Our method represents hair\nstrands as line segments growing from the scalp and optimizes them using a\nnovel differentiable rendering algorithm. To robustly optimize a substantial\nnumber of slender explicit geometries, we introduce 3D orientation estimation\nutilizing global optimization, strand initialization based on Laplace's\nequation, and reparameterization that leverages geometric connectivity and\nspatial proximity. Unlike existing optimization-based methods, our method is\ncapable of reconstructing internal hair flow in an absolute direction. Our\nmethod exhibits robust and accurate inverse rendering, surpassing the quality\nof existing methods and significantly improving processing speed.\n","authors":["Yusuke Takimoto","Hikari Takehara","Hiroyuki Sato","Zihao Zhu","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.17496v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.20026v1","updated":"2024-03-29T07:28:50Z","published":"2024-03-29T07:28:50Z","title":"FSMR: A Feature Swapping Multi-modal Reasoning Approach with Joint\n  Textual and Visual Clues","summary":"  Multi-modal reasoning plays a vital role in bridging the gap between textual\nand visual information, enabling a deeper understanding of the context. This\npaper presents the Feature Swapping Multi-modal Reasoning (FSMR) model,\ndesigned to enhance multi-modal reasoning through feature swapping. FSMR\nleverages a pre-trained visual-language model as an encoder, accommodating both\ntext and image inputs for effective feature representation from both\nmodalities. It introduces a unique feature swapping module, enabling the\nexchange of features between identified objects in images and corresponding\nvocabulary words in text, thereby enhancing the model's comprehension of the\ninterplay between images and text. To further bolster its multi-modal alignment\ncapabilities, FSMR incorporates a multi-modal cross-attention mechanism,\nfacilitating the joint modeling of textual and visual information. During\ntraining, we employ image-text matching and cross-entropy losses to ensure\nsemantic consistency between visual and language elements. Extensive\nexperiments on the PMR dataset demonstrate FSMR's superiority over\nstate-of-the-art baseline models across various performance metrics.\n","authors":["Shuang Li","Jiahua Wang","Lijie Wen"],"pdf_url":"https://arxiv.org/pdf/2403.20026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10908v2","updated":"2024-03-29T07:24:23Z","published":"2023-12-18T03:34:07Z","title":"CLOVA: A Closed-Loop Visual Assistant with Tool Usage and Update","summary":"  Utilizing large language models (LLMs) to compose off-the-shelf visual tools\nrepresents a promising avenue of research for developing robust visual\nassistants capable of addressing diverse visual tasks. However, these methods\noften overlook the potential for continual learning, typically by freezing the\nutilized tools, thus limiting their adaptation to environments requiring new\nknowledge. To tackle this challenge, we propose CLOVA, a Closed-Loop Visual\nAssistant, which operates within a framework encompassing inference,\nreflection, and learning phases. During the inference phase, LLMs generate\nprograms and execute corresponding tools to complete assigned tasks. In the\nreflection phase, a multimodal global-local reflection scheme analyzes human\nfeedback to determine which tools require updating. Lastly, the learning phase\nemploys three flexible approaches to automatically gather training data and\nintroduces a novel prompt tuning scheme to update the tools, allowing CLOVA to\nefficiently acquire new knowledge. Experimental findings demonstrate that CLOVA\nsurpasses existing tool-usage methods by 5% in visual question answering and\nmultiple-image reasoning, by 10% in knowledge tagging, and by 20% in image\nediting. These results underscore the significance of the continual learning\ncapability in general visual assistants.\n","authors":["Zhi Gao","Yuntao Du","Xintong Zhang","Xiaojian Ma","Wenjuan Han","Song-Chun Zhu","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2312.10908v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.15082v3","updated":"2024-03-29T07:20:42Z","published":"2024-03-22T10:06:31Z","title":"Cell Variational Information Bottleneck Network","summary":"  In this work, we propose Cell Variational Information Bottleneck Network\n(cellVIB), a convolutional neural network using information bottleneck\nmechanism, which can be combined with the latest feedforward network\narchitecture in an end-to-end training method. Our Cell Variational Information\nBottleneck Network is constructed by stacking VIB cells, which generate feature\nmaps with uncertainty. As layers going deeper, the regularization effect will\ngradually increase, instead of directly adding excessive regular constraints to\nthe output layer of the model as in Deep VIB. Under each VIB cell, the\nfeedforward process learns an independent mean term and an standard deviation\nterm, and predicts the Gaussian distribution based on them. The feedback\nprocess is based on reparameterization trick for effective training. This work\nperforms an extensive analysis on MNIST dataset to verify the effectiveness of\neach VIB cells, and provides an insightful analysis on how the VIB cells affect\nmutual information. Experiments conducted on CIFAR-10 also prove that our\ncellVIB is robust against noisy labels during training and against corrupted\nimages during testing. Then, we validate our method on PACS dataset, whose\nresults show that the VIB cells can significantly improve the generalization\nperformance of the basic model. Finally, in a more complex representation\nlearning task, face recognition, our network structure has also achieved very\ncompetitive results.\n","authors":["Zhonghua Zhai","Chen Ju","Jinsong Lan","Shuai Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.15082v3.pdf","comment":"Found errors in the article, therefore postponing publication for now"},{"id":"http://arxiv.org/abs/2403.20022v1","updated":"2024-03-29T07:16:34Z","published":"2024-03-29T07:16:34Z","title":"Psychometry: An Omnifit Model for Image Reconstruction from Human Brain\n  Activity","summary":"  Reconstructing the viewed images from human brain activity bridges human and\ncomputer vision through the Brain-Computer Interface. The inherent variability\nin brain function between individuals leads existing literature to focus on\nacquiring separate models for each individual using their respective brain\nsignal data, ignoring commonalities between these data. In this article, we\ndevise Psychometry, an omnifit model for reconstructing images from functional\nMagnetic Resonance Imaging (fMRI) obtained from different subjects. Psychometry\nincorporates an omni mixture-of-experts (Omni MoE) module where all the experts\nwork together to capture the inter-subject commonalities, while each expert\nassociated with subject-specific parameters copes with the individual\ndifferences. Moreover, Psychometry is equipped with a retrieval-enhanced\ninference strategy, termed Ecphory, which aims to enhance the learned fMRI\nrepresentation via retrieving from prestored subject-specific memories. These\ndesigns collectively render Psychometry omnifit and efficient, enabling it to\ncapture both inter-subject commonality and individual specificity across\nsubjects. As a result, the enhanced fMRI representations serve as conditional\nsignals to guide a generation model to reconstruct high-quality and realistic\nimages, establishing Psychometry as state-of-the-art in terms of both\nhigh-level and low-level metrics.\n","authors":["Ruijie Quan","Wenguan Wang","Zhibo Tian","Fan Ma","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2403.20022v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.20018v1","updated":"2024-03-29T07:14:14Z","published":"2024-03-29T07:14:14Z","title":"SCINeRF: Neural Radiance Fields from a Snapshot Compressive Image","summary":"  In this paper, we explore the potential of Snapshot Compressive Imaging (SCI)\ntechnique for recovering the underlying 3D scene representation from a single\ntemporal compressed image. SCI is a cost-effective method that enables the\nrecording of high-dimensional data, such as hyperspectral or temporal\ninformation, into a single image using low-cost 2D imaging sensors. To achieve\nthis, a series of specially designed 2D masks are usually employed, which not\nonly reduces storage requirements but also offers potential privacy protection.\nInspired by this, to take one step further, our approach builds upon the\npowerful 3D scene representation capabilities of neural radiance fields (NeRF).\nSpecifically, we formulate the physical imaging process of SCI as part of the\ntraining of NeRF, allowing us to exploit its impressive performance in\ncapturing complex scene structures. To assess the effectiveness of our method,\nwe conduct extensive evaluations using both synthetic data and real data\ncaptured by our SCI system. Extensive experimental results demonstrate that our\nproposed approach surpasses the state-of-the-art methods in terms of image\nreconstruction and novel view image synthesis. Moreover, our method also\nexhibits the ability to restore high frame-rate multi-view consistent images by\nleveraging SCI and the rendering capabilities of NeRF. The code is available at\nhttps://github.com/WU-CVGL/SCINeRF.\n","authors":["Yunhao Li","Xiaodong Wang","Ping Wang","Xin Yuan","Peidong Liu"],"pdf_url":"https://arxiv.org/pdf/2403.20018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20013v1","updated":"2024-03-29T06:58:57Z","published":"2024-03-29T06:58:57Z","title":"DerainNeRF: 3D Scene Estimation with Adhesive Waterdrop Removal","summary":"  When capturing images through the glass during rainy or snowy weather\nconditions, the resulting images often contain waterdrops adhered on the glass\nsurface, and these waterdrops significantly degrade the image quality and\nperformance of many computer vision algorithms. To tackle these limitations, we\npropose a method to reconstruct the clear 3D scene implicitly from multi-view\nimages degraded by waterdrops. Our method exploits an attention network to\npredict the location of waterdrops and then train a Neural Radiance Fields to\nrecover the 3D scene implicitly. By leveraging the strong scene representation\ncapabilities of NeRF, our method can render high-quality novel-view images with\nwaterdrops removed. Extensive experimental results on both synthetic and real\ndatasets show that our method is able to generate clear 3D scenes and\noutperforms existing state-of-the-art (SOTA) image adhesive waterdrop removal\nmethods.\n","authors":["Yunhao Li","Jing Wu","Lingzhe Zhao","Peidong Liu"],"pdf_url":"https://arxiv.org/pdf/2403.20013v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20012v1","updated":"2024-03-29T06:53:52Z","published":"2024-03-29T06:53:52Z","title":"Colorful Cutout: Enhancing Image Data Augmentation with Curriculum\n  Learning","summary":"  Data augmentation is one of the regularization strategies for the training of\ndeep learning models, which enhances generalizability and prevents overfitting,\nleading to performance improvement. Although researchers have proposed various\ndata augmentation techniques, they often lack consideration for the difficulty\nof augmented data. Recently, another line of research suggests incorporating\nthe concept of curriculum learning with data augmentation in the field of\nnatural language processing. In this study, we adopt curriculum data\naugmentation for image data augmentation and propose colorful cutout, which\ngradually increases the noise and difficulty introduced in the augmented image.\nOur experimental results highlight the possibility of curriculum data\naugmentation for image data. We publicly released our source code to improve\nthe reproducibility of our study.\n","authors":["Juhwan Choi","YoungBin Kim"],"pdf_url":"https://arxiv.org/pdf/2403.20012v1.pdf","comment":"ICLR 2024 Tiny Papers"},{"id":"http://arxiv.org/abs/2403.12236v2","updated":"2024-03-29T06:41:07Z","published":"2024-03-18T20:33:44Z","title":"Improving Generalization via Meta-Learning on Hard Samples","summary":"  Learned reweighting (LRW) approaches to supervised learning use an\noptimization criterion to assign weights for training instances, in order to\nmaximize performance on a representative validation dataset. We pose and\nformalize the problem of optimized selection of the validation set used in LRW\ntraining, to improve classifier generalization. In particular, we show that\nusing hard-to-classify instances in the validation set has both a theoretical\nconnection to, and strong empirical evidence of generalization. We provide an\nefficient algorithm for training this meta-optimized model, as well as a simple\ntrain-twice heuristic for careful comparative study. We demonstrate that LRW\nwith easy validation data performs consistently worse than LRW with hard\nvalidation data, establishing the validity of our meta-optimization problem.\nOur proposed algorithm outperforms a wide range of baselines on a range of\ndatasets and domain shift challenges (Imagenet-1K, CIFAR-100, Clothing-1M,\nCAMELYON, WILDS, etc.), with ~1% gains using VIT-B on Imagenet. We also show\nthat using naturally hard examples for validation (Imagenet-R / Imagenet-A) in\nLRW training for Imagenet improves performance on both clean and naturally hard\ntest instances by 1-2%. Secondary analyses show that using hard validation data\nin an LRW framework improves margins on test data, hinting at the mechanism\nunderlying our empirical gains. We believe this work opens up new research\ndirections for the meta-optimization of meta-learning in a supervised learning\ncontext.\n","authors":["Nishant Jain","Arun S. Suggala","Pradeep Shenoy"],"pdf_url":"https://arxiv.org/pdf/2403.12236v2.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.20002v1","updated":"2024-03-29T06:33:13Z","published":"2024-03-29T06:33:13Z","title":"Grounding and Enhancing Grid-based Models for Neural Fields","summary":"  Many contemporary studies utilize grid-based models for neural field\nrepresentation, but a systematic analysis of grid-based models is still\nmissing, hindering the improvement of those models. Therefore, this paper\nintroduces a theoretical framework for grid-based models. This framework points\nout that these models' approximation and generalization behaviors are\ndetermined by grid tangent kernels (GTK), which are intrinsic properties of\ngrid-based models. The proposed framework facilitates a consistent and\nsystematic analysis of diverse grid-based models. Furthermore, the introduced\nframework motivates the development of a novel grid-based model named the\nMultiplicative Fourier Adaptive Grid (MulFAGrid). The numerical analysis\ndemonstrates that MulFAGrid exhibits a lower generalization bound than its\npredecessors, indicating its robust generalization performance. Empirical\nstudies reveal that MulFAGrid achieves state-of-the-art performance in various\ntasks, including 2D image fitting, 3D signed distance field (SDF)\nreconstruction, and novel view synthesis, demonstrating superior representation\nability. The project website is available at\nhttps://sites.google.com/view/cvpr24-2034-submission/home.\n","authors":["Zelin Zhao","Fenglei Fan","Wenlong Liao","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2403.20002v1.pdf","comment":"Accepted in CVPR24"},{"id":"http://arxiv.org/abs/2312.13528v2","updated":"2024-03-29T05:57:33Z","published":"2023-12-21T02:01:19Z","title":"DyBluRF: Dynamic Deblurring Neural Radiance Fields for Blurry Monocular\n  Video","summary":"  Neural Radiance Fields (NeRF), initially developed for static scenes, have\ninspired many video novel view synthesis techniques. However, the challenge for\nvideo view synthesis arises from motion blur, a consequence of object or camera\nmovement during exposure, which hinders the precise synthesis of sharp\nspatio-temporal views. In response, we propose a novel dynamic deblurring NeRF\nframework for blurry monocular video, called DyBluRF, consisting of a Base Ray\nInitialization (BRI) stage and a Motion Decomposition-based Deblurring (MDD)\nstage. Our DyBluRF is the first that handles the novel view synthesis for\nblurry monocular video with a novel two-stage framework. In the BRI stage, we\ncoarsely reconstruct dynamic 3D scenes and jointly initialize the base ray,\nwhich is further used to predict latent sharp rays, using the inaccurate camera\npose information from the given blurry frames. In the MDD stage, we introduce a\nnovel Incremental Latent Sharp-rays Prediction (ILSP) approach for the blurry\nmonocular video frames by decomposing the latent sharp rays into global camera\nmotion and local object motion components. We further propose two loss\nfunctions for effective geometry regularization and decomposition of static and\ndynamic scene components without any mask supervision. Experiments show that\nDyBluRF outperforms qualitatively and quantitatively the SOTA methods.\n","authors":["Minh-Quan Viet Bui","Jongmin Park","Jihyong Oh","Munchurl Kim"],"pdf_url":"https://arxiv.org/pdf/2312.13528v2.pdf","comment":"The first two authors contributed equally to this work (equal\n  contribution). The last two authors advised equally to this work. Please\n  visit our project page at https://kaist-viclab.github.io/dyblurf-site/"},{"id":"http://arxiv.org/abs/2403.19985v1","updated":"2024-03-29T05:39:47Z","published":"2024-03-29T05:39:47Z","title":"Stable Surface Regularization for Fast Few-Shot NeRF","summary":"  This paper proposes an algorithm for synthesizing novel views under few-shot\nsetup. The main concept is to develop a stable surface regularization technique\ncalled Annealing Signed Distance Function (ASDF), which anneals the surface in\na coarse-to-fine manner to accelerate convergence speed. We observe that the\nEikonal loss - which is a widely known geometric regularization - requires\ndense training signal to shape different level-sets of SDF, leading to\nlow-fidelity results under few-shot training. In contrast, the proposed surface\nregularization successfully reconstructs scenes and produce high-fidelity\ngeometry with stable training. Our method is further accelerated by utilizing\ngrid representation and monocular geometric priors. Finally, the proposed\napproach is up to 45 times faster than existing few-shot novel view synthesis\nmethods, and it produces comparable results in the ScanNet dataset and\nNeRF-Real dataset.\n","authors":["Byeongin Joung","Byeong-Uk Lee","Jaesung Choe","Ukcheol Shin","Minjun Kang","Taeyeop Lee","In So Kweon","Kuk-Jin Yoon"],"pdf_url":"https://arxiv.org/pdf/2403.19985v1.pdf","comment":"3DV 2024"},{"id":"http://arxiv.org/abs/2403.19983v1","updated":"2024-03-29T05:35:04Z","published":"2024-03-29T05:35:04Z","title":"A multi-stage semi-supervised learning for ankle fracture classification\n  on CT images","summary":"  Because of the complicated mechanism of ankle injury, it is very difficult to\ndiagnose ankle fracture in clinic. In order to simplify the process of fracture\ndiagnosis, an automatic diagnosis model of ankle fracture was proposed.\nFirstly, a tibia-fibula segmentation network is proposed for the joint\ntibiofibular region of the ankle joint, and the corresponding segmentation\ndataset is established on the basis of fracture data. Secondly, the image\nregistration method is used to register the bone segmentation mask with the\nnormal bone mask. Finally, a semi-supervised classifier is constructed to make\nfull use of a large number of unlabeled data to classify ankle fractures.\nExperiments show that the proposed method can segment fractures with fracture\nlines accurately and has better performance than the general method. At the\nsame time, this method is superior to classification network in several\nindexes.\n","authors":["Hongzhi Liu","Guicheng Li","Jiacheng Nie","Hui Tang","Chunfeng Yang","Qianjin Feng","Hailin Xu","Yang Chen"],"pdf_url":"https://arxiv.org/pdf/2403.19983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19980v1","updated":"2024-03-29T05:23:34Z","published":"2024-03-29T05:23:34Z","title":"A Parallel Attention Network for Cattle Face Recognition","summary":"  Cattle face recognition holds paramount significance in domains such as\nanimal husbandry and behavioral research. Despite significant progress in\nconfined environments, applying these accomplishments in wild settings remains\nchallenging. Thus, we create the first large-scale cattle face recognition\ndataset, ICRWE, for wild environments. It encompasses 483 cattle and 9,816\nhigh-resolution image samples. Each sample undergoes annotation for face\nfeatures, light conditions, and face orientation. Furthermore, we introduce a\nnovel parallel attention network, PANet. Comprising several cascaded\nTransformer modules, each module incorporates two parallel Position Attention\nModules (PAM) and Feature Mapping Modules (FMM). PAM focuses on local and\nglobal features at each image position through parallel channel attention, and\nFMM captures intricate feature patterns through non-linear mappings.\nExperimental results indicate that PANet achieves a recognition accuracy of\n88.03% on the ICRWE dataset, establishing itself as the current\nstate-of-the-art approach. The source code is available in the supplementary\nmaterials.\n","authors":["Jiayu Li","Xuechao Zou","Shiying Wang","Ben Chen","Junliang Xing","Pin Tao"],"pdf_url":"https://arxiv.org/pdf/2403.19980v1.pdf","comment":"Accepted by ICME 2024"},{"id":"http://arxiv.org/abs/2403.19979v1","updated":"2024-03-29T05:23:12Z","published":"2024-03-29T05:23:12Z","title":"Semantically-Shifted Incremental Adapter-Tuning is A Continual\n  ViTransformer","summary":"  Class-incremental learning (CIL) aims to enable models to continuously learn\nnew classes while overcoming catastrophic forgetting. The introduction of\npre-trained models has brought new tuning paradigms to CIL. In this paper, we\nrevisit different parameter-efficient tuning (PET) methods within the context\nof continual learning. We observe that adapter tuning demonstrates superiority\nover prompt-based methods, even without parameter expansion in each learning\nsession. Motivated by this, we propose incrementally tuning the shared adapter\nwithout imposing parameter update constraints, enhancing the learning capacity\nof the backbone. Additionally, we employ feature sampling from stored\nprototypes to retrain a unified classifier, further improving its performance.\nWe estimate the semantic shift of old prototypes without access to past samples\nand update stored prototypes session by session. Our proposed method eliminates\nmodel expansion and avoids retaining any image samples. It surpasses previous\npre-trained model-based CIL methods and demonstrates remarkable continual\nlearning capabilities. Experimental results on five CIL benchmarks validate the\neffectiveness of our approach, achieving state-of-the-art (SOTA) performance.\n","authors":["Yuwen Tan","Qinhao Zhou","Xiang Xiang","Ke Wang","Yuchuan Wu","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2403.19979v1.pdf","comment":"To appear at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.16558v2","updated":"2024-03-29T05:12:45Z","published":"2024-03-25T09:17:15Z","title":"Elysium: Exploring Object-level Perception in Videos via MLLM","summary":"  Multi-modal Large Language Models (MLLMs) have demonstrated their ability to\nperceive objects in still images, but their application in video-related tasks,\nsuch as object tracking, remains understudied. This lack of exploration is\nprimarily due to two key challenges. Firstly, extensive pretraining on\nlarge-scale video datasets is required to equip MLLMs with the capability to\nperceive objects across multiple frames and understand inter-frame\nrelationships. Secondly, processing a large number of frames within the context\nwindow of Large Language Models (LLMs) can impose a significant computational\nburden. To address the first challenge, we introduce ElysiumTrack-1M, a\nlarge-scale video dataset supported for three tasks: Single Object Tracking\n(SOT), Referring Single Object Tracking (RSOT), and Video Referring Expression\nGeneration (Video-REG). ElysiumTrack-1M contains 1.27 million annotated video\nframes with corresponding object boxes and descriptions. Leveraging this\ndataset, we conduct training of MLLMs and propose a token-compression model\nT-Selector to tackle the second challenge. Our proposed approach, Elysium:\nExploring Object-level Perception in Videos via MLLM, is an end-to-end\ntrainable MLLM that attempts to conduct object-level tasks in videos without\nrequiring any additional plug-in or expert models. All codes and datasets are\navailable at https://github.com/Hon-Wong/Elysium.\n","authors":["Han Wang","Yanjie Wang","Yongjie Ye","Yuxiang Nie","Can Huang"],"pdf_url":"https://arxiv.org/pdf/2403.16558v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19976v1","updated":"2024-03-29T04:58:56Z","published":"2024-03-29T04:58:56Z","title":"eTraM: Event-based Traffic Monitoring Dataset","summary":"  Event cameras, with their high temporal and dynamic range and minimal memory\nusage, have found applications in various fields. However, their potential in\nstatic traffic monitoring remains largely unexplored. To facilitate this\nexploration, we present eTraM - a first-of-its-kind, fully event-based traffic\nmonitoring dataset. eTraM offers 10 hr of data from different traffic scenarios\nin various lighting and weather conditions, providing a comprehensive overview\nof real-world situations. Providing 2M bounding box annotations, it covers\neight distinct classes of traffic participants, ranging from vehicles to\npedestrians and micro-mobility. eTraM's utility has been assessed using\nstate-of-the-art methods for traffic participant detection, including RVT, RED,\nand YOLOv8. We quantitatively evaluate the ability of event-based models to\ngeneralize on nighttime and unseen scenes. Our findings substantiate the\ncompelling potential of leveraging event cameras for traffic monitoring,\nopening new avenues for research and application. eTraM is available at\nhttps://eventbasedvision.github.io/eTraM\n","authors":["Aayush Atul Verma","Bharatesh Chakravarthi","Arpitsinh Vaghela","Hua Wei","Yezhou Yang"],"pdf_url":"https://arxiv.org/pdf/2403.19976v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19975v1","updated":"2024-03-29T04:58:33Z","published":"2024-03-29T04:58:33Z","title":"Context-Aware Integration of Language and Visual References for Natural\n  Language Tracking","summary":"  Tracking by natural language specification (TNL) aims to consistently\nlocalize a target in a video sequence given a linguistic description in the\ninitial frame. Existing methodologies perform language-based and template-based\nmatching for target reasoning separately and merge the matching results from\ntwo sources, which suffer from tracking drift when language and visual\ntemplates miss-align with the dynamic target state and ambiguity in the later\nmerging stage. To tackle the issues, we propose a joint multi-modal tracking\nframework with 1) a prompt modulation module to leverage the complementarity\nbetween temporal visual templates and language expressions, enabling precise\nand context-aware appearance and linguistic cues, and 2) a unified target\ndecoding module to integrate the multi-modal reference cues and executes the\nintegrated queries on the search image to predict the target location in an\nend-to-end manner directly. This design ensures spatio-temporal consistency by\nleveraging historical visual information and introduces an integrated solution,\ngenerating predictions in a single step. Extensive experiments conducted on\nTNL2K, OTB-Lang, LaSOT, and RefCOCOg validate the efficacy of our proposed\napproach. The results demonstrate competitive performance against\nstate-of-the-art methods for both tracking and grounding.\n","authors":["Yanyan Shao","Shuting He","Qi Ye","Yuchao Feng","Wenhan Luo","Jiming Chen"],"pdf_url":"https://arxiv.org/pdf/2403.19975v1.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2311.17132v2","updated":"2024-03-29T04:55:51Z","published":"2023-11-28T18:03:27Z","title":"TransNeXt: Robust Foveal Visual Perception for Vision Transformers","summary":"  Due to the depth degradation effect in residual connections, many efficient\nVision Transformers models that rely on stacking layers for information\nexchange often fail to form sufficient information mixing, leading to unnatural\nvisual perception. To address this issue, in this paper, we propose Aggregated\nAttention, a biomimetic design-based token mixer that simulates biological\nfoveal vision and continuous eye movement while enabling each token on the\nfeature map to have a global perception. Furthermore, we incorporate learnable\ntokens that interact with conventional queries and keys, which further\ndiversifies the generation of affinity matrices beyond merely relying on the\nsimilarity between queries and keys. Our approach does not rely on stacking for\ninformation exchange, thus effectively avoiding depth degradation and achieving\nnatural visual perception. Additionally, we propose Convolutional GLU, a\nchannel mixer that bridges the gap between GLU and SE mechanism, which empowers\neach token to have channel attention based on its nearest neighbor image\nfeatures, enhancing local modeling capability and model robustness. We combine\naggregated attention and convolutional GLU to create a new visual backbone\ncalled TransNeXt. Extensive experiments demonstrate that our TransNeXt achieves\nstate-of-the-art performance across multiple model sizes. At a resolution of\n$224^2$, TransNeXt-Tiny attains an ImageNet accuracy of 84.0%, surpassing\nConvNeXt-B with 69% fewer parameters. Our TransNeXt-Base achieves an ImageNet\naccuracy of 86.2% and an ImageNet-A accuracy of 61.6% at a resolution of\n$384^2$, a COCO object detection mAP of 57.1, and an ADE20K semantic\nsegmentation mIoU of 54.7.\n","authors":["Dai Shi"],"pdf_url":"https://arxiv.org/pdf/2311.17132v2.pdf","comment":"CVPR 2024 Camera-ready Version. Project Page:\n  https://github.com/DaiShiResearch/TransNeXt"},{"id":"http://arxiv.org/abs/2312.11461v2","updated":"2024-03-29T04:32:57Z","published":"2023-12-18T18:59:12Z","title":"GAvatar: Animatable 3D Gaussian Avatars with Implicit Mesh Learning","summary":"  Gaussian splatting has emerged as a powerful 3D representation that harnesses\nthe advantages of both explicit (mesh) and implicit (NeRF) 3D representations.\nIn this paper, we seek to leverage Gaussian splatting to generate realistic\nanimatable avatars from textual descriptions, addressing the limitations (e.g.,\nflexibility and efficiency) imposed by mesh or NeRF-based representations.\nHowever, a naive application of Gaussian splatting cannot generate high-quality\nanimatable avatars and suffers from learning instability; it also cannot\ncapture fine avatar geometries and often leads to degenerate body parts. To\ntackle these problems, we first propose a primitive-based 3D Gaussian\nrepresentation where Gaussians are defined inside pose-driven primitives to\nfacilitate animation. Second, to stabilize and amortize the learning of\nmillions of Gaussians, we propose to use neural implicit fields to predict the\nGaussian attributes (e.g., colors). Finally, to capture fine avatar geometries\nand extract detailed meshes, we propose a novel SDF-based implicit mesh\nlearning approach for 3D Gaussians that regularizes the underlying geometries\nand extracts highly detailed textured meshes. Our proposed method, GAvatar,\nenables the large-scale generation of diverse animatable avatars using only\ntext prompts. GAvatar significantly surpasses existing methods in terms of both\nappearance and geometry quality, and achieves extremely fast rendering (100\nfps) at 1K resolution.\n","authors":["Ye Yuan","Xueting Li","Yangyi Huang","Shalini De Mello","Koki Nagano","Jan Kautz","Umar Iqbal"],"pdf_url":"https://arxiv.org/pdf/2312.11461v2.pdf","comment":"CVPR 2024. Project website: https://nvlabs.github.io/GAvatar"},{"id":"http://arxiv.org/abs/2403.19969v1","updated":"2024-03-29T04:28:06Z","published":"2024-03-29T04:28:06Z","title":"Separate, Dynamic and Differentiable (SMART) Pruner for Block/Output\n  Channel Pruning on Computer Vision Tasks","summary":"  Deep Neural Network (DNN) pruning has emerged as a key strategy to reduce\nmodel size, improve inference latency, and lower power consumption on DNN\naccelerators. Among various pruning techniques, block and output channel\npruning have shown significant potential in accelerating hardware performance.\nHowever, their accuracy often requires further improvement. In response to this\nchallenge, we introduce a separate, dynamic and differentiable (SMART) pruner.\nThis pruner stands out by utilizing a separate, learnable probability mask for\nweight importance ranking, employing a differentiable Top k operator to achieve\ntarget sparsity, and leveraging a dynamic temperature parameter trick to escape\nfrom non-sparse local minima. In our experiments, the SMART pruner consistently\ndemonstrated its superiority over existing pruning methods across a wide range\nof tasks and models on block and output channel pruning. Additionally, we\nextend our testing to Transformer-based models in N:M pruning scenarios, where\nSMART pruner also yields state-of-the-art results, demonstrating its\nadaptability and robustness across various neural network architectures, and\npruning types.\n","authors":["Guanhua Ding","Zexi Ye","Zhen Zhong","Gang Li","David Shao"],"pdf_url":"https://arxiv.org/pdf/2403.19969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10521v3","updated":"2024-03-29T04:21:27Z","published":"2024-03-15T17:59:53Z","title":"P-MapNet: Far-seeing Map Generator Enhanced by both SDMap and HDMap\n  Priors","summary":"  Autonomous vehicles are gradually entering city roads today, with the help of\nhigh-definition maps (HDMaps). However, the reliance on HDMaps prevents\nautonomous vehicles from stepping into regions without this expensive digital\ninfrastructure. This fact drives many researchers to study online HDMap\ngeneration algorithms, but the performance of these algorithms at far regions\nis still unsatisfying. We present P-MapNet, in which the letter P highlights\nthe fact that we focus on incorporating map priors to improve model\nperformance. Specifically, we exploit priors in both SDMap and HDMap. On one\nhand, we extract weakly aligned SDMap from OpenStreetMap, and encode it as an\nadditional conditioning branch. Despite the misalignment challenge, our\nattention-based architecture adaptively attends to relevant SDMap skeletons and\nsignificantly improves performance. On the other hand, we exploit a masked\nautoencoder to capture the prior distribution of HDMap, which can serve as a\nrefinement module to mitigate occlusions and artifacts. We benchmark on the\nnuScenes and Argoverse2 datasets. Through comprehensive experiments, we show\nthat: (1) our SDMap prior can improve online map generation performance, using\nboth rasterized (by up to $+18.73$ $\\rm mIoU$) and vectorized (by up to $+8.50$\n$\\rm mAP$) output representations. (2) our HDMap prior can improve map\nperceptual metrics by up to $6.34\\%$. (3) P-MapNet can be switched into\ndifferent inference modes that covers different regions of the\naccuracy-efficiency trade-off landscape. (4) P-MapNet is a far-seeing solution\nthat brings larger improvements on longer ranges. Codes and models are publicly\navailable at https://jike5.github.io/P-MapNet.\n","authors":["Zhou Jiang","Zhenxin Zhu","Pengfei Li","Huan-ang Gao","Tianyuan Yuan","Yongliang Shi","Hang Zhao","Hao Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.10521v3.pdf","comment":"Code: https://jike5.github.io/P-MapNet"},{"id":"http://arxiv.org/abs/2211.13398v3","updated":"2024-03-29T04:13:49Z","published":"2022-11-24T03:27:00Z","title":"CPPF++: Uncertainty-Aware Sim2Real Object Pose Estimation by Vote\n  Aggregation","summary":"  Object pose estimation constitutes a critical area within the domain of 3D\nvision. While contemporary state-of-the-art methods that leverage real-world\npose annotations have demonstrated commendable performance, the procurement of\nsuch real training data incurs substantial costs. This paper focuses on a\nspecific setting wherein only 3D CAD models are utilized as a priori knowledge,\ndevoid of any background or clutter information. We introduce a novel method,\nCPPF++, designed for sim-to-real pose estimation. This method builds upon the\nfoundational point-pair voting scheme of CPPF, reformulating it through a\nprobabilistic view. To address the challenge posed by vote collision, we\npropose a novel approach that involves modeling the voting uncertainty by\nestimating the probabilistic distribution of each point pair within the\ncanonical space. Furthermore, we augment the contextual information provided by\neach voting unit through the introduction of N-point tuples. To enhance the\nrobustness and accuracy of the model, we incorporate several innovative\nmodules, including noisy pair filtering, online alignment optimization, and a\ntuple feature ensemble. Alongside these methodological advancements, we\nintroduce a new category-level pose estimation dataset, named DiversePose 300.\nEmpirical evidence demonstrates that our method significantly surpasses\nprevious sim-to-real approaches and achieves comparable or superior performance\non novel datasets. Our code is available on https://github.com/qq456cvb/CPPF2.\n","authors":["Yang You","Wenhao He","Jin Liu","Hongkai Xiong","Weiming Wang","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2211.13398v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19967v1","updated":"2024-03-29T04:10:07Z","published":"2024-03-29T04:10:07Z","title":"Rewrite the Stars","summary":"  Recent studies have drawn attention to the untapped potential of the \"star\noperation\" (element-wise multiplication) in network design. While intuitive\nexplanations abound, the foundational rationale behind its application remains\nlargely unexplored. Our study attempts to reveal the star operation's ability\nto map inputs into high-dimensional, non-linear feature spaces -- akin to\nkernel tricks -- without widening the network. We further introduce StarNet, a\nsimple yet powerful prototype, demonstrating impressive performance and low\nlatency under compact network structure and efficient budget. Like stars in the\nsky, the star operation appears unremarkable but holds a vast universe of\npotential. Our work encourages further exploration across tasks, with codes\navailable at https://github.com/ma-xu/Rewrite-the-Stars.\n","authors":["Xu Ma","Xiyang Dai","Yue Bai","Yizhou Wang","Yun Fu"],"pdf_url":"https://arxiv.org/pdf/2403.19967v1.pdf","comment":"Accepted by CVPR 2024. Codes are made publically available at\n  https://github.com/ma-xu/Rewrite-the-Stars"},{"id":"http://arxiv.org/abs/2311.16714v2","updated":"2024-03-29T04:07:25Z","published":"2023-11-28T11:53:56Z","title":"Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld","summary":"  While large language models (LLMs) excel in a simulated world of texts, they\nstruggle to interact with the more realistic world without perceptions of other\nmodalities such as visual or audio signals. Although vision-language models\n(VLMs) integrate LLM modules (1) aligned with static image features, and (2)\nmay possess prior knowledge of world dynamics (as demonstrated in the text\nworld), they have not been trained in an embodied visual world and thus cannot\nalign with its dynamics. On the other hand, training an embodied agent in a\nnoisy visual world without expert guidance is often challenging and\ninefficient. In this paper, we train a VLM agent living in a visual world using\nan LLM agent excelling in a parallel text world. Specifically, we distill LLM's\nreflection outcomes (improved actions by analyzing mistakes) in a text world's\ntasks to finetune the VLM on the same tasks of the visual world, resulting in\nan Embodied Multi-Modal Agent (EMMA) quickly adapting to the visual world\ndynamics. Such cross-modality imitation learning between the two parallel\nworlds is achieved by a novel DAgger-DPO algorithm, enabling EMMA to generalize\nto a broad scope of new tasks without any further guidance from the LLM expert.\nExtensive evaluations on the ALFWorld benchmark's diverse tasks highlight\nEMMA's superior performance to SOTA VLM-based agents, e.g., 20%-70% improvement\nin the success rate.\n","authors":["Yijun Yang","Tianyi Zhou","Kanxue Li","Dapeng Tao","Lusong Li","Li Shen","Xiaodong He","Jing Jiang","Yuhui Shi"],"pdf_url":"https://arxiv.org/pdf/2311.16714v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.19966v1","updated":"2024-03-29T04:02:51Z","published":"2024-03-29T04:02:51Z","title":"Multi-task Magnetic Resonance Imaging Reconstruction using Meta-learning","summary":"  Using single-task deep learning methods to reconstruct Magnetic Resonance\nImaging (MRI) data acquired with different imaging sequences is inherently\nchallenging. The trained deep learning model typically lacks generalizability,\nand the dissimilarity among image datasets with different types of contrast\nleads to suboptimal learning performance. This paper proposes a meta-learning\napproach to efficiently learn image features from multiple MR image datasets.\nOur algorithm can perform multi-task learning to simultaneously reconstruct MR\nimages acquired using different imaging sequences with different image\ncontrasts. The experiment results demonstrate the ability of our new\nmeta-learning reconstruction method to successfully reconstruct\nhighly-undersampled k-space data from multiple MRI datasets simultaneously,\noutperforming other compelling reconstruction methods previously developed for\nsingle-task learning.\n","authors":["Wanyu Bian","Albert Jang","Fang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.19966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19964v1","updated":"2024-03-29T03:56:19Z","published":"2024-03-29T03:56:19Z","title":"FairRAG: Fair Human Generation via Fair Retrieval Augmentation","summary":"  Existing text-to-image generative models reflect or even amplify societal\nbiases ingrained in their training data. This is especially concerning for\nhuman image generation where models are biased against certain demographic\ngroups. Existing attempts to rectify this issue are hindered by the inherent\nlimitations of the pre-trained models and fail to substantially improve\ndemographic diversity. In this work, we introduce Fair Retrieval Augmented\nGeneration (FairRAG), a novel framework that conditions pre-trained generative\nmodels on reference images retrieved from an external image database to improve\nfairness in human generation. FairRAG enables conditioning through a\nlightweight linear module that projects reference images into the textual\nspace. To enhance fairness, FairRAG applies simple-yet-effective debiasing\nstrategies, providing images from diverse demographic groups during the\ngenerative process. Extensive experiments demonstrate that FairRAG outperforms\nexisting methods in terms of demographic diversity, image-text alignment, and\nimage fidelity while incurring minimal computational overhead during inference.\n","authors":["Robik Shrestha","Yang Zou","Qiuyu Chen","Zhiheng Li","Yusheng Xie","Siqi Deng"],"pdf_url":"https://arxiv.org/pdf/2403.19964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19963v1","updated":"2024-03-29T03:48:35Z","published":"2024-03-29T03:48:35Z","title":"Efficient Modulation for Vision Networks","summary":"  In this work, we present efficient modulation, a novel design for efficient\nvision networks. We revisit the modulation mechanism, which operates input\nthrough convolutional context modeling and feature projection layers, and fuses\nfeatures via element-wise multiplication and an MLP block. We demonstrate that\nthe modulation mechanism is particularly well suited for efficient networks and\nfurther tailor the modulation design by proposing the efficient modulation\n(EfficientMod) block, which is considered the essential building block for our\nnetworks. Benefiting from the prominent representational ability of modulation\nmechanism and the proposed efficient design, our network can accomplish better\ntrade-offs between accuracy and efficiency and set new state-of-the-art\nperformance in the zoo of efficient networks. When integrating EfficientMod\nwith the vanilla self-attention block, we obtain the hybrid architecture which\nfurther improves the performance without loss of efficiency. We carry out\ncomprehensive experiments to verify EfficientMod's performance. With fewer\nparameters, our EfficientMod-s performs 0.6 top-1 accuracy better than\nEfficientFormerV2-s2 and is 25% faster on GPU, and 2.9 better than\nMobileViTv2-1.0 at the same GPU latency. Additionally, our method presents a\nnotable improvement in downstream tasks, outperforming EfficientFormerV2-s by\n3.6 mIoU on the ADE20K benchmark. Code and checkpoints are available at\nhttps://github.com/ma-xu/EfficientMod.\n","authors":["Xu Ma","Xiyang Dai","Jianwei Yang","Bin Xiao","Yinpeng Chen","Yun Fu","Lu Yuan"],"pdf_url":"https://arxiv.org/pdf/2403.19963v1.pdf","comment":"Accepted by ICLR 2024. Codes are made publically available at\n  https://github.com/ma-xu/EfficientMod"},{"id":"http://arxiv.org/abs/2310.05916v4","updated":"2024-03-29T03:40:47Z","published":"2023-10-09T17:59:04Z","title":"Interpreting CLIP's Image Representation via Text-Based Decomposition","summary":"  We investigate the CLIP image encoder by analyzing how individual model\ncomponents affect the final representation. We decompose the image\nrepresentation as a sum across individual image patches, model layers, and\nattention heads, and use CLIP's text representation to interpret the summands.\nInterpreting the attention heads, we characterize each head's role by\nautomatically finding text representations that span its output space, which\nreveals property-specific roles for many heads (e.g. location or shape). Next,\ninterpreting the image patches, we uncover an emergent spatial localization\nwithin CLIP. Finally, we use this understanding to remove spurious features\nfrom CLIP and to create a strong zero-shot image segmenter. Our results\nindicate that a scalable understanding of transformer models is attainable and\ncan be used to repair and improve models.\n","authors":["Yossi Gandelsman","Alexei A. Efros","Jacob Steinhardt"],"pdf_url":"https://arxiv.org/pdf/2310.05916v4.pdf","comment":"Project page and code:\n  https://yossigandelsman.github.io/clip_decomposition/"},{"id":"http://arxiv.org/abs/2403.19949v1","updated":"2024-03-29T03:15:31Z","published":"2024-03-29T03:15:31Z","title":"FairCLIP: Harnessing Fairness in Vision-Language Learning","summary":"  Fairness is a critical concern in deep learning, especially in healthcare,\nwhere these models influence diagnoses and treatment decisions. Although\nfairness has been investigated in the vision-only domain, the fairness of\nmedical vision-language (VL) models remains unexplored due to the scarcity of\nmedical VL datasets for studying fairness. To bridge this research gap, we\nintroduce the first fair vision-language medical dataset FairVLMed that\nprovides detailed demographic attributes, ground-truth labels, and clinical\nnotes to facilitate an in-depth examination of fairness within VL foundation\nmodels. Using FairVLMed, we conduct a comprehensive fairness analysis of two\nwidely-used VL models (CLIP and BLIP2), pre-trained on both natural and medical\ndomains, across four different protected attributes. Our results highlight\nsignificant biases in all VL models, with Asian, Male, Non-Hispanic, and\nSpanish being the preferred subgroups across the protected attributes of race,\ngender, ethnicity, and language, respectively. In order to alleviate these\nbiases, we propose FairCLIP, an optimal-transport-based approach that achieves\na favorable trade-off between performance and fairness by reducing the Sinkhorn\ndistance between the overall sample distribution and the distributions\ncorresponding to each demographic group. As the first VL dataset of its kind,\nFairVLMed holds the potential to catalyze advancements in the development of\nmachine learning models that are both ethically aware and clinically effective.\nOur dataset and code are available at\nhttps://ophai.hms.harvard.edu/datasets/fairvlmed10k.\n","authors":["Yan Luo","Min Shi","Muhammad Osama Khan","Muhammad Muneeb Afzal","Hao Huang","Shuaihang Yuan","Yu Tian","Luo Song","Ava Kouhana","Tobias Elze","Yi Fang","Mengyu Wang"],"pdf_url":"https://arxiv.org/pdf/2403.19949v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.19944v1","updated":"2024-03-29T02:55:07Z","published":"2024-03-29T02:55:07Z","title":"Binarized Low-light Raw Video Enhancement","summary":"  Recently, deep neural networks have achieved excellent performance on\nlow-light raw video enhancement. However, they often come with high\ncomputational complexity and large memory costs, which hinder their\napplications on resource-limited devices. In this paper, we explore the\nfeasibility of applying the extremely compact binary neural network (BNN) to\nlow-light raw video enhancement. Nevertheless, there are two main issues with\nbinarizing video enhancement models. One is how to fuse the temporal\ninformation to improve low-light denoising without complex modules. The other\nis how to narrow the performance gap between binary convolutions with the full\nprecision ones. To address the first issue, we introduce a spatial-temporal\nshift operation, which is easy-to-binarize and effective. The temporal shift\nefficiently aggregates the features of neighbor frames and the spatial shift\nhandles the misalignment caused by the large motion in videos. For the second\nissue, we present a distribution-aware binary convolution, which captures the\ndistribution characteristics of real-valued input and incorporates them into\nplain binary convolutions to alleviate the degradation in performance.\nExtensive quantitative and qualitative experiments have shown our\nhigh-efficiency binarized low-light raw video enhancement method can attain a\npromising performance.\n","authors":["Gengchen Zhang","Yulun Zhang","Xin Yuan","Ying Fu"],"pdf_url":"https://arxiv.org/pdf/2403.19944v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.19935v1","updated":"2024-03-29T02:42:22Z","published":"2024-03-29T02:42:22Z","title":"CP HDR: A feature point detection and description library for LDR and\n  HDR images","summary":"  In computer vision, characteristics refer to image regions with unique\nproperties, such as corners, edges, textures, or areas with high contrast.\nThese regions can be represented through feature points (FPs). FP detection and\ndescription are fundamental steps to many computer vision tasks. Most FP\ndetection and description methods use low dynamic range (LDR) images,\nsufficient for most applications involving digital images. However, LDR images\nmay have saturated pixels in scenes with extreme light conditions, which\ndegrade FP detection. On the other hand, high dynamic range (HDR) images\nusually present a greater dynamic range but FP detection algorithms do not take\nadvantage of all the information in such images. In this study, we present a\nsystematic review of image detection and description algorithms that use HDR\nimages as input. We developed a library called CP_HDR that implements the\nHarris corner detector, SIFT detector and descriptor, and two modifications of\nthose algorithms specialized in HDR images, called SIFT for HDR (SfHDR) and\nHarris for HDR (HfHDR). Previous studies investigated the use of HDR images in\nFP detection, but we did not find studies investigating the use of HDR images\nin FP description. Using uniformity, repeatability rate, mean average\nprecision, and matching rate metrics, we compared the performance of the CP_HDR\nalgorithms using LDR and HDR images. We observed an increase in the uniformity\nof the distribution of FPs among the high-light, mid-light, and low-light areas\nof the images. The results show that using HDR images as input to detection\nalgorithms improves performance and that SfHDR and HfHDR enhance FP\ndescription.\n","authors":["Artur Santos Nascimento","Valter Guilherme Silva de Souza","Daniel Oliveira Dantas","Beatriz Trinchão Andrade"],"pdf_url":"https://arxiv.org/pdf/2403.19935v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19080v2","updated":"2024-03-29T02:31:10Z","published":"2024-03-28T01:05:06Z","title":"MMCert: Provable Defense against Adversarial Attacks to Multi-modal\n  Models","summary":"  Different from a unimodal model whose input is from a single modality, the\ninput (called multi-modal input) of a multi-modal model is from multiple\nmodalities such as image, 3D points, audio, text, etc. Similar to unimodal\nmodels, many existing studies show that a multi-modal model is also vulnerable\nto adversarial perturbation, where an attacker could add small perturbation to\nall modalities of a multi-modal input such that the multi-modal model makes\nincorrect predictions for it. Existing certified defenses are mostly designed\nfor unimodal models, which achieve sub-optimal certified robustness guarantees\nwhen extended to multi-modal models as shown in our experimental results. In\nour work, we propose MMCert, the first certified defense against adversarial\nattacks to a multi-modal model. We derive a lower bound on the performance of\nour MMCert under arbitrary adversarial attacks with bounded perturbations to\nboth modalities (e.g., in the context of auto-driving, we bound the number of\nchanged pixels in both RGB image and depth image). We evaluate our MMCert using\ntwo benchmark datasets: one for the multi-modal road segmentation task and the\nother for the multi-modal emotion recognition task. Moreover, we compare our\nMMCert with a state-of-the-art certified defense extended from unimodal models.\nOur experimental results show that our MMCert outperforms the baseline.\n","authors":["Yanting Wang","Hongye Fu","Wei Zou","Jinyuan Jia"],"pdf_url":"https://arxiv.org/pdf/2403.19080v2.pdf","comment":"To appear in CVPR'24"},{"id":"http://arxiv.org/abs/2403.19926v1","updated":"2024-03-29T02:26:22Z","published":"2024-03-29T02:26:22Z","title":"Video-Based Human Pose Regression via Decoupled Space-Time Aggregation","summary":"  By leveraging temporal dependency in video sequences, multi-frame human pose\nestimation algorithms have demonstrated remarkable results in complicated\nsituations, such as occlusion, motion blur, and video defocus. These algorithms\nare predominantly based on heatmaps, resulting in high computation and storage\nrequirements per frame, which limits their flexibility and real-time\napplication in video scenarios, particularly on edge devices. In this paper, we\ndevelop an efficient and effective video-based human pose regression method,\nwhich bypasses intermediate representations such as heatmaps and instead\ndirectly maps the input to the output joint coordinates. Despite the inherent\nspatial correlation among adjacent joints of the human pose, the temporal\ntrajectory of each individual joint exhibits relative independence. In light of\nthis, we propose a novel Decoupled Space-Time Aggregation network (DSTA) to\nseparately capture the spatial contexts between adjacent joints and the\ntemporal cues of each individual joint, thereby avoiding the conflation of\nspatiotemporal dimensions. Concretely, DSTA learns a dedicated feature token\nfor each joint to facilitate the modeling of their spatiotemporal dependencies.\nWith the proposed joint-wise local-awareness attention mechanism, our method is\ncapable of efficiently and flexibly utilizing the spatial dependency of\nadjacent joints and the temporal dependency of each joint itself. Extensive\nexperiments demonstrate the superiority of our method. Compared to previous\nregression-based single-frame human pose estimation methods, DSTA significantly\nenhances performance, achieving an 8.9 mAP improvement on PoseTrack2017.\nFurthermore, our approach either surpasses or is on par with the\nstate-of-the-art heatmap-based multi-frame human pose estimation methods.\nProject page: https://github.com/zgspose/DSTA.\n","authors":["Jijie He","Wenwu Yang"],"pdf_url":"https://arxiv.org/pdf/2403.19926v1.pdf","comment":"12 papgers, 3 figures"},{"id":"http://arxiv.org/abs/2403.19924v1","updated":"2024-03-29T02:22:54Z","published":"2024-03-29T02:22:54Z","title":"SceneTracker: Long-term Scene Flow Estimation Network","summary":"  Considering the complementarity of scene flow estimation in the spatial\ndomain's focusing capability and 3D object tracking in the temporal domain's\ncoherence, this study aims to address a comprehensive new task that can\nsimultaneously capture fine-grained and long-term 3D motion in an online\nmanner: long-term scene flow estimation (LSFE). We introduce SceneTracker, a\nnovel learning-based LSFE network that adopts an iterative approach to\napproximate the optimal trajectory. Besides, it dynamically indexes and\nconstructs appearance and depth correlation features simultaneously and employs\nthe Transformer to explore and utilize long-range connections within and\nbetween trajectories. With detailed experiments, SceneTracker shows superior\ncapabilities in handling 3D spatial occlusion and depth noise interference,\nhighly tailored to the LSFE task's needs. The code for SceneTracker is\navailable at https://github.com/wwsource/SceneTracker.\n","authors":["Bo Wang","Jian Li","Yang Yu","Li Liu","Zhenping Sun","Dewen Hu"],"pdf_url":"https://arxiv.org/pdf/2403.19924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17095v2","updated":"2024-03-29T02:18:40Z","published":"2023-11-28T06:42:58Z","title":"Emergent Open-Vocabulary Semantic Segmentation from Off-the-shelf\n  Vision-Language Models","summary":"  From image-text pairs, large-scale vision-language models (VLMs) learn to\nimplicitly associate image regions with words, which prove effective for tasks\nlike visual question answering. However, leveraging the learned association for\nopen-vocabulary semantic segmentation remains a challenge. In this paper, we\npropose a simple, yet extremely effective, training-free technique,\nPlug-and-Play Open-Vocabulary Semantic Segmentation (PnP-OVSS) for this task.\nPnP-OVSS leverages a VLM with direct text-to-image cross-attention and an\nimage-text matching loss. To balance between over-segmentation and\nunder-segmentation, we introduce Salience Dropout; by iteratively dropping\npatches that the model is most attentive to, we are able to better resolve the\nentire extent of the segmentation mask. \\shortname{} does not require any\nneural network training and performs hyperparameter tuning without the need for\nany segmentation annotations, even for a validation set. PnP-OVSS demonstrates\nsubstantial improvements over comparable baselines (+29.4% mIoU on Pascal VOC,\n+13.2% mIoU on Pascal Context, +14.0% mIoU on MS COCO, and +11.4% mIoU on\nADE-20K.) and even outperforms most baselines that conduct additional network\ntraining on top of pretrained VLMs. Our codebase is at\nhttps://github.com/letitiabanana/PnP-OVSS.\n","authors":["Jiayun Luo","Siddhesh Khandelwal","Leonid Sigal","Boyang Li"],"pdf_url":"https://arxiv.org/pdf/2311.17095v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.19920v1","updated":"2024-03-29T02:17:09Z","published":"2024-03-29T02:17:09Z","title":"MI-NeRF: Learning a Single Face NeRF from Multiple Identities","summary":"  In this work, we introduce a method that learns a single dynamic neural\nradiance field (NeRF) from monocular talking face videos of multiple\nidentities. NeRFs have shown remarkable results in modeling the 4D dynamics and\nappearance of human faces. However, they require per-identity optimization.\nAlthough recent approaches have proposed techniques to reduce the training and\nrendering time, increasing the number of identities can be expensive. We\nintroduce MI-NeRF (multi-identity NeRF), a single unified network that models\ncomplex non-rigid facial motion for multiple identities, using only monocular\nvideos of arbitrary length. The core premise in our method is to learn the\nnon-linear interactions between identity and non-identity specific information\nwith a multiplicative module. By training on multiple videos simultaneously,\nMI-NeRF not only reduces the total training time compared to standard\nsingle-identity NeRFs, but also demonstrates robustness in synthesizing novel\nexpressions for any input identity. We present results for both facial\nexpression transfer and talking face video synthesis. Our method can be further\npersonalized for a target identity given only a short video.\n","authors":["Aggelina Chatziagapi","Grigorios G. Chrysos","Dimitris Samaras"],"pdf_url":"https://arxiv.org/pdf/2403.19920v1.pdf","comment":"Project page: https://aggelinacha.github.io/MI-NeRF/"},{"id":"http://arxiv.org/abs/2403.19919v1","updated":"2024-03-29T02:10:38Z","published":"2024-03-29T02:10:38Z","title":"Diff-Reg v1: Diffusion Matching Model for Registration Problem","summary":"  Establishing reliable correspondences is essential for registration tasks\nsuch as 3D and 2D3D registration. Existing methods commonly leverage geometric\nor semantic point features to generate potential correspondences. However,\nthese features may face challenges such as large deformation, scale\ninconsistency, and ambiguous matching problems (e.g., symmetry). Additionally,\nmany previous methods, which rely on single-pass prediction, may struggle with\nlocal minima in complex scenarios. To mitigate these challenges, we introduce a\ndiffusion matching model for robust correspondence construction. Our approach\ntreats correspondence estimation as a denoising diffusion process within the\ndoubly stochastic matrix space, which gradually denoises (refines) a doubly\nstochastic matching matrix to the ground-truth one for high-quality\ncorrespondence estimation. It involves a forward diffusion process that\ngradually introduces Gaussian noise into the ground truth matching matrix and a\nreverse denoising process that iteratively refines the noisy matching matrix.\nIn particular, the feature extraction from the backbone occurs only once during\nthe inference phase. Our lightweight denoising module utilizes the same feature\nat each reverse sampling step. Evaluation of our method on both 3D and 2D3D\nregistration tasks confirms its effectiveness.\n","authors":["Qianliang Wu","Haobo Jiang","Lei Luo","Jun Li","Yaqing Ding","Jin Xie","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2403.19919v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2401.00436"},{"id":"http://arxiv.org/abs/2403.19915v1","updated":"2024-03-29T02:03:00Z","published":"2024-03-29T02:03:00Z","title":"Using Images as Covariates: Measuring Curb Appeal with Deep Learning","summary":"  This paper details an innovative methodology to integrate image data into\ntraditional econometric models. Motivated by forecasting sales prices for\nresidential real estate, we harness the power of deep learning to add\n\"information\" contained in images as covariates. Specifically, images of homes\nwere categorized and encoded using an ensemble of image classifiers (ResNet-50,\nVGG16, MobileNet, and Inception V3). Unique features presented within each\nimage were further encoded through panoptic segmentation. Forecasts from a\nneural network trained on the encoded data results in improved out-of-sample\npredictive power. We also combine these image-based forecasts with standard\nhedonic real estate property and location characteristics, resulting in a\nunified dataset. We show that image-based forecasts increase the accuracy of\nhedonic forecasts when encoded features are regarded as additional covariates.\nWe also attempt to \"explain\" which covariates the image-based forecasts are\nmost highly correlated with. The study exemplifies the benefits of\ninterdisciplinary methodologies, merging machine learning and econometrics to\nharness untapped data sources for more accurate forecasting.\n","authors":["Ardyn Nordstrom","Morgan Nordstrom","Matthew D. Webb"],"pdf_url":"https://arxiv.org/pdf/2403.19915v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19912v1","updated":"2024-03-29T01:46:11Z","published":"2024-03-29T01:46:11Z","title":"Automated Identification and Segmentation of Hi Sources in CRAFTS Using\n  Deep Learning Method","summary":"  We introduce a machine learning-based method for extracting HI sources from\n3D spectral data, and construct a dedicated dataset of HI sources from CRAFTS.\nOur custom dataset provides comprehensive resources for HI source detection.\nUtilizing the 3D-Unet segmentation architecture, our method reliably identifies\nand segments HI sources, achieving notable performance metrics with recall\nrates reaching 91.6% and accuracy levels at 95.7%. These outcomes substantiate\nthe value of our custom dataset and the efficacy of our proposed network in\nidentifying HI source. Our code is publicly available at\nhttps://github.com/fishszh/HISF.\n","authors":["Zihao Song","Huaxi Chen","Donghui Quan","Di Li","Yinghui Zheng","Shulei Ni","Yunchuan Chen","Yun Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.19912v1.pdf","comment":"6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2402.13729v3","updated":"2024-03-29T01:42:02Z","published":"2024-02-21T11:46:16Z","title":"Hybrid Video Diffusion Models with 2D Triplane and 3D Wavelet\n  Representation","summary":"  Generating high-quality videos that synthesize desired realistic content is a\nchallenging task due to their intricate high-dimensionality and complexity of\nvideos. Several recent diffusion-based methods have shown comparable\nperformance by compressing videos to a lower-dimensional latent space, using\ntraditional video autoencoder architecture. However, such method that employ\nstandard frame-wise 2D and 3D convolution fail to fully exploit the\nspatio-temporal nature of videos. To address this issue, we propose a novel\nhybrid video diffusion model, called HVDM, which can capture spatio-temporal\ndependencies more effectively. The HVDM is trained by a hybrid video\nautoencoder which extracts a disentangled representation of the video\nincluding: (i) a global context information captured by a 2D projected latent\n(ii) a local volume information captured by 3D convolutions with wavelet\ndecomposition (iii) a frequency information for improving the video\nreconstruction. Based on this disentangled representation, our hybrid\nautoencoder provide a more comprehensive video latent enriching the generated\nvideos with fine structures and details. Experiments on video generation\nbenchamarks (UCF101, SkyTimelapse, and TaiChi) demonstrate that the proposed\napproach achieves state-of-the-art video generation quality, showing a wide\nrange of video applications (e.g., long video generation, image-to-video, and\nvideo dynamics control).\n","authors":["Kihong Kim","Haneol Lee","Jihye Park","Seyeon Kim","Kwanghee Lee","Seungryong Kim","Jaejun Yoo"],"pdf_url":"https://arxiv.org/pdf/2402.13729v3.pdf","comment":"17 pages, 13 figures"},{"id":"http://arxiv.org/abs/2311.15153v4","updated":"2024-03-29T01:18:37Z","published":"2023-11-26T01:05:55Z","title":"Predicting Gradient is Better: Exploring Self-Supervised Learning for\n  SAR ATR with a Joint-Embedding Predictive Architecture","summary":"  The growing Synthetic Aperture Radar (SAR) data has the potential to build a\nfoundation model through Self-Supervised Learning (SSL) methods, which can\nachieve various SAR Automatic Target Recognition (ATR) tasks with pre-training\nin large-scale unlabeled data and fine-tuning in small labeled samples. SSL\naims to construct supervision signals directly from the data, which minimizes\nthe need for expensive expert annotation and maximizes the use of the expanding\ndata pool for a foundational model. This study investigates an effective SSL\nmethod for SAR ATR, which can pave the way for a foundation model in SAR ATR.\nThe primary obstacles faced in SSL for SAR ATR are the small targets in remote\nsensing and speckle noise in SAR images, corresponding to the SSL approach and\nsignals. To overcome these challenges, we present a novel Joint-Embedding\nPredictive Architecture for SAR ATR (SAR-JEPA), which leverages local masked\npatches to predict the multi-scale SAR gradient representations of unseen\ncontext. The key aspect of SAR-JEPA is integrating SAR domain features to\nensure high-quality self-supervised signals as target features. Besides, we\nemploy local masks and multi-scale features to accommodate the various small\ntargets in remote sensing. By fine-tuning and evaluating our framework on three\ntarget recognition datasets (vehicle, ship, and aircraft) with four other\ndatasets as pre-training, we demonstrate its outperformance over other SSL\nmethods and its effectiveness with increasing SAR data. This study showcases\nthe potential of SSL for SAR target recognition across diverse targets, scenes,\nand sensors.\n","authors":["Weijie Li","Yang Wei","Tianpeng Liu","Yuenan Hou","Yuxuan Li","Zhen Liu","Yongxiang Liu","Li Liu"],"pdf_url":"https://arxiv.org/pdf/2311.15153v4.pdf","comment":"Our codes at https://github.com/waterdisappear/SAR-JEPA"},{"id":"http://arxiv.org/abs/2403.19905v1","updated":"2024-03-29T01:11:56Z","published":"2024-03-29T01:11:56Z","title":"Classification of Diabetic Retinopathy using Pre-Trained Deep Learning\n  Models","summary":"  Diabetic Retinopathy (DR) stands as the leading cause of blindness globally,\nparticularly affecting individuals between the ages of 20 and 70. This paper\npresents a Computer-Aided Diagnosis (CAD) system designed for the automatic\nclassification of retinal images into five distinct classes: Normal, Mild,\nModerate, Severe, and Proliferative Diabetic Retinopathy (PDR). The proposed\nsystem leverages Convolutional Neural Networks (CNNs) employing pre-trained\ndeep learning models. Through the application of fine-tuning techniques, our\nmodel is trained on fundus images of diabetic retinopathy with resolutions of\n350x350x3 and 224x224x3. Experimental results obtained on the Kaggle platform,\nutilizing resources comprising 4 CPUs, 17 GB RAM, and 1 GB Disk, demonstrate\nthe efficacy of our approach. The achieved Area Under the Curve (AUC) values\nfor CNN, MobileNet, VGG-16, InceptionV3, and InceptionResNetV2 models are 0.50,\n0.70, 0.53, 0.63, and 0.69, respectively.\n","authors":["Inas Al-Kamachy","Prof. Dr. Reza Hassanpour","Prof. Roya Choupani"],"pdf_url":"https://arxiv.org/pdf/2403.19905v1.pdf","comment":"3 pages, 1 figure, 1 table"},{"id":"http://arxiv.org/abs/2310.10375v2","updated":"2024-03-29T01:08:12Z","published":"2023-10-16T13:16:09Z","title":"GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers","summary":"  As transformers are equivariant to the permutation of input tokens, encoding\nthe positional information of tokens is necessary for many tasks. However,\nsince existing positional encoding schemes have been initially designed for NLP\ntasks, their suitability for vision tasks, which typically exhibit different\nstructural properties in their data, is questionable. We argue that existing\npositional encoding schemes are suboptimal for 3D vision tasks, as they do not\nrespect their underlying 3D geometric structure. Based on this hypothesis, we\npropose a geometry-aware attention mechanism that encodes the geometric\nstructure of tokens as relative transformation determined by the geometric\nrelationship between queries and key-value pairs. By evaluating on multiple\nnovel view synthesis (NVS) datasets in the sparse wide-baseline multi-view\nsetting, we show that our attention, called Geometric Transform Attention\n(GTA), improves learning efficiency and performance of state-of-the-art\ntransformer-based NVS models without any additional learned parameters and only\nminor computational overhead.\n","authors":["Takeru Miyato","Bernhard Jaeger","Max Welling","Andreas Geiger"],"pdf_url":"https://arxiv.org/pdf/2310.10375v2.pdf","comment":"Published as a conference paper at ICLR 2024"},{"id":"http://arxiv.org/abs/2403.19904v1","updated":"2024-03-29T01:07:20Z","published":"2024-03-29T01:07:20Z","title":"Fully Geometric Panoramic Localization","summary":"  We introduce a lightweight and accurate localization method that only\nutilizes the geometry of 2D-3D lines. Given a pre-captured 3D map, our approach\nlocalizes a panorama image, taking advantage of the holistic 360 view. The\nsystem mitigates potential privacy breaches or domain discrepancies by avoiding\ntrained or hand-crafted visual descriptors. However, as lines alone can be\nambiguous, we express distinctive yet compact spatial contexts from\nrelationships between lines, namely the dominant directions of parallel lines\nand the intersection between non-parallel lines. The resulting representations\nare efficient in processing time and memory compared to conventional visual\ndescriptor-based methods. Given the groups of dominant line directions and\ntheir intersections, we accelerate the search process to test thousands of pose\ncandidates in less than a millisecond without sacrificing accuracy. We\nempirically show that the proposed 2D-3D matching can localize panoramas for\nchallenging scenes with similar structures, dramatic domain shifts or\nillumination changes. Our fully geometric approach does not involve extensive\nparameter tuning or neural network training, making it a practical algorithm\nthat can be readily deployed in the real world. Project page including the code\nis available through this link: https://82magnolia.github.io/fgpl/.\n","authors":["Junho Kim","Jiwon Jeong","Young Min Kim"],"pdf_url":"https://arxiv.org/pdf/2403.19904v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.19902v1","updated":"2024-03-29T01:05:23Z","published":"2024-03-29T01:05:23Z","title":"Heterogeneous Network Based Contrastive Learning Method for PolSAR Land\n  Cover Classification","summary":"  Polarimetric synthetic aperture radar (PolSAR) image interpretation is widely\nused in various fields. Recently, deep learning has made significant progress\nin PolSAR image classification. Supervised learning (SL) requires a large\namount of labeled PolSAR data with high quality to achieve better performance,\nhowever, manually labeled data is insufficient. This causes the SL to fail into\noverfitting and degrades its generalization performance. Furthermore, the\nscattering confusion problem is also a significant challenge that attracts more\nattention. To solve these problems, this article proposes a Heterogeneous\nNetwork based Contrastive Learning method(HCLNet). It aims to learn high-level\nrepresentation from unlabeled PolSAR data for few-shot classification according\nto multi-features and superpixels. Beyond the conventional CL, HCLNet\nintroduces the heterogeneous architecture for the first time to utilize\nheterogeneous PolSAR features better. And it develops two easy-to-use plugins\nto narrow the domain gap between optics and PolSAR, including feature filter\nand superpixel-based instance discrimination, which the former is used to\nenhance the complementarity of multi-features, and the latter is used to\nincrease the diversity of negative samples. Experiments demonstrate the\nsuperiority of HCLNet on three widely used PolSAR benchmark datasets compared\nwith state-of-the-art methods. Ablation studies also verify the importance of\neach component. Besides, this work has implications for how to efficiently\nutilize the multi-features of PolSAR data to learn better high-level\nrepresentation in CL and how to construct networks suitable for PolSAR data\nbetter.\n","authors":["Jianfeng Cai","Yue Ma","Zhixi Feng","Shuyuan Yang"],"pdf_url":"https://arxiv.org/pdf/2403.19902v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15585v3","updated":"2024-03-29T00:44:18Z","published":"2024-03-22T19:19:51Z","title":"MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis","summary":"  Chest X-ray images are commonly used for predicting acute and chronic\ncardiopulmonary conditions, but efforts to integrate them with structured\nclinical data face challenges due to incomplete electronic health records\n(EHR). This paper introduces MedPromptX, the first model to integrate\nmultimodal large language models (MLLMs), few-shot prompting (FP) and visual\ngrounding (VG) to combine imagery with EHR data for chest X-ray diagnosis. A\npre-trained MLLM is utilized to complement the missing EHR information,\nproviding a comprehensive understanding of patients' medical history.\nAdditionally, FP reduces the necessity for extensive training of MLLMs while\neffectively tackling the issue of hallucination. Nevertheless, the process of\ndetermining the optimal number of few-shot examples and selecting high-quality\ncandidates can be burdensome, yet it profoundly influences model performance.\nHence, we propose a new technique that dynamically refines few-shot data for\nreal-time adjustment to new patient scenarios. Moreover, VG aids in focusing\nthe model's attention on relevant regions of interest in X-ray images,\nenhancing the identification of abnormalities. We release MedPromptX-VQA, a new\nin-context visual question answering dataset encompassing interleaved image and\nEHR data derived from MIMIC-IV and MIMIC-CXR databases. Results demonstrate the\nSOTA performance of MedPromptX, achieving an 11% improvement in F1-score\ncompared to the baselines. Code and data are available at\nhttps://github.com/BioMedIA-MBZUAI/MedPromptX\n","authors":["Mai A. Shaaban","Adnan Khan","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2403.15585v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19898v1","updated":"2024-03-29T00:40:12Z","published":"2024-03-29T00:40:12Z","title":"Structure Matters: Tackling the Semantic Discrepancy in Diffusion Models\n  for Image Inpainting","summary":"  Denoising diffusion probabilistic models for image inpainting aim to add the\nnoise to the texture of image during the forward process and recover masked\nregions with unmasked ones of the texture via the reverse denoising\nprocess.Despite the meaningful semantics generation,the existing arts suffer\nfrom the semantic discrepancy between masked and unmasked regions, since the\nsemantically dense unmasked texture fails to be completely degraded while the\nmasked regions turn to the pure noise in diffusion process,leading to the large\ndiscrepancy between them.In this paper,we aim to answer how unmasked semantics\nguide texture denoising process;together with how to tackle the semantic\ndiscrepancy,to facilitate the consistent and meaningful semantics generation.To\nthis end,we propose a novel structure-guided diffusion model named\nStrDiffusion,to reformulate the conventional texture denoising process under\nstructure guidance to derive a simplified denoising objective for image\ninpainting,while revealing:1) the semantically sparse structure is beneficial\nto tackle semantic discrepancy in early stage, while dense texture generates\nreasonable semantics in late stage;2) the semantics from unmasked regions\nessentially offer the time-dependent structure guidance for the texture\ndenoising process,benefiting from the time-dependent sparsity of the structure\nsemantics.For the denoising process,a structure-guided neural network is\ntrained to estimate the simplified denoising objective by exploiting the\nconsistency of the denoised structure between masked and unmasked\nregions.Besides,we devise an adaptive resampling strategy as a formal criterion\nas whether structure is competent to guide the texture denoising process,while\nregulate their semantic correlations.Extensive experiments validate the merits\nof StrDiffusion over the state-of-the-arts.Our code is available at\nhttps://github.com/htyjers/StrDiffusion.\n","authors":["Haipeng Liu","Yang Wang","Biao Qian","Meng Wang","Yong Rui"],"pdf_url":"https://arxiv.org/pdf/2403.19898v1.pdf","comment":"15 pages, 10 figures, to appear CVPR 2024"},{"id":"http://arxiv.org/abs/2403.19897v1","updated":"2024-03-29T00:36:38Z","published":"2024-03-29T00:36:38Z","title":"Disentangling Racial Phenotypes: Fine-Grained Control of Race-related\n  Facial Phenotype Characteristics","summary":"  Achieving an effective fine-grained appearance variation over 2D facial\nimages, whilst preserving facial identity, is a challenging task due to the\nhigh complexity and entanglement of common 2D facial feature encoding spaces.\nDespite these challenges, such fine-grained control, by way of disentanglement\nis a crucial enabler for data-driven racial bias mitigation strategies across\nmultiple automated facial analysis tasks, as it allows to analyse, characterise\nand synthesise human facial diversity. In this paper, we propose a novel GAN\nframework to enable fine-grained control over individual race-related phenotype\nattributes of the facial images. Our framework factors the latent (feature)\nspace into elements that correspond to race-related facial phenotype\nrepresentations, thereby separating phenotype aspects (e.g. skin, hair colour,\nnose, eye, mouth shapes), which are notoriously difficult to annotate robustly\nin real-world facial data. Concurrently, we also introduce a high quality\naugmented, diverse 2D face image dataset drawn from CelebA-HQ for GAN training.\nUnlike prior work, our framework only relies upon 2D imagery and related\nparameters to achieve state-of-the-art individual control over race-related\nphenotype attributes with improved photo-realistic output.\n","authors":["Seyma Yucer","Amir Atapour Abarghouei","Noura Al Moubayed","Toby P. Breckon"],"pdf_url":"https://arxiv.org/pdf/2403.19897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19896v1","updated":"2024-03-29T00:33:37Z","published":"2024-03-29T00:33:37Z","title":"Nonlinearity Enhanced Adaptive Activation Function","summary":"  A simply implemented activation function with even cubic nonlinearity is\nintroduced that increases the accuracy of neural networks without substantial\nadditional computational resources. This is partially enabled through an\napparent tradeoff between convergence and accuracy. The activation function\ngeneralizes the standard RELU function by introducing additional degrees of\nfreedom through optimizable parameters that enable the degree of nonlinearity\nto be adjusted. The associated accuracy enhancement is quantified in the\ncontext of the MNIST digit data set through a comparison with standard\ntechniques.\n","authors":["David Yevick"],"pdf_url":"https://arxiv.org/pdf/2403.19896v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19893v1","updated":"2024-03-29T00:28:26Z","published":"2024-03-29T00:28:26Z","title":"PLoc: A New Evaluation Criterion Based on Physical Location for\n  Autonomous Driving Datasets","summary":"  Autonomous driving has garnered significant attention as a key research area\nwithin artificial intelligence. In the context of autonomous driving scenarios,\nthe varying physical locations of objects correspond to different levels of\ndanger. However, conventional evaluation criteria for automatic driving object\ndetection often overlook the crucial aspect of an object's physical location,\nleading to evaluation results that may not accurately reflect the genuine\nthreat posed by the object to the autonomous driving vehicle. To enhance the\nsafety of autonomous driving, this paper introduces a novel evaluation\ncriterion based on physical location information, termed PLoc. This criterion\ntranscends the limitations of traditional criteria by acknowledging that the\nphysical location of pedestrians in autonomous driving scenarios can provide\nvaluable safety-related information. Furthermore, this paper presents a newly\nre-annotated dataset (ApolloScape-R) derived from ApolloScape. ApolloScape-R\ninvolves the relabeling of pedestrians based on the significance of their\nphysical location. The dataset is utilized to assess the performance of various\nobject detection models under the proposed PLoc criterion. Experimental results\ndemonstrate that the average accuracy of all object detection models in\nidentifying a person situated in the travel lane of an autonomous vehicle is\nlower than that for a person on a sidewalk. The dataset is publicly available\nat https://github.com/lnyrlyed/ApolloScape-R.git\n","authors":["Ruining Yang","Yuqi Peng"],"pdf_url":"https://arxiv.org/pdf/2403.19893v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19888v1","updated":"2024-03-29T00:05:13Z","published":"2024-03-29T00:05:13Z","title":"MambaMixer: Efficient Selective State Space Models with Dual Token and\n  Channel Selection","summary":"  Recent advances in deep learning have mainly relied on Transformers due to\ntheir data dependency and ability to learn at scale. The attention module in\nthese architectures, however, exhibits quadratic time and space in input size,\nlimiting their scalability for long-sequence modeling. Despite recent attempts\nto design efficient and effective architecture backbone for multi-dimensional\ndata, such as images and multivariate time series, existing models are either\ndata independent, or fail to allow inter- and intra-dimension communication.\nRecently, State Space Models (SSMs), and more specifically Selective State\nSpace Models, with efficient hardware-aware implementation, have shown\npromising potential for long sequence modeling. Motivated by the success of\nSSMs, we present MambaMixer, a new architecture with data-dependent weights\nthat uses a dual selection mechanism across tokens and channels, called\nSelective Token and Channel Mixer. MambaMixer connects selective mixers using a\nweighted averaging mechanism, allowing layers to have direct access to early\nfeatures. As a proof of concept, we design Vision MambaMixer (ViM2) and Time\nSeries MambaMixer (TSM2) architectures based on the MambaMixer block and\nexplore their performance in various vision and time series forecasting tasks.\nOur results underline the importance of selective mixing across both tokens and\nchannels. In ImageNet classification, object detection, and semantic\nsegmentation tasks, ViM2 achieves competitive performance with well-established\nvision models and outperforms SSM-based vision models. In time series\nforecasting, TSM2 achieves outstanding performance compared to state-of-the-art\nmethods while demonstrating significantly improved computational cost. These\nresults show that while Transformers, cross-channel attention, and MLPs are\nsufficient for good performance in time series forecasting, neither is\nnecessary.\n","authors":["Ali Behrouz","Michele Santacatterina","Ramin Zabih"],"pdf_url":"https://arxiv.org/pdf/2403.19888v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2303.13612v2","updated":"2024-03-29T23:42:05Z","published":"2023-03-23T18:55:43Z","title":"NOPE: Novel Object Pose Estimation from a Single Image","summary":"  The practicality of 3D object pose estimation remains limited for many\napplications due to the need for prior knowledge of a 3D model and a training\nperiod for new objects. To address this limitation, we propose an approach that\ntakes a single image of a new object as input and predicts the relative pose of\nthis object in new images without prior knowledge of the object's 3D model and\nwithout requiring training time for new objects and categories. We achieve this\nby training a model to directly predict discriminative embeddings for\nviewpoints surrounding the object. This prediction is done using a simple U-Net\narchitecture with attention and conditioned on the desired pose, which yields\nextremely fast inference. We compare our approach to state-of-the-art methods\nand show it outperforms them both in terms of accuracy and robustness. Our\nsource code is publicly available at https://github.com/nv-nguyen/nope\n","authors":["Van Nguyen Nguyen","Thibault Groueix","Yinlin Hu","Mathieu Salzmann","Vincent Lepetit"],"pdf_url":"https://arxiv.org/pdf/2303.13612v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2402.18528v2","updated":"2024-03-29T23:41:13Z","published":"2024-02-28T18:08:03Z","title":"Gradient Reweighting: Towards Imbalanced Class-Incremental Learning","summary":"  Class-Incremental Learning (CIL) trains a model to continually recognize new\nclasses from non-stationary data while retaining learned knowledge. A major\nchallenge of CIL arises when applying to real-world data characterized by\nnon-uniform distribution, which introduces a dual imbalance problem involving\n(i) disparities between stored exemplars of old tasks and new class data\n(inter-phase imbalance), and (ii) severe class imbalances within each\nindividual task (intra-phase imbalance). We show that this dual imbalance issue\ncauses skewed gradient updates with biased weights in FC layers, thus inducing\nover/under-fitting and catastrophic forgetting in CIL. Our method addresses it\nby reweighting the gradients towards balanced optimization and unbiased\nclassifier learning. Additionally, we observe imbalanced forgetting where\nparadoxically the instance-rich classes suffer higher performance degradation\nduring CIL due to a larger amount of training data becoming unavailable in\nsubsequent learning phases. To tackle this, we further introduce a\ndistribution-aware knowledge distillation loss to mitigate forgetting by\naligning output logits proportionally with the distribution of lost training\ndata. We validate our method on CIFAR-100, ImageNetSubset, and Food101 across\nvarious evaluation protocols and demonstrate consistent improvements compared\nto existing works, showing great potential to apply CIL in real-world scenarios\nwith enhanced robustness and effectiveness.\n","authors":["Jiangpeng He","Fengqing Zhu"],"pdf_url":"https://arxiv.org/pdf/2402.18528v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2401.10171v2","updated":"2024-03-29T22:47:59Z","published":"2024-01-18T18:01:19Z","title":"SHINOBI: Shape and Illumination using Neural Object Decomposition via\n  BRDF Optimization In-the-wild","summary":"  We present SHINOBI, an end-to-end framework for the reconstruction of shape,\nmaterial, and illumination from object images captured with varying lighting,\npose, and background. Inverse rendering of an object based on unconstrained\nimage collections is a long-standing challenge in computer vision and graphics\nand requires a joint optimization over shape, radiance, and pose. We show that\nan implicit shape representation based on a multi-resolution hash encoding\nenables faster and robust shape reconstruction with joint camera alignment\noptimization that outperforms prior work. Further, to enable the editing of\nillumination and object reflectance (i.e. material) we jointly optimize BRDF\nand illumination together with the object's shape. Our method is class-agnostic\nand works on in-the-wild image collections of objects to produce relightable 3D\nassets for several use cases such as AR/VR, movies, games, etc. Project page:\nhttps://shinobi.aengelhardt.com Video:\nhttps://www.youtube.com/watch?v=iFENQ6AcYd8&feature=youtu.be\n","authors":["Andreas Engelhardt","Amit Raj","Mark Boss","Yunzhi Zhang","Abhishek Kar","Yuanzhen Li","Deqing Sun","Ricardo Martin Brualla","Jonathan T. Barron","Hendrik P. A. Lensch","Varun Jampani"],"pdf_url":"https://arxiv.org/pdf/2401.10171v2.pdf","comment":"Accepted by IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR 2024). Updated supplementary material and acknowledgements"},{"id":"http://arxiv.org/abs/2403.17173v2","updated":"2024-03-29T22:46:03Z","published":"2024-03-25T20:39:58Z","title":"Task2Box: Box Embeddings for Modeling Asymmetric Task Relationships","summary":"  Modeling and visualizing relationships between tasks or datasets is an\nimportant step towards solving various meta-tasks such as dataset discovery,\nmulti-tasking, and transfer learning. However, many relationships, such as\ncontainment and transferability, are naturally asymmetric and current\napproaches for representation and visualization (e.g., t-SNE) do not readily\nsupport this. We propose Task2Box, an approach to represent tasks using box\nembeddings -- axis-aligned hyperrectangles in low dimensional spaces -- that\ncan capture asymmetric relationships between them through volumetric overlaps.\nWe show that Task2Box accurately predicts unseen hierarchical relationships\nbetween nodes in ImageNet and iNaturalist datasets, as well as transferability\nbetween tasks in the Taskonomy benchmark. We also show that box embeddings\nestimated from task representations (e.g., CLIP, Task2Vec, or attribute based)\ncan be used to predict relationships between unseen tasks more accurately than\nclassifiers trained on the same representations, as well as handcrafted\nasymmetric distances (e.g., KL divergence). This suggests that low-dimensional\nbox embeddings can effectively capture these task relationships and have the\nadded advantage of being interpretable. We use the approach to visualize\nrelationships among publicly available image classification datasets on popular\ndataset hosting platform called Hugging Face.\n","authors":["Rangel Daroya","Aaron Sun","Subhransu Maji"],"pdf_url":"https://arxiv.org/pdf/2403.17173v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.08237v2","updated":"2024-03-29T22:29:03Z","published":"2023-01-19T18:54:43Z","title":"LoCoNet: Long-Short Context Network for Active Speaker Detection","summary":"  Active Speaker Detection (ASD) aims to identify who is speaking in each frame\nof a video. ASD reasons from audio and visual information from two contexts:\nlong-term intra-speaker context and short-term inter-speaker context. Long-term\nintra-speaker context models the temporal dependencies of the same speaker,\nwhile short-term inter-speaker context models the interactions of speakers in\nthe same scene. These two contexts are complementary to each other and can help\ninfer the active speaker. Motivated by these observations, we propose LoCoNet,\na simple yet effective Long-Short Context Network that models the long-term\nintra-speaker context and short-term inter-speaker context. We use\nself-attention to model long-term intra-speaker context due to its\neffectiveness in modeling long-range dependencies, and convolutional blocks\nthat capture local patterns to model short-term inter-speaker context.\nExtensive experiments show that LoCoNet achieves state-of-the-art performance\non multiple datasets, achieving an mAP of 95.2%(+1.1%) on AVA-ActiveSpeaker,\n68.1%(+22%) on Columbia dataset, 97.2%(+2.8%) on Talkies dataset and\n59.7%(+8.0%) on Ego4D dataset. Moreover, in challenging cases where multiple\nspeakers are present, or face of active speaker is much smaller than other\nfaces in the same scene, LoCoNet outperforms previous state-of-the-art methods\nby 3.4% on the AVA-ActiveSpeaker dataset. The code will be released at\nhttps://github.com/SJTUwxz/LoCoNet_ASD.\n","authors":["Xizi Wang","Feng Cheng","Gedas Bertasius","David Crandall"],"pdf_url":"https://arxiv.org/pdf/2301.08237v2.pdf","comment":"accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.19076v2","updated":"2024-03-29T21:33:39Z","published":"2024-03-28T00:34:56Z","title":"Tiny Machine Learning: Progress and Futures","summary":"  Tiny Machine Learning (TinyML) is a new frontier of machine learning. By\nsqueezing deep learning models into billions of IoT devices and\nmicrocontrollers (MCUs), we expand the scope of AI applications and enable\nubiquitous intelligence. However, TinyML is challenging due to hardware\nconstraints: the tiny memory resource makes it difficult to hold deep learning\nmodels designed for cloud and mobile platforms. There is also limited compiler\nand inference engine support for bare-metal devices. Therefore, we need to\nco-design the algorithm and system stack to enable TinyML. In this review, we\nwill first discuss the definition, challenges, and applications of TinyML. We\nthen survey the recent progress in TinyML and deep learning on MCUs. Next, we\nwill introduce MCUNet, showing how we can achieve ImageNet-scale AI\napplications on IoT devices with system-algorithm co-design. We will further\nextend the solution from inference to training and introduce tiny on-device\ntraining techniques. Finally, we present future directions in this area.\nToday's large model might be tomorrow's tiny model. The scope of TinyML should\nevolve and adapt over time.\n","authors":["Ji Lin","Ligeng Zhu","Wei-Ming Chen","Wei-Chen Wang","Song Han"],"pdf_url":"https://arxiv.org/pdf/2403.19076v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2206.15472"},{"id":"http://arxiv.org/abs/2403.08848v2","updated":"2024-03-29T21:22:06Z","published":"2024-03-13T16:57:04Z","title":"FocusMAE: Gallbladder Cancer Detection from Ultrasound Videos with\n  Focused Masked Autoencoders","summary":"  In recent years, automated Gallbladder Cancer (GBC) detection has gained the\nattention of researchers. Current state-of-the-art (SOTA) methodologies relying\non ultrasound sonography (US) images exhibit limited generalization,\nemphasizing the need for transformative approaches. We observe that individual\nUS frames may lack sufficient information to capture disease manifestation.\nThis study advocates for a paradigm shift towards video-based GBC detection,\nleveraging the inherent advantages of spatiotemporal representations. Employing\nthe Masked Autoencoder (MAE) for representation learning, we address\nshortcomings in conventional image-based methods. We propose a novel design\ncalled FocusMAE to systematically bias the selection of masking tokens from\nhigh-information regions, fostering a more refined representation of\nmalignancy. Additionally, we contribute the most extensive US video dataset for\nGBC detection. We also note that, this is the first study on US video-based GBC\ndetection. We validate the proposed methods on the curated dataset, and report\na new state-of-the-art (SOTA) accuracy of 96.4% for the GBC detection problem,\nagainst an accuracy of 84% by current Image-based SOTA - GBCNet, and RadFormer,\nand 94.7% by Video-based SOTA - AdaMAE. We further demonstrate the generality\nof the proposed FocusMAE on a public CT-based Covid detection dataset,\nreporting an improvement in accuracy by 3.3% over current baselines. The source\ncode and pretrained models are available at:\nhttps://gbc-iitd.github.io/focusmae\n","authors":["Soumen Basu","Mayuna Gupta","Chetan Madan","Pankaj Gupta","Chetan Arora"],"pdf_url":"https://arxiv.org/pdf/2403.08848v2.pdf","comment":"To Appear at CVPR 2024"},{"id":"http://arxiv.org/abs/2402.01858v2","updated":"2024-03-29T21:18:37Z","published":"2024-02-02T19:28:33Z","title":"Explaining latent representations of generative models with large\n  multimodal models","summary":"  Learning interpretable representations of data generative latent factors is\nan important topic for the development of artificial intelligence. With the\nrise of the large multimodal model, it can align images with text to generate\nanswers. In this work, we propose a framework to comprehensively explain each\nlatent variable in the generative models using a large multimodal model. We\nfurther measure the uncertainty of our generated explanations, quantitatively\nevaluate the performance of explanation generation among multiple large\nmultimodal models, and qualitatively visualize the variations of each latent\nvariable to learn the disentanglement effects of different generative models on\nexplanations. Finally, we discuss the explanatory capabilities and limitations\nof state-of-the-art large multimodal models.\n","authors":["Mengdan Zhu","Zhenke Liu","Bo Pan","Abhinav Angirekula","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.01858v2.pdf","comment":"ICLR 2024 Workshop Paper on Reliable and Responsible Foundation\n  Models"},{"id":"http://arxiv.org/abs/2311.10696v2","updated":"2024-03-29T20:17:29Z","published":"2023-11-17T18:28:32Z","title":"Versatile Medical Image Segmentation Learned from Multi-Source Datasets\n  via Model Self-Disambiguation","summary":"  A versatile medical image segmentation model applicable to images acquired\nwith diverse equipment and protocols can facilitate model deployment and\nmaintenance. However, building such a model typically demands a large, diverse,\nand fully annotated dataset, which is challenging to obtain due to the\nlabor-intensive nature of data curation. To address this challenge, we propose\na cost-effective alternative that harnesses multi-source data with only partial\nor sparse segmentation labels for training, substantially reducing the cost of\ndeveloping a versatile model. We devise strategies for model\nself-disambiguation, prior knowledge incorporation, and imbalance mitigation to\ntackle challenges associated with inconsistently labeled multi-source data,\nincluding label ambiguity and modality, dataset, and class imbalances.\nExperimental results on a multi-modal dataset compiled from eight different\nsources for abdominal structure segmentation have demonstrated the\neffectiveness and superior performance of our method compared to\nstate-of-the-art alternative approaches. We anticipate that its cost-saving\nfeatures, which optimize the utilization of existing annotated data and reduce\nannotation efforts for new data, will have a significant impact in the field.\n","authors":["Xiaoyang Chen","Hao Zheng","Yuemeng Li","Yuncong Ma","Liang Ma","Hongming Li","Yong Fan"],"pdf_url":"https://arxiv.org/pdf/2311.10696v2.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2201.09929v3","updated":"2024-03-29T20:10:38Z","published":"2022-01-24T19:37:04Z","title":"Euclidean and Affine Curve Reconstruction","summary":"  We consider practical aspects of reconstructing planar curves with prescribed\nEuclidean or affine curvatures. These curvatures are invariant under the\nspecial Euclidean group and the equi-affine groups, respectively, and play an\nimportant role in computer vision and shape analysis. We discuss and implement\nalgorithms for such reconstruction, and give estimates on how close\nreconstructed curves are relative to the closeness of their curvatures in\nappropriate metrics. Several illustrative examples are provided.\n","authors":["Jose Agudelo","Brooke Dippold","Ian Klein","Alex Kokot","Eric Geiger","Irina Kogan"],"pdf_url":"https://arxiv.org/pdf/2201.09929v3.pdf","comment":"This paper is a result of an REU project conducted at the North\n  Carolina State University in the Summer and Fall 2020. This version has\n  several minor corrections"},{"id":"http://arxiv.org/abs/2312.03816v3","updated":"2024-03-29T19:50:38Z","published":"2023-12-06T18:56:14Z","title":"AVID: Any-Length Video Inpainting with Diffusion Model","summary":"  Recent advances in diffusion models have successfully enabled text-guided\nimage inpainting. While it seems straightforward to extend such editing\ncapability into the video domain, there have been fewer works regarding\ntext-guided video inpainting. Given a video, a masked region at its initial\nframe, and an editing prompt, it requires a model to do infilling at each frame\nfollowing the editing guidance while keeping the out-of-mask region intact.\nThere are three main challenges in text-guided video inpainting: ($i$) temporal\nconsistency of the edited video, ($ii$) supporting different inpainting types\nat different structural fidelity levels, and ($iii$) dealing with variable\nvideo length. To address these challenges, we introduce Any-Length Video\nInpainting with Diffusion Model, dubbed as AVID. At its core, our model is\nequipped with effective motion modules and adjustable structure guidance, for\nfixed-length video inpainting. Building on top of that, we propose a novel\nTemporal MultiDiffusion sampling pipeline with a middle-frame attention\nguidance mechanism, facilitating the generation of videos with any desired\nduration. Our comprehensive experiments show our model can robustly deal with\nvarious inpainting types at different video duration ranges, with high quality.\nMore visualization results are made publicly available at\nhttps://zhang-zx.github.io/AVID/ .\n","authors":["Zhixing Zhang","Bichen Wu","Xiaoyan Wang","Yaqiao Luo","Luxin Zhang","Yinan Zhao","Peter Vajda","Dimitris Metaxas","Licheng Yu"],"pdf_url":"https://arxiv.org/pdf/2312.03816v3.pdf","comment":"Project website: https://zhang-zx.github.io/AVID/"},{"id":"http://arxiv.org/abs/2402.07245v2","updated":"2024-03-29T19:47:50Z","published":"2024-02-11T17:09:21Z","title":"Semi-Mamba-UNet: Pixel-Level Contrastive and Pixel-Level\n  Cross-Supervised Visual Mamba-based UNet for Semi-Supervised Medical Image\n  Segmentation","summary":"  Medical image segmentation is essential in diagnostics, treatment planning,\nand healthcare, with deep learning offering promising advancements. Notably,\nConvolutional Neural Network (CNN) excel in capturing local image features,\nwhereas Vision Transformer (ViT) adeptly model long-range dependencies through\nmulti-head self-attention mechanisms. Despite their strengths, both CNN and ViT\nface challenges in efficiently processing long-range dependencies within\nmedical images, often requiring substantial computational resources. This\nissue, combined with the high cost and limited availability of expert\nannotations, poses significant obstacles to achieving precise segmentation. To\naddress these challenges, this paper introduces the Semi-Mamba-UNet, which\nintegrates a visual mamba-based UNet architecture with a conventional UNet into\na semi-supervised learning (SSL) framework. This innovative SSL approach\nleverages dual networks to jointly generate pseudo labels and cross supervise\neach other, drawing inspiration from consistency regularization techniques.\nFurthermore, we introduce a self-supervised pixel-level contrastive learning\nstrategy, employing a projector pair to further enhance feature learning\ncapabilities. Our comprehensive evaluation on a publicly available MRI cardiac\nsegmentation dataset, comparing against various SSL frameworks with different\nUNet-based segmentation networks, highlights the superior performance of\nSemi-Mamba-UNet. The source code has been made publicly accessible.\n","authors":["Chao Ma","Ziyang Wang"],"pdf_url":"https://arxiv.org/pdf/2402.07245v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12194v2","updated":"2024-03-29T19:37:18Z","published":"2023-11-20T21:20:37Z","title":"DiffAvatar: Simulation-Ready Garment Optimization with Differentiable\n  Simulation","summary":"  The realism of digital avatars is crucial in enabling telepresence\napplications with self-expression and customization. While physical simulations\ncan produce realistic motions for clothed humans, they require high-quality\ngarment assets with associated physical parameters for cloth simulations.\nHowever, manually creating these assets and calibrating their parameters is\nlabor-intensive and requires specialized expertise. Current methods focus on\nreconstructing geometry, but don't generate complete assets for physics-based\napplications. To address this gap, we propose \\papername,~a novel approach that\nperforms body and garment co-optimization using differentiable simulation. By\nintegrating physical simulation into the optimization loop and accounting for\nthe complex nonlinear behavior of cloth and its intricate interaction with the\nbody, our framework recovers body and garment geometry and extracts important\nmaterial parameters in a physically plausible way. Our experiments demonstrate\nthat our approach generates realistic clothing and body shape suitable for\ndownstream applications. We provide additional insights and results on our\nwebpage: https://people.csail.mit.edu/liyifei/publication/diffavatar/\n","authors":["Yifei Li","Hsiao-yu Chen","Egor Larionov","Nikolaos Sarafianos","Wojciech Matusik","Tuur Stuyck"],"pdf_url":"https://arxiv.org/pdf/2311.12194v2.pdf","comment":"CVPR 2024; Project page:\n  https://people.csail.mit.edu/liyifei/publication/diffavatar/"},{"id":"http://arxiv.org/abs/2403.11821v2","updated":"2024-03-29T19:27:23Z","published":"2024-03-18T14:24:20Z","title":"Evaluating Text-to-Image Synthesis: Survey and Taxonomy of Image Quality\n  Metrics","summary":"  Recent advances in text-to-image synthesis enabled through a combination of\nlanguage and vision foundation models have led to a proliferation of the tools\navailable and an increased attention to the field. When conducting\ntext-to-image synthesis, a central goal is to ensure that the content between\ntext and image is aligned. As such, there exist numerous evaluation metrics\nthat aim to mimic human judgement. However, it is often unclear which metric to\nuse for evaluating text-to-image synthesis systems as their evaluation is\nhighly nuanced. In this work, we provide a comprehensive overview of existing\ntext-to-image evaluation metrics. Based on our findings, we propose a new\ntaxonomy for categorizing these metrics. Our taxonomy is grounded in the\nassumption that there are two main quality criteria, namely compositionality\nand generality, which ideally map to human preferences. Ultimately, we derive\nguidelines for practitioners conducting text-to-image evaluation, discuss open\nchallenges of evaluation mechanisms, and surface limitations of current\nmetrics.\n","authors":["Sebastian Hartwig","Dominik Engel","Leon Sick","Hannah Kniesel","Tristan Payer","Poonam Poonam","Michael Glöckler","Alex Bäuerle","Timo Ropinski"],"pdf_url":"https://arxiv.org/pdf/2403.11821v2.pdf","comment":"preprint, 21 pages, 2 figures, 1 table"},{"id":"http://arxiv.org/abs/2403.15571v2","updated":"2024-03-29T19:14:36Z","published":"2024-03-22T18:52:10Z","title":"Augmented Reality Warnings in Roadway Work Zones: Evaluating the Effect\n  of Modality on Worker Reaction Times","summary":"  Given the aging highway infrastructure requiring extensive rebuilding and\nenhancements, and the consequent rise in the number of work zones, there is an\nurgent need to develop advanced safety systems to protect workers. While\nAugmented Reality (AR) holds significant potential for delivering warnings to\nworkers, its integration into roadway work zones remains relatively unexplored.\nThe primary objective of this study is to improve safety measures within\nroadway work zones by conducting an extensive analysis of how different\ncombinations of multimodal AR warnings influence the reaction times of workers.\nThis paper addresses this gap through a series of experiments that aim to\nreplicate the distinctive conditions of roadway work zones, both in real-world\nand virtual reality environments. Our approach comprises three key components:\nan advanced AR system prototype, a VR simulation of AR functionality within the\nwork zone environment, and the Wizard of Oz technique to synchronize user\nexperiences across experiments. To assess reaction times, we leverage both the\nsimple reaction time (SRT) technique and an innovative vision-based metric that\nutilizes real-time pose estimation. By conducting five experiments in\ncontrolled outdoor work zones and indoor VR settings, our study provides\nvaluable information on how various multimodal AR warnings impact workers\nreaction times. Furthermore, our findings reveal the disparities in reaction\ntimes between VR simulations and real-world scenarios, thereby gauging VR's\ncapability to mirror the dynamics of roadway work zones. Furthermore, our\nresults substantiate the potential and reliability of vision-based reaction\ntime measurements. These insights resonate well with those derived using the\nSRT technique, underscoring the viability of this approach for tangible\nreal-world uses.\n","authors":["Sepehr Sabeti","Fatemeh Banani Ardecani","Omidreza Shoghli"],"pdf_url":"https://arxiv.org/pdf/2403.15571v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01482v2","updated":"2024-03-29T18:52:59Z","published":"2024-01-03T01:11:16Z","title":"Incorporating Geo-Diverse Knowledge into Prompting for Increased\n  Geographical Robustness in Object Recognition","summary":"  Existing object recognition models have been shown to lack robustness in\ndiverse geographical scenarios due to domain shifts in design and context.\nClass representations need to be adapted to more accurately reflect an object\nconcept under these shifts. In the absence of training data from target\ngeographies, we hypothesize that geographically diverse descriptive knowledge\nof categories can enhance robustness. For this purpose, we explore the\nfeasibility of probing a large language model for geography-based object\nknowledge, and we examine the effects of integrating knowledge into zero-shot\nand learnable soft prompting with CLIP. Within this exploration, we propose\ngeography knowledge regularization to ensure that soft prompts trained on a\nsource set of geographies generalize to an unseen target set. Accuracy gains\nover prompting baselines on DollarStreet while training only on Europe data are\nup to +2.8/1.2/1.6 on target data from Africa/Asia/Americas, and +4.6 overall\non the hardest classes. Competitive performance is shown vs. few-shot target\ntraining, and analysis is provided to direct future study of geographical\nrobustness.\n","authors":["Kyle Buettner","Sina Malakouti","Xiang Lorraine Li","Adriana Kovashka"],"pdf_url":"https://arxiv.org/pdf/2401.01482v2.pdf","comment":"To appear in IEEE/CVF Computer Vision and Pattern Recognition\n  Conference (CVPR), 2024"},{"id":"http://arxiv.org/abs/2403.17801v2","updated":"2024-03-29T18:45:35Z","published":"2024-03-26T15:40:05Z","title":"Towards 3D Vision with Low-Cost Single-Photon Cameras","summary":"  We present a method for reconstructing 3D shape of arbitrary Lambertian\nobjects based on measurements by miniature, energy-efficient, low-cost\nsingle-photon cameras. These cameras, operating as time resolved image sensors,\nilluminate the scene with a very fast pulse of diffuse light and record the\nshape of that pulse as it returns back from the scene at a high temporal\nresolution. We propose to model this image formation process, account for its\nnon-idealities, and adapt neural rendering to reconstruct 3D geometry from a\nset of spatially distributed sensors with known poses. We show that our\napproach can successfully recover complex 3D shapes from simulated data. We\nfurther demonstrate 3D object reconstruction from real-world captures,\nutilizing measurements from a commodity proximity sensor. Our work draws a\nconnection between image-based modeling and active range scanning and is a step\ntowards 3D vision with single-photon cameras.\n","authors":["Fangzhou Mu","Carter Sifferman","Sacha Jungerman","Yiquan Li","Mark Han","Michael Gleicher","Mohit Gupta","Yin Li"],"pdf_url":"https://arxiv.org/pdf/2403.17801v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00604v2","updated":"2024-03-29T18:33:30Z","published":"2023-12-31T23:04:25Z","title":"SteinDreamer: Variance Reduction for Text-to-3D Score Distillation via\n  Stein Identity","summary":"  Score distillation has emerged as one of the most prevalent approaches for\ntext-to-3D asset synthesis. Essentially, score distillation updates 3D\nparameters by lifting and back-propagating scores averaged over different\nviews. In this paper, we reveal that the gradient estimation in score\ndistillation is inherent to high variance. Through the lens of variance\nreduction, the effectiveness of SDS and VSD can be interpreted as applications\nof various control variates to the Monte Carlo estimator of the distilled\nscore. Motivated by this rethinking and based on Stein's identity, we propose a\nmore general solution to reduce variance for score distillation, termed Stein\nScore Distillation (SSD). SSD incorporates control variates constructed by\nStein identity, allowing for arbitrary baseline functions. This enables us to\ninclude flexible guidance priors and network architectures to explicitly\noptimize for variance reduction. In our experiments, the overall pipeline,\ndubbed SteinDreamer, is implemented by instantiating the control variate with a\nmonocular depth estimator. The results suggest that SSD can effectively reduce\nthe distillation variance and consistently improve visual quality for both\nobject- and scene-level generation. Moreover, we demonstrate that SteinDreamer\nachieves faster convergence than existing methods due to more stable gradient\nupdates.\n","authors":["Peihao Wang","Zhiwen Fan","Dejia Xu","Dilin Wang","Sreyas Mohan","Forrest Iandola","Rakesh Ranjan","Yilei Li","Qiang Liu","Zhangyang Wang","Vikas Chandra"],"pdf_url":"https://arxiv.org/pdf/2401.00604v2.pdf","comment":"Project page: https://vita-group.github.io/SteinDreamer/"},{"id":"http://arxiv.org/abs/2307.08919v3","updated":"2024-03-29T18:19:36Z","published":"2023-07-18T01:31:47Z","title":"Systematic comparison of semi-supervised and self-supervised learning\n  for medical image classification","summary":"  In typical medical image classification problems, labeled data is scarce\nwhile unlabeled data is more available. Semi-supervised learning and\nself-supervised learning are two different research directions that can improve\naccuracy by learning from extra unlabeled data. Recent methods from both\ndirections have reported significant gains on traditional benchmarks. Yet past\nbenchmarks do not focus on medical tasks and rarely compare self- and semi-\nmethods together on an equal footing. Furthermore, past benchmarks often handle\nhyperparameter tuning suboptimally. First, they may not tune hyperparameters at\nall, leading to underfitting. Second, when tuning does occur, it often\nunrealistically uses a labeled validation set that is much larger than the\ntraining set. Therefore currently published rankings might not always\ncorroborate with their practical utility This study contributes a systematic\nevaluation of self- and semi- methods with a unified experimental protocol\nintended to guide a practitioner with scarce overall labeled data and a limited\ncompute budget. We answer two key questions: Can hyperparameter tuning be\neffective with realistic-sized validation sets? If so, when all methods are\ntuned well, which self- or semi-supervised methods achieve the best accuracy?\nOur study compares 13 representative semi- and self-supervised methods to\nstrong labeled-set-only baselines on 4 medical datasets. From 20000+ GPU hours\nof computation, we provide valuable best practices to resource-constrained\npractitioners: hyperparameter tuning is effective, and the semi-supervised\nmethod known as MixMatch delivers the most reliable gains across 4 datasets.\n","authors":["Zhe Huang","Ruijie Jiang","Shuchin Aeron","Michael C. Hughes"],"pdf_url":"https://arxiv.org/pdf/2307.08919v3.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18784v2","updated":"2024-03-29T18:16:29Z","published":"2024-03-27T17:32:04Z","title":"SplatFace: Gaussian Splat Face Reconstruction Leveraging an Optimizable\n  Surface","summary":"  We present SplatFace, a novel Gaussian splatting framework designed for 3D\nhuman face reconstruction without reliance on accurate pre-determined geometry.\nOur method is designed to simultaneously deliver both high-quality novel view\nrendering and accurate 3D mesh reconstructions. We incorporate a generic 3D\nMorphable Model (3DMM) to provide a surface geometric structure, making it\npossible to reconstruct faces with a limited set of input images. We introduce\na joint optimization strategy that refines both the Gaussians and the morphable\nsurface through a synergistic non-rigid alignment process. A novel distance\nmetric, splat-to-surface, is proposed to improve alignment by considering both\nthe Gaussian position and covariance. The surface information is also utilized\nto incorporate a world-space densification process, resulting in superior\nreconstruction quality. Our experimental analysis demonstrates that the\nproposed method is competitive with both other Gaussian splatting techniques in\nnovel view synthesis and other 3D reconstruction methods in producing 3D face\nmeshes with high geometric precision.\n","authors":["Jiahao Luo","Jing Liu","James Davis"],"pdf_url":"https://arxiv.org/pdf/2403.18784v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00909v2","updated":"2024-03-29T18:04:37Z","published":"2023-12-31T22:47:06Z","title":"Taming Mode Collapse in Score Distillation for Text-to-3D Generation","summary":"  Despite the remarkable performance of score distillation in text-to-3D\ngeneration, such techniques notoriously suffer from view inconsistency issues,\nalso known as \"Janus\" artifact, where the generated objects fake each view with\nmultiple front faces. Although empirically effective methods have approached\nthis problem via score debiasing or prompt engineering, a more rigorous\nperspective to explain and tackle this problem remains elusive. In this paper,\nwe reveal that the existing score distillation-based text-to-3D generation\nframeworks degenerate to maximal likelihood seeking on each view independently\nand thus suffer from the mode collapse problem, manifesting as the Janus\nartifact in practice. To tame mode collapse, we improve score distillation by\nre-establishing the entropy term in the corresponding variational objective,\nwhich is applied to the distribution of rendered images. Maximizing the entropy\nencourages diversity among different views in generated 3D assets, thereby\nmitigating the Janus problem. Based on this new objective, we derive a new\nupdate rule for 3D score distillation, dubbed Entropic Score Distillation\n(ESD). We theoretically reveal that ESD can be simplified and implemented by\njust adopting the classifier-free guidance trick upon variational score\ndistillation. Although embarrassingly straightforward, our extensive\nexperiments successfully demonstrate that ESD can be an effective treatment for\nJanus artifacts in score distillation.\n","authors":["Peihao Wang","Dejia Xu","Zhiwen Fan","Dilin Wang","Sreyas Mohan","Forrest Iandola","Rakesh Ranjan","Yilei Li","Qiang Liu","Zhangyang Wang","Vikas Chandra"],"pdf_url":"https://arxiv.org/pdf/2401.00909v2.pdf","comment":"Project page: https://vita-group.github.io/3D-Mode-Collapse/"},{"id":"http://arxiv.org/abs/2401.06407v2","updated":"2024-03-29T18:02:27Z","published":"2024-01-12T07:04:44Z","title":"UAV-Borne Mapping Algorithms for Low-Altitude and High-Speed Drone\n  Applications","summary":"  This article presents an analysis of current state-of-the-art sensors and how\nthese sensors work with several mapping algorithms for UAV (Unmanned Aerial\nVehicle) applications, focusing on low-altitude and high-speed scenarios. A new\nexperimental construct is created using highly realistic environments made\npossible by integrating the AirSim simulator with Google 3D maps models using\nthe Cesium Tiles plugin. Experiments are conducted in this high-realism\nsimulated environment to evaluate the performance of three distinct mapping\nalgorithms: (1) Direct Sparse Odometry (DSO), (2) Stereo DSO (SDSO), and (3)\nDSO Lite (DSOL). Experimental results evaluate algorithms based on their\nmeasured geometric accuracy and computational speed. The results provide\nvaluable insights into the strengths and limitations of each algorithm.\nFindings quantify compromises in UAV algorithm selection, allowing researchers\nto find the mapping solution best suited to their application, which often\nrequires a compromise between computational performance and the density and\naccuracy of geometric map estimates. Results indicate that for UAVs with\nrestrictive computing resources, DSOL is the best option. For systems with\npayload capacity and modest compute resources, SDSO is the best option. If only\none camera is available, DSO is the option to choose for applications that\nrequire dense mapping results.\n","authors":["Jincheng Zhang","Artur Wolek","Andrew R. Willis"],"pdf_url":"https://arxiv.org/pdf/2401.06407v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00762v2","updated":"2024-03-29T18:02:03Z","published":"2024-03-01T18:59:03Z","title":"Point Cloud Mamba: Point Cloud Learning via State Space Model","summary":"  In this work, for the first time, we demonstrate that Mamba-based point cloud\nmethods can outperform point-based methods. Mamba exhibits strong global\nmodeling capabilities and linear computational complexity, making it highly\nattractive for point cloud analysis. To enable more effective processing of 3-D\npoint cloud data by Mamba, we propose a novel Consistent Traverse Serialization\nto convert point clouds into 1-D point sequences while ensuring that\nneighboring points in the sequence are also spatially adjacent. Consistent\nTraverse Serialization yields six variants by permuting the order of x, y, and\nz coordinates, and the synergistic use of these variants aids Mamba in\ncomprehensively observing point cloud data. Furthermore, to assist Mamba in\nhandling point sequences with different orders more effectively, we introduce\npoint prompts to inform Mamba of the sequence's arrangement rules. Finally, we\npropose positional encoding based on spatial coordinate mapping to inject\npositional information into point cloud sequences better. Based on these\nimprovements, we construct a point cloud network named Point Cloud Mamba, which\ncombines local and global modeling. Point Cloud Mamba surpasses the SOTA\npoint-based method PointNeXt and achieves new SOTA performance on the\nScanObjectNN, ModelNet40, and ShapeNetPart datasets.\n","authors":["Tao Zhang","Xiangtai Li","Haobo Yuan","Shunping Ji","Shuicheng Yan"],"pdf_url":"https://arxiv.org/pdf/2403.00762v2.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2403.20298v1","updated":"2024-03-29T17:15:21Z","published":"2024-03-29T17:15:21Z","title":"Review-Based Cross-Domain Recommendation via Hyperbolic Embedding and\n  Hierarchy-Aware Domain Disentanglement","summary":"  The issue of data sparsity poses a significant challenge to recommender\nsystems. In response to this, algorithms that leverage side information such as\nreview texts have been proposed. Furthermore, Cross-Domain Recommendation\n(CDR), which captures domain-shareable knowledge and transfers it from a richer\ndomain (source) to a sparser one (target), has received notable attention.\nNevertheless, the majority of existing methodologies assume a Euclidean\nembedding space, encountering difficulties in accurately representing richer\ntext information and managing complex interactions between users and items.\nThis paper advocates a hyperbolic CDR approach based on review texts for\nmodeling user-item relationships. We first emphasize that conventional\ndistance-based domain alignment techniques may cause problems because small\nmodifications in hyperbolic geometry result in magnified perturbations,\nultimately leading to the collapse of hierarchical structures. To address this\nchallenge, we propose hierarchy-aware embedding and domain alignment schemes\nthat adjust the scale to extract domain-shareable information without\ndisrupting structural forms. The process involves the initial embedding of\nreview texts in hyperbolic space, followed by feature extraction incorporating\ndegree-based normalization and structure alignment. We conducted extensive\nexperiments to substantiate the efficiency, robustness, and scalability of our\nproposed model in comparison to state-of-the-art baselines.\n","authors":["Yoonhyuk Choi"],"pdf_url":"https://arxiv.org/pdf/2403.20298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20296v1","updated":"2024-03-29T17:13:18Z","published":"2024-03-29T17:13:18Z","title":"Aiming at the Target: Filter Collaborative Information for Cross-Domain\n  Recommendation","summary":"  Cross-domain recommender (CDR) systems aim to enhance the performance of the\ntarget domain by utilizing data from other related domains. However, irrelevant\ninformation from the source domain may instead degrade target domain\nperformance, which is known as the negative transfer problem. There have been\nsome attempts to address this problem, mostly by designing adaptive\nrepresentations for overlapped users. Whereas, representation adaptions solely\nrely on the expressive capacity of the CDR model, lacking explicit constraint\nto filter the irrelevant source-domain collaborative information for the target\ndomain.\n  In this paper, we propose a novel Collaborative information regularized User\nTransformation (CUT) framework to tackle the negative transfer problem by\ndirectly filtering users' collaborative information. In CUT, user similarity in\nthe target domain is adopted as a constraint for user transformation learning\nto filter the user collaborative information from the source domain. CUT first\nlearns user similarity relationships from the target domain. Then,\nsource-target information transfer is guided by the user similarity, where we\ndesign a user transformation layer to learn target-domain user representations\nand a contrastive loss to supervise the user collaborative information\ntransferred. The results show significant performance improvement of CUT\ncompared with SOTA single and cross-domain methods. Further analysis of the\ntarget-domain results illustrates that CUT can effectively alleviate the\nnegative transfer problem.\n","authors":["Hanyu Li","Weizhi Ma","Peijie Sun","Jiayu Li","Cunxiang Yin","Yancheng He","Guoqiang Xu","Min Zhang","Shaoping Ma"],"pdf_url":"https://arxiv.org/pdf/2403.20296v1.pdf","comment":"Accepted by SIGIR 2024"},{"id":"http://arxiv.org/abs/2403.20222v1","updated":"2024-03-29T15:07:21Z","published":"2024-03-29T15:07:21Z","title":"Shallow Cross-Encoders for Low-Latency Retrieval","summary":"  Transformer-based Cross-Encoders achieve state-of-the-art effectiveness in\ntext retrieval. However, Cross-Encoders based on large transformer models (such\nas BERT or T5) are computationally expensive and allow for scoring only a small\nnumber of documents within a reasonably small latency window. However, keeping\nsearch latencies low is important for user satisfaction and energy usage. In\nthis paper, we show that weaker shallow transformer models (i.e., transformers\nwith a limited number of layers) actually perform better than full-scale models\nwhen constrained to these practical low-latency settings since they can\nestimate the relevance of more documents in the same time budget. We further\nshow that shallow transformers may benefit from the generalized Binary\nCross-Entropy (gBCE) training scheme, which has recently demonstrated success\nfor recommendation tasks. Our experiments with TREC Deep Learning passage\nranking query sets demonstrate significant improvements in shallow and\nfull-scale models in low-latency scenarios. For example, when the latency limit\nis 25ms per query, MonoBERT-Large (a cross-encoder based on a full-scale BERT\nmodel) is only able to achieve NDCG@10 of 0.431 on TREC DL 2019, while\nTinyBERT-gBCE (a cross-encoder based on TinyBERT trained with gBCE) reaches\nNDCG@10 of 0.652, a +51% gain over MonoBERT-Large. We also show that shallow\nCross-Encoders are effective even when used without a GPU (e.g., with CPU\ninference, NDCG@10 decreases only by 3% compared to GPU inference with 50ms\nlatency), which makes Cross-Encoders practical to run even without specialized\nhardware acceleration.\n","authors":["Aleksandr V. Petrov","Sean MacAvaney","Craig Macdonald"],"pdf_url":"https://arxiv.org/pdf/2403.20222v1.pdf","comment":"Accepted by ECIR2024"},{"id":"http://arxiv.org/abs/2403.11624v3","updated":"2024-03-29T14:20:17Z","published":"2024-03-18T09:56:00Z","title":"Dual-Channel Multiplex Graph Neural Networks for Recommendation","summary":"  Efficient recommender systems play a crucial role in accurately capturing\nuser and item attributes that mirror individual preferences. Some existing\nrecommendation techniques have started to shift their focus towards modeling\nvarious types of interaction relations between users and items in real-world\nrecommendation scenarios, such as clicks, marking favorites, and purchases on\nonline shopping platforms. Nevertheless, these approaches still grapple with\ntwo significant shortcomings: (1) Insufficient modeling and exploitation of the\nimpact of various behavior patterns formed by multiplex relations between users\nand items on representation learning, and (2) ignoring the effect of different\nrelations in the behavior patterns on the target relation in recommender system\nscenarios. In this study, we introduce a novel recommendation framework,\nDual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the\naforementioned challenges. It incorporates an explicit behavior pattern\nrepresentation learner to capture the behavior patterns composed of multiplex\nuser-item interaction relations, and includes a relation chain representation\nlearning and a relation chain-aware encoder to discover the impact of various\nauxiliary relations on the target relation, the dependencies between different\nrelations, and mine the appropriate order of relations in a behavior pattern.\nExtensive experiments on three real-world datasets demonstrate that our \\model\nsurpasses various state-of-the-art recommendation methods. It outperforms the\nbest baselines by 10.06\\% and 12.15\\% on average across all datasets in terms\nof R@10 and N@10 respectively.\n","authors":["Xiang Li","Chaofan Fu","Zhongying Zhao","Guanjie Zheng","Chao Huang","Junyu Dong","Yanwei Yu"],"pdf_url":"https://arxiv.org/pdf/2403.11624v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20197v1","updated":"2024-03-29T14:19:26Z","published":"2024-03-29T14:19:26Z","title":"Dual Simplex Volume Maximization for Simplex-Structured Matrix\n  Factorization","summary":"  Simplex-structured matrix factorization (SSMF) is a generalization of\nnonnegative matrix factorization, a fundamental interpretable data analysis\nmodel, and has applications in hyperspectral unmixing and topic modeling. To\nobtain identifiable solutions, a standard approach is to find minimum-volume\nsolutions. By taking advantage of the duality/polarity concept for polytopes,\nwe convert minimum-volume SSMF in the primal space to a maximum-volume problem\nin the dual space. We first prove the identifiability of this maximum-volume\ndual problem. Then, we use this dual formulation to provide a novel\noptimization approach which bridges the gap between two existing families of\nalgorithms for SSMF, namely volume minimization and facet identification.\nNumerical experiments show that the proposed approach performs favorably\ncompared to the state-of-the-art SSMF algorithms.\n","authors":["Maryam Abdolali","Giovanni Barbarino","Nicolas Gillis"],"pdf_url":"https://arxiv.org/pdf/2403.20197v1.pdf","comment":"31 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.20107v1","updated":"2024-03-29T10:40:45Z","published":"2024-03-29T10:40:45Z","title":"Robust Federated Contrastive Recommender System against Model Poisoning\n  Attack","summary":"  Federated Recommender Systems (FedRecs) have garnered increasing attention\nrecently, thanks to their privacy-preserving benefits. However, the\ndecentralized and open characteristics of current FedRecs present two dilemmas.\nFirst, the performance of FedRecs is compromised due to highly sparse on-device\ndata for each client. Second, the system's robustness is undermined by the\nvulnerability to model poisoning attacks launched by malicious users. In this\npaper, we introduce a novel contrastive learning framework designed to fully\nleverage the client's sparse data through embedding augmentation, referred to\nas CL4FedRec. Unlike previous contrastive learning approaches in FedRecs that\nnecessitate clients to share their private parameters, our CL4FedRec aligns\nwith the basic FedRec learning protocol, ensuring compatibility with most\nexisting FedRec implementations. We then evaluate the robustness of FedRecs\nequipped with CL4FedRec by subjecting it to several state-of-the-art model\npoisoning attacks. Surprisingly, our observations reveal that contrastive\nlearning tends to exacerbate the vulnerability of FedRecs to these attacks.\nThis is attributed to the enhanced embedding uniformity, making the polluted\ntarget item embedding easily proximate to popular items. Based on this insight,\nwe propose an enhanced and robust version of CL4FedRec (rCL4FedRec) by\nintroducing a regularizer to maintain the distance among item embeddings with\ndifferent popularity levels. Extensive experiments conducted on four commonly\nused recommendation datasets demonstrate that CL4FedRec significantly enhances\nboth the model's performance and the robustness of FedRecs.\n","authors":["Wei Yuan","Chaoqun Yang","Liang Qu","Guanhua Ye","Quoc Viet Hung Nguyen","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2403.20107v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20095v1","updated":"2024-03-29T10:18:19Z","published":"2024-03-29T10:18:19Z","title":"KGUF: Simple Knowledge-aware Graph-based Recommender with User-based\n  Semantic Features Filtering","summary":"  The recent integration of Graph Neural Networks (GNNs) into recommendation\nhas led to a novel family of Collaborative Filtering (CF) approaches, namely\nGraph Collaborative Filtering (GCF). Following the same GNNs wave, recommender\nsystems exploiting Knowledge Graphs (KGs) have also been successfully empowered\nby the GCF rationale to combine the representational power of GNNs with the\nsemantics conveyed by KGs, giving rise to Knowledge-aware Graph Collaborative\nFiltering (KGCF), which use KGs to mine hidden user intent. Nevertheless,\nempirical evidence suggests that computing and combining user-level intent\nmight not always be necessary, as simpler approaches can yield comparable or\nsuperior results while keeping explicit semantic features. Under this\nperspective, user historical preferences become essential to refine the KG and\nretain the most discriminating features, thus leading to concise item\nrepresentation. Driven by the assumptions above, we propose KGUF, a KGCF model\nthat learns latent representations of semantic features in the KG to better\ndefine the item profile. By leveraging user profiles through decision trees,\nKGUF effectively retains only those features relevant to users. Results on\nthree datasets justify KGUF's rationale, as our approach is able to reach\nperformance comparable or superior to SOTA methods while maintaining a simpler\nformalization. Link to the repository: https://github.com/sisinflab/KGUF.\n","authors":["Salvatore Bufi","Alberto Carlo Maria Mancino","Antonio Ferrara","Daniele Malitesta","Tommaso Di Noia","Eugenio Di Sciascio"],"pdf_url":"https://arxiv.org/pdf/2403.20095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08217v2","updated":"2024-03-29T06:31:04Z","published":"2024-01-16T09:04:17Z","title":"LLM-Guided Multi-View Hypergraph Learning for Human-Centric Explainable\n  Recommendation","summary":"  As personalized recommendation systems become vital in the age of information\noverload, traditional methods relying solely on historical user interactions\noften fail to fully capture the multifaceted nature of human interests. To\nenable more human-centric modeling of user preferences, this work proposes a\nnovel explainable recommendation framework, i.e., LLMHG, synergizing the\nreasoning capabilities of large language models (LLMs) and the structural\nadvantages of hypergraph neural networks. By effectively profiling and\ninterpreting the nuances of individual user interests, our framework pioneers\nenhancements to recommendation systems with increased explainability. We\nvalidate that explicitly accounting for the intricacies of human preferences\nallows our human-centric and explainable LLMHG approach to consistently\noutperform conventional models across diverse real-world datasets. The proposed\nplug-and-play enhancement framework delivers immediate gains in recommendation\nperformance while offering a pathway to apply advanced LLMs for better\ncapturing the complexity of human interests across machine learning\napplications.\n","authors":["Zhixuan Chu","Yan Wang","Qing Cui","Longfei Li","Wenqing Chen","Zhan Qin","Kui Ren"],"pdf_url":"https://arxiv.org/pdf/2401.08217v2.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.13001v3","updated":"2024-03-29T05:51:53Z","published":"2023-10-06T12:31:05Z","title":"Conversational Financial Information Retrieval Model (ConFIRM)","summary":"  With the exponential growth in large language models (LLMs), leveraging their\nemergent properties for specialized domains like finance merits exploration.\nHowever, regulated fields such as finance pose unique constraints, requiring\ndomain-optimized frameworks. We present ConFIRM, an LLM-based conversational\nfinancial information retrieval model tailored for query intent classification\nand knowledge base labeling.\n  ConFIRM comprises two modules:\n  1) a method to synthesize finance domain-specific question-answer pairs, and\n  2) evaluation of parameter efficient fine-tuning approaches for the query\nclassification task. We generate a dataset of over 4000 samples, assessing\naccuracy on a separate test set.\n  ConFIRM achieved over 90% accuracy, essential for regulatory compliance.\nConFIRM provides a data-efficient solution to extract precise query intent for\nfinancial dialog systems.\n","authors":["Stephen Choi","William Gazeley","Siu Ho Wong","Tingting Li"],"pdf_url":"https://arxiv.org/pdf/2310.13001v3.pdf","comment":"10 pages, 2 figures, 2 tables, 2 appendices"},{"id":"http://arxiv.org/abs/2403.19899v1","updated":"2024-03-29T00:45:42Z","published":"2024-03-29T00:45:42Z","title":"Inclusive Design Insights from a Preliminary Image-Based Conversational\n  Search Systems Evaluation","summary":"  The digital realm has witnessed the rise of various search modalities, among\nwhich the Image-Based Conversational Search System stands out. This research\ndelves into the design, implementation, and evaluation of this specific system,\njuxtaposing it against its text-based and mixed counterparts. A diverse\nparticipant cohort ensures a broad evaluation spectrum. Advanced tools\nfacilitate emotion analysis, capturing user sentiments during interactions,\nwhile structured feedback sessions offer qualitative insights. Results indicate\nthat while the text-based system minimizes user confusion, the image-based\nsystem presents challenges in direct information interpretation. However, the\nmixed system achieves the highest engagement, suggesting an optimal blend of\nvisual and textual information. Notably, the potential of these systems,\nespecially the image-based modality, to assist individuals with intellectual\ndisabilities is highlighted. The study concludes that the Image-Based\nConversational Search System, though challenging in some aspects, holds\npromise, especially when integrated into a mixed system, offering both clarity\nand engagement.\n","authors":["Yue Zheng","Lei Yu","Junmian Chen","Tianyu Xia","Yuanyuan Yin","Shan Wang","Haiming Liu"],"pdf_url":"https://arxiv.org/pdf/2403.19899v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19889v1","updated":"2024-03-29T00:14:46Z","published":"2024-03-29T00:14:46Z","title":"Towards a Robust Retrieval-Based Summarization System","summary":"  This paper describes an investigation of the robustness of large language\nmodels (LLMs) for retrieval augmented generation (RAG)-based summarization\ntasks. While LLMs provide summarization capabilities, their performance in\ncomplex, real-world scenarios remains under-explored. Our first contribution is\nLogicSumm, an innovative evaluation framework incorporating realistic scenarios\nto assess LLM robustness during RAG-based summarization. Based on limitations\nidentified by LogiSumm, we then developed SummRAG, a comprehensive system to\ncreate training dialogues and fine-tune a model to enhance robustness within\nLogicSumm's scenarios. SummRAG is an example of our goal of defining structured\nmethods to test the capabilities of an LLM, rather than addressing issues in a\none-off fashion. Experimental results confirm the power of SummRAG, showcasing\nimproved logical coherence and summarization quality. Data, corresponding model\nweights, and Python code are available online.\n","authors":["Shengjie Liu","Jing Wu","Jingyuan Bao","Wenyi Wang","Naira Hovakimyan","Christopher G Healey"],"pdf_url":"https://arxiv.org/pdf/2403.19889v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2403.20331v1","updated":"2024-03-29T17:59:53Z","published":"2024-03-29T17:59:53Z","title":"Unsolvable Problem Detection: Evaluating Trustworthiness of Vision\n  Language Models","summary":"  This paper introduces a novel and significant challenge for Vision Language\nModels (VLMs), termed Unsolvable Problem Detection (UPD). UPD examines the\nVLM's ability to withhold answers when faced with unsolvable problems in the\ncontext of Visual Question Answering (VQA) tasks. UPD encompasses three\ndistinct settings: Absent Answer Detection (AAD), Incompatible Answer Set\nDetection (IASD), and Incompatible Visual Question Detection (IVQD). To deeply\ninvestigate the UPD problem, extensive experiments indicate that most VLMs,\nincluding GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying\nextents, highlighting significant room for the improvements. To address UPD, we\nexplore both training-free and training-based solutions, offering new insights\ninto their effectiveness and limitations. We hope our insights, together with\nfuture efforts within the proposed UPD settings, will enhance the broader\nunderstanding and development of more practical and reliable VLMs.\n","authors":["Atsuyuki Miyai","Jingkang Yang","Jingyang Zhang","Yifei Ming","Qing Yu","Go Irie","Yixuan Li","Hai Li","Ziwei Liu","Kiyoharu Aizawa"],"pdf_url":"https://arxiv.org/pdf/2403.20331v1.pdf","comment":"Code: https://github.com/AtsuMiyai/UPD"},{"id":"http://arxiv.org/abs/2403.20329v1","updated":"2024-03-29T17:59:06Z","published":"2024-03-29T17:59:06Z","title":"ReALM: Reference Resolution As Language Modeling","summary":"  Reference resolution is an important problem, one that is essential to\nunderstand and successfully handle context of different kinds. This context\nincludes both previous turns and context that pertains to non-conversational\nentities, such as entities on the user's screen or those running in the\nbackground. While LLMs have been shown to be extremely powerful for a variety\nof tasks, their use in reference resolution, particularly for\nnon-conversational entities, remains underutilized. This paper demonstrates how\nLLMs can be used to create an extremely effective system to resolve references\nof various types, by showing how reference resolution can be converted into a\nlanguage modeling problem, despite involving forms of entities like those on\nscreen that are not traditionally conducive to being reduced to a text-only\nmodality. We demonstrate large improvements over an existing system with\nsimilar functionality across different types of references, with our smallest\nmodel obtaining absolute gains of over 5% for on-screen references. We also\nbenchmark against GPT-3.5 and GPT-4, with our smallest model achieving\nperformance comparable to that of GPT-4, and our larger models substantially\noutperforming it.\n","authors":["Joel Ruben Antony Moniz","Soundarya Krishnan","Melis Ozyildirim","Prathamesh Saraf","Halim Cagri Ates","Yuan Zhang","Hong Yu","Nidhi Rajshree"],"pdf_url":"https://arxiv.org/pdf/2403.20329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20328v1","updated":"2024-03-29T17:59:05Z","published":"2024-03-29T17:59:05Z","title":"Learning Visual Quadrupedal Loco-Manipulation from Demonstrations","summary":"  Quadruped robots are progressively being integrated into human environments.\nDespite the growing locomotion capabilities of quadrupedal robots, their\ninteraction with objects in realistic scenes is still limited. While additional\nrobotic arms on quadrupedal robots enable manipulating objects, they are\nsometimes redundant given that a quadruped robot is essentially a mobile unit\nequipped with four limbs, each possessing 3 degrees of freedom (DoFs). Hence,\nwe aim to empower a quadruped robot to execute real-world manipulation tasks\nusing only its legs. We decompose the loco-manipulation process into a\nlow-level reinforcement learning (RL)-based controller and a high-level\nBehavior Cloning (BC)-based planner. By parameterizing the manipulation\ntrajectory, we synchronize the efforts of the upper and lower layers, thereby\nleveraging the advantages of both RL and BC. Our approach is validated through\nsimulations and real-world experiments, demonstrating the robot's ability to\nperform tasks that demand mobility and high precision, such as lifting a basket\nfrom the ground while moving, closing a dishwasher, pressing a button, and\npushing a door. Project website: https://zhengmaohe.github.io/leg-manip\n","authors":["Zhengmao He","Kun Lei","Yanjie Ze","Koushil Sreenath","Zhongyu Li","Huazhe Xu"],"pdf_url":"https://arxiv.org/pdf/2403.20328v1.pdf","comment":"Project website: https://zhengmaohe.github.io/leg-manip"},{"id":"http://arxiv.org/abs/2403.20324v1","updated":"2024-03-29T17:51:50Z","published":"2024-03-29T17:51:50Z","title":"Localising the Seizure Onset Zone from Single-Pulse Electrical\n  Stimulation Responses with a Transformer","summary":"  Epilepsy is one of the most common neurological disorders, and many patients\nrequire surgical intervention when medication fails to control seizures. For\neffective surgical outcomes, precise localisation of the epileptogenic focus -\noften approximated through the Seizure Onset Zone (SOZ) - is critical yet\nremains a challenge. Active probing through electrical stimulation is already\nstandard clinical practice for identifying epileptogenic areas. This paper\nadvances the application of deep learning for SOZ localisation using Single\nPulse Electrical Stimulation (SPES) responses. We achieve this by introducing\nTransformer models that incorporate cross-channel attention. We evaluate these\nmodels on held-out patient test sets to assess their generalisability to unseen\npatients and electrode placements.\n  Our study makes three key contributions: Firstly, we implement an existing\ndeep learning model to compare two SPES analysis paradigms - namely, divergent\nand convergent. These paradigms evaluate outward and inward effective\nconnections, respectively. Our findings reveal a notable improvement in moving\nfrom a divergent (AUROC: 0.574) to a convergent approach (AUROC: 0.666),\nmarking the first application of the latter in this context. Secondly, we\ndemonstrate the efficacy of the Transformer models in handling heterogeneous\nelectrode placements, increasing the AUROC to 0.730. Lastly, by incorporating\ninter-trial variability, we further refine the Transformer models, with an\nAUROC of 0.745, yielding more consistent predictions across patients. These\nadvancements provide a deeper insight into SOZ localisation and represent a\nsignificant step in modelling patient-specific intracranial EEG electrode\nplacements in SPES. Future work will explore integrating these models into\nclinical decision-making processes to bridge the gap between deep learning\nresearch and practical healthcare applications.\n","authors":["Jamie Norris","Aswin Chari","Gerald Cooray","Martin Tisdall","Karl Friston","Richard Rosch"],"pdf_url":"https://arxiv.org/pdf/2403.20324v1.pdf","comment":"15 pages, 7 figures, submitted to CHIL 2024"},{"id":"http://arxiv.org/abs/2310.11256v2","updated":"2024-03-29T17:50:17Z","published":"2023-10-17T13:22:36Z","title":"Gromov-Wassertein-like Distances in the Gaussian Mixture Models Space","summary":"  The Gromov-Wasserstein (GW) distance is frequently used in machine learning\nto compare distributions across distinct metric spaces. Despite its utility, it\nremains computationally intensive, especially for large-scale problems.\nRecently, a novel Wasserstein distance specifically tailored for Gaussian\nmixture models and known as MW (mixture Wasserstein) has been introduced by\nseveral authors. In scenarios where data exhibit clustering, this approach\nsimplifies to a small-scale discrete optimal transport problem, which\ncomplexity depends solely on the number of Gaussian components in the GMMs.\nThis paper aims to extend MW by introducing new Gromov-type distances. These\ndistances are designed to be isometry-invariant in Euclidean spaces and are\napplicable for comparing GMMs across different dimensional spaces. Our first\ncontribution is the Mixture Gromov Wasserstein distance (MGW), which can be\nviewed as a Gromovized version of MW. This new distance has a straightforward\ndiscrete formulation, making it highly efficient for estimating distances\nbetween GMMs in practical applications. To facilitate the derivation of a\ntransport plan between GMMs, we present a second distance, the Embedded\nWasserstein distance (EW). This distance turns out to be closely related to\nseveral recent alternatives to Gromov-Wasserstein. We show that EW can be\nadapted to derive a distance as well as optimal transportation plans between\nGMMs. We demonstrate the efficiency of these newly proposed distances on medium\nto large-scale problems, including shape matching and hyperspectral image color\ntransfer.\n","authors":["Antoine Salmona","Julie Delon","Agnès Desolneux"],"pdf_url":"https://arxiv.org/pdf/2310.11256v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2403.20320v1","updated":"2024-03-29T17:43:58Z","published":"2024-03-29T17:43:58Z","title":"MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning","summary":"  Adapting models pre-trained on large-scale datasets to a variety of\ndownstream tasks is a common strategy in deep learning. Consequently,\nparameter-efficient fine-tuning methods have emerged as a promising way to\nadapt pre-trained models to different tasks while training only a minimal\nnumber of parameters. While most of these methods are designed for single-task\nadaptation, parameter-efficient training in Multi-Task Learning (MTL)\narchitectures is still unexplored. In this paper, we introduce MTLoRA, a novel\nframework for parameter-efficient training of MTL models. MTLoRA employs\nTask-Agnostic and Task-Specific Low-Rank Adaptation modules, which effectively\ndisentangle the parameter space in MTL fine-tuning, thereby enabling the model\nto adeptly handle both task specialization and interaction within MTL contexts.\nWe applied MTLoRA to hierarchical-transformer-based MTL architectures, adapting\nthem to multiple downstream dense prediction tasks. Our extensive experiments\non the PASCAL dataset show that MTLoRA achieves higher accuracy on downstream\ntasks compared to fully fine-tuning the MTL model while reducing the number of\ntrainable parameters by 3.6x. Furthermore, MTLoRA establishes a Pareto-optimal\ntrade-off between the number of trainable parameters and the accuracy of the\ndownstream tasks, outperforming current state-of-the-art parameter-efficient\ntraining methods in both accuracy and efficiency. Our code is publicly\navailable.\n","authors":["Ahmed Agiza","Marina Neseem","Sherief Reda"],"pdf_url":"https://arxiv.org/pdf/2403.20320v1.pdf","comment":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2024"},{"id":"http://arxiv.org/abs/2304.05301v2","updated":"2024-03-29T17:34:59Z","published":"2023-04-11T15:50:54Z","title":"TACOS: Topology-Aware Collective Algorithm Synthesizer for Distributed\n  Machine Learning","summary":"  The surge of artificial intelligence, specifically large language models, has\nled to a rapid advent towards the development of large-scale machine learning\ntraining clusters. Collective communications within these clusters tend to be\nheavily bandwidth-bound, necessitating techniques to optimally utilize the\navailable network bandwidth. This puts the routing algorithm for the collective\nat the forefront of determining the performance. Unfortunately, communication\nlibraries used in distributed machine learning today are limited by a fixed set\nof routing algorithms. This constraints collective performance within the\ndomain of next-generation training clusters that employ intricate,\nheterogeneous, and asymmetric, large-scale topologies. Further, the emergence\nof irregular topologies attributed to runtime phenomena such as device failures\nserves to compound the complexity of the challenge. To this end, this paper\nintroduces TACOS, an automated synthesizer that generates topology-aware\ncollective algorithms for common distributed machine learning collectives\nacross arbitrary input network topologies. TACOS was able to synthesize\nAll-Reduce algorithm for a heterogeneous 512-NPU system in just 6.09 minutes\nwhile achieving performance improvement up to 4.27x over state-of-the-art prior\nwork. TACOS exhibits high scalability, with synthesis time scaling\nquadratically with the number of NPUs. In contrast to prior works' NP-hard\napproaches, TACOS with 40K NPUs completes in 2.52 hours.\n","authors":["William Won","Midhilesh Elavazhagan","Sudarshan Srinivasan","Ajaya Durg","Samvit Kaul","Swati Gupta","Tushar Krishna"],"pdf_url":"https://arxiv.org/pdf/2304.05301v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18274v2","updated":"2024-03-29T17:34:40Z","published":"2023-10-27T16:59:51Z","title":"LipSim: A Provably Robust Perceptual Similarity Metric","summary":"  Recent years have seen growing interest in developing and applying perceptual\nsimilarity metrics. Research has shown the superiority of perceptual metrics\nover pixel-wise metrics in aligning with human perception and serving as a\nproxy for the human visual system. On the other hand, as perceptual metrics\nrely on neural networks, there is a growing concern regarding their resilience,\ngiven the established vulnerability of neural networks to adversarial attacks.\nIt is indeed logical to infer that perceptual metrics may inherit both the\nstrengths and shortcomings of neural networks. In this work, we demonstrate the\nvulnerability of state-of-the-art perceptual similarity metrics based on an\nensemble of ViT-based feature extractors to adversarial attacks. We then\npropose a framework to train a robust perceptual similarity metric called\nLipSim (Lipschitz Similarity Metric) with provable guarantees. By leveraging\n1-Lipschitz neural networks as the backbone, LipSim provides guarded areas\naround each data point and certificates for all perturbations within an\n$\\ell_2$ ball. Finally, a comprehensive set of experiments shows the\nperformance of LipSim in terms of natural and certified scores and on the image\nretrieval application. The code is available at\nhttps://github.com/SaraGhazanfari/LipSim.\n","authors":["Sara Ghazanfari","Alexandre Araujo","Prashanth Krishnamurthy","Farshad Khorrami","Siddharth Garg"],"pdf_url":"https://arxiv.org/pdf/2310.18274v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20298v1","updated":"2024-03-29T17:15:21Z","published":"2024-03-29T17:15:21Z","title":"Review-Based Cross-Domain Recommendation via Hyperbolic Embedding and\n  Hierarchy-Aware Domain Disentanglement","summary":"  The issue of data sparsity poses a significant challenge to recommender\nsystems. In response to this, algorithms that leverage side information such as\nreview texts have been proposed. Furthermore, Cross-Domain Recommendation\n(CDR), which captures domain-shareable knowledge and transfers it from a richer\ndomain (source) to a sparser one (target), has received notable attention.\nNevertheless, the majority of existing methodologies assume a Euclidean\nembedding space, encountering difficulties in accurately representing richer\ntext information and managing complex interactions between users and items.\nThis paper advocates a hyperbolic CDR approach based on review texts for\nmodeling user-item relationships. We first emphasize that conventional\ndistance-based domain alignment techniques may cause problems because small\nmodifications in hyperbolic geometry result in magnified perturbations,\nultimately leading to the collapse of hierarchical structures. To address this\nchallenge, we propose hierarchy-aware embedding and domain alignment schemes\nthat adjust the scale to extract domain-shareable information without\ndisrupting structural forms. The process involves the initial embedding of\nreview texts in hyperbolic space, followed by feature extraction incorporating\ndegree-based normalization and structure alignment. We conducted extensive\nexperiments to substantiate the efficiency, robustness, and scalability of our\nproposed model in comparison to state-of-the-art baselines.\n","authors":["Yoonhyuk Choi"],"pdf_url":"https://arxiv.org/pdf/2403.20298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04855v2","updated":"2024-03-29T17:10:55Z","published":"2023-11-08T17:50:17Z","title":"Algorithms for Non-Negative Matrix Factorization on Noisy Data With\n  Negative Values","summary":"  Non-negative matrix factorization (NMF) is a dimensionality reduction\ntechnique that has shown promise for analyzing noisy data, especially\nastronomical data. For these datasets, the observed data may contain negative\nvalues due to noise even when the true underlying physical signal is strictly\npositive. Prior NMF work has not treated negative data in a statistically\nconsistent manner, which becomes problematic for low signal-to-noise data with\nmany negative values. In this paper we present two algorithms, Shift-NMF and\nNearly-NMF, that can handle both the noisiness of the input data and also any\nintroduced negativity. Both of these algorithms use the negative data space\nwithout clipping, and correctly recover non-negative signals without any\nintroduced positive offset that occurs when clipping negative data. We\ndemonstrate this numerically on both simple and more realistic examples, and\nprove that both algorithms have monotonically decreasing update rules.\n","authors":["Dylan Green","Stephen Bailey"],"pdf_url":"https://arxiv.org/pdf/2311.04855v2.pdf","comment":"12 pages, 8 figures. Submitted to IEEE Transactions on Signal\n  Processing. Updated version after reviewer comments, expanding paper with a\n  new section and a new appendix as well as more equations. Algorithm\n  derivation flow was significantly altered to be more tractable"},{"id":"http://arxiv.org/abs/2403.20287v1","updated":"2024-03-29T16:58:13Z","published":"2024-03-29T16:58:13Z","title":"Benchmarking Counterfactual Image Generation","summary":"  Counterfactual image generation is pivotal for understanding the causal\nrelations of variables, with applications in interpretability and generation of\nunbiased synthetic data. However, evaluating image generation is a\nlong-standing challenge in itself. The need to evaluate counterfactual\ngeneration compounds on this challenge, precisely because counterfactuals, by\ndefinition, are hypothetical scenarios without observable ground truths. In\nthis paper, we present a novel comprehensive framework aimed at benchmarking\ncounterfactual image generation methods. We incorporate metrics that focus on\nevaluating diverse aspects of counterfactuals, such as composition,\neffectiveness, minimality of interventions, and image realism. We assess the\nperformance of three distinct conditional image generation model types, based\non the Structural Causal Model paradigm. Our work is accompanied by a\nuser-friendly Python package which allows to further evaluate and benchmark\nexisting and future counterfactual image generation methods. Our framework is\nextendable to additional SCM and other causal methods, generative models, and\ndatasets.\n","authors":["Thomas Melistas","Nikos Spyrou","Nefeli Gkouti","Pedro Sanchez","Athanasios Vlontzos","Giorgos Papanastasiou","Sotirios A. Tsaftaris"],"pdf_url":"https://arxiv.org/pdf/2403.20287v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15905v4","updated":"2024-03-29T16:53:58Z","published":"2024-03-23T18:19:02Z","title":"Towards Low-Energy Adaptive Personalization for Resource-Constrained\n  Devices","summary":"  The personalization of machine learning (ML) models to address data drift is\na significant challenge in the context of Internet of Things (IoT)\napplications. Presently, most approaches focus on fine-tuning either the full\nbase model or its last few layers to adapt to new data, while often neglecting\nenergy costs. However, various types of data drift exist, and fine-tuning the\nfull base model or the last few layers may not result in optimal performance in\ncertain scenarios. We propose Target Block Fine-Tuning (TBFT), a low-energy\nadaptive personalization framework designed for resource-constrained devices.\nWe categorize data drift and personalization into three types: input-level,\nfeature-level, and output-level. For each type, we fine-tune different blocks\nof the model to achieve optimal performance with reduced energy costs.\nSpecifically, input-, feature-, and output-level correspond to fine-tuning the\nfront, middle, and rear blocks of the model. We evaluate TBFT on a ResNet\nmodel, three datasets, three different training sizes, and a Raspberry Pi.\nCompared with the $Block Avg$, where each block is fine-tuned individually and\ntheir performance improvements are averaged, TBFT exhibits an improvement in\nmodel accuracy by an average of 15.30% whilst saving 41.57% energy consumption\non average compared with full fine-tuning.\n","authors":["Yushan Huang","Josh Millar","Yuxuan Long","Yuchen Zhao","Hamed Haddadi"],"pdf_url":"https://arxiv.org/pdf/2403.15905v4.pdf","comment":"Accepetd to The 4th Workshop on Machine Learning and Systems\n  (EuroMLSys '24)"},{"id":"http://arxiv.org/abs/2403.20284v1","updated":"2024-03-29T16:53:11Z","published":"2024-03-29T16:53:11Z","title":"LayerNorm: A key component in parameter-efficient fine-tuning","summary":"  Fine-tuning a pre-trained model, such as Bidirectional Encoder\nRepresentations from Transformers (BERT), has been proven to be an effective\nmethod for solving many natural language processing (NLP) tasks. However, due\nto the large number of parameters in many state-of-the-art NLP models,\nincluding BERT, the process of fine-tuning is computationally expensive. One\nattractive solution to this issue is parameter-efficient fine-tuning, which\ninvolves modifying only a minimal segment of the model while keeping the\nremainder unchanged. Yet, it remains unclear which segment of the BERT model is\ncrucial for fine-tuning. In this paper, we first analyze different components\nin the BERT model to pinpoint which one undergoes the most significant changes\nafter fine-tuning. We find that output LayerNorm changes more than any other\ncomponents when fine-tuned for different General Language Understanding\nEvaluation (GLUE) tasks. Then we show that only fine-tuning the LayerNorm can\nreach comparable, or in some cases better, performance to full fine-tuning and\nother parameter-efficient fine-tuning methods. Moreover, we use Fisher\ninformation to determine the most critical subset of LayerNorm and demonstrate\nthat many NLP tasks in the GLUE benchmark can be solved by fine-tuning only a\nsmall portion of LayerNorm with negligible performance degradation.\n","authors":["Taha ValizadehAslani","Hualou Liang"],"pdf_url":"https://arxiv.org/pdf/2403.20284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20280v1","updated":"2024-03-29T16:49:40Z","published":"2024-03-29T16:49:40Z","title":"Sparse multimodal fusion with modal channel attention","summary":"  The ability of masked multimodal transformer architectures to learn a robust\nembedding space when modality samples are sparsely aligned is studied by\nmeasuring the quality of generated embedding spaces as a function of modal\nsparsity. An extension to the masked multimodal transformer model is proposed\nwhich incorporates modal-incomplete channels in the multihead attention\nmechanism called modal channel attention (MCA). Two datasets with 4 modalities\nare used, CMU-MOSEI for multimodal sentiment recognition and TCGA for\nmultiomics. Models are shown to learn uniform and aligned embedding spaces with\nonly two out of four modalities in most samples. It was found that, even with\nno modal sparsity, the proposed MCA mechanism improves the quality of generated\nembedding spaces, recall metrics, and subsequent performance on downstream\ntasks.\n","authors":["Josiah Bjorgaard"],"pdf_url":"https://arxiv.org/pdf/2403.20280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04670v2","updated":"2024-03-29T16:39:28Z","published":"2023-12-07T20:11:03Z","title":"Rapid Motor Adaptation for Robotic Manipulator Arms","summary":"  Developing generalizable manipulation skills is a core challenge in embodied\nAI. This includes generalization across diverse task configurations,\nencompassing variations in object shape, density, friction coefficient, and\nexternal disturbances such as forces applied to the robot. Rapid Motor\nAdaptation (RMA) offers a promising solution to this challenge. It posits that\nessential hidden variables influencing an agent's task performance, such as\nobject mass and shape, can be effectively inferred from the agent's action and\nproprioceptive history. Drawing inspiration from RMA in locomotion and in-hand\nrotation, we use depth perception to develop agents tailored for rapid motor\nadaptation in a variety of manipulation tasks. We evaluated our agents on four\nchallenging tasks from the Maniskill2 benchmark, namely pick-and-place\noperations with hundreds of objects from the YCB and EGAD datasets, peg\ninsertion with precise position and orientation, and operating a variety of\nfaucets and handles, with customized environment variations. Empirical results\ndemonstrate that our agents surpass state-of-the-art methods like automatic\ndomain randomization and vision-based policies, obtaining better generalization\nperformance and sample efficiency.\n","authors":["Yichao Liang","Kevin Ellis","João Henriques"],"pdf_url":"https://arxiv.org/pdf/2312.04670v2.pdf","comment":"Accepted at CVPR 2024. 12 pages"},{"id":"http://arxiv.org/abs/2403.20266v1","updated":"2024-03-29T16:16:48Z","published":"2024-03-29T16:16:48Z","title":"Latxa: An Open Language Model and Evaluation Suite for Basque","summary":"  We introduce Latxa, a family of large language models for Basque ranging from\n7 to 70 billion parameters. Latxa is based on Llama 2, which we continue\npretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens.\nAddressing the scarcity of high-quality benchmarks for Basque, we further\nintroduce 4 multiple choice evaluation datasets: EusProficiency, comprising\n5,169 questions from official language proficiency exams; EusReading,\ncomprising 352 reading comprehension questions; EusTrivia, comprising 1,715\ntrivia questions from 5 knowledge areas; and EusExams, comprising 16,774\nquestions from public examinations. In our extensive evaluation, Latxa\noutperforms all previous open models we compare to by a large margin. In\naddition, it is competitive with GPT-4 Turbo in language proficiency and\nunderstanding, despite lagging behind in reading comprehension and\nknowledge-intensive tasks. Both the Latxa family of models, as well as our new\npretraining corpora and evaluation datasets, are publicly available under open\nlicenses at https://github.com/hitz-zentroa/latxa. Our suite enables\nreproducible research on methods to build LLMs for low-resource languages.\n","authors":["Julen Etxaniz","Oscar Sainz","Naiara Perez","Itziar Aldabe","German Rigau","Eneko Agirre","Aitor Ormazabal","Mikel Artetxe","Aitor Soroa"],"pdf_url":"https://arxiv.org/pdf/2403.20266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16970v2","updated":"2024-03-29T16:14:41Z","published":"2024-03-25T17:31:12Z","title":"Joint chest X-ray diagnosis and clinical visual attention prediction\n  with multi-stage cooperative learning: enhancing interpretability","summary":"  As deep learning has become the state-of-the-art for computer-assisted\ndiagnosis, interpretability of the automatic decisions is crucial for clinical\ndeployment. While various methods were proposed in this domain, visual\nattention maps of clinicians during radiological screening offer a unique asset\nto provide important insights and can potentially enhance the quality of\ncomputer-assisted diagnosis. With this paper, we introduce a novel\ndeep-learning framework for joint disease diagnosis and prediction of\ncorresponding visual saliency maps for chest X-ray scans. Specifically, we\ndesigned a novel dual-encoder multi-task UNet, which leverages both a\nDenseNet201 backbone and a Residual and Squeeze-and-Excitation block-based\nencoder to extract diverse features for saliency map prediction, and a\nmulti-scale feature-fusion classifier to perform disease classification. To\ntackle the issue of asynchronous training schedules of individual tasks in\nmulti-task learning, we proposed a multi-stage cooperative learning strategy,\nwith contrastive learning for feature encoder pretraining to boost performance.\nExperiments show that our proposed method outperformed existing techniques for\nchest X-ray diagnosis and the quality of visual saliency map prediction.\n","authors":["Zirui Qiu","Hassan Rivaz","Yiming Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.16970v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20262v1","updated":"2024-03-29T16:13:31Z","published":"2024-03-29T16:13:31Z","title":"ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language\n  Models","summary":"  Research on Large Language Models (LLMs) has recently witnessed an increasing\ninterest in extending models' context size to better capture dependencies\nwithin long documents. While benchmarks have been proposed to assess long-range\nabilities, existing efforts primarily considered generic tasks that are not\nnecessarily aligned with real-world applications. In contrast, our work\nproposes a new benchmark for long-context LLMs focused on a practical meeting\nassistant scenario. In this scenario, the long contexts consist of transcripts\nobtained by automatic speech recognition, presenting unique challenges for LLMs\ndue to the inherent noisiness and oral nature of such data. Our benchmark,\nnamed ELITR-Bench, augments the existing ELITR corpus' transcripts with 271\nmanually crafted questions and their ground-truth answers. Our experiments with\nrecent long-context LLMs on ELITR-Bench highlight a gap between open-source and\nproprietary models, especially when questions are asked sequentially within a\nconversation. We also provide a thorough analysis of our GPT-4-based evaluation\nmethod, encompassing insights from a crowdsourcing study. Our findings suggest\nthat while GPT-4's evaluation scores are correlated with human judges', its\nability to differentiate among more than three score levels may be limited.\n","authors":["Thibaut Thonet","Jos Rozen","Laurent Besacier"],"pdf_url":"https://arxiv.org/pdf/2403.20262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20261v1","updated":"2024-03-29T16:10:34Z","published":"2024-03-29T16:10:34Z","title":"FABind+: Enhancing Molecular Docking through Improved Pocket Prediction\n  and Pose Generation","summary":"  Molecular docking is a pivotal process in drug discovery. While traditional\ntechniques rely on extensive sampling and simulation governed by physical\nprinciples, these methods are often slow and costly. The advent of deep\nlearning-based approaches has shown significant promise, offering increases in\nboth accuracy and efficiency. Building upon the foundational work of FABind, a\nmodel designed with a focus on speed and accuracy, we present FABind+, an\nenhanced iteration that largely boosts the performance of its predecessor. We\nidentify pocket prediction as a critical bottleneck in molecular docking and\npropose a novel methodology that significantly refines pocket prediction,\nthereby streamlining the docking process. Furthermore, we introduce\nmodifications to the docking module to enhance its pose generation\ncapabilities. In an effort to bridge the gap with conventional\nsampling/generative methods, we incorporate a simple yet effective sampling\ntechnique coupled with a confidence model, requiring only minor adjustments to\nthe regression framework of FABind. Experimental results and analysis reveal\nthat FABind+ remarkably outperforms the original FABind, achieves competitive\nstate-of-the-art performance, and delivers insightful modeling strategies. This\ndemonstrates FABind+ represents a substantial step forward in molecular docking\nand drug discovery. Our code is in https://github.com/QizhiPei/FABind.\n","authors":["Kaiyuan Gao","Qizhi Pei","Jinhua Zhu","Tao Qin","Kun He","Tie-Yan Liu","Lijun Wu"],"pdf_url":"https://arxiv.org/pdf/2403.20261v1.pdf","comment":"17 pages, 14 figures, 5 tables"},{"id":"http://arxiv.org/abs/2403.20253v1","updated":"2024-03-29T15:59:11Z","published":"2024-03-29T15:59:11Z","title":"MedCLIP-SAM: Bridging Text and Image Towards Universal Medical Image\n  Segmentation","summary":"  Medical image segmentation of anatomical structures and pathology is crucial\nin modern clinical diagnosis, disease study, and treatment planning. To date,\ngreat progress has been made in deep learning-based segmentation techniques,\nbut most methods still lack data efficiency, generalizability, and\ninteractability. Consequently, the development of new, precise segmentation\nmethods that demand fewer labeled datasets is of utmost importance in medical\nimage analysis. Recently, the emergence of foundation models, such as CLIP and\nSegment-Anything-Model (SAM), with comprehensive cross-domain representation\nopened the door for interactive and universal image segmentation. However,\nexploration of these models for data-efficient medical image segmentation is\nstill limited, but is highly necessary. In this paper, we propose a novel\nframework, called MedCLIP-SAM that combines CLIP and SAM models to generate\nsegmentation of clinical scans using text prompts in both zero-shot and weakly\nsupervised settings. To achieve this, we employed a new Decoupled Hard Negative\nNoise Contrastive Estimation (DHN-NCE) loss to fine-tune the BiomedCLIP model\nand the recent gScoreCAM to generate prompts to obtain segmentation masks from\nSAM in a zero-shot setting. Additionally, we explored the use of zero-shot\nsegmentation labels in a weakly supervised paradigm to improve the segmentation\nquality further. By extensively testing three diverse segmentation tasks and\nmedical image modalities (breast tumor ultrasound, brain tumor MRI, and lung\nX-ray), our proposed framework has demonstrated excellent accuracy.\n","authors":["Taha Koleilat","Hojat Asgariandehkordi","Hassan Rivaz","Yiming Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.20253v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.20252v1","updated":"2024-03-29T15:58:46Z","published":"2024-03-29T15:58:46Z","title":"Using LLMs to Model the Beliefs and Preferences of Targeted Populations","summary":"  We consider the problem of aligning a large language model (LLM) to model the\npreferences of a human population. Modeling the beliefs, preferences, and\nbehaviors of a specific population can be useful for a variety of different\napplications, such as conducting simulated focus groups for new products,\nconducting virtual surveys, and testing behavioral interventions, especially\nfor interventions that are expensive, impractical, or unethical. Existing work\nhas had mixed success using LLMs to accurately model human behavior in\ndifferent contexts. We benchmark and evaluate two well-known fine-tuning\napproaches and evaluate the resulting populations on their ability to match the\npreferences of real human respondents on a survey of preferences for battery\nelectric vehicles (BEVs). We evaluate our models against their ability to match\npopulation-wide statistics as well as their ability to match individual\nresponses, and we investigate the role of temperature in controlling the\ntrade-offs between these two. Additionally, we propose and evaluate a novel\nloss term to improve model performance on responses that require a numeric\nresponse.\n","authors":["Keiichi Namikoshi","Alex Filipowicz","David A. Shamma","Rumen Iliev","Candice L. Hogan","Nikos Arechiga"],"pdf_url":"https://arxiv.org/pdf/2403.20252v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20250v1","updated":"2024-03-29T15:55:06Z","published":"2024-03-29T15:55:06Z","title":"Optimal Policy Learning with Observational Data in Multi-Action\n  Scenarios: Estimation, Risk Preference, and Potential Failures","summary":"  This paper deals with optimal policy learning (OPL) with observational data,\ni.e. data-driven optimal decision-making, in multi-action (or multi-arm)\nsettings, where a finite set of decision options is available. It is organized\nin three parts, where I discuss respectively: estimation, risk preference, and\npotential failures. The first part provides a brief review of the key\napproaches to estimating the reward (or value) function and optimal policy\nwithin this context of analysis. Here, I delineate the identification\nassumptions and statistical properties related to offline optimal policy\nlearning estimators. In the second part, I delve into the analysis of decision\nrisk. This analysis reveals that the optimal choice can be influenced by the\ndecision maker's attitude towards risks, specifically in terms of the trade-off\nbetween reward conditional mean and conditional variance. Here, I present an\napplication of the proposed model to real data, illustrating that the average\nregret of a policy with multi-valued treatment is contingent on the\ndecision-maker's attitude towards risk. The third part of the paper discusses\nthe limitations of optimal data-driven decision-making by highlighting\nconditions under which decision-making can falter. This aspect is linked to the\nfailure of the two fundamental assumptions essential for identifying the\noptimal choice: (i) overlapping, and (ii) unconfoundedness. Some conclusions\nend the paper.\n","authors":["Giovanni Cerulli"],"pdf_url":"https://arxiv.org/pdf/2403.20250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.09244v2","updated":"2024-03-29T15:54:02Z","published":"2023-07-18T13:23:23Z","title":"Energy Efficient Deep Multi-Label ON/OFF Classification of Low Frequency\n  Metered Home Appliances","summary":"  Non-intrusive load monitoring (NILM) is the process of obtaining\nappliance-level data from a single metering point, measuring total electricity\nconsumption of a household or a business. Appliance-level data can be directly\nused for demand response applications and energy management systems as well as\nfor awareness raising and motivation for improvements in energy efficiency.\nRecently, classical machine learning and deep learning (DL) techniques became\nvery popular and proved as highly effective for NILM classification, but with\nthe growing complexity these methods are faced with significant computational\nand energy demands during both their training and operation. In this paper, we\nintroduce a novel DL model aimed at enhanced multi-label classification of NILM\nwith improved computation and energy efficiency. We also propose an evaluation\nmethodology for comparison of different models using data synthesized from the\nmeasurement datasets so as to better represent real-world scenarios. Compared\nto the state-of-the-art, the proposed model has its energy consumption reduced\nby more than 23% while providing on average approximately 8 percentage points\nin performance improvement when evaluating on data derived from REFIT and\nUK-DALE datasets. We also show a 12 percentage point performance advantage of\nthe proposed DL based model over a random forest model and observe performance\ndegradation with the increase of the number of devices in the household, namely\nwith each additional 5 devices, the average performance degrades by\napproximately 7 percentage points.\n","authors":["Anže Pirnat","Blaž Bertalanič","Gregor Cerar","Mihael Mohorčič","Carolina Fortuna"],"pdf_url":"https://arxiv.org/pdf/2307.09244v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20246v1","updated":"2024-03-29T15:45:25Z","published":"2024-03-29T15:45:25Z","title":"Enhancing Dimension-Reduced Scatter Plots with Class and Feature\n  Centroids","summary":"  Dimension reduction is increasingly applied to high-dimensional biomedical\ndata to improve its interpretability. When datasets are reduced to two\ndimensions, each observation is assigned an x and y coordinates and is\nrepresented as a point on a scatter plot. A significant challenge lies in\ninterpreting the meaning of the x and y axes due to the complexities inherent\nin dimension reduction. This study addresses this challenge by using the x and\ny coordinates derived from dimension reduction to calculate class and feature\ncentroids, which can be overlaid onto the scatter plots. This method connects\nthe low-dimension space to the original high-dimensional space. We illustrate\nthe utility of this approach with data derived from the phenotypes of three\nneurogenetic diseases and demonstrate how the addition of class and feature\ncentroids increases the interpretability of scatter plots.\n","authors":["Daniel B. Hier","Tayo Obafemi-Ajayi","Gayla R. Olbricht","Devin M. Burns","Sasha Petrenko","Donald C. Wunsch II"],"pdf_url":"https://arxiv.org/pdf/2403.20246v1.pdf","comment":"Submitted to 46th Annual International Conference of the IEEE\n  Engineering in Medicine and Biology Society"},{"id":"http://arxiv.org/abs/2312.05269v2","updated":"2024-03-29T15:44:05Z","published":"2023-12-07T19:19:25Z","title":"LifelongMemory: Leveraging LLMs for Answering Queries in Long-form\n  Egocentric Videos","summary":"  In this paper we introduce LifelongMemory, a new framework for accessing\nlong-form egocentric videographic memory through natural language question\nanswering and retrieval. LifelongMemory generates concise video activity\ndescriptions of the camera wearer and leverages the zero-shot capabilities of\npretrained large language models to perform reasoning over long-form video\ncontext. Furthermore, Lifelong Memory uses a confidence and explanation module\nto produce confident, high-quality, and interpretable answers. Our approach\nachieves state-of-the-art performance on the EgoSchema benchmark for question\nanswering and is highly competitive on the natural language query (NLQ)\nchallenge of Ego4D. Code is available at\nhttps://github.com/Agentic-Learning-AI-Lab/lifelong-memory.\n","authors":["Ying Wang","Yanlai Yang","Mengye Ren"],"pdf_url":"https://arxiv.org/pdf/2312.05269v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20233v1","updated":"2024-03-29T15:22:03Z","published":"2024-03-29T15:22:03Z","title":"Functional Bilevel Optimization for Machine Learning","summary":"  In this paper, we introduce a new functional point of view on bilevel\noptimization problems for machine learning, where the inner objective is\nminimized over a function space. These types of problems are most often solved\nby using methods developed in the parametric setting, where the inner objective\nis strongly convex with respect to the parameters of the prediction function.\nThe functional point of view does not rely on this assumption and notably\nallows using over-parameterized neural networks as the inner prediction\nfunction. We propose scalable and efficient algorithms for the functional\nbilevel optimization problem and illustrate the benefits of our approach on\ninstrumental regression and reinforcement learning tasks, which admit natural\nfunctional bilevel structures.\n","authors":["Ieva Petrulionyte","Julien Mairal","Michael Arbel"],"pdf_url":"https://arxiv.org/pdf/2403.20233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20230v1","updated":"2024-03-29T15:20:33Z","published":"2024-03-29T15:20:33Z","title":"An FPGA-Based Reconfigurable Accelerator for Convolution-Transformer\n  Hybrid EfficientViT","summary":"  Vision Transformers (ViTs) have achieved significant success in computer\nvision. However, their intensive computations and massive memory footprint\nchallenge ViTs' deployment on embedded devices, calling for efficient ViTs.\nAmong them, EfficientViT, the state-of-the-art one, features a\nConvolution-Transformer hybrid architecture, enhancing both accuracy and\nhardware efficiency. Unfortunately, existing accelerators cannot fully exploit\nthe hardware benefits of EfficientViT due to its unique architecture. In this\npaper, we propose an FPGA-based accelerator for EfficientViT to advance the\nhardware efficiency frontier of ViTs. Specifically, we design a reconfigurable\narchitecture to efficiently support various operation types, including\nlightweight convolutions and attention, boosting hardware utilization.\nAdditionally, we present a time-multiplexed and pipelined dataflow to\nfacilitate both intra- and inter-layer fusions, reducing off-chip data access\ncosts. Experimental results show that our accelerator achieves up to 780.2 GOPS\nin throughput and 105.1 GOPS/W in energy efficiency at 200MHz on the Xilinx\nZCU102 FPGA, which significantly outperforms prior works.\n","authors":["Haikuo Shao","Huihong Shi","Wendong Mao","Zhongfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.20230v1.pdf","comment":"To appear in the 2024 IEEE International Symposium on Circuits and\n  Systems (ISCAS 2024)"},{"id":"http://arxiv.org/abs/2403.20221v1","updated":"2024-03-29T15:05:57Z","published":"2024-03-29T15:05:57Z","title":"Graph Neural Aggregation-diffusion with Metastability","summary":"  Continuous graph neural models based on differential equations have expanded\nthe architecture of graph neural networks (GNNs). Due to the connection between\ngraph diffusion and message passing, diffusion-based models have been widely\nstudied. However, diffusion naturally drives the system towards an equilibrium\nstate, leading to issues like over-smoothing. To this end, we propose GRADE\ninspired by graph aggregation-diffusion equations, which includes the delicate\nbalance between nonlinear diffusion and aggregation induced by interaction\npotentials. The node representations obtained through aggregation-diffusion\nequations exhibit metastability, indicating that features can aggregate into\nmultiple clusters. In addition, the dynamics within these clusters can persist\nfor long time periods, offering the potential to alleviate over-smoothing\neffects. This nonlinear diffusion in our model generalizes existing\ndiffusion-based models and establishes a connection with classical GNNs. We\nprove that GRADE achieves competitive performance across various benchmarks and\nalleviates the over-smoothing issue in GNNs evidenced by the enhanced Dirichlet\nenergy.\n","authors":["Kaiyuan Cui","Xinyan Wang","Zicheng Zhang","Weichen Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.20221v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2402.00786v4","updated":"2024-03-29T14:56:42Z","published":"2024-02-01T17:17:55Z","title":"CroissantLLM: A Truly Bilingual French-English Language Model","summary":"  We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T\nEnglish and French tokens, to bring to the research and industrial community a\nhigh-performance, fully open-sourced bilingual model that runs swiftly on\nconsumer-grade local hardware. To that end, we pioneer the approach of training\nan intrinsically bilingual model with a 1:1 English-to-French pretraining data\nratio, a custom tokenizer, and bilingual finetuning datasets. We release the\ntraining dataset, notably containing a French split with manually curated,\nhigh-quality, and varied data sources. To assess performance outside of\nEnglish, we craft a novel benchmark, FrenchBench, consisting of an array of\nclassification and generation tasks, covering various orthogonal aspects of\nmodel performance in the French Language. Additionally, rooted in transparency\nand to foster further Large Language Model research, we release codebases, and\ndozens of checkpoints across various model sizes, training data distributions,\nand training steps, as well as fine-tuned Chat models, and strong translation\nmodels. We evaluate our model through the FMTI framework, and validate 81 % of\nthe transparency criteria, far beyond the scores of even most open initiatives.\nThis work enriches the NLP landscape, breaking away from previous\nEnglish-centric work in order to strengthen our understanding of\nmultilinguality in language models.\n","authors":["Manuel Faysse","Patrick Fernandes","Nuno M. Guerreiro","António Loison","Duarte M. Alves","Caio Corro","Nicolas Boizard","João Alves","Ricardo Rei","Pedro H. Martins","Antoni Bigata Casademunt","François Yvon","André F. T. Martins","Gautier Viaud","Céline Hudelot","Pierre Colombo"],"pdf_url":"https://arxiv.org/pdf/2402.00786v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20212v1","updated":"2024-03-29T14:47:54Z","published":"2024-03-29T14:47:54Z","title":"On Size and Hardness Generalization in Unsupervised Learning for the\n  Travelling Salesman Problem","summary":"  We study the generalization capability of Unsupervised Learning in solving\nthe Travelling Salesman Problem (TSP). We use a Graph Neural Network (GNN)\ntrained with a surrogate loss function to generate an embedding for each node.\nWe use these embeddings to construct a heat map that indicates the likelihood\nof each edge being part of the optimal route. We then apply local search to\ngenerate our final predictions. Our investigation explores how different\ntraining instance sizes, embedding dimensions, and distributions influence the\noutcomes of Unsupervised Learning methods. Our results show that training with\nlarger instance sizes and increasing embedding dimensions can build a more\neffective representation, enhancing the model's ability to solve TSP.\nFurthermore, in evaluating generalization across different distributions, we\nfirst determine the hardness of various distributions and explore how different\nhardnesses affect the final results. Our findings suggest that models trained\non harder instances exhibit better generalization capabilities, highlighting\nthe importance of selecting appropriate training instances in solving TSP using\nUnsupervised Learning.\n","authors":["Yimeng Min","Carla P. Gomes"],"pdf_url":"https://arxiv.org/pdf/2403.20212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.14426v2","updated":"2024-03-29T14:45:11Z","published":"2022-12-29T18:57:05Z","title":"Restricting to the chip architecture maintains the quantum neural\n  network accuracy","summary":"  In the era of noisy intermediate-scale quantum devices, variational quantum\nalgorithms (VQAs) stand as a prominent strategy for constructing quantum\nmachine learning models. These models comprise both a quantum and a classical\ncomponent. The quantum facet is characterized by a parametrization $U$,\ntypically derived from the composition of various quantum gates. On the other\nhand, the classical component involves an optimizer that adjusts the parameters\nof $U$ to minimize a cost function $C$. Despite the extensive applications of\nVQAs, several critical questions persist, such as determining the optimal gate\nsequence, devising efficient parameter optimization strategies, selecting\nappropriate cost functions, and understanding the influence of quantum chip\narchitectures on the final results. This article aims to address the last\nquestion, emphasizing that, in general, the cost function tends to converge\ntowards an average value as the utilized parameterization approaches a\n$2$-design. Consequently, when the parameterization closely aligns with a\n$2$-design, the quantum neural network model's outcome becomes less dependent\non the specific parametrization. This insight leads to the possibility of\nleveraging the inherent architecture of quantum chips to define the\nparametrization for VQAs. By doing so, the need for additional swap gates is\nmitigated, consequently reducing the depth of VQAs and minimizing associated\nerrors.\n","authors":["Lucas Friedrich","Jonas Maziero"],"pdf_url":"https://arxiv.org/pdf/2212.14426v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20208v1","updated":"2024-03-29T14:41:21Z","published":"2024-03-29T14:41:21Z","title":"Unleashing the Potential of Large Language Models for Predictive Tabular\n  Tasks in Data Science","summary":"  In the domain of data science, the predictive tasks of classification,\nregression, and imputation of missing values are commonly encountered\nchallenges associated with tabular data. This research endeavors to apply Large\nLanguage Models (LLMs) towards addressing these predictive tasks. Despite their\nproficiency in comprehending natural language, LLMs fall short in dealing with\nstructured tabular data. This limitation stems from their lacking exposure to\nthe intricacies of tabular data during their foundational training. Our\nresearch aims to mitigate this gap by compiling a comprehensive corpus of\ntables annotated with instructions and executing large-scale training of\nLlama-2 on this enriched dataset. Furthermore, we investigate the practical\napplication of applying the trained model to zero-shot prediction, few-shot\nprediction, and in-context learning scenarios. Through extensive experiments,\nour methodology has shown significant improvements over existing benchmarks.\nThese advancements highlight the efficacy of tailoring LLM training to solve\ntable-related problems in data science, thereby establishing a new benchmark in\nthe utilization of LLMs for enhancing tabular intelligence.\n","authors":["Yazheng Yang","Yuqi Wang","Sankalok Sen","Lei Li","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2403.20208v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2010.11970v4","updated":"2024-03-29T14:40:04Z","published":"2020-10-22T18:08:58Z","title":"Two-sample Test using Projected Wasserstein Distance","summary":"  We develop a projected Wasserstein distance for the two-sample test, a\nfundamental problem in statistics and machine learning: given two sets of\nsamples, to determine whether they are from the same distribution. In\nparticular, we aim to circumvent the curse of dimensionality in Wasserstein\ndistance: when the dimension is high, it has diminishing testing power, which\nis inherently due to the slow concentration property of Wasserstein metrics in\nthe high dimension space. A key contribution is to couple optimal projection to\nfind the low dimensional linear mapping to maximize the Wasserstein distance\nbetween projected probability distributions. We characterize the theoretical\nproperty of the finite-sample convergence rate on IPMs and present practical\nalgorithms for computing this metric. Numerical examples validate our\ntheoretical results.\n","authors":["Jie Wang","Rui Gao","Yao Xie"],"pdf_url":"https://arxiv.org/pdf/2010.11970v4.pdf","comment":"10 pages, 3 figures. Accepted in ISIT-21, typo in Proposition 3 has\n  been corrected"},{"id":"http://arxiv.org/abs/2309.02671v3","updated":"2024-03-29T14:36:03Z","published":"2023-09-06T02:40:33Z","title":"RLSynC: Offline-Online Reinforcement Learning for Synthon Completion","summary":"  Retrosynthesis is the process of determining the set of reactant molecules\nthat can react to form a desired product. Semi-template-based retrosynthesis\nmethods, which imitate the reverse logic of synthesis reactions, first predict\nthe reaction centers in the products, and then complete the resulting synthons\nback into reactants. We develop a new offline-online reinforcement learning\nmethod RLSynC for synthon completion in semi-template-based methods. RLSynC\nassigns one agent to each synthon, all of which complete the synthons by\nconducting actions step by step in a synchronized fashion. RLSynC learns the\npolicy from both offline training episodes and online interactions, which\nallows RLSynC to explore new reaction spaces. RLSynC uses a standalone forward\nsynthesis model to evaluate the likelihood of the predicted reactants in\nsynthesizing a product, and thus guides the action search. Our results\ndemonstrate that RLSynC can outperform state-of-the-art synthon completion\nmethods with improvements as high as 14.9%, highlighting its potential in\nsynthesis planning.\n","authors":["Frazier N. Baker","Ziqi Chen","Daniel Adu-Ampratwum","Xia Ning"],"pdf_url":"https://arxiv.org/pdf/2309.02671v3.pdf","comment":"32 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.20202v1","updated":"2024-03-29T14:31:36Z","published":"2024-03-29T14:31:36Z","title":"Voice Signal Processing for Machine Learning. The Case of Speaker\n  Isolation","summary":"  The widespread use of automated voice assistants along with other recent\ntechnological developments have increased the demand for applications that\nprocess audio signals and human voice in particular. Voice recognition tasks\nare typically performed using artificial intelligence and machine learning\nmodels. Even though end-to-end models exist, properly pre-processing the signal\ncan greatly reduce the complexity of the task and allow it to be solved with a\nsimpler ML model and fewer computational resources. However, ML engineers who\nwork on such tasks might not have a background in signal processing which is an\nentirely different area of expertise.\n  The objective of this work is to provide a concise comparative analysis of\nFourier and Wavelet transforms that are most commonly used as signal\ndecomposition methods for audio processing tasks. Metrics for evaluating speech\nintelligibility are also discussed, namely Scale-Invariant Signal-to-Distortion\nRatio (SI-SDR), Perceptual Evaluation of Speech Quality (PESQ), and Short-Time\nObjective Intelligibility (STOI). The level of detail in the exposition is\nmeant to be sufficient for an ML engineer to make informed decisions when\nchoosing, fine-tuning, and evaluating a decomposition method for a specific ML\nmodel. The exposition contains mathematical definitions of the relevant\nconcepts accompanied with intuitive non-mathematical explanations in order to\nmake the text more accessible to engineers without deep expertise in signal\nprocessing. Formal mathematical definitions and proofs of theorems are\nintentionally omitted in order to keep the text concise.\n","authors":["Radan Ganchev"],"pdf_url":"https://arxiv.org/pdf/2403.20202v1.pdf","comment":"MSc. thesis. for associated source code, see\n  https://github.com/rganchev/speech-signal-processing-for-ml"},{"id":"http://arxiv.org/abs/2308.12044v5","updated":"2024-03-29T14:25:29Z","published":"2023-08-23T10:08:52Z","title":"A multiobjective continuation method to compute the regularization path\n  of deep neural networks","summary":"  Sparsity is a highly desired feature in deep neural networks (DNNs) since it\nensures numerical efficiency, improves the interpretability of models (due to\nthe smaller number of relevant features), and robustness. For linear models, it\nis well known that there exists a \\emph{regularization path} connecting the\nsparsest solution in terms of the $\\ell^1$ norm, i.e., zero weights and the\nnon-regularized solution. Very recently, there was a first attempt to extend\nthe concept of regularization paths to DNNs by means of treating the empirical\nloss and sparsity ($\\ell^1$ norm) as two conflicting criteria and solving the\nresulting multiobjective optimization problem for low-dimensional DNN. However,\ndue to the non-smoothness of the $\\ell^1$ norm and the high number of\nparameters, this approach is not very efficient from a computational\nperspective for high-dimensional DNNs. To overcome this limitation, we present\nan algorithm that allows for the approximation of the entire Pareto front for\nthe above-mentioned objectives in a very efficient manner for high-dimensional\nDNNs with millions of parameters. We present numerical examples using both\ndeterministic and stochastic gradients. We furthermore demonstrate that\nknowledge of the regularization path allows for a well-generalizing network\nparametrization. To the best of our knowledge, this is the first algorithm to\ncompute the regularization path for non-convex multiobjective optimization\nproblems (MOPs) with millions of degrees of freedom.\n","authors":["Augustina C. Amakor","Konstantin Sonntag","Sebastian Peitz"],"pdf_url":"https://arxiv.org/pdf/2308.12044v5.pdf","comment":"7 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.11624v3","updated":"2024-03-29T14:20:17Z","published":"2024-03-18T09:56:00Z","title":"Dual-Channel Multiplex Graph Neural Networks for Recommendation","summary":"  Efficient recommender systems play a crucial role in accurately capturing\nuser and item attributes that mirror individual preferences. Some existing\nrecommendation techniques have started to shift their focus towards modeling\nvarious types of interaction relations between users and items in real-world\nrecommendation scenarios, such as clicks, marking favorites, and purchases on\nonline shopping platforms. Nevertheless, these approaches still grapple with\ntwo significant shortcomings: (1) Insufficient modeling and exploitation of the\nimpact of various behavior patterns formed by multiplex relations between users\nand items on representation learning, and (2) ignoring the effect of different\nrelations in the behavior patterns on the target relation in recommender system\nscenarios. In this study, we introduce a novel recommendation framework,\nDual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the\naforementioned challenges. It incorporates an explicit behavior pattern\nrepresentation learner to capture the behavior patterns composed of multiplex\nuser-item interaction relations, and includes a relation chain representation\nlearning and a relation chain-aware encoder to discover the impact of various\nauxiliary relations on the target relation, the dependencies between different\nrelations, and mine the appropriate order of relations in a behavior pattern.\nExtensive experiments on three real-world datasets demonstrate that our \\model\nsurpasses various state-of-the-art recommendation methods. It outperforms the\nbest baselines by 10.06\\% and 12.15\\% on average across all datasets in terms\nof R@10 and N@10 respectively.\n","authors":["Xiang Li","Chaofan Fu","Zhongying Zhao","Guanjie Zheng","Chao Huang","Junyu Dong","Yanwei Yu"],"pdf_url":"https://arxiv.org/pdf/2403.11624v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20197v1","updated":"2024-03-29T14:19:26Z","published":"2024-03-29T14:19:26Z","title":"Dual Simplex Volume Maximization for Simplex-Structured Matrix\n  Factorization","summary":"  Simplex-structured matrix factorization (SSMF) is a generalization of\nnonnegative matrix factorization, a fundamental interpretable data analysis\nmodel, and has applications in hyperspectral unmixing and topic modeling. To\nobtain identifiable solutions, a standard approach is to find minimum-volume\nsolutions. By taking advantage of the duality/polarity concept for polytopes,\nwe convert minimum-volume SSMF in the primal space to a maximum-volume problem\nin the dual space. We first prove the identifiability of this maximum-volume\ndual problem. Then, we use this dual formulation to provide a novel\noptimization approach which bridges the gap between two existing families of\nalgorithms for SSMF, namely volume minimization and facet identification.\nNumerical experiments show that the proposed approach performs favorably\ncompared to the state-of-the-art SSMF algorithms.\n","authors":["Maryam Abdolali","Giovanni Barbarino","Nicolas Gillis"],"pdf_url":"https://arxiv.org/pdf/2403.20197v1.pdf","comment":"31 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.20195v1","updated":"2024-03-29T14:17:30Z","published":"2024-03-29T14:17:30Z","title":"Enhancing Lithological Mapping with Spatially Constrained Bayesian\n  Network (SCB-Net): An Approach for Field Data-Constrained Predictions with\n  Uncertainty Evaluation","summary":"  Geological maps are an extremely valuable source of information for the Earth\nsciences. They provide insights into mineral exploration, vulnerability to\nnatural hazards, and many other applications. These maps are created using\nnumerical or conceptual models that use geological observations to extrapolate\ndata. Geostatistical techniques have traditionally been used to generate\nreliable predictions that take into account the spatial patterns inherent in\nthe data. However, as the number of auxiliary variables increases, these\nmethods become more labor-intensive. Additionally, traditional machine learning\nmethods often struggle with spatially correlated data and extracting valuable\nnon-linear information from geoscientific datasets. To address these\nlimitations, a new architecture called the Spatially Constrained Bayesian\nNetwork (SCB-Net) has been developed. The SCB-Net aims to effectively exploit\nthe information from auxiliary variables while producing spatially constrained\npredictions. It is made up of two parts, the first part focuses on learning\nunderlying patterns in the auxiliary variables while the second part integrates\nground-truth data and the learned embeddings from the first part. Moreover, to\nassess model uncertainty, a technique called Monte Carlo dropout is used as a\nBayesian approximation. The SCB-Net has been applied to two selected areas in\nnorthern Quebec, Canada, and has demonstrated its potential in generating\nfield-data-constrained lithological maps while allowing assessment of\nprediction uncertainty for decision-making. This study highlights the promising\nadvancements of deep neural networks in geostatistics, particularly in handling\ncomplex spatial feature learning tasks, leading to improved spatial information\ntechniques.\n","authors":["Victor Silva dos Santos","Erwan Gloaguen","Shiva Tirdad"],"pdf_url":"https://arxiv.org/pdf/2403.20195v1.pdf","comment":"17 pages, 3559 words, 14 figures"},{"id":"http://arxiv.org/abs/2403.19591v2","updated":"2024-03-29T14:13:11Z","published":"2024-03-28T17:13:47Z","title":"Genetic Quantization-Aware Approximation for Non-Linear Operations in\n  Transformers","summary":"  Non-linear functions are prevalent in Transformers and their lightweight\nvariants, incurring substantial and frequently underestimated hardware costs.\nPrevious state-of-the-art works optimize these operations by piece-wise linear\napproximation and store the parameters in look-up tables (LUT), but most of\nthem require unfriendly high-precision arithmetics such as FP/INT 32 and lack\nconsideration of integer-only INT quantization. This paper proposed a genetic\nLUT-Approximation algorithm namely GQA-LUT that can automatically determine the\nparameters with quantization awareness. The results demonstrate that GQA-LUT\nachieves negligible degradation on the challenging semantic segmentation task\nfor both vanilla and linear Transformer models. Besides, proposed GQA-LUT\nenables the employment of INT8-based LUT-Approximation that achieves an area\nsavings of 81.3~81.7% and a power reduction of 79.3~80.2% compared to the\nhigh-precision FP/INT 32 alternatives. Code is available at https://\ngithub.com/PingchengDong/GQA-LUT.\n","authors":["Pingcheng Dong","Yonghao Tan","Dong Zhang","Tianwei Ni","Xuejiao Liu","Yu Liu","Peng Luo","Luhong Liang","Shih-Yang Liu","Xijie Huang","Huaiyu Zhu","Yun Pan","Fengwei An","Kwang-Ting Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.19591v2.pdf","comment":"61st ACM/IEEE Design Automation Conference (DAC) 2024"},{"id":"http://arxiv.org/abs/2403.20190v1","updated":"2024-03-29T14:09:59Z","published":"2024-03-29T14:09:59Z","title":"Homomorphic WiSARDs: Efficient Weightless Neural Network training over\n  encrypted data","summary":"  The widespread application of machine learning algorithms is a matter of\nincreasing concern for the data privacy research community, and many have\nsought to develop privacy-preserving techniques for it. Among existing\napproaches, the homomorphic evaluation of ML algorithms stands out by\nperforming operations directly over encrypted data, enabling strong guarantees\nof confidentiality. The homomorphic evaluation of inference algorithms is\npractical even for relatively deep Convolution Neural Networks (CNNs). However,\ntraining is still a major challenge, with current solutions often resorting to\nlightweight algorithms that can be unfit for solving more complex problems,\nsuch as image recognition. This work introduces the homomorphic evaluation of\nWilkie, Stonham, and Aleksander's Recognition Device (WiSARD) and subsequent\nWeightless Neural Networks (WNNs) for training and inference on encrypted data.\nCompared to CNNs, WNNs offer better performance with a relatively small\naccuracy drop. We develop a complete framework for it, including several\nbuilding blocks that can be of independent interest. Our framework achieves\n91.7% accuracy on the MNIST dataset after only 3.5 minutes of encrypted\ntraining (multi-threaded), going up to 93.8% in 3.5 hours. For the HAM10000\ndataset, we achieve 67.9% accuracy in just 1.5 minutes, going up to 69.9% after\n1 hour. Compared to the state of the art on the HE evaluation of CNN training,\nGlyph (Lou et al., NeurIPS 2020), these results represent a speedup of up to\n1200 times with an accuracy loss of at most 5.4%. For HAM10000, we even\nachieved a 0.65% accuracy improvement while being 60 times faster than Glyph.\nWe also provide solutions for small-scale encrypted training. In a single\nthread on a desktop machine using less than 200MB of memory, we train over 1000\nMNIST images in 12 minutes or over the entire Wisconsin Breast Cancer dataset\nin just 11 seconds.\n","authors":["Leonardo Neumann","Antonio Guimarães","Diego F. Aranha","Edson Borin"],"pdf_url":"https://arxiv.org/pdf/2403.20190v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20188v1","updated":"2024-03-29T14:05:40Z","published":"2024-03-29T14:05:40Z","title":"Distributed Swarm Learning for Edge Internet of Things","summary":"  The rapid growth of Internet of Things (IoT) has led to the widespread\ndeployment of smart IoT devices at wireless edge for collaborative machine\nlearning tasks, ushering in a new era of edge learning. With a huge number of\nhardware-constrained IoT devices operating in resource-limited wireless\nnetworks, edge learning encounters substantial challenges, including\ncommunication and computation bottlenecks, device and data heterogeneity,\nsecurity risks, privacy leakages, non-convex optimization, and complex wireless\nenvironments. To address these issues, this article explores a novel framework\nknown as distributed swarm learning (DSL), which combines artificial\nintelligence and biological swarm intelligence in a holistic manner. By\nharnessing advanced signal processing and communications, DSL provides\nefficient solutions and robust tools for large-scale IoT at the edge of\nwireless networks.\n","authors":["Yue Wang","Zhi Tian","FXin Fan","Zhipeng Cai","Cameron Nowzari","Kai Zeng"],"pdf_url":"https://arxiv.org/pdf/2403.20188v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2210.16705"},{"id":"http://arxiv.org/abs/2403.20184v1","updated":"2024-03-29T13:59:34Z","published":"2024-03-29T13:59:34Z","title":"Exploring Pathological Speech Quality Assessment with ASR-Powered\n  Wav2Vec2 in Data-Scarce Context","summary":"  Automatic speech quality assessment has raised more attention as an\nalternative or support to traditional perceptual clinical evaluation. However,\nmost research so far only gains good results on simple tasks such as binary\nclassification, largely due to data scarcity. To deal with this challenge,\ncurrent works tend to segment patients' audio files into many samples to\naugment the datasets. Nevertheless, this approach has limitations, as it\nindirectly relates overall audio scores to individual segments. This paper\nintroduces a novel approach where the system learns at the audio level instead\nof segments despite data scarcity. This paper proposes to use the pre-trained\nWav2Vec2 architecture for both SSL, and ASR as feature extractor in speech\nassessment. Carried out on the HNC dataset, our ASR-driven approach established\na new baseline compared with other approaches, obtaining average $MSE=0.73$ and\n$MSE=1.15$ for the prediction of intelligibility and severity scores\nrespectively, using only 95 training samples. It shows that the ASR based\nWav2Vec2 model brings the best results and may indicate a strong correlation\nbetween ASR and speech quality assessment. We also measure its ability on\nvariable segment durations and speech content, exploring factors influencing\nits decision.\n","authors":["Tuan Nguyen","Corinne Fredouille","Alain Ghio","Mathieu Balaguer","Virginie Woisard"],"pdf_url":"https://arxiv.org/pdf/2403.20184v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.10424v2","updated":"2024-03-29T13:48:44Z","published":"2024-03-15T15:58:37Z","title":"Structured Evaluation of Synthetic Tabular Data","summary":"  Tabular data is common yet typically incomplete, small in volume, and\naccess-restricted due to privacy concerns. Synthetic data generation offers\npotential solutions. Many metrics exist for evaluating the quality of synthetic\ntabular data; however, we lack an objective, coherent interpretation of the\nmany metrics. To address this issue, we propose an evaluation framework with a\nsingle, mathematical objective that posits that the synthetic data should be\ndrawn from the same distribution as the observed data. Through various\nstructural decomposition of the objective, this framework allows us to reason\nfor the first time the completeness of any set of metrics, as well as unifies\nexisting metrics, including those that stem from fidelity considerations,\ndownstream application, and model-based approaches. Moreover, the framework\nmotivates model-free baselines and a new spectrum of metrics. We evaluate\nstructurally informed synthesizers and synthesizers powered by deep learning.\nUsing our structured framework, we show that synthetic data generators that\nexplicitly represent tabular structure outperform other methods, especially on\nsmaller datasets.\n","authors":["Scott Cheng-Hsin Yang","Baxter Eaves","Michael Schmidt","Ken Swanson","Patrick Shafto"],"pdf_url":"https://arxiv.org/pdf/2403.10424v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20156v1","updated":"2024-03-29T13:05:59Z","published":"2024-03-29T13:05:59Z","title":"CAESAR: Enhancing Federated RL in Heterogeneous MDPs through\n  Convergence-Aware Sampling with Screening","summary":"  In this study, we delve into Federated Reinforcement Learning (FedRL) in the\ncontext of value-based agents operating across diverse Markov Decision\nProcesses (MDPs). Existing FedRL methods typically aggregate agents' learning\nby averaging the value functions across them to improve their performance.\nHowever, this aggregation strategy is suboptimal in heterogeneous environments\nwhere agents converge to diverse optimal value functions. To address this\nproblem, we introduce the Convergence-AwarE SAmpling with scReening (CAESAR)\naggregation scheme designed to enhance the learning of individual agents across\nvaried MDPs. CAESAR is an aggregation strategy used by the server that combines\nconvergence-aware sampling with a screening mechanism. By exploiting the fact\nthat agents learning in identical MDPs are converging to the same optimal value\nfunction, CAESAR enables the selective assimilation of knowledge from more\nproficient counterparts, thereby significantly enhancing the overall learning\nefficiency. We empirically validate our hypothesis and demonstrate the\neffectiveness of CAESAR in enhancing the learning efficiency of agents, using\nboth a custom-built GridWorld environment and the classical FrozenLake-v1 task,\neach presenting varying levels of environmental heterogeneity.\n","authors":["Hei Yi Mak","Flint Xiaofeng Fan","Luca A. Lanzendörfer","Cheston Tan","Wei Tsang Ooi","Roger Wattenhofer"],"pdf_url":"https://arxiv.org/pdf/2403.20156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.09591v2","updated":"2024-03-29T13:04:03Z","published":"2023-07-18T19:56:20Z","title":"Gradient strikes back: How filtering out high frequencies improves\n  explanations","summary":"  Attribution methods correspond to a class of explainability methods (XAI)\nthat aim to assess how individual inputs contribute to a model's\ndecision-making process. We have identified a significant limitation in one\ntype of attribution methods, known as \"white-box\" methods. Although highly\nefficient, these methods rely on a gradient signal that is often contaminated\nby high-frequency noise. To overcome this limitation, we introduce a new\napproach called \"FORGrad\". This simple method effectively filters out noise\nartifacts by using optimal cut-off frequencies tailored to the unique\ncharacteristics of each model architecture. Our findings show that FORGrad\nconsistently enhances the performance of already existing white-box methods,\nenabling them to compete effectively with more accurate yet computationally\ndemanding \"black-box\" methods. We anticipate that our research will foster\nbroader adoption of simpler and more efficient white-box methods for\nexplainability, offering a better balance between faithfulness and\ncomputational efficiency.\n","authors":["Sabine Muzellec","Thomas Fel","Victor Boutin","Léo andéol","Rufin VanRullen","Thomas Serre"],"pdf_url":"https://arxiv.org/pdf/2307.09591v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.08396v5","updated":"2024-03-29T12:50:38Z","published":"2023-05-15T07:23:54Z","title":"MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation","summary":"  Since their emergence, Convolutional Neural Networks (CNNs) have made\nsignificant strides in medical image analysis. However, the local nature of the\nconvolution operator may pose a limitation for capturing global and long-range\ninteractions in CNNs. Recently, Transformers have gained popularity in the\ncomputer vision community and also in medical image segmentation due to their\nability to process global features effectively. The scalability issues of the\nself-attention mechanism and lack of the CNN-like inductive bias may have\nlimited their adoption. Therefore, hybrid Vision transformers\n(CNN-Transformer), exploiting the advantages of both Convolution and\nSelf-attention Mechanisms, have gained importance. In this work, we present\nMaxViT-UNet, a new Encoder-Decoder based UNet type hybrid vision transformer\n(CNN-Transformer) for medical image segmentation. The proposed Hybrid Decoder\nis designed to harness the power of both the convolution and self-attention\nmechanisms at each decoding stage with a nominal memory and computational\nburden. The inclusion of multi-axis self-attention, within each decoder stage,\nsignificantly enhances the discriminating capacity between the object and\nbackground regions, thereby helping in improving the segmentation efficiency.\nIn the Hybrid Decoder, a new block is also proposed. The fusion process\ncommences by integrating the upsampled lower-level decoder features, obtained\nthrough transpose convolution, with the skip-connection features derived from\nthe hybrid encoder. Subsequently, the fused features undergo refinement through\nthe utilization of a multi-axis attention mechanism. The proposed decoder block\nis repeated multiple times to segment the nuclei regions progressively.\nExperimental results on MoNuSeg18 and MoNuSAC20 datasets demonstrate the\neffectiveness of the proposed technique.\n","authors":["Abdul Rehman Khan","Asifullah Khan"],"pdf_url":"https://arxiv.org/pdf/2305.08396v5.pdf","comment":"19 pages, 6 figures, 5 tables"},{"id":"http://arxiv.org/abs/2403.12687v2","updated":"2024-03-29T12:45:27Z","published":"2024-03-19T12:45:52Z","title":"Audio-Visual Compound Expression Recognition Method based on Late\n  Modality Fusion and Rule-based Decision","summary":"  This paper presents the results of the SUN team for the Compound Expressions\nRecognition Challenge of the 6th ABAW Competition. We propose a novel\naudio-visual method for compound expression recognition. Our method relies on\nemotion recognition models that fuse modalities at the emotion probability\nlevel, while decisions regarding the prediction of compound expressions are\nbased on predefined rules. Notably, our method does not use any training data\nspecific to the target task. Thus, the problem is a zero-shot classification\ntask. The method is evaluated in multi-corpus training and cross-corpus\nvalidation setups. Using our proposed method is achieved an F1-score value\nequals to 22.01% on the C-EXPR-DB test subset. Our findings from the challenge\ndemonstrate that the proposed method can potentially form a basis for\ndeveloping intelligent tools for annotating audio-visual data in the context of\nhuman's basic and compound emotions.\n","authors":["Elena Ryumina","Maxim Markitantov","Dmitry Ryumin","Heysem Kaya","Alexey Karpov"],"pdf_url":"https://arxiv.org/pdf/2403.12687v2.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.20150v1","updated":"2024-03-29T12:37:57Z","published":"2024-03-29T12:37:57Z","title":"TFB: Towards Comprehensive and Fair Benchmarking of Time Series\n  Forecasting Methods","summary":"  Time series are generated in diverse domains such as economic, traffic,\nhealth, and energy, where forecasting of future values has numerous important\napplications. Not surprisingly, many forecasting methods are being proposed. To\nensure progress, it is essential to be able to study and compare such methods\nempirically in a comprehensive and reliable manner. To achieve this, we propose\nTFB, an automated benchmark for Time Series Forecasting (TSF) methods. TFB\nadvances the state-of-the-art by addressing shortcomings related to datasets,\ncomparison methods, and evaluation pipelines: 1) insufficient coverage of data\ndomains, 2) stereotype bias against traditional methods, and 3) inconsistent\nand inflexible pipelines. To achieve better domain coverage, we include\ndatasets from 10 different domains: traffic, electricity, energy, the\nenvironment, nature, economic, stock markets, banking, health, and the web. We\nalso provide a time series characterization to ensure that the selected\ndatasets are comprehensive. To remove biases against some methods, we include a\ndiverse range of methods, including statistical learning, machine learning, and\ndeep learning methods, and we also support a variety of evaluation strategies\nand metrics to ensure a more comprehensive evaluations of different methods. To\nsupport the integration of different methods into the benchmark and enable fair\ncomparisons, TFB features a flexible and scalable pipeline that eliminates\nbiases. Next, we employ TFB to perform a thorough evaluation of 21 Univariate\nTime Series Forecasting (UTSF) methods on 8,068 univariate time series and 14\nMultivariate Time Series Forecasting (MTSF) methods on 25 datasets. The\nbenchmark code and data are available at\nhttps://github.com/decisionintelligence/TFB.\n","authors":["Xiangfei Qiu","Jilin Hu","Lekui Zhou","Xingjian Wu","Junyang Du","Buang Zhang","Chenjuan Guo","Aoying Zhou","Christian S. Jensen","Zhenli Sheng","Bin Yang"],"pdf_url":"https://arxiv.org/pdf/2403.20150v1.pdf","comment":"Accepted by PVLDB 2024"},{"id":"http://arxiv.org/abs/2403.20149v1","updated":"2024-03-29T12:34:57Z","published":"2024-03-29T12:34:57Z","title":"Conformal Prediction for Stochastic Decision-Making of PV Power in\n  Electricity Markets","summary":"  This paper studies the use of conformal prediction (CP), an emerging\nprobabilistic forecasting method, for day-ahead photovoltaic power predictions\nto enhance participation in electricity markets. First, machine learning models\nare used to construct point predictions. Thereafter, several variants of CP are\nimplemented to quantify the uncertainty of those predictions by creating CP\nintervals and cumulative distribution functions. Optimal quantity bids for the\nelectricity market are estimated using several bidding strategies under\nuncertainty, namely: trust-the-forecast, worst-case, Newsvendor and expected\nutility maximization (EUM). Results show that CP in combination with k-nearest\nneighbors and/or Mondrian binning outperforms its corresponding linear quantile\nregressors. Using CP in combination with certain bidding strategies can yield\nhigh profit with minimal energy imbalance. In concrete, using conformal\npredictive systems with k-nearest neighbors and Mondrian binning after random\nforest regression yields the best profit and imbalance regardless of the\ndecision-making strategy. Combining this uncertainty quantification method with\nthe EUM strategy with conditional value at risk (CVaR) can yield up to 93\\% of\nthe potential profit with minimal energy imbalance.\n","authors":["Yvet Renkema","Nico Brinkel","Tarek Alskaif"],"pdf_url":"https://arxiv.org/pdf/2403.20149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02977v3","updated":"2024-03-29T12:28:46Z","published":"2024-02-05T12:58:29Z","title":"Variational Flow Models: Flowing in Your Style","summary":"  We introduce \"posterior flows\" - generalizations of \"probability flows\" to a\nbroader class of stochastic processes not necessarily diffusion processes - and\npropose a systematic training-free method to transform the posterior flow of a\n\"linear\" stochastic process characterized by the equation Xt = at * X0 + st *\nX1 into a straight constant-speed (SC) flow, reminiscent of Rectified Flow.\nThis transformation facilitates fast sampling along the original posterior flow\nwithout training a new model of the SC flow. The flexibility of our approach\nallows us to extend our transformation to inter-convert two posterior flows\nfrom distinct \"linear\" stochastic processes. Moreover, we can easily integrate\nhigh-order numerical solvers into the transformed SC flow, further enhancing\nsampling accuracy and efficiency. Rigorous theoretical analysis and extensive\nexperimental results substantiate the advantages of our framework.\n","authors":["Kien Do","Duc Kieu","Toan Nguyen","Dang Nguyen","Hung Le","Dung Nguyen","Thin Nguyen"],"pdf_url":"https://arxiv.org/pdf/2402.02977v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20139v1","updated":"2024-03-29T12:16:01Z","published":"2024-03-29T12:16:01Z","title":"Designing Poisson Integrators Through Machine Learning","summary":"  This paper presents a general method to construct Poisson integrators, i.e.,\nintegrators that preserve the underlying Poisson geometry. We assume the\nPoisson manifold is integrable, meaning there is a known local symplectic\ngroupoid for which the Poisson manifold serves as the set of units. Our\nconstructions build upon the correspondence between Poisson diffeomorphisms and\nLagrangian bisections, which allows us to reformulate the design of Poisson\nintegrators as solutions to a certain PDE (Hamilton-Jacobi). The main novelty\nof this work is to understand the Hamilton-Jacobi PDE as an optimization\nproblem, whose solution can be easily approximated using machine learning\nrelated techniques. This research direction aligns with the current trend in\nthe PDE and machine learning communities, as initiated by Physics- Informed\nNeural Networks, advocating for designs that combine both physical modeling\n(the Hamilton-Jacobi PDE) and data.\n","authors":["Miguel Vaquero","David Martín de Diego","Jorge Cortés"],"pdf_url":"https://arxiv.org/pdf/2403.20139v1.pdf","comment":"5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2401.00320v2","updated":"2024-03-29T12:04:52Z","published":"2023-12-30T20:52:20Z","title":"DXAI: Explaining Classification by Image Decomposition","summary":"  We propose a new way to explain and to visualize neural network\nclassification through a decomposition-based explainable AI (DXAI). Instead of\nproviding an explanation heatmap, our method yields a decomposition of the\nimage into class-agnostic and class-distinct parts, with respect to the data\nand chosen classifier. Following a fundamental signal processing paradigm of\nanalysis and synthesis, the original image is the sum of the decomposed parts.\nWe thus obtain a radically different way of explaining classification. The\nclass-agnostic part ideally is composed of all image features which do not\nposses class information, where the class-distinct part is its complementary.\nThis new visualization can be more helpful and informative in certain\nscenarios, especially when the attributes are dense, global and additive in\nnature, for instance, when colors or textures are essential for class\ndistinction. Code is available at https://github.com/dxai2024/dxai.\n","authors":["Elnatan Kadar","Guy Gilboa"],"pdf_url":"https://arxiv.org/pdf/2401.00320v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20130v1","updated":"2024-03-29T11:44:14Z","published":"2024-03-29T11:44:14Z","title":"Sound event localization and classification using WASN in Outdoor\n  Environment","summary":"  Deep learning-based sound event localization and classification is an\nemerging research area within wireless acoustic sensor networks. However,\ncurrent methods for sound event localization and classification typically rely\non a single microphone array, making them susceptible to signal attenuation and\nenvironmental noise, which limits their monitoring range. Moreover, methods\nusing multiple microphone arrays often focus solely on source localization,\nneglecting the aspect of sound event classification. In this paper, we propose\na deep learning-based method that employs multiple features and attention\nmechanisms to estimate the location and class of sound source. We introduce a\nSoundmap feature to capture spatial information across multiple frequency\nbands. We also use the Gammatone filter to generate acoustic features more\nsuitable for outdoor environments. Furthermore, we integrate attention\nmechanisms to learn channel-wise relationships and temporal dependencies within\nthe acoustic features. To evaluate our proposed method, we conduct experiments\nusing simulated datasets with different levels of noise and size of monitoring\nareas, as well as different arrays and source positions. The experimental\nresults demonstrate the superiority of our proposed method over\nstate-of-the-art methods in both sound event classification and sound source\nlocalization tasks. And we provide further analysis to explain the reasons for\nthe observed errors.\n","authors":["Dongzhe Zhang","Jianfeng Chen","Jisheng Bai","Mou Wang"],"pdf_url":"https://arxiv.org/pdf/2403.20130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20124v1","updated":"2024-03-29T11:27:37Z","published":"2024-03-29T11:27:37Z","title":"Application of Machine Learning Algorithms in Classifying Postoperative\n  Success in Metabolic Bariatric Surgery: A Comprehensive Study","summary":"  Objectives: Metabolic Bariatric Surgery (MBS) is a critical intervention for\npatients living with obesity and related health issues. Accurate classification\nand prediction of patient outcomes are vital for optimizing treatment\nstrategies. This study presents a novel machine learning approach to classify\npatients in the context of metabolic bariatric surgery, providing insights into\nthe efficacy of different models and variable types. Methods: Various machine\nlearning models, including GaussianNB, ComplementNB, KNN, Decision Tree, KNN\nwith RandomOverSampler, and KNN with SMOTE, were applied to a dataset of 73\npatients. The dataset, comprising psychometric, socioeconomic, and analytical\nvariables, was analyzed to determine the most efficient predictive model. The\nstudy also explored the impact of different variable groupings and oversampling\ntechniques. Results: Experimental results indicate average accuracy values as\nhigh as 66.7% for the best model. Enhanced versions of KNN and Decision Tree,\nalong with variations of KNN such as RandomOverSampler and SMOTE, yielded the\nbest results. Conclusions: The study unveils a promising avenue for classifying\npatients in the realm of metabolic bariatric surgery. The results underscore\nthe importance of selecting appropriate variables and employing diverse\napproaches to achieve optimal performance. The developed system holds potential\nas a tool to assist healthcare professionals in decision-making, thereby\nenhancing metabolic bariatric surgery outcomes. These findings lay the\ngroundwork for future collaboration between hospitals and healthcare entities\nto improve patient care through the utilization of machine learning algorithms.\nMoreover, the findings suggest room for improvement, potentially achievable\nwith a larger dataset and careful parameter tuning.\n","authors":["José Alberto Benítez-Andrades","Camino Prada-García","Rubén García-Fernández","María D. Ballesteros-Pomar","María-Inmaculada González-Alonso","Antonio Serrano-García"],"pdf_url":"https://arxiv.org/pdf/2403.20124v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20122v1","updated":"2024-03-29T11:23:10Z","published":"2024-03-29T11:23:10Z","title":"Learning using granularity statistical invariants for classification","summary":"  Learning using statistical invariants (LUSI) is a new learning paradigm,\nwhich adopts weak convergence mechanism, and can be applied to a wider range of\nclassification problems. However, the computation cost of invariant matrices in\nLUSI is high for large-scale datasets during training. To settle this issue,\nthis paper introduces a granularity statistical invariant for LUSI, and\ndevelops a new learning paradigm called learning using granularity statistical\ninvariants (LUGSI). LUGSI employs both strong and weak convergence mechanisms,\ntaking a perspective of minimizing expected risk. As far as we know, it is the\nfirst time to construct granularity statistical invariants. Compared to LUSI,\nthe introduction of this new statistical invariant brings two advantages.\nFirstly, it enhances the structural information of the data. Secondly, LUGSI\ntransforms a large invariant matrix into a smaller one by maximizing the\ndistance between classes, achieving feasibility for large-scale datasets\nclassification problems and significantly enhancing the training speed of model\noperations. Experimental results indicate that LUGSI not only exhibits improved\ngeneralization capabilities but also demonstrates faster training speed,\nparticularly for large-scale datasets.\n","authors":["Ting-Ting Zhu","Yuan-Hai Shao","Chun-Na Li","Tian Liu"],"pdf_url":"https://arxiv.org/pdf/2403.20122v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20112v1","updated":"2024-03-29T10:49:02Z","published":"2024-03-29T10:49:02Z","title":"Segmentation, Classification and Interpretation of Breast Cancer Medical\n  Images using Human-in-the-Loop Machine Learning","summary":"  This paper explores the application of Human-in-the-Loop (HITL) strategies in\ntraining machine learning models in the medical domain. In this case a\ndoctor-in-the-loop approach is proposed to leverage human expertise in dealing\nwith large and complex data. Specifically, the paper deals with the integration\nof genomic data and Whole Slide Imaging (WSI) analysis of breast cancer. Three\ndifferent tasks were developed: segmentation of histopathological images,\nclassification of this images regarding the genomic subtype of the cancer and,\nfinally, interpretation of the machine learning results. The involvement of a\npathologist helped us to develop a better segmentation model and to enhance the\nexplainatory capabilities of the models, but the classification results were\nsuboptimal, highlighting the limitations of this approach: despite involving\nhuman experts, complex domains can still pose challenges, and a HITL approach\nmay not always be effective.\n","authors":["David Vázquez-Lema","Eduardo Mosqueira-Rey","Elena Hernández-Pereira","Carlos Fernández-Lozano","Fernando Seara-Romera","Jorge Pombo-Otero"],"pdf_url":"https://arxiv.org/pdf/2403.20112v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20109v1","updated":"2024-03-29T10:44:51Z","published":"2024-03-29T10:44:51Z","title":"Mol-AIR: Molecular Reinforcement Learning with Adaptive Intrinsic\n  Rewards for Goal-directed Molecular Generation","summary":"  Optimizing techniques for discovering molecular structures with desired\nproperties is crucial in artificial intelligence(AI)-based drug discovery.\nCombining deep generative models with reinforcement learning has emerged as an\neffective strategy for generating molecules with specific properties. Despite\nits potential, this approach is ineffective in exploring the vast chemical\nspace and optimizing particular chemical properties. To overcome these\nlimitations, we present Mol-AIR, a reinforcement learning-based framework using\nadaptive intrinsic rewards for effective goal-directed molecular generation.\nMol-AIR leverages the strengths of both history-based and learning-based\nintrinsic rewards by exploiting random distillation network and counting-based\nstrategies. In benchmark tests, Mol-AIR demonstrates superior performance over\nexisting approaches in generating molecules with desired properties without any\nprior knowledge, including penalized LogP, QED, and celecoxib similarity. We\nbelieve that Mol-AIR represents a significant advancement in drug discovery,\noffering a more efficient path to discovering novel therapeutics.\n","authors":["Jinyeong Park","Jaegyoon Ahn","Jonghwan Choi","Jibum Kim"],"pdf_url":"https://arxiv.org/pdf/2403.20109v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20106v1","updated":"2024-03-29T10:40:41Z","published":"2024-03-29T10:40:41Z","title":"Aggregating Local and Global Features via Selective State Spaces Model\n  for Efficient Image Deblurring","summary":"  Image deblurring is a process of restoring a high quality image from the\ncorresponding blurred image. Significant progress in this field has been made\npossible by the emergence of various effective deep learning models, including\nCNNs and Transformers. However, these methods often face the dilemma between\neliminating long-range blur degradation perturbations and maintaining\ncomputational efficiency, which hinders their practical application. To address\nthis issue, we propose an efficient image deblurring network that leverages\nselective structured state spaces model to aggregate enriched and accurate\nfeatures. Specifically, we design an aggregate local and global block\n(ALGBlock) to capture and fuse both local invariant properties and non-local\ninformation. The ALGBlock consists of two blocks: (1) The local block models\nlocal connectivity using simplified channel attention. (2) The global block\ncaptures long-range dependency features with linear complexity through\nselective structured state spaces. Nevertheless, we note that the image details\nare local features of images, we accentuate the local part for restoration by\nrecalibrating the weight when aggregating the two branches for recovery.\nExperimental results demonstrate that the proposed method outperforms\nstate-of-the-art approaches on widely used benchmarks, highlighting its\nsuperior performance.\n","authors":["Hu Gao","Depeng Dang"],"pdf_url":"https://arxiv.org/pdf/2403.20106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20101v1","updated":"2024-03-29T10:31:32Z","published":"2024-03-29T10:31:32Z","title":"RealKIE: Five Novel Datasets for Enterprise Key Information Extraction","summary":"  We introduce RealKIE, a benchmark of five challenging datasets aimed at\nadvancing key information extraction methods, with an emphasis on enterprise\napplications. The datasets include a diverse range of documents including SEC\nS1 Filings, US Non-disclosure Agreements, UK Charity Reports, FCC Invoices, and\nResource Contracts. Each presents unique challenges: poor text serialization,\nsparse annotations in long documents, and complex tabular layouts. These\ndatasets provide a realistic testing ground for key information extraction\ntasks like investment analysis and legal data processing.\n  In addition to presenting these datasets, we offer an in-depth description of\nthe annotation process, document processing techniques, and baseline modeling\napproaches. This contribution facilitates the development of NLP models capable\nof handling practical challenges and supports further research into information\nextraction technologies applicable to industry-specific problems.\n  The annotated data and OCR outputs are available to download at\nhttps://indicodatasolutions.github.io/RealKIE/ code to reproduce the baselines\nwill be available shortly.\n","authors":["Benjamin Townsend","Madison May","Christopher Wells"],"pdf_url":"https://arxiv.org/pdf/2403.20101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08585v2","updated":"2024-03-29T10:15:47Z","published":"2024-03-13T14:42:06Z","title":"Improving Implicit Regularization of SGD with Preconditioning for Least\n  Square Problems","summary":"  Stochastic gradient descent (SGD) exhibits strong algorithmic regularization\neffects in practice and plays an important role in the generalization of modern\nmachine learning. However, prior research has revealed instances where the\ngeneralization performance of SGD is worse than ridge regression due to uneven\noptimization along different dimensions. Preconditioning offers a natural\nsolution to this issue by rebalancing optimization across different directions.\nYet, the extent to which preconditioning can enhance the generalization\nperformance of SGD and whether it can bridge the existing gap with ridge\nregression remains uncertain. In this paper, we study the generalization\nperformance of SGD with preconditioning for the least squared problem. We make\na comprehensive comparison between preconditioned SGD and (standard \\&\npreconditioned) ridge regression. Our study makes several key contributions\ntoward understanding and improving SGD with preconditioning. First, we\nestablish excess risk bounds (generalization performance) for preconditioned\nSGD and ridge regression under an arbitrary preconditions matrix. Second,\nleveraging the excessive risk characterization of preconditioned SGD and ridge\nregression, we show that (through construction) there exists a simple\npreconditioned matrix that can outperform (standard \\& preconditioned) ridge\nregression. Finally, we show that our proposed preconditioning matrix is\nstraightforward enough to allow robust estimation from finite samples while\nmaintaining a theoretical advantage over ridge regression. Our empirical\nresults align with our theoretical findings, collectively showcasing the\nenhanced regularization effect of preconditioned SGD.\n","authors":["Junwei Su","Difan Zou","Chuan Wu"],"pdf_url":"https://arxiv.org/pdf/2403.08585v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.13417v5","updated":"2024-03-29T09:57:24Z","published":"2023-02-26T22:10:23Z","title":"Training neural networks with structured noise improves classification\n  and generalization","summary":"  The beneficial role of noise-injection in learning is a consolidated concept\nin the field of artificial neural networks, suggesting that even biological\nsystems might take advantage of similar mechanisms to optimize their\nperformance. The training-with-noise algorithm proposed by Gardner and\ncollaborators is an emblematic example of a noise-injection procedure in\nrecurrent networks, which can be used to model biological neural systems. We\nshow how adding structure to noisy training data can substantially improve the\nalgorithm performance, allowing the network to approach perfect retrieval of\nthe memories and wide basins of attraction, even in the scenario of maximal\ninjected noise. We also prove that the so-called Hebbian Unlearning rule\ncoincides with the training-with-noise algorithm when noise is maximal and data\nare stable fixed points of the network dynamics.\n","authors":["Marco Benedetti","Enrico Ventura"],"pdf_url":"https://arxiv.org/pdf/2302.13417v5.pdf","comment":"17 pages, 10 figures"},{"id":"http://arxiv.org/abs/2310.13240v2","updated":"2024-03-29T09:49:02Z","published":"2023-10-20T02:48:29Z","title":"Transparency challenges in policy evaluation with causal machine\n  learning -- improving usability and accountability","summary":"  Causal machine learning tools are beginning to see use in real-world policy\nevaluation tasks to flexibly estimate treatment effects. One issue with these\nmethods is that the machine learning models used are generally black boxes,\ni.e., there is no globally interpretable way to understand how a model makes\nestimates. This is a clear problem in policy evaluation applications,\nparticularly in government, because it is difficult to understand whether such\nmodels are functioning in ways that are fair, based on the correct\ninterpretation of evidence and transparent enough to allow for accountability\nif things go wrong. However, there has been little discussion of transparency\nproblems in the causal machine learning literature and how these might be\novercome. This paper explores why transparency issues are a problem for causal\nmachine learning in public policy evaluation applications and considers ways\nthese problems might be addressed through explainable AI tools and by\nsimplifying models in line with interpretable AI principles. It then applies\nthese ideas to a case-study using a causal forest model to estimate conditional\naverage treatment effects for a hypothetical change in the school leaving age\nin Australia. It shows that existing tools for understanding black-box\npredictive models are poorly suited to causal machine learning and that\nsimplifying the model to make it interpretable leads to an unacceptable\nincrease in error (in this application). It concludes that new tools are needed\nto properly understand causal machine learning models and the algorithms that\nfit them.\n","authors":["Patrick Rehill","Nicholas Biddle"],"pdf_url":"https://arxiv.org/pdf/2310.13240v2.pdf","comment":"31 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.20078v1","updated":"2024-03-29T09:19:52Z","published":"2024-03-29T09:19:52Z","title":"Negative Label Guided OOD Detection with Pretrained Vision-Language\n  Models","summary":"  Out-of-distribution (OOD) detection aims at identifying samples from unknown\nclasses, playing a crucial role in trustworthy models against errors on\nunexpected inputs. Extensive research has been dedicated to exploring OOD\ndetection in the vision modality. Vision-language models (VLMs) can leverage\nboth textual and visual information for various multi-modal applications,\nwhereas few OOD detection methods take into account information from the text\nmodality. In this paper, we propose a novel post hoc OOD detection method,\ncalled NegLabel, which takes a vast number of negative labels from extensive\ncorpus databases. We design a novel scheme for the OOD score collaborated with\nnegative labels. Theoretical analysis helps to understand the mechanism of\nnegative labels. Extensive experiments demonstrate that our method NegLabel\nachieves state-of-the-art performance on various OOD detection benchmarks and\ngeneralizes well on multiple VLM architectures. Furthermore, our method\nNegLabel exhibits remarkable robustness against diverse domain shifts. The\ncodes are available at https://github.com/tmlr-group/NegLabel.\n","authors":["Xue Jiang","Feng Liu","Zhen Fang","Hong Chen","Tongliang Liu","Feng Zheng","Bo Han"],"pdf_url":"https://arxiv.org/pdf/2403.20078v1.pdf","comment":"ICLR 2024 Spotlight"},{"id":"http://arxiv.org/abs/2403.20075v1","updated":"2024-03-29T09:17:40Z","published":"2024-03-29T09:17:40Z","title":"Adaptive Decentralized Federated Learning in Energy and Latency\n  Constrained Wireless Networks","summary":"  In Federated Learning (FL), with parameter aggregated by a central node, the\ncommunication overhead is a substantial concern. To circumvent this limitation\nand alleviate the single point of failure within the FL framework, recent\nstudies have introduced Decentralized Federated Learning (DFL) as a viable\nalternative. Considering the device heterogeneity, and energy cost associated\nwith parameter aggregation, in this paper, the problem on how to efficiently\nleverage the limited resources available to enhance the model performance is\ninvestigated. Specifically, we formulate a problem that minimizes the loss\nfunction of DFL while considering energy and latency constraints. The proposed\nsolution involves optimizing the number of local training rounds across diverse\ndevices with varying resource budgets. To make this problem tractable, we first\nanalyze the convergence of DFL with edge devices with different rounds of local\ntraining. The derived convergence bound reveals the impact of the rounds of\nlocal training on the model performance. Then, based on the derived bound, the\nclosed-form solutions of rounds of local training in different devices are\nobtained. Meanwhile, since the solutions require the energy cost of aggregation\nas low as possible, we modify different graph-based aggregation schemes to\nsolve this energy consumption minimization problem, which can be applied to\ndifferent communication scenarios. Finally, a DFL framework which jointly\nconsiders the optimized rounds of local training and the energy-saving\naggregation scheme is proposed. Simulation results show that, the proposed\nalgorithm achieves a better performance than the conventional schemes with\nfixed rounds of local training, and consumes less energy than other traditional\naggregation schemes.\n","authors":["Zhigang Yan","Dong Li"],"pdf_url":"https://arxiv.org/pdf/2403.20075v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20058v1","updated":"2024-03-29T08:47:49Z","published":"2024-03-29T08:47:49Z","title":"Revolutionizing Disease Diagnosis with simultaneous functional PET/MR\n  and Deeply Integrated Brain Metabolic, Hemodynamic, and Perfusion Networks","summary":"  Simultaneous functional PET/MR (sf-PET/MR) presents a cutting-edge multimodal\nneuroimaging technique. It provides an unprecedented opportunity for\nconcurrently monitoring and integrating multifaceted brain networks built by\nspatiotemporally covaried metabolic activity, neural activity, and cerebral\nblood flow (perfusion). Albeit high scientific/clinical values, short in\nhardware accessibility of PET/MR hinders its applications, let alone modern\nAI-based PET/MR fusion models. Our objective is to develop a clinically\nfeasible AI-based disease diagnosis model trained on comprehensive sf-PET/MR\ndata with the power of, during inferencing, allowing single modality input\n(e.g., PET only) as well as enforcing multimodal-based accuracy. To this end,\nwe propose MX-ARM, a multimodal MiXture-of-experts Alignment and Reconstruction\nModel. It is modality detachable and exchangeable, allocating different\nmulti-layer perceptrons dynamically (\"mixture of experts\") through learnable\nweights to learn respective representations from different modalities. Such\ndesign will not sacrifice model performance in uni-modal situation. To fully\nexploit the inherent complex and nonlinear relation among modalities while\nproducing fine-grained representations for uni-modal inference, we subsequently\nadd a modal alignment module to line up a dominant modality (e.g., PET) with\nrepresentations of auxiliary modalities (MR). We further adopt multimodal\nreconstruction to promote the quality of learned features. Experiments on\nprecious multimodal sf-PET/MR data for Mild Cognitive Impairment diagnosis\nshowcase the efficacy of our model toward clinically feasible precision\nmedicine.\n","authors":["Luoyu Wang","Yitian Tao","Qing Yang","Yan Liang","Siwei Liu","Hongcheng Shi","Dinggang Shen","Han Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.20058v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2209.11964v2","updated":"2024-03-29T08:46:46Z","published":"2022-09-24T08:57:10Z","title":"Strong Transferable Adversarial Attacks via Ensembled Asymptotically\n  Normal Distribution Learning","summary":"  Strong adversarial examples are crucial for evaluating and enhancing the\nrobustness of deep neural networks. However, the performance of popular attacks\nis usually sensitive, for instance, to minor image transformations, stemming\nfrom limited information -- typically only one input example, a handful of\nwhite-box source models, and undefined defense strategies. Hence, the crafted\nadversarial examples are prone to overfit the source model, which hampers their\ntransferability to unknown architectures. In this paper, we propose an approach\nnamed Multiple Asymptotically Normal Distribution Attacks (MultiANDA) which\nexplicitly characterize adversarial perturbations from a learned distribution.\nSpecifically, we approximate the posterior distribution over the perturbations\nby taking advantage of the asymptotic normality property of stochastic gradient\nascent (SGA), then employ the deep ensemble strategy as an effective proxy for\nBayesian marginalization in this process, aiming to estimate a mixture of\nGaussians that facilitates a more thorough exploration of the potential\noptimization space. The approximated posterior essentially describes the\nstationary distribution of SGA iterations, which captures the geometric\ninformation around the local optimum. Thus, MultiANDA allows drawing an\nunlimited number of adversarial perturbations for each input and reliably\nmaintains the transferability. Our proposed method outperforms ten\nstate-of-the-art black-box attacks on deep learning models with or without\ndefenses through extensive experiments on seven normally trained and seven\ndefense models.\n","authors":["Zhengwei Fang","Rui Wang","Tao Huang","Liping Jing"],"pdf_url":"https://arxiv.org/pdf/2209.11964v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16460v2","updated":"2024-03-29T08:46:16Z","published":"2024-03-25T06:43:28Z","title":"FedAC: An Adaptive Clustered Federated Learning Framework for\n  Heterogeneous Data","summary":"  Clustered federated learning (CFL) is proposed to mitigate the performance\ndeterioration stemming from data heterogeneity in federated learning (FL) by\ngrouping similar clients for cluster-wise model training. However, current CFL\nmethods struggle due to inadequate integration of global and intra-cluster\nknowledge and the absence of an efficient online model similarity metric, while\ntreating the cluster count as a fixed hyperparameter limits flexibility and\nrobustness. In this paper, we propose an adaptive CFL framework, named FedAC,\nwhich (1) efficiently integrates global knowledge into intra-cluster learning\nby decoupling neural networks and utilizing distinct aggregation methods for\neach submodule, significantly enhancing performance; (2) includes a\ncosteffective online model similarity metric based on dimensionality reduction;\n(3) incorporates a cluster number fine-tuning module for improved adaptability\nand scalability in complex, heterogeneous environments. Extensive experiments\nshow that FedAC achieves superior empirical performance, increasing the test\naccuracy by around 1.82% and 12.67% on CIFAR-10 and CIFAR-100 datasets,\nrespectively, under different non-IID settings compared to SOTA methods.\n","authors":["Yuxin Zhang","Haoyu Chen","Zheng Lin","Zhe Chen","Jin Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.16460v2.pdf","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.20047v1","updated":"2024-03-29T08:33:05Z","published":"2024-03-29T08:33:05Z","title":"Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real\n  World","summary":"  Sparse training has emerged as a promising method for resource-efficient deep\nneural networks (DNNs) in real-world applications. However, the reliability of\nsparse models remains a crucial concern, particularly in detecting unknown\nout-of-distribution (OOD) data. This study addresses the knowledge gap by\ninvestigating the reliability of sparse training from an OOD perspective and\nreveals that sparse training exacerbates OOD unreliability. The lack of unknown\ninformation and the sparse constraints hinder the effective exploration of\nweight space and accurate differentiation between known and unknown knowledge.\nTo tackle these challenges, we propose a new unknown-aware sparse training\nmethod, which incorporates a loss modification, auto-tuning strategy, and a\nvoting scheme to guide weight space exploration and mitigate confusion between\nknown and unknown information without incurring significant additional costs or\nrequiring access to additional OOD data. Theoretical insights demonstrate how\nour method reduces model confidence when faced with OOD samples. Empirical\nexperiments across multiple datasets, model architectures, and sparsity levels\nvalidate the effectiveness of our method, with improvements of up to\n\\textbf{8.4\\%} in AUROC while maintaining comparable or higher accuracy and\ncalibration. This research enhances the understanding and readiness of sparse\nDNNs for deployment in resource-limited applications. Our code is available on:\n\\url{https://github.com/StevenBoys/MOON}.\n","authors":["Bowen Lei","Dongkuan Xu","Ruqi Zhang","Bani Mallick"],"pdf_url":"https://arxiv.org/pdf/2403.20047v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.14178v3","updated":"2024-03-29T08:13:38Z","published":"2023-04-27T13:27:01Z","title":"mPLUG-Owl: Modularization Empowers Large Language Models with\n  Multimodality","summary":"  Large language models (LLMs) have demonstrated impressive zero-shot abilities\non a variety of open-ended tasks, while recent research has also explored the\nuse of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl,\na novel training paradigm that equips LLMs with multi-modal abilities through\nmodularized learning of foundation LLM, a visual knowledge module, and a visual\nabstractor module. This approach can support multiple modalities and facilitate\ndiverse unimodal and multimodal abilities through modality collaboration. The\ntraining paradigm of mPLUG-Owl involves a two-stage method for aligning image\nand text, which learns visual knowledge with the assistance of LLM while\nmaintaining and even improving the generation abilities of LLM. In the first\nstage, the visual knowledge module and abstractor module are trained with a\nfrozen LLM module to align the image and text. In the second stage,\nlanguage-only and multi-modal supervised datasets are used to jointly fine-tune\na low-rank adaption (LoRA) module on LLM and the abstractor module by freezing\nthe visual knowledge module. We carefully build a visually-related instruction\nevaluation set OwlEval. Experimental results show that our model outperforms\nexisting multi-modal models, demonstrating mPLUG-Owl's impressive instruction\nand visual understanding ability, multi-turn conversation ability, and\nknowledge reasoning ability. Besides, we observe some unexpected and exciting\nabilities such as multi-image correlation and scene text understanding, which\nmakes it possible to leverage it for harder real scenarios, such as vision-only\ndocument comprehension. Our code, pre-trained model, instruction-tuned models,\nand evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The\nonline demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.\n","authors":["Qinghao Ye","Haiyang Xu","Guohai Xu","Jiabo Ye","Ming Yan","Yiyang Zhou","Junyang Wang","Anwen Hu","Pengcheng Shi","Yaya Shi","Chenliang Li","Yuanhong Xu","Hehong Chen","Junfeng Tian","Qi Qian","Ji Zhang","Fei Huang","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2304.14178v3.pdf","comment":"Working in Process"},{"id":"http://arxiv.org/abs/2403.20033v1","updated":"2024-03-29T07:59:33Z","published":"2024-03-29T07:59:33Z","title":"A novel decision fusion approach for sale price prediction using Elastic\n  Net and MOPSO","summary":"  Price prediction algorithms propose prices for every product or service\naccording to market trends, projected demand, and other characteristics,\nincluding government rules, international transactions, and speculation and\nexpectation. As the dependent variable in price prediction, it is affected by\nseveral independent and correlated variables which may challenge the price\nprediction. To overcome this challenge, machine learning algorithms allow more\naccurate price prediction without explicitly modeling the relatedness between\nvariables. However, as inputs increase, it challenges the existing machine\nlearning approaches regarding computing efficiency and prediction\neffectiveness. Hence, this study introduces a novel decision level fusion\napproach to select informative variables in price prediction. The suggested\nmetaheuristic algorithm balances two competitive objective functions, which are\ndefined to improve the prediction utilized variables and reduce the error rate\nsimultaneously. To generate Pareto optimal solutions, an Elastic net approach\nis employed to eliminate unrelated and redundant variables to increase the\naccuracy. Afterward, we propose a novel method for combining solutions and\nensuring that a subset of features is optimal. Two various real datasets\nevaluate the proposed price prediction method. The results support the\nsuggested superiority of the model concerning its relative root mean square\nerror and adjusted correlation coefficient.\n","authors":["Amir Eshaghi Chaleshtori"],"pdf_url":"https://arxiv.org/pdf/2403.20033v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20020v1","updated":"2024-03-29T07:15:30Z","published":"2024-03-29T07:15:30Z","title":"Nonparametric Bellman Mappings for Reinforcement Learning: Application\n  to Robust Adaptive Filtering","summary":"  This paper designs novel nonparametric Bellman mappings in reproducing kernel\nHilbert spaces (RKHSs) for reinforcement learning (RL). The proposed mappings\nbenefit from the rich approximating properties of RKHSs, adopt no assumptions\non the statistics of the data owing to their nonparametric nature, require no\nknowledge on transition probabilities of Markov decision processes, and may\noperate without any training data. Moreover, they allow for sampling on-the-fly\nvia the design of trajectory samples, re-use past test data via experience\nreplay, effect dimensionality reduction by random Fourier features, and enable\ncomputationally lightweight operations to fit into efficient online or\ntime-adaptive learning. The paper offers also a variational framework to design\nthe free parameters of the proposed Bellman mappings, and shows that\nappropriate choices of those parameters yield several popular Bellman-mapping\ndesigns. As an application, the proposed mappings are employed to offer a novel\nsolution to the problem of countering outliers in adaptive filtering. More\nspecifically, with no prior information on the statistics of the outliers and\nno training data, a policy-iteration algorithm is introduced to select online,\nper time instance, the ``optimal'' coefficient p in the\nleast-mean-p-power-error method. Numerical tests on synthetic data showcase, in\nmost of the cases, the superior performance of the proposed solution over\nseveral RL and non-RL schemes.\n","authors":["Yuki Akiyama","Minh Vu","Konstantinos Slavakis"],"pdf_url":"https://arxiv.org/pdf/2403.20020v1.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2403.20016v1","updated":"2024-03-29T07:03:10Z","published":"2024-03-29T07:03:10Z","title":"EnCoMP: Enhanced Covert Maneuver Planning using Offline Reinforcement\n  Learning","summary":"  Cover navigation in complex environments is a critical challenge for\nautonomous robots, requiring the identification and utilization of\nenvironmental cover while maintaining efficient navigation. We propose an\nenhanced navigation system that enables robots to identify and utilize natural\nand artificial environmental features as cover, thereby minimizing exposure to\npotential threats. Our perception pipeline leverages LiDAR data to generate\nhigh-fidelity cover maps and potential threat maps, providing a comprehensive\nunderstanding of the surrounding environment. We train an offline reinforcement\nlearning model using a diverse dataset collected from real-world environments,\nlearning a robust policy that evaluates the quality of candidate actions based\non their ability to maximize cover utilization, minimize exposure to threats,\nand reach the goal efficiently. Extensive real-world experiments demonstrate\nthe superiority of our approach in terms of success rate, cover utilization,\nexposure minimization, and navigation efficiency compared to state-of-the-art\nmethods.\n","authors":["Jumman Hossain","Abu-Zaher Faridee","Nirmalya Roy"],"pdf_url":"https://arxiv.org/pdf/2403.20016v1.pdf","comment":"Paper under review for CVPR Workshop"},{"id":"http://arxiv.org/abs/2403.20009v1","updated":"2024-03-29T06:48:30Z","published":"2024-03-29T06:48:30Z","title":"On Large Language Models' Hallucination with Regard to Known Facts","summary":"  Large language models are successful in answering factoid questions but are\nalso prone to hallucination.We investigate the phenomenon of LLMs possessing\ncorrect answer knowledge yet still hallucinating from the perspective of\ninference dynamics, an area not previously covered in studies on\nhallucinations.We are able to conduct this analysis via two key ideas.First, we\nidentify the factual questions that query the same triplet knowledge but result\nin different answers. The difference between the model behaviors on the correct\nand incorrect outputs hence suggests the patterns when hallucinations happen.\nSecond, to measure the pattern, we utilize mappings from the residual streams\nto vocabulary space. We reveal the different dynamics of the output token\nprobabilities along the depths of layers between the correct and hallucinated\ncases. In hallucinated cases, the output token's information rarely\ndemonstrates abrupt increases and consistent superiority in the later stages of\nthe model. Leveraging the dynamic curve as a feature, we build a classifier\ncapable of accurately detecting hallucinatory predictions with an 88\\% success\nrate. Our study shed light on understanding the reasons for LLMs'\nhallucinations on their known facts, and more importantly, on accurately\npredicting when they are hallucinating.\n","authors":["Che Jiang","Biqing Qi","Xiangyu Hong","Dayuan Fu","Yang Cheng","Fandong Meng","Mo Yu","Bowen Zhou","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.20009v1.pdf","comment":"Accepted by NAACL 2024 MainConference"},{"id":"http://arxiv.org/abs/2403.12236v2","updated":"2024-03-29T06:41:07Z","published":"2024-03-18T20:33:44Z","title":"Improving Generalization via Meta-Learning on Hard Samples","summary":"  Learned reweighting (LRW) approaches to supervised learning use an\noptimization criterion to assign weights for training instances, in order to\nmaximize performance on a representative validation dataset. We pose and\nformalize the problem of optimized selection of the validation set used in LRW\ntraining, to improve classifier generalization. In particular, we show that\nusing hard-to-classify instances in the validation set has both a theoretical\nconnection to, and strong empirical evidence of generalization. We provide an\nefficient algorithm for training this meta-optimized model, as well as a simple\ntrain-twice heuristic for careful comparative study. We demonstrate that LRW\nwith easy validation data performs consistently worse than LRW with hard\nvalidation data, establishing the validity of our meta-optimization problem.\nOur proposed algorithm outperforms a wide range of baselines on a range of\ndatasets and domain shift challenges (Imagenet-1K, CIFAR-100, Clothing-1M,\nCAMELYON, WILDS, etc.), with ~1% gains using VIT-B on Imagenet. We also show\nthat using naturally hard examples for validation (Imagenet-R / Imagenet-A) in\nLRW training for Imagenet improves performance on both clean and naturally hard\ntest instances by 1-2%. Secondary analyses show that using hard validation data\nin an LRW framework improves margins on test data, hinting at the mechanism\nunderlying our empirical gains. We believe this work opens up new research\ndirections for the meta-optimization of meta-learning in a supervised learning\ncontext.\n","authors":["Nishant Jain","Arun S. Suggala","Pradeep Shenoy"],"pdf_url":"https://arxiv.org/pdf/2403.12236v2.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.19996v1","updated":"2024-03-29T06:24:07Z","published":"2024-03-29T06:24:07Z","title":"DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT\n  Sensor Data","summary":"  Internet of Things (IoT) sensor data or readings evince variations in\ntimestamp range, sampling frequency, geographical location, unit of\nmeasurement, etc. Such presented sequence data heterogeneity makes it difficult\nfor traditional time series classification algorithms to perform well.\nTherefore, addressing the heterogeneity challenge demands learning not only the\nsub-patterns (local features) but also the overall pattern (global feature). To\naddress the challenge of classifying heterogeneous IoT sensor data (e.g.,\ncategorizing sensor data types like temperature and humidity), we propose a\nnovel deep learning model that incorporates both Convolutional Neural Network\nand Bi-directional Gated Recurrent Unit to learn local and global features\nrespectively, in an end-to-end manner. Through rigorous experimentation on\nheterogeneous IoT sensor datasets, we validate the effectiveness of our\nproposed model, which outperforms recent state-of-the-art classification\nmethods as well as several machine learning and deep learning baselines. In\nparticular, the model achieves an average absolute improvement of 3.37% in\nAccuracy and 2.85% in F1-Score across datasets\n","authors":["Muhammad Sakib Khan Inan","Kewen Liao","Haifeng Shen","Prem Prakash Jayaraman","Dimitrios Georgakopoulos","Ming Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2403.19996v1.pdf","comment":"Accepted for Publication and Presented in EAI MobiQuitous 2023 - 20th\n  EAI International Conference on Mobile and Ubiquitous Systems: Computing,\n  Networking and Services"},{"id":"http://arxiv.org/abs/2308.10806v2","updated":"2024-03-29T06:15:41Z","published":"2023-08-21T15:53:38Z","title":"DFWLayer: Differentiable Frank-Wolfe Optimization Layer","summary":"  Differentiable optimization has received a significant amount of attention\ndue to its foundational role in the domain of machine learning based on neural\nnetworks. This paper proposes a differentiable layer, named Differentiable\nFrank-Wolfe Layer (DFWLayer), by rolling out the Frank-Wolfe method, a\nwell-known optimization algorithm which can solve constrained optimization\nproblems without projections and Hessian matrix computations, thus leading to\nan efficient way of dealing with large-scale convex optimization problems with\nnorm constraints. Experimental results demonstrate that the DFWLayer not only\nattains competitive accuracy in solutions and gradients but also consistently\nadheres to constraints.\n","authors":["Zixuan Liu","Liu Liu","Xueqian Wang","Peilin Zhao"],"pdf_url":"https://arxiv.org/pdf/2308.10806v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.13001v3","updated":"2024-03-29T05:51:53Z","published":"2023-10-06T12:31:05Z","title":"Conversational Financial Information Retrieval Model (ConFIRM)","summary":"  With the exponential growth in large language models (LLMs), leveraging their\nemergent properties for specialized domains like finance merits exploration.\nHowever, regulated fields such as finance pose unique constraints, requiring\ndomain-optimized frameworks. We present ConFIRM, an LLM-based conversational\nfinancial information retrieval model tailored for query intent classification\nand knowledge base labeling.\n  ConFIRM comprises two modules:\n  1) a method to synthesize finance domain-specific question-answer pairs, and\n  2) evaluation of parameter efficient fine-tuning approaches for the query\nclassification task. We generate a dataset of over 4000 samples, assessing\naccuracy on a separate test set.\n  ConFIRM achieved over 90% accuracy, essential for regulatory compliance.\nConFIRM provides a data-efficient solution to extract precise query intent for\nfinancial dialog systems.\n","authors":["Stephen Choi","William Gazeley","Siu Ho Wong","Tingting Li"],"pdf_url":"https://arxiv.org/pdf/2310.13001v3.pdf","comment":"10 pages, 2 figures, 2 tables, 2 appendices"},{"id":"http://arxiv.org/abs/2403.19979v1","updated":"2024-03-29T05:23:12Z","published":"2024-03-29T05:23:12Z","title":"Semantically-Shifted Incremental Adapter-Tuning is A Continual\n  ViTransformer","summary":"  Class-incremental learning (CIL) aims to enable models to continuously learn\nnew classes while overcoming catastrophic forgetting. The introduction of\npre-trained models has brought new tuning paradigms to CIL. In this paper, we\nrevisit different parameter-efficient tuning (PET) methods within the context\nof continual learning. We observe that adapter tuning demonstrates superiority\nover prompt-based methods, even without parameter expansion in each learning\nsession. Motivated by this, we propose incrementally tuning the shared adapter\nwithout imposing parameter update constraints, enhancing the learning capacity\nof the backbone. Additionally, we employ feature sampling from stored\nprototypes to retrain a unified classifier, further improving its performance.\nWe estimate the semantic shift of old prototypes without access to past samples\nand update stored prototypes session by session. Our proposed method eliminates\nmodel expansion and avoids retaining any image samples. It surpasses previous\npre-trained model-based CIL methods and demonstrates remarkable continual\nlearning capabilities. Experimental results on five CIL benchmarks validate the\neffectiveness of our approach, achieving state-of-the-art (SOTA) performance.\n","authors":["Yuwen Tan","Qinhao Zhou","Xiang Xiang","Ke Wang","Yuchuan Wu","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2403.19979v1.pdf","comment":"To appear at CVPR 2024"},{"id":"http://arxiv.org/abs/2312.10359v2","updated":"2024-03-29T04:44:44Z","published":"2023-12-16T07:27:07Z","title":"Conformer-Based Speech Recognition On Extreme Edge-Computing Devices","summary":"  With increasingly more powerful compute capabilities and resources in today's\ndevices, traditionally compute-intensive automatic speech recognition (ASR) has\nbeen moving from the cloud to devices to better protect user privacy. However,\nit is still challenging to implement on-device ASR on resource-constrained\ndevices, such as smartphones, smart wearables, and other small home automation\ndevices. In this paper, we propose a series of model architecture adaptions,\nneural network graph transformations, and numerical optimizations to fit an\nadvanced Conformer based end-to-end streaming ASR system on\nresource-constrained devices without accuracy degradation. We achieve over 5.26\ntimes faster than realtime (0.19 RTF) speech recognition on small wearables\nwhile minimizing energy consumption and achieving state-of-the-art accuracy.\nThe proposed methods are widely applicable to other transformer-based\nserver-free AI applications. In addition, we provide a complete theory on\noptimal pre-normalizers that numerically stabilize layer normalization in any\nLp-norm using any floating point precision.\n","authors":["Mingbin Xu","Alex Jin","Sicheng Wang","Mu Su","Tim Ng","Henry Mason","Shiyi Han","Zhihong Lei Yaqiao Deng","Zhen Huang","Mahesh Krishnamoorthy"],"pdf_url":"https://arxiv.org/pdf/2312.10359v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11461v2","updated":"2024-03-29T04:32:57Z","published":"2023-12-18T18:59:12Z","title":"GAvatar: Animatable 3D Gaussian Avatars with Implicit Mesh Learning","summary":"  Gaussian splatting has emerged as a powerful 3D representation that harnesses\nthe advantages of both explicit (mesh) and implicit (NeRF) 3D representations.\nIn this paper, we seek to leverage Gaussian splatting to generate realistic\nanimatable avatars from textual descriptions, addressing the limitations (e.g.,\nflexibility and efficiency) imposed by mesh or NeRF-based representations.\nHowever, a naive application of Gaussian splatting cannot generate high-quality\nanimatable avatars and suffers from learning instability; it also cannot\ncapture fine avatar geometries and often leads to degenerate body parts. To\ntackle these problems, we first propose a primitive-based 3D Gaussian\nrepresentation where Gaussians are defined inside pose-driven primitives to\nfacilitate animation. Second, to stabilize and amortize the learning of\nmillions of Gaussians, we propose to use neural implicit fields to predict the\nGaussian attributes (e.g., colors). Finally, to capture fine avatar geometries\nand extract detailed meshes, we propose a novel SDF-based implicit mesh\nlearning approach for 3D Gaussians that regularizes the underlying geometries\nand extracts highly detailed textured meshes. Our proposed method, GAvatar,\nenables the large-scale generation of diverse animatable avatars using only\ntext prompts. GAvatar significantly surpasses existing methods in terms of both\nappearance and geometry quality, and achieves extremely fast rendering (100\nfps) at 1K resolution.\n","authors":["Ye Yuan","Xueting Li","Yangyi Huang","Shalini De Mello","Koki Nagano","Jan Kautz","Umar Iqbal"],"pdf_url":"https://arxiv.org/pdf/2312.11461v2.pdf","comment":"CVPR 2024. Project website: https://nvlabs.github.io/GAvatar"},{"id":"http://arxiv.org/abs/2403.19969v1","updated":"2024-03-29T04:28:06Z","published":"2024-03-29T04:28:06Z","title":"Separate, Dynamic and Differentiable (SMART) Pruner for Block/Output\n  Channel Pruning on Computer Vision Tasks","summary":"  Deep Neural Network (DNN) pruning has emerged as a key strategy to reduce\nmodel size, improve inference latency, and lower power consumption on DNN\naccelerators. Among various pruning techniques, block and output channel\npruning have shown significant potential in accelerating hardware performance.\nHowever, their accuracy often requires further improvement. In response to this\nchallenge, we introduce a separate, dynamic and differentiable (SMART) pruner.\nThis pruner stands out by utilizing a separate, learnable probability mask for\nweight importance ranking, employing a differentiable Top k operator to achieve\ntarget sparsity, and leveraging a dynamic temperature parameter trick to escape\nfrom non-sparse local minima. In our experiments, the SMART pruner consistently\ndemonstrated its superiority over existing pruning methods across a wide range\nof tasks and models on block and output channel pruning. Additionally, we\nextend our testing to Transformer-based models in N:M pruning scenarios, where\nSMART pruner also yields state-of-the-art results, demonstrating its\nadaptability and robustness across various neural network architectures, and\npruning types.\n","authors":["Guanhua Ding","Zexi Ye","Zhen Zhong","Gang Li","David Shao"],"pdf_url":"https://arxiv.org/pdf/2403.19969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.02856v4","updated":"2024-03-29T04:18:14Z","published":"2021-12-06T08:27:54Z","title":"Doubly Optimal No-Regret Online Learning in Strongly Monotone Games with\n  Bandit Feedback","summary":"  We consider online no-regret learning in unknown games with bandit feedback,\nwhere each player can only observe its reward at each time -- determined by all\nplayers' current joint action -- rather than its gradient. We focus on the\nclass of \\textit{smooth and strongly monotone} games and study optimal\nno-regret learning therein. Leveraging self-concordant barrier functions, we\nfirst construct a new bandit learning algorithm and show that it achieves the\nsingle-agent optimal regret of $\\tilde{\\Theta}(n\\sqrt{T})$ under smooth and\nstrongly concave reward functions ($n \\geq 1$ is the problem dimension). We\nthen show that if each player applies this no-regret learning algorithm in\nstrongly monotone games, the joint action converges in the \\textit{last\niterate} to the unique Nash equilibrium at a rate of\n$\\tilde{\\Theta}(nT^{-1/2})$. Prior to our work, the best-known convergence rate\nin the same class of games is $\\tilde{O}(n^{2/3}T^{-1/3})$ (achieved by a\ndifferent algorithm), thus leaving open the problem of optimal no-regret\nlearning algorithms (since the known lower bound is $\\Omega(nT^{-1/2})$). Our\nresults thus settle this open problem and contribute to the broad landscape of\nbandit game-theoretical learning by identifying the first doubly optimal bandit\nlearning algorithm, in that it achieves (up to log factors) both optimal regret\nin the single-agent learning and optimal last-iterate convergence rate in the\nmulti-agent learning. We also present preliminary numerical results on several\napplication problems to demonstrate the efficacy of our algorithm in terms of\niteration count.\n","authors":["Wenjia Ba","Tianyi Lin","Jiawei Zhang","Zhengyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2112.02856v4.pdf","comment":"43 pages, 4 figures"},{"id":"http://arxiv.org/abs/2211.13398v3","updated":"2024-03-29T04:13:49Z","published":"2022-11-24T03:27:00Z","title":"CPPF++: Uncertainty-Aware Sim2Real Object Pose Estimation by Vote\n  Aggregation","summary":"  Object pose estimation constitutes a critical area within the domain of 3D\nvision. While contemporary state-of-the-art methods that leverage real-world\npose annotations have demonstrated commendable performance, the procurement of\nsuch real training data incurs substantial costs. This paper focuses on a\nspecific setting wherein only 3D CAD models are utilized as a priori knowledge,\ndevoid of any background or clutter information. We introduce a novel method,\nCPPF++, designed for sim-to-real pose estimation. This method builds upon the\nfoundational point-pair voting scheme of CPPF, reformulating it through a\nprobabilistic view. To address the challenge posed by vote collision, we\npropose a novel approach that involves modeling the voting uncertainty by\nestimating the probabilistic distribution of each point pair within the\ncanonical space. Furthermore, we augment the contextual information provided by\neach voting unit through the introduction of N-point tuples. To enhance the\nrobustness and accuracy of the model, we incorporate several innovative\nmodules, including noisy pair filtering, online alignment optimization, and a\ntuple feature ensemble. Alongside these methodological advancements, we\nintroduce a new category-level pose estimation dataset, named DiversePose 300.\nEmpirical evidence demonstrates that our method significantly surpasses\nprevious sim-to-real approaches and achieves comparable or superior performance\non novel datasets. Our code is available on https://github.com/qq456cvb/CPPF2.\n","authors":["Yang You","Wenhao He","Jin Liu","Hongkai Xiong","Weiming Wang","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2211.13398v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.12420v2","updated":"2024-03-29T04:13:25Z","published":"2023-04-24T19:52:42Z","title":"Sample-Efficient and Surrogate-Based Design Optimization of Underwater\n  Vehicle Hulls","summary":"  Physics simulations like computational fluid dynamics (CFD) are a\ncomputational bottleneck in computer-aided design (CAD) optimization processes.\nTo overcome this bottleneck, one requires either an optimization framework that\nis highly sample-efficient, or a fast data-driven proxy (surrogate model) for\nlong-running simulations. Both approaches have benefits and limitations.\nBayesian optimization is often used for sample efficiency, but it solves one\nspecific problem and struggles with transferability; alternatively, surrogate\nmodels can offer fast and often more generalizable solutions for CFD problems,\nbut gathering data for and training such models can be computationally\ndemanding. In this work, we leverage recent advances in optimization and\nartificial intelligence (AI) to explore both of these potential approaches, in\nthe context of designing an optimal unmanned underwater vehicle (UUV) hull. Our\nstudy finds that the Bayesian Optimization-Lower Condition Bound (BO-LCB)\nalgorithm is the most sample-efficient optimization framework and has the best\nconvergence behavior of those considered. Subsequently, we show that our\nDNN-based surrogate model predicts drag force on test data in tight agreement\nwith CFD simulations, with a mean absolute percentage error (MAPE) of 1.85%.\nCombining these results, we demonstrate a two-orders-of-magnitude speedup (with\ncomparable accuracy) for the design optimization process when the surrogate\nmodel is used. To our knowledge, this is the first study applying Bayesian\noptimization and DNN-based surrogate modeling to the problem of UUV design\noptimization, and we share our developments as open-source software.\n","authors":["Harsh Vardhan","David Hyde","Umesh Timalsina","Peter Volgyesi","Janos Sztipanovits"],"pdf_url":"https://arxiv.org/pdf/2304.12420v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19964v1","updated":"2024-03-29T03:56:19Z","published":"2024-03-29T03:56:19Z","title":"FairRAG: Fair Human Generation via Fair Retrieval Augmentation","summary":"  Existing text-to-image generative models reflect or even amplify societal\nbiases ingrained in their training data. This is especially concerning for\nhuman image generation where models are biased against certain demographic\ngroups. Existing attempts to rectify this issue are hindered by the inherent\nlimitations of the pre-trained models and fail to substantially improve\ndemographic diversity. In this work, we introduce Fair Retrieval Augmented\nGeneration (FairRAG), a novel framework that conditions pre-trained generative\nmodels on reference images retrieved from an external image database to improve\nfairness in human generation. FairRAG enables conditioning through a\nlightweight linear module that projects reference images into the textual\nspace. To enhance fairness, FairRAG applies simple-yet-effective debiasing\nstrategies, providing images from diverse demographic groups during the\ngenerative process. Extensive experiments demonstrate that FairRAG outperforms\nexisting methods in terms of demographic diversity, image-text alignment, and\nimage fidelity while incurring minimal computational overhead during inference.\n","authors":["Robik Shrestha","Yang Zou","Qiuyu Chen","Zhiheng Li","Yusheng Xie","Siqi Deng"],"pdf_url":"https://arxiv.org/pdf/2403.19964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19962v1","updated":"2024-03-29T03:48:12Z","published":"2024-03-29T03:48:12Z","title":"Enhancing the General Agent Capabilities of Low-Parameter LLMs through\n  Tuning and Multi-Branch Reasoning","summary":"  Open-source pre-trained Large Language Models (LLMs) exhibit strong language\nunderstanding and generation capabilities, making them highly successful in a\nvariety of tasks. However, when used as agents for dealing with complex\nproblems in the real world, their performance is far inferior to large\ncommercial models such as ChatGPT and GPT-4. As intelligent agents, LLMs need\nto have the capabilities of task planning, long-term memory, and the ability to\nleverage external tools to achieve satisfactory performance. Various methods\nhave been proposed to enhance the agent capabilities of LLMs. On the one hand,\nmethods involve constructing agent-specific data and fine-tuning the models. On\nthe other hand, some methods focus on designing prompts that effectively\nactivate the reasoning abilities of the LLMs. We explore both strategies on the\n7B and 13B models. We propose a comprehensive method for constructing\nagent-specific data using GPT-4. Through supervised fine-tuning with\nconstructed data, we find that for these models with a relatively small number\nof parameters, supervised fine-tuning can significantly reduce hallucination\noutputs and formatting errors in agent tasks. Furthermore, techniques such as\nmulti-path reasoning and task decomposition can effectively decrease problem\ncomplexity and enhance the performance of LLMs as agents. We evaluate our\nmethod on five agent tasks of AgentBench and achieve satisfactory results.\n","authors":["Qinhao Zhou","Zihan Zhang","Xiang Xiang","Ke Wang","Yuchuan Wu","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2403.19962v1.pdf","comment":"To appear at NAACL 2024"},{"id":"http://arxiv.org/abs/2309.16971v4","updated":"2024-03-29T03:44:48Z","published":"2023-09-29T04:41:27Z","title":"Multi-Resolution Active Learning of Fourier Neural Operators","summary":"  Fourier Neural Operator (FNO) is a popular operator learning framework. It\nnot only achieves the state-of-the-art performance in many tasks, but also is\nefficient in training and prediction. However, collecting training data for the\nFNO can be a costly bottleneck in practice, because it often demands expensive\nphysical simulations. To overcome this problem, we propose Multi-Resolution\nActive learning of FNO (MRA-FNO), which can dynamically select the input\nfunctions and resolutions to lower the data cost as much as possible while\noptimizing the learning efficiency. Specifically, we propose a probabilistic\nmulti-resolution FNO and use ensemble Monte-Carlo to develop an effective\nposterior inference algorithm. To conduct active learning, we maximize a\nutility-cost ratio as the acquisition function to acquire new examples and\nresolutions at each step. We use moment matching and the matrix determinant\nlemma to enable tractable, efficient utility computation. Furthermore, we\ndevelop a cost annealing framework to avoid over-penalizing high-resolution\nqueries at the early stage. The over-penalization is severe when the cost\ndifference is significant between the resolutions, which renders active\nlearning often stuck at low-resolution queries and inferior performance. Our\nmethod overcomes this problem and applies to general multi-fidelity active\nlearning and optimization problems. We have shown the advantage of our method\nin several benchmark operator learning tasks. The code is available at\nhttps://github.com/shib0li/MRA-FNO.\n","authors":["Shibo Li","Xin Yu","Wei Xing","Mike Kirby","Akil Narayan","Shandian Zhe"],"pdf_url":"https://arxiv.org/pdf/2309.16971v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06530v4","updated":"2024-03-29T03:37:04Z","published":"2023-03-12T01:12:43Z","title":"Making Batch Normalization Great in Federated Deep Learning","summary":"  Batch Normalization (BN) is widely used in {centralized} deep learning to\nimprove convergence and generalization. However, in {federated} learning (FL)\nwith decentralized data, prior work has observed that training with BN could\nhinder performance and suggested replacing it with Group Normalization (GN). In\nthis paper, we revisit this substitution by expanding the empirical study\nconducted in prior work. Surprisingly, we find that BN outperforms GN in many\nFL settings. The exceptions are high-frequency communication and extreme\nnon-IID regimes. We reinvestigate factors that are believed to cause this\nproblem, including the mismatch of BN statistics across clients and the\ndeviation of gradients during local training. We empirically identify a simple\npractice that could reduce the impacts of these factors while maintaining the\nstrength of BN. Our approach, which we named FIXBN, is fairly easy to\nimplement, without any additional training or communication costs, and performs\nfavorably across a wide range of FL settings. We hope that our study could\nserve as a valuable reference for future practical usage and theoretical\nanalysis in FL.\n","authors":["Jike Zhong","Hong-You Chen","Wei-Lun Chao"],"pdf_url":"https://arxiv.org/pdf/2303.06530v4.pdf","comment":"An extended version of the workshop paper in NeurIPS 2023\n  (https://federated-learning.org/fl@fm-neurips-2023/)"},{"id":"http://arxiv.org/abs/2403.19950v1","updated":"2024-03-29T03:16:29Z","published":"2024-03-29T03:16:29Z","title":"Coverage-Guaranteed Prediction Sets for Out-of-Distribution Data","summary":"  Out-of-distribution (OOD) generalization has attracted increasing research\nattention in recent years, due to its promising experimental results in\nreal-world applications. In this paper,we study the confidence set prediction\nproblem in the OOD generalization setting. Split conformal prediction (SCP) is\nan efficient framework for handling the confidence set prediction problem.\nHowever, the validity of SCP requires the examples to be exchangeable, which is\nviolated in the OOD setting. Empirically, we show that trivially applying SCP\nresults in a failure to maintain the marginal coverage when the unseen target\ndomain is different from the source domain. To address this issue, we develop a\nmethod for forming confident prediction sets in the OOD setting and\ntheoretically prove the validity of our method. Finally, we conduct experiments\non simulated data to empirically verify the correctness of our theory and the\nvalidity of our proposed method.\n","authors":["Xin Zou","Weiwei Liu"],"pdf_url":"https://arxiv.org/pdf/2403.19950v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12400v2","updated":"2024-03-29T02:58:19Z","published":"2023-12-19T18:35:33Z","title":"New Classes of the Greedy-Applicable Arm Feature Distributions in the\n  Sparse Linear Bandit Problem","summary":"  We consider the sparse contextual bandit problem where arm feature affects\nreward through the inner product of sparse parameters. Recent studies have\ndeveloped sparsity-agnostic algorithms based on the greedy arm selection\npolicy. However, the analysis of these algorithms requires strong assumptions\non the arm feature distribution to ensure that the greedily selected samples\nare sufficiently diverse; One of the most common assumptions, relaxed symmetry,\nimposes approximate origin-symmetry on the distribution, which cannot allow\ndistributions that has origin-asymmetric support. In this paper, we show that\nthe greedy algorithm is applicable to a wider range of the arm feature\ndistributions from two aspects. Firstly, we show that a mixture distribution\nthat has a greedy-applicable component is also greedy-applicable. Second, we\npropose new distribution classes, related to Gaussian mixture, discrete, and\nradial distribution, for which the sample diversity is guaranteed. The proposed\nclasses can describe distributions with origin-asymmetric support and, in\nconjunction with the first claim, provide theoretical guarantees of the greedy\npolicy for a very wide range of the arm feature distributions.\n","authors":["Koji Ichikawa","Shinji Ito","Daisuke Hatano","Hanna Sumita","Takuro Fukunaga","Naonori Kakimura","Ken-ichi Kawarabayashi"],"pdf_url":"https://arxiv.org/pdf/2312.12400v2.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2307.02766v2","updated":"2024-03-29T02:55:22Z","published":"2023-07-06T04:27:16Z","title":"Temporal Difference Learning for High-Dimensional PIDEs with Jumps","summary":"  In this paper, we propose a deep learning framework for solving\nhigh-dimensional partial integro-differential equations (PIDEs) based on the\ntemporal difference learning. We introduce a set of Levy processes and\nconstruct a corresponding reinforcement learning model. To simulate the entire\nprocess, we use deep neural networks to represent the solutions and non-local\nterms of the equations. Subsequently, we train the networks using the temporal\ndifference error, termination condition, and properties of the non-local terms\nas the loss function. The relative error of the method reaches O(10^{-3}) in\n100-dimensional experiments and O(10^{-4}) in one-dimensional pure jump\nproblems. Additionally, our method demonstrates the advantages of low\ncomputational cost and robustness, making it well-suited for addressing\nproblems with different forms and intensities of jumps.\n","authors":["Liwei Lu","Hailong Guo","Xu Yang","Yi Zhu"],"pdf_url":"https://arxiv.org/pdf/2307.02766v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19943v1","updated":"2024-03-29T02:54:41Z","published":"2024-03-29T02:54:41Z","title":"TDANet: A Novel Temporal Denoise Convolutional Neural Network With\n  Attention for Fault Diagnosis","summary":"  Fault diagnosis plays a crucial role in maintaining the operational integrity\nof mechanical systems, preventing significant losses due to unexpected\nfailures. As intelligent manufacturing and data-driven approaches evolve, Deep\nLearning (DL) has emerged as a pivotal technique in fault diagnosis research,\nrecognized for its ability to autonomously extract complex features. However,\nthe practical application of current fault diagnosis methods is challenged by\nthe complexity of industrial environments. This paper proposed the Temporal\nDenoise Convolutional Neural Network With Attention (TDANet), designed to\nimprove fault diagnosis performance in noise environments. This model\ntransforms one-dimensional signals into two-dimensional tensors based on their\nperiodic properties, employing multi-scale 2D convolution kernels to extract\nsignal information both within and across periods. This method enables\neffective identification of signal characteristics that vary over multiple time\nscales. The TDANet incorporates a Temporal Variable Denoise (TVD) module with\nresidual connections and a Multi-head Attention Fusion (MAF) module, enhancing\nthe saliency of information within noisy data and maintaining effective fault\ndiagnosis performance. Evaluation on two datasets, CWRU (single sensor) and\nReal aircraft sensor fault (multiple sensors), demonstrates that the TDANet\nmodel significantly outperforms existing deep learning approaches in terms of\ndiagnostic accuracy under noisy environments.\n","authors":["Zhongzhi Li","Rong Fan","Jingqi Tu","Jinyi Ma","Jianliang Ai","Yiqun Dong"],"pdf_url":"https://arxiv.org/pdf/2403.19943v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03223v4","updated":"2024-03-29T02:47:29Z","published":"2023-10-05T00:45:04Z","title":"TacoGFN: Target-conditioned GFlowNet for Structure-based Drug Design","summary":"  Searching the vast chemical space for drug-like and synthesizable molecules\nwith high binding affinity to a protein pocket is a challenging task in drug\ndiscovery. Recently, molecular deep generative models have been introduced\nwhich promise to be more efficient than exhaustive virtual screening, by\ndirectly generating molecules based on the protein structure. However, since\nthey learn the distribution of a limited protein-ligand complex dataset, the\nexisting methods struggle with generating novel molecules with significant\nproperty improvements. In this paper, we frame the generation task as a\nReinforcement Learning task, where the goal is to search the wider chemical\nspace for molecules with desirable properties as opposed to fitting a training\ndata distribution. More specifically, we propose TacoGFN, a Generative Flow\nNetwork conditioned on protein pocket structure, using binding affinity,\ndrug-likeliness and synthesizability measures as our reward. Empirically, our\nmethod outperforms state-of-art methods on the CrossDocked2020 benchmark for\nevery molecular property (Vina score, QED, SA), while significantly improving\nthe generation time. TacoGFN achieves $-8.82$ in median docking score and\n$52.63\\%$ in Novel Hit Rate.\n","authors":["Tony Shen","Seonghwan Seo","Grayson Lee","Mohit Pandey","Jason R Smith","Artem Cherkasov","Woo Youn Kim","Martin Ester"],"pdf_url":"https://arxiv.org/pdf/2310.03223v4.pdf","comment":"Accepted at NeurIPS 2023 AID3 and at NeurIPS 2023 GenBio as Spotlight"},{"id":"http://arxiv.org/abs/2403.19928v1","updated":"2024-03-29T02:32:15Z","published":"2024-03-29T02:32:15Z","title":"DiJiang: Efficient Large Language Models through Compact Kernelization","summary":"  In an effort to reduce the computational load of Transformers, research on\nlinear attention has gained significant momentum. However, the improvement\nstrategies for attention mechanisms typically necessitate extensive retraining,\nwhich is impractical for large language models with a vast array of parameters.\nIn this paper, we present DiJiang, a novel Frequency Domain Kernelization\napproach that enables the transformation of a pre-trained vanilla Transformer\ninto a linear complexity model with little training costs. By employing a\nweighted Quasi-Monte Carlo method for sampling, the proposed approach\ntheoretically offers superior approximation efficiency. To further reduce the\ntraining computational complexity, our kernelization is based on Discrete\nCosine Transform (DCT) operations. Extensive experiments demonstrate that the\nproposed method achieves comparable performance to the original Transformer,\nbut with significantly reduced training costs and much faster inference speeds.\nOur DiJiang-7B achieves comparable performance with LLaMA2-7B on various\nbenchmark while requires only about 1/50 training cost. Code is available at\nhttps://github.com/YuchuanTian/DiJiang.\n","authors":["Hanting Chen","Zhicheng Liu","Xutao Wang","Yuchuan Tian","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2403.19928v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19925v1","updated":"2024-03-29T02:25:55Z","published":"2024-03-29T02:25:55Z","title":"Decision Mamba: Reinforcement Learning via Sequence Modeling with\n  Selective State Spaces","summary":"  Decision Transformer, a promising approach that applies Transformer\narchitectures to reinforcement learning, relies on causal self-attention to\nmodel sequences of states, actions, and rewards. While this method has shown\ncompetitive results, this paper investigates the integration of the Mamba\nframework, known for its advanced capabilities in efficient and effective\nsequence modeling, into the Decision Transformer architecture, focusing on the\npotential performance enhancements in sequential decision-making tasks. Our\nstudy systematically evaluates this integration by conducting a series of\nexperiments across various decision-making environments, comparing the modified\nDecision Transformer, Decision Mamba, with its traditional counterpart. This\nwork contributes to the advancement of sequential decision-making models,\nsuggesting that the architecture and training methodology of neural networks\ncan significantly impact their performance in complex tasks, and highlighting\nthe potential of Mamba as a valuable tool for improving the efficacy of\nTransformer-based models in reinforcement learning scenarios.\n","authors":["Toshihiro Ota"],"pdf_url":"https://arxiv.org/pdf/2403.19925v1.pdf","comment":"8 pages, 1 figure"},{"id":"http://arxiv.org/abs/2403.19918v1","updated":"2024-03-29T02:10:19Z","published":"2024-03-29T02:10:19Z","title":"CtRL-Sim: Reactive and Controllable Driving Agents with Offline\n  Reinforcement Learning","summary":"  Evaluating autonomous vehicle stacks (AVs) in simulation typically involves\nreplaying driving logs from real-world recorded traffic. However, agents\nreplayed from offline data do not react to the actions of the AV, and their\nbehaviour cannot be easily controlled to simulate counterfactual scenarios.\nExisting approaches have attempted to address these shortcomings by proposing\nmethods that rely on heuristics or learned generative models of real-world data\nbut these approaches either lack realism or necessitate costly iterative\nsampling procedures to control the generated behaviours. In this work, we take\nan alternative approach and propose CtRL-Sim, a method that leverages\nreturn-conditioned offline reinforcement learning within a physics-enhanced\nNocturne simulator to efficiently generate reactive and controllable traffic\nagents. Specifically, we process real-world driving data through the Nocturne\nsimulator to generate a diverse offline reinforcement learning dataset,\nannotated with various reward terms. With this dataset, we train a\nreturn-conditioned multi-agent behaviour model that allows for fine-grained\nmanipulation of agent behaviours by modifying the desired returns for the\nvarious reward components. This capability enables the generation of a wide\nrange of driving behaviours beyond the scope of the initial dataset, including\nthose representing adversarial behaviours. We demonstrate that CtRL-Sim can\nefficiently generate diverse and realistic safety-critical scenarios while\nproviding fine-grained control over agent behaviours. Further, we show that\nfine-tuning our model on simulated safety-critical scenarios generated by our\nmodel enhances this controllability.\n","authors":["Luke Rowe","Roger Girgis","Anthony Gosselin","Bruno Carrez","Florian Golemo","Felix Heide","Liam Paull","Christopher Pal"],"pdf_url":"https://arxiv.org/pdf/2403.19918v1.pdf","comment":"20 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2402.03646v3","updated":"2024-03-29T02:01:11Z","published":"2024-02-06T02:45:13Z","title":"Lens: A Foundation Model for Network Traffic in Cybersecurity","summary":"  Network traffic refers to the amount of data being sent and received over the\ninternet or any system that connects computers. Analyzing and understanding\nnetwork traffic is vital for improving network security and management.\nHowever, the analysis of network traffic is challenging due to the diverse\nnature of data packets, which often feature heterogeneous headers and encrypted\npayloads lacking semantics. To capture the latent semantics of traffic, a few\nstudies have adopted pre-training techniques based on the Transformer encoder\nor decoder to learn the representations from massive traffic data. However,\nthese methods typically excel in traffic understanding (classification) or\ntraffic generation tasks. To address this issue, we develop Lens, a foundation\nmodel for network traffic that leverages the T5 architecture to learn the\npre-trained representations from large-scale unlabeled data. Harnessing the\nstrength of the encoder-decoder framework, which captures the global\ninformation while preserving the generative ability, our model can better learn\nthe representations from raw data. To further enhance pre-training\neffectiveness, we design a novel loss that combines three distinct tasks:\nMasked Span Prediction (MSP), Packet Order Prediction (POP), and Homologous\nTraffic Prediction (HTP). Evaluation results across various benchmark datasets\ndemonstrate that the proposed Lens outperforms the baselines in most downstream\ntasks related to both traffic understanding and generation. Notably, it also\nrequires much less labeled data for fine-tuning compared to current methods.\n","authors":["Qineng Wang","Chen Qian","Xiaochang Li","Ziyu Yao","Huajie Shao"],"pdf_url":"https://arxiv.org/pdf/2402.03646v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19913v1","updated":"2024-03-29T01:53:24Z","published":"2024-03-29T01:53:24Z","title":"MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of\n  Large Language Models","summary":"  Large language models such as ChatGPT and GPT-4 have recently achieved\nastonishing performance on a variety of natural language processing tasks. In\nthis paper, we propose MANGO, a benchmark to evaluate their capabilities to\nperform text-based mapping and navigation. Our benchmark includes 53 mazes\ntaken from a suite of textgames: each maze is paired with a walkthrough that\nvisits every location but does not cover all possible paths. The task is\nquestion-answering: for each maze, a large language model reads the walkthrough\nand answers hundreds of mapping and navigation questions such as \"How should\nyou go to Attic from West of House?\" and \"Where are we if we go north and east\nfrom Cellar?\". Although these questions are easy to humans, it turns out that\neven GPT-4, the best-to-date language model, performs poorly at answering them.\nFurther, our experiments suggest that a strong mapping and navigation ability\nwould benefit large language models in performing relevant downstream tasks,\nsuch as playing textgames. Our MANGO benchmark will facilitate future research\non methods that improve the mapping and navigation capabilities of language\nmodels. We host our leaderboard, data, code, and evaluation program at\nhttps://mango.ttic.edu and https://github.com/oaklight/mango/.\n","authors":["Peng Ding","Jiading Fang","Peng Li","Kangrui Wang","Xiaochen Zhou","Mo Yu","Jing Li","Matthew R. Walter","Hongyuan Mei"],"pdf_url":"https://arxiv.org/pdf/2403.19913v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19907v1","updated":"2024-03-29T01:25:05Z","published":"2024-03-29T01:25:05Z","title":"Beyond the Known: Novel Class Discovery for Open-world Graph Learning","summary":"  Node classification on graphs is of great importance in many applications.\nDue to the limited labeling capability and evolution in real-world open\nscenarios, novel classes can emerge on unlabeled testing nodes. However, little\nattention has been paid to novel class discovery on graphs. Discovering novel\nclasses is challenging as novel and known class nodes are correlated by edges,\nwhich makes their representations indistinguishable when applying message\npassing GNNs. Furthermore, the novel classes lack labeling information to guide\nthe learning process. In this paper, we propose a novel method Open-world gRAph\nneuraL network (ORAL) to tackle these challenges. ORAL first detects\ncorrelations between classes through semi-supervised prototypical learning.\nInter-class correlations are subsequently eliminated by the prototypical\nattention network, leading to distinctive representations for different\nclasses. Furthermore, to fully explore multi-scale graph features for\nalleviating label deficiencies, ORAL generates pseudo-labels by aligning and\nensembling label estimations from multiple stacked prototypical attention\nnetworks. Extensive experiments on several benchmark datasets show the\neffectiveness of our proposed method.\n","authors":["Yucheng Jin","Yun Xiong","Juncheng Fang","Xixi Wu","Dongxiao He","Xing Jia","Bingchen Zhao","Philip Yu"],"pdf_url":"https://arxiv.org/pdf/2403.19907v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10375v2","updated":"2024-03-29T01:08:12Z","published":"2023-10-16T13:16:09Z","title":"GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers","summary":"  As transformers are equivariant to the permutation of input tokens, encoding\nthe positional information of tokens is necessary for many tasks. However,\nsince existing positional encoding schemes have been initially designed for NLP\ntasks, their suitability for vision tasks, which typically exhibit different\nstructural properties in their data, is questionable. We argue that existing\npositional encoding schemes are suboptimal for 3D vision tasks, as they do not\nrespect their underlying 3D geometric structure. Based on this hypothesis, we\npropose a geometry-aware attention mechanism that encodes the geometric\nstructure of tokens as relative transformation determined by the geometric\nrelationship between queries and key-value pairs. By evaluating on multiple\nnovel view synthesis (NVS) datasets in the sparse wide-baseline multi-view\nsetting, we show that our attention, called Geometric Transform Attention\n(GTA), improves learning efficiency and performance of state-of-the-art\ntransformer-based NVS models without any additional learned parameters and only\nminor computational overhead.\n","authors":["Takeru Miyato","Bernhard Jaeger","Max Welling","Andreas Geiger"],"pdf_url":"https://arxiv.org/pdf/2310.10375v2.pdf","comment":"Published as a conference paper at ICLR 2024"},{"id":"http://arxiv.org/abs/2403.19897v1","updated":"2024-03-29T00:36:38Z","published":"2024-03-29T00:36:38Z","title":"Disentangling Racial Phenotypes: Fine-Grained Control of Race-related\n  Facial Phenotype Characteristics","summary":"  Achieving an effective fine-grained appearance variation over 2D facial\nimages, whilst preserving facial identity, is a challenging task due to the\nhigh complexity and entanglement of common 2D facial feature encoding spaces.\nDespite these challenges, such fine-grained control, by way of disentanglement\nis a crucial enabler for data-driven racial bias mitigation strategies across\nmultiple automated facial analysis tasks, as it allows to analyse, characterise\nand synthesise human facial diversity. In this paper, we propose a novel GAN\nframework to enable fine-grained control over individual race-related phenotype\nattributes of the facial images. Our framework factors the latent (feature)\nspace into elements that correspond to race-related facial phenotype\nrepresentations, thereby separating phenotype aspects (e.g. skin, hair colour,\nnose, eye, mouth shapes), which are notoriously difficult to annotate robustly\nin real-world facial data. Concurrently, we also introduce a high quality\naugmented, diverse 2D face image dataset drawn from CelebA-HQ for GAN training.\nUnlike prior work, our framework only relies upon 2D imagery and related\nparameters to achieve state-of-the-art individual control over race-related\nphenotype attributes with improved photo-realistic output.\n","authors":["Seyma Yucer","Amir Atapour Abarghouei","Noura Al Moubayed","Toby P. Breckon"],"pdf_url":"https://arxiv.org/pdf/2403.19897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19896v1","updated":"2024-03-29T00:33:37Z","published":"2024-03-29T00:33:37Z","title":"Nonlinearity Enhanced Adaptive Activation Function","summary":"  A simply implemented activation function with even cubic nonlinearity is\nintroduced that increases the accuracy of neural networks without substantial\nadditional computational resources. This is partially enabled through an\napparent tradeoff between convergence and accuracy. The activation function\ngeneralizes the standard RELU function by introducing additional degrees of\nfreedom through optimizable parameters that enable the degree of nonlinearity\nto be adjusted. The associated accuracy enhancement is quantified in the\ncontext of the MNIST digit data set through a comparison with standard\ntechniques.\n","authors":["David Yevick"],"pdf_url":"https://arxiv.org/pdf/2403.19896v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19895v1","updated":"2024-03-29T00:29:57Z","published":"2024-03-29T00:29:57Z","title":"An Information-Theoretic Framework for Out-of-Distribution\n  Generalization","summary":"  We study the Out-of-Distribution (OOD) generalization in machine learning and\npropose a general framework that provides information-theoretic generalization\nbounds. Our framework interpolates freely between Integral Probability Metric\n(IPM) and $f$-divergence, which naturally recovers some known results\n(including Wasserstein- and KL-bounds), as well as yields new generalization\nbounds. Moreover, we show that our framework admits an optimal transport\ninterpretation. When evaluated in two concrete examples, the proposed bounds\neither strictly improve upon existing bounds in some cases or recover the best\namong existing OOD generalization bounds.\n","authors":["Wenliang Liu","Guanding Yu","Lele Wang","Renjie Liao"],"pdf_url":"https://arxiv.org/pdf/2403.19895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19060v2","updated":"2024-03-29T00:25:03Z","published":"2024-03-27T23:55:02Z","title":"Towards Human-Centered Construction Robotics: An RL-Driven Companion\n  Robot For Contextually Assisting Carpentry Workers","summary":"  In the dynamic construction industry, traditional robotic integration has\nprimarily focused on automating specific tasks, often overlooking the\ncomplexity and variability of human aspects in construction workflows. This\npaper introduces a human-centered approach with a \"work companion rover\"\ndesigned to assist construction workers within their existing practices, aiming\nto enhance safety and workflow fluency while respecting construction labor's\nskilled nature. We conduct an in-depth study on deploying a robotic system in\ncarpentry formwork, showcasing a prototype that emphasizes mobility, safety,\nand comfortable worker-robot collaboration in dynamic environments through a\ncontextual Reinforcement Learning (RL)-driven modular framework. Our research\nadvances robotic applications in construction, advocating for collaborative\nmodels where adaptive robots support rather than replace humans, underscoring\nthe potential for an interactive and collaborative human-robot workforce.\n","authors":["Yuning Wu","Jiaying Wei","Jean Oh","Daniel Cardoso Llach"],"pdf_url":"https://arxiv.org/pdf/2403.19060v2.pdf","comment":"8 pages, 9 figures. This work has been submitted to the IEEE for\n  possible publication. Copyright may be transferred without notice, after\n  which this version may no longer be accessible"},{"id":"http://arxiv.org/abs/2403.19889v1","updated":"2024-03-29T00:14:46Z","published":"2024-03-29T00:14:46Z","title":"Towards a Robust Retrieval-Based Summarization System","summary":"  This paper describes an investigation of the robustness of large language\nmodels (LLMs) for retrieval augmented generation (RAG)-based summarization\ntasks. While LLMs provide summarization capabilities, their performance in\ncomplex, real-world scenarios remains under-explored. Our first contribution is\nLogicSumm, an innovative evaluation framework incorporating realistic scenarios\nto assess LLM robustness during RAG-based summarization. Based on limitations\nidentified by LogiSumm, we then developed SummRAG, a comprehensive system to\ncreate training dialogues and fine-tune a model to enhance robustness within\nLogicSumm's scenarios. SummRAG is an example of our goal of defining structured\nmethods to test the capabilities of an LLM, rather than addressing issues in a\none-off fashion. Experimental results confirm the power of SummRAG, showcasing\nimproved logical coherence and summarization quality. Data, corresponding model\nweights, and Python code are available online.\n","authors":["Shengjie Liu","Jing Wu","Jingyuan Bao","Wenyi Wang","Naira Hovakimyan","Christopher G Healey"],"pdf_url":"https://arxiv.org/pdf/2403.19889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19888v1","updated":"2024-03-29T00:05:13Z","published":"2024-03-29T00:05:13Z","title":"MambaMixer: Efficient Selective State Space Models with Dual Token and\n  Channel Selection","summary":"  Recent advances in deep learning have mainly relied on Transformers due to\ntheir data dependency and ability to learn at scale. The attention module in\nthese architectures, however, exhibits quadratic time and space in input size,\nlimiting their scalability for long-sequence modeling. Despite recent attempts\nto design efficient and effective architecture backbone for multi-dimensional\ndata, such as images and multivariate time series, existing models are either\ndata independent, or fail to allow inter- and intra-dimension communication.\nRecently, State Space Models (SSMs), and more specifically Selective State\nSpace Models, with efficient hardware-aware implementation, have shown\npromising potential for long sequence modeling. Motivated by the success of\nSSMs, we present MambaMixer, a new architecture with data-dependent weights\nthat uses a dual selection mechanism across tokens and channels, called\nSelective Token and Channel Mixer. MambaMixer connects selective mixers using a\nweighted averaging mechanism, allowing layers to have direct access to early\nfeatures. As a proof of concept, we design Vision MambaMixer (ViM2) and Time\nSeries MambaMixer (TSM2) architectures based on the MambaMixer block and\nexplore their performance in various vision and time series forecasting tasks.\nOur results underline the importance of selective mixing across both tokens and\nchannels. In ImageNet classification, object detection, and semantic\nsegmentation tasks, ViM2 achieves competitive performance with well-established\nvision models and outperforms SSM-based vision models. In time series\nforecasting, TSM2 achieves outstanding performance compared to state-of-the-art\nmethods while demonstrating significantly improved computational cost. These\nresults show that while Transformers, cross-channel attention, and MLPs are\nsufficient for good performance in time series forecasting, neither is\nnecessary.\n","authors":["Ali Behrouz","Michele Santacatterina","Ramin Zabih"],"pdf_url":"https://arxiv.org/pdf/2403.19888v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2310.06588v2","updated":"2024-03-29T23:53:28Z","published":"2023-10-10T12:53:48Z","title":"FTFT: Efficient and Robust Fine-Tuning by Transferring Training Dynamics","summary":"  Despite the massive success of fine-tuning Pre-trained Language Models\n(PLMs), they remain susceptible to out-of-distribution input. Dataset\ncartography is a simple yet effective dual-model approach that improves the\nrobustness of fine-tuned PLMs. It involves fine-tuning a model on the original\ntraining set (i.e. reference model), selecting a subset of important training\ninstances based on the training dynamics, and fine-tuning again only on these\nselected examples (i.e. main model). However, this approach requires\nfine-tuning the same model twice, which is computationally expensive for large\nPLMs. In this paper, we show that (1) training dynamics are highly transferable\nacross model sizes and pre-training methods, and that (2) fine-tuning main\nmodels using these selected training instances achieves higher training\nefficiency than empirical risk minimization (ERM). Building on these\nobservations, we propose a novel fine-tuning approach: Fine-Tuning by\ntransFerring Training dynamics (FTFT). Compared with dataset cartography, FTFT\nuses more efficient reference models and aggressive early stopping. FTFT\nachieves robustness improvements over ERM while lowering the training cost by\nup to $\\sim 50\\%$.\n","authors":["Yupei Du","Albert Gatt","Dong Nguyen"],"pdf_url":"https://arxiv.org/pdf/2310.06588v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19076v2","updated":"2024-03-29T21:33:39Z","published":"2024-03-28T00:34:56Z","title":"Tiny Machine Learning: Progress and Futures","summary":"  Tiny Machine Learning (TinyML) is a new frontier of machine learning. By\nsqueezing deep learning models into billions of IoT devices and\nmicrocontrollers (MCUs), we expand the scope of AI applications and enable\nubiquitous intelligence. However, TinyML is challenging due to hardware\nconstraints: the tiny memory resource makes it difficult to hold deep learning\nmodels designed for cloud and mobile platforms. There is also limited compiler\nand inference engine support for bare-metal devices. Therefore, we need to\nco-design the algorithm and system stack to enable TinyML. In this review, we\nwill first discuss the definition, challenges, and applications of TinyML. We\nthen survey the recent progress in TinyML and deep learning on MCUs. Next, we\nwill introduce MCUNet, showing how we can achieve ImageNet-scale AI\napplications on IoT devices with system-algorithm co-design. We will further\nextend the solution from inference to training and introduce tiny on-device\ntraining techniques. Finally, we present future directions in this area.\nToday's large model might be tomorrow's tiny model. The scope of TinyML should\nevolve and adapt over time.\n","authors":["Ji Lin","Ligeng Zhu","Wei-Ming Chen","Wei-Chen Wang","Song Han"],"pdf_url":"https://arxiv.org/pdf/2403.19076v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2206.15472"},{"id":"http://arxiv.org/abs/2212.04612v3","updated":"2024-03-29T21:33:29Z","published":"2022-12-09T00:32:46Z","title":"Training Data Influence Analysis and Estimation: A Survey","summary":"  Good models require good training data. For overparameterized deep models,\nthe causal relationship between training data and model predictions is\nincreasingly opaque and poorly understood. Influence analysis partially\ndemystifies training's underlying interactions by quantifying the amount each\ntraining instance alters the final model. Measuring the training data's\ninfluence exactly can be provably hard in the worst case; this has led to the\ndevelopment and use of influence estimators, which only approximate the true\ninfluence. This paper provides the first comprehensive survey of training data\ninfluence analysis and estimation. We begin by formalizing the various, and in\nplaces orthogonal, definitions of training data influence. We then organize\nstate-of-the-art influence analysis methods into a taxonomy; we describe each\nof these methods in detail and compare their underlying assumptions, asymptotic\ncomplexities, and overall strengths and weaknesses. Finally, we propose future\nresearch directions to make influence analysis more useful in practice as well\nas more theoretically and empirically sound. A curated, up-to-date list of\nresources related to influence analysis is available at\nhttps://github.com/ZaydH/influence_analysis_papers.\n","authors":["Zayd Hammoudeh","Daniel Lowd"],"pdf_url":"https://arxiv.org/pdf/2212.04612v3.pdf","comment":"Published in Springer journal \"Machine Learning\""},{"id":"http://arxiv.org/abs/2402.01858v2","updated":"2024-03-29T21:18:37Z","published":"2024-02-02T19:28:33Z","title":"Explaining latent representations of generative models with large\n  multimodal models","summary":"  Learning interpretable representations of data generative latent factors is\nan important topic for the development of artificial intelligence. With the\nrise of the large multimodal model, it can align images with text to generate\nanswers. In this work, we propose a framework to comprehensively explain each\nlatent variable in the generative models using a large multimodal model. We\nfurther measure the uncertainty of our generated explanations, quantitatively\nevaluate the performance of explanation generation among multiple large\nmultimodal models, and qualitatively visualize the variations of each latent\nvariable to learn the disentanglement effects of different generative models on\nexplanations. Finally, we discuss the explanatory capabilities and limitations\nof state-of-the-art large multimodal models.\n","authors":["Mengdan Zhu","Zhenke Liu","Bo Pan","Abhinav Angirekula","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.01858v2.pdf","comment":"ICLR 2024 Workshop Paper on Reliable and Responsible Foundation\n  Models"},{"id":"http://arxiv.org/abs/2309.12701v3","updated":"2024-03-29T21:06:26Z","published":"2023-09-22T08:18:08Z","title":"Interpretable Decision Tree Search as a Markov Decision Process","summary":"  Finding an optimal decision tree for a supervised learning task is a\nchallenging combinatorial problem to solve at scale. It was recently proposed\nto frame the problem as a Markov Decision Problem (MDP) and use deep\nreinforcement learning to tackle scaling. Unfortunately, these methods are not\ncompetitive with the current branch-and-bound state-of-the-art. We propose\ninstead to scale the resolution of such MDPs using an information-theoretic\ntests generating function that heuristically, and dynamically for every state,\nlimits the set of admissible test actions to a few good candidates. As a\nsolver, we show empirically that our algorithm is at the very least competitive\nwith branch-and-bound alternatives. As a machine learning tool, a key advantage\nof our approach is to solve for multiple complexity-performance trade-offs at\nvirtually no additional cost. With such a set of solutions, a user can then\nselect the tree that generalizes best and which has the interpretability level\nthat best suits their needs, which no current branch-and-bound method allows.\n","authors":["Hector Kohler","Riad Akrour","Philippe Preux"],"pdf_url":"https://arxiv.org/pdf/2309.12701v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10800v2","updated":"2024-03-29T20:34:26Z","published":"2024-03-16T04:19:48Z","title":"Model Reprogramming Outperforms Fine-tuning on Out-of-distribution Data\n  in Text-Image Encoders","summary":"  When evaluating the performance of a pre-trained model transferred to a\ndownstream task, it is imperative to assess not only the in-distribution (ID)\naccuracy of the downstream model but also its capacity to generalize and\nidentify out-of-distribution (OOD) samples. In this paper, we unveil the hidden\ncosts associated with intrusive fine-tuning techniques. Specifically, we\ndemonstrate that commonly used fine-tuning methods not only distort the\nrepresentations necessary for generalizing to covariate-shifted OOD samples\n(OOD generalization) but also distort the representations necessary for\ndetecting semantically-shifted OOD samples (OOD detection). To address these\nchallenges, we introduce a new model reprogramming approach for fine-tuning,\nwhich we name Reprogrammer. Reprogrammer aims to improve the holistic\nperformance of the downstream model across ID, OOD generalization, and OOD\ndetection tasks. Our empirical evidence reveals that Reprogrammer is less\nintrusive and yields superior downstream models. Furthermore, we demonstrate\nthat by appending an additional representation residual connection to\nReprogrammer, we can further preserve pre-training representations, resulting\nin an even more safe and robust downstream model capable of excelling in many\nID classification, OOD generalization, and OOD detection settings.\n","authors":["Andrew Geng","Pin-Yu Chen"],"pdf_url":"https://arxiv.org/pdf/2403.10800v2.pdf","comment":"Accepted in SatML 2024"},{"id":"http://arxiv.org/abs/2402.11686v2","updated":"2024-03-29T19:16:16Z","published":"2024-02-18T19:31:26Z","title":"Learning the Topology and Behavior of Discrete Dynamical Systems","summary":"  Discrete dynamical systems are commonly used to model the spread of\ncontagions on real-world networks. Under the PAC framework, existing research\nhas studied the problem of learning the behavior of a system, assuming that the\nunderlying network is known. In this work, we focus on a more challenging\nsetting: to learn both the behavior and the underlying topology of a black-box\nsystem. We show that, in general, this learning problem is computationally\nintractable. On the positive side, we present efficient learning methods under\nthe PAC model when the underlying graph of the dynamical system belongs to some\nclasses. Further, we examine a relaxed setting where the topology of an unknown\nsystem is partially observed. For this case, we develop an efficient PAC\nlearner to infer the system and establish the sample complexity. Lastly, we\npresent a formal analysis of the expressive power of the hypothesis class of\ndynamical systems where both the topology and behavior are unknown, using the\nwell-known formalism of the Natarajan dimension. Our results provide a\ntheoretical foundation for learning both the behavior and topology of discrete\ndynamical systems.\n","authors":["Zirou Qiu","Abhijin Adiga","Madhav V. Marathe","S. S. Ravi","Daniel J. Rosenkrantz","Richard E. Stearns","Anil Vullikanti"],"pdf_url":"https://arxiv.org/pdf/2402.11686v2.pdf","comment":"Accepted at AAAI-24"},{"id":"http://arxiv.org/abs/2311.17929v6","updated":"2024-03-29T19:14:18Z","published":"2023-11-25T22:26:58Z","title":"New Online Communities: Graph Deep Learning on Anonymous Voting Networks\n  to Identify Sybils in Polycentric Governance","summary":"  This research examines the polycentric governance of digital assets in\nblockchain-based Decentralized Autonomous Organizations (DAOs). It offers a\ntheoretical framework and addresses a critical challenge facing decentralized\ngovernance by developing a method to identify Sybils, or spurious identities.\nSybils pose significant organizational sustainability threats to DAOs and\nother, commons-based online communities, and threat models are identified. The\nexperimental method uses an autoencoder architecture and graph deep learning\ntechniques to identify Sybil activity in a DAO governance dataset\n(snapshot.org). Specifically, a Graph Convolutional Neural Network (GCNN)\nlearned voting behaviours and a fast vector clustering algorithm used\nhigh-dimensional embeddings to identify similar nodes in a graph. The results\nreveal that deep learning can effectively identify Sybils, reducing the voting\ngraph by 2-5%. This research underscores the importance of Sybil resistance in\nDAOs, identifies challenges and opportunities for forensics and analysis of\nanonymous networks, and offers a novel perspective on decentralized governance,\ninforming future policy, regulation, and governance practices.\n","authors":["Quinn DuPont"],"pdf_url":"https://arxiv.org/pdf/2311.17929v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.00185v2","updated":"2024-03-29T19:10:30Z","published":"2023-07-01T01:07:20Z","title":"Interpretable Constructive Algorithm for Random Weight Neural Networks","summary":"  In this paper, an interpretable construction method (IC) with geometric\ninformation is proposed to address a significant drawback of incremental random\nweight neural networks (IRWNNs), which is the difficulty in interpreting the\nblack-box process of hidden parameter selection.The IC utilises geometric\nrelationships to randomly assign hidden parameters, which improves\ninterpretability. In addition, IC employs a node pooling strategy to select the\nnodes that will both facilitate network convergence. The article also\ndemonstrates the general approximation properties of IC and presents a\nlightweight version tailored for large-scale data modelling tasks. Experimental\nresults on six benchmark datasets and one numerical simulation dataset\ndemonstrate the superior performance of IC compared to other constructive\nalgorithms in terms of modelling speed, accuracy and network structure. In\naddition, the effectiveness of IC is validated by two real-world industrial\napplications.\n","authors":["Jing Nan","Wei Dai","Guan Yuan","Ping Zhou"],"pdf_url":"https://arxiv.org/pdf/2307.00185v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.15043v2","updated":"2024-03-29T19:01:27Z","published":"2024-01-26T18:13:57Z","title":"Health Text Simplification: An Annotated Corpus for Digestive Cancer\n  Education and Novel Strategies for Reinforcement Learning","summary":"  Objective: The reading level of health educational materials significantly\ninfluences the understandability and accessibility of the information,\nparticularly for minoritized populations. Many patient educational resources\nsurpass the reading level and complexity of widely accepted standards. There is\na critical need for high-performing text simplification models in health\ninformation to enhance dissemination and literacy. This need is particularly\nacute in cancer education, where effective prevention and screening education\ncan substantially reduce morbidity and mortality.\n  Methods: We introduce Simplified Digestive Cancer (SimpleDC), a parallel\ncorpus of cancer education materials tailored for health text simplification\nresearch, comprising educational content from the American Cancer Society,\nCenters for Disease Control and Prevention, and National Cancer Institute.\nUtilizing SimpleDC alongside the existing Med-EASi corpus, we explore Large\nLanguage Model (LLM)-based simplification methods, including fine-tuning,\nreinforcement learning (RL), reinforcement learning with human feedback (RLHF),\ndomain adaptation, and prompt-based approaches. Our experimentation encompasses\nLlama 2 and GPT-4. A novel RLHF reward function is introduced, featuring a\nlightweight model adept at distinguishing between original and simplified\ntexts, thereby enhancing the model's effectiveness with unlabeled data.\n  Results: Fine-tuned Llama 2 models demonstrated high performance across\nvarious metrics. Our innovative RLHF reward function surpassed existing RL text\nsimplification reward functions in effectiveness. The results underscore that\nRL/RLHF can augment fine-tuning, facilitating model training on unlabeled text\nand improving performance.\n","authors":["Md Mushfiqur Rahman","Mohammad Sabik Irbaz","Kai North","Michelle S. Williams","Marcos Zampieri","Kevin Lybarger"],"pdf_url":"https://arxiv.org/pdf/2401.15043v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04808v2","updated":"2024-03-29T18:56:23Z","published":"2023-11-08T16:30:52Z","title":"A Lightweight Architecture for Real-Time Neuronal-Spike Classification","summary":"  Electrophysiological recordings of neural activity in a mouse's brain are\nvery popular among neuroscientists for understanding brain function. One\nparticular area of interest is acquiring recordings from the Purkinje cells in\nthe cerebellum in order to understand brain injuries and the loss of motor\nfunctions. However, current setups for such experiments do not allow the mouse\nto move freely and, thus, do not capture its natural behaviour since they have\na wired connection between the animal's head stage and an acquisition device.\nIn this work, we propose a lightweight neuronal-spike detection and\nclassification architecture that leverages on the unique characteristics of the\nPurkinje cells to discard unneeded information from the sparse neural data in\nreal time. This allows the (condensed) data to be easily stored on a removable\nstorage device on the head stage, alleviating the need for wires. Synthesis\nresults reveal a >95% overall classification accuracy while still resulting in\na small-form-factor design, which allows for the free movement of mice during\nexperiments. Moreover, the power-efficient nature of the design and the usage\nof STT-RAM (Spin Transfer Torque Magnetic Random Access Memory) as the\nremovable storage allows the head stage to easily operate on a tiny battery for\nup to approximately 4 days.\n","authors":["Muhammad Ali Siddiqi","David Vrijenhoek","Lennart P. L. Landsmeer","Job van der Kleij","Anteneh Gebregiorgis","Vincenzo Romano","Rajendra Bishnoi","Said Hamdioui","Christos Strydis"],"pdf_url":"https://arxiv.org/pdf/2311.04808v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01482v2","updated":"2024-03-29T18:52:59Z","published":"2024-01-03T01:11:16Z","title":"Incorporating Geo-Diverse Knowledge into Prompting for Increased\n  Geographical Robustness in Object Recognition","summary":"  Existing object recognition models have been shown to lack robustness in\ndiverse geographical scenarios due to domain shifts in design and context.\nClass representations need to be adapted to more accurately reflect an object\nconcept under these shifts. In the absence of training data from target\ngeographies, we hypothesize that geographically diverse descriptive knowledge\nof categories can enhance robustness. For this purpose, we explore the\nfeasibility of probing a large language model for geography-based object\nknowledge, and we examine the effects of integrating knowledge into zero-shot\nand learnable soft prompting with CLIP. Within this exploration, we propose\ngeography knowledge regularization to ensure that soft prompts trained on a\nsource set of geographies generalize to an unseen target set. Accuracy gains\nover prompting baselines on DollarStreet while training only on Europe data are\nup to +2.8/1.2/1.6 on target data from Africa/Asia/Americas, and +4.6 overall\non the hardest classes. Competitive performance is shown vs. few-shot target\ntraining, and analysis is provided to direct future study of geographical\nrobustness.\n","authors":["Kyle Buettner","Sina Malakouti","Xiang Lorraine Li","Adriana Kovashka"],"pdf_url":"https://arxiv.org/pdf/2401.01482v2.pdf","comment":"To appear in IEEE/CVF Computer Vision and Pattern Recognition\n  Conference (CVPR), 2024"},{"id":"http://arxiv.org/abs/2209.07028v3","updated":"2024-03-29T18:50:04Z","published":"2022-09-15T03:41:09Z","title":"Estimating large causal polytrees from small samples","summary":"  We consider the problem of estimating a large causal polytree from a\nrelatively small i.i.d. sample. This is motivated by the problem of determining\ncausal structure when the number of variables is very large compared to the\nsample size, such as in gene regulatory networks. We give an algorithm that\nrecovers the tree with high accuracy in such settings. The algorithm works\nunder essentially no distributional or modeling assumptions other than some\nmild non-degeneracy conditions.\n","authors":["Sourav Chatterjee","Mathukumalli Vidyasagar"],"pdf_url":"https://arxiv.org/pdf/2209.07028v3.pdf","comment":"26 pages. An R package has been developed (see link in the article),\n  and a real data example has been added"},{"id":"http://arxiv.org/abs/2210.09903v5","updated":"2024-03-29T18:25:28Z","published":"2022-10-18T14:43:44Z","title":"Online Convex Optimization with Unbounded Memory","summary":"  Online convex optimization (OCO) is a widely used framework in online\nlearning. In each round, the learner chooses a decision in a convex set and an\nadversary chooses a convex loss function, and then the learner suffers the loss\nassociated with their current decision. However, in many applications the\nlearner's loss depends not only on the current decision but on the entire\nhistory of decisions until that point. The OCO framework and its existing\ngeneralizations do not capture this, and they can only be applied to many\nsettings of interest after a long series of approximation arguments. They also\nleave open the question of whether the dependence on memory is tight because\nthere are no non-trivial lower bounds. In this work we introduce a\ngeneralization of the OCO framework, \"Online Convex Optimization with Unbounded\nMemory\", that captures long-term dependence on past decisions. We introduce the\nnotion of $p$-effective memory capacity, $H_p$, that quantifies the maximum\ninfluence of past decisions on present losses. We prove an $O(\\sqrt{H_p T})$\nupper bound on the policy regret and a matching (worst-case) lower bound. As a\nspecial case, we prove the first non-trivial lower bound for OCO with finite\nmemory \\citep{anavaHM2015online}, which could be of independent interest, and\nalso improve existing upper bounds. We demonstrate the broad applicability of\nour framework by using it to derive regret bounds, and to improve and simplify\nexisting regret bound derivations, for a variety of online learning problems\nincluding online linear control and an online variant of performative\nprediction.\n","authors":["Raunak Kumar","Sarah Dean","Robert Kleinberg"],"pdf_url":"https://arxiv.org/pdf/2210.09903v5.pdf","comment":"Proceedings of the 37th Conference on Neural Information Processing\n  Systems (NeurIPS 2023)"},{"id":"http://arxiv.org/abs/2307.08919v3","updated":"2024-03-29T18:19:36Z","published":"2023-07-18T01:31:47Z","title":"Systematic comparison of semi-supervised and self-supervised learning\n  for medical image classification","summary":"  In typical medical image classification problems, labeled data is scarce\nwhile unlabeled data is more available. Semi-supervised learning and\nself-supervised learning are two different research directions that can improve\naccuracy by learning from extra unlabeled data. Recent methods from both\ndirections have reported significant gains on traditional benchmarks. Yet past\nbenchmarks do not focus on medical tasks and rarely compare self- and semi-\nmethods together on an equal footing. Furthermore, past benchmarks often handle\nhyperparameter tuning suboptimally. First, they may not tune hyperparameters at\nall, leading to underfitting. Second, when tuning does occur, it often\nunrealistically uses a labeled validation set that is much larger than the\ntraining set. Therefore currently published rankings might not always\ncorroborate with their practical utility This study contributes a systematic\nevaluation of self- and semi- methods with a unified experimental protocol\nintended to guide a practitioner with scarce overall labeled data and a limited\ncompute budget. We answer two key questions: Can hyperparameter tuning be\neffective with realistic-sized validation sets? If so, when all methods are\ntuned well, which self- or semi-supervised methods achieve the best accuracy?\nOur study compares 13 representative semi- and self-supervised methods to\nstrong labeled-set-only baselines on 4 medical datasets. From 20000+ GPU hours\nof computation, we provide valuable best practices to resource-constrained\npractitioners: hyperparameter tuning is effective, and the semi-supervised\nmethod known as MixMatch delivers the most reliable gains across 4 datasets.\n","authors":["Zhe Huang","Ruijie Jiang","Shuchin Aeron","Michael C. Hughes"],"pdf_url":"https://arxiv.org/pdf/2307.08919v3.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2402.16562v2","updated":"2024-03-29T18:05:17Z","published":"2024-02-26T13:39:04Z","title":"Q-FOX Learning: Breaking Tradition in Reinforcement Learning","summary":"  Reinforcement learning (RL) is a subset of artificial intelligence (AI) where\nagents learn the best action by interacting with the environment, making it\nsuitable for tasks that do not require labeled data or direct supervision.\nHyperparameters (HP) tuning refers to choosing the best parameter that leads to\noptimal solutions in RL algorithms. Manual or random tuning of the HP may be a\ncrucial process because variations in this parameter lead to changes in the\noverall learning aspects and different rewards. In this paper, a novel and\nautomatic HP-tuning method called Q-FOX is proposed. This uses both the FOX\noptimizer, a new optimization method inspired by nature that mimics red foxes'\nhunting behavior, and the commonly used, easy-to-implement RL Q-learning\nalgorithm to solve the problem of HP tuning. Moreover, a new objective function\nis proposed which prioritizes the reward over the mean squared error (MSE) and\nlearning time (steps). Q-FOX has been evaluated on two OpenAI Gym environment\ncontrol tasks: Cart Pole and Frozen Lake. It exposed greater cumulative rewards\nthan HP tuning with other optimizers, such as PSO, GA, Bee, or randomly\nselected HP. The cumulative reward for the Cart Pole task was 32.08, and for\nthe Frozen Lake task was 0.95. Despite the robustness of Q-FOX, it has\nlimitations. It cannot be used directly in real-word problems before choosing\nthe HP in a simulation environment because its processes work iteratively,\nmaking it time-consuming. The results indicate that Q-FOX has played an\nessential role in HP tuning for RL algorithms to effectively solve different\ncontrol tasks.\n","authors":["Mahmood A. Jumaah","Yossra H. Ali","Tarik A. Rashid"],"pdf_url":"https://arxiv.org/pdf/2402.16562v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00909v2","updated":"2024-03-29T18:04:37Z","published":"2023-12-31T22:47:06Z","title":"Taming Mode Collapse in Score Distillation for Text-to-3D Generation","summary":"  Despite the remarkable performance of score distillation in text-to-3D\ngeneration, such techniques notoriously suffer from view inconsistency issues,\nalso known as \"Janus\" artifact, where the generated objects fake each view with\nmultiple front faces. Although empirically effective methods have approached\nthis problem via score debiasing or prompt engineering, a more rigorous\nperspective to explain and tackle this problem remains elusive. In this paper,\nwe reveal that the existing score distillation-based text-to-3D generation\nframeworks degenerate to maximal likelihood seeking on each view independently\nand thus suffer from the mode collapse problem, manifesting as the Janus\nartifact in practice. To tame mode collapse, we improve score distillation by\nre-establishing the entropy term in the corresponding variational objective,\nwhich is applied to the distribution of rendered images. Maximizing the entropy\nencourages diversity among different views in generated 3D assets, thereby\nmitigating the Janus problem. Based on this new objective, we derive a new\nupdate rule for 3D score distillation, dubbed Entropic Score Distillation\n(ESD). We theoretically reveal that ESD can be simplified and implemented by\njust adopting the classifier-free guidance trick upon variational score\ndistillation. Although embarrassingly straightforward, our extensive\nexperiments successfully demonstrate that ESD can be an effective treatment for\nJanus artifacts in score distillation.\n","authors":["Peihao Wang","Dejia Xu","Zhiwen Fan","Dilin Wang","Sreyas Mohan","Forrest Iandola","Rakesh Ranjan","Yilei Li","Qiang Liu","Zhangyang Wang","Vikas Chandra"],"pdf_url":"https://arxiv.org/pdf/2401.00909v2.pdf","comment":"Project page: https://vita-group.github.io/3D-Mode-Collapse/"}],"Multimedia":[{"id":"http://arxiv.org/abs/2310.05737v3","updated":"2024-03-29T17:44:41Z","published":"2023-10-09T14:10:29Z","title":"Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation","summary":"  While Large Language Models (LLMs) are the dominant models for generative\ntasks in language, they do not perform as well as diffusion models on image and\nvideo generation. To effectively use LLMs for visual generation, one crucial\ncomponent is the visual tokenizer that maps pixel-space inputs to discrete\ntokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, a\nvideo tokenizer designed to generate concise and expressive tokens for both\nvideos and images using a common token vocabulary. Equipped with this new\ntokenizer, we show that LLMs outperform diffusion models on standard image and\nvideo generation benchmarks including ImageNet and Kinetics. In addition, we\ndemonstrate that our tokenizer surpasses the previously top-performing video\ntokenizer on two more tasks: (1) video compression comparable to the\nnext-generation video codec (VCC) according to human evaluations, and (2)\nlearning effective representations for action recognition tasks.\n","authors":["Lijun Yu","José Lezama","Nitesh B. Gundavarapu","Luca Versari","Kihyuk Sohn","David Minnen","Yong Cheng","Vighnesh Birodkar","Agrim Gupta","Xiuye Gu","Alexander G. Hauptmann","Boqing Gong","Ming-Hsuan Yang","Irfan Essa","David A. Ross","Lu Jiang"],"pdf_url":"https://arxiv.org/pdf/2310.05737v3.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.10897v2","updated":"2024-03-29T14:49:11Z","published":"2024-03-16T11:21:24Z","title":"Rethinking Multi-view Representation Learning via Distilled\n  Disentangling","summary":"  Multi-view representation learning aims to derive robust representations that\nare both view-consistent and view-specific from diverse data sources. This\npaper presents an in-depth analysis of existing approaches in this domain,\nhighlighting a commonly overlooked aspect: the redundancy between\nview-consistent and view-specific representations. To this end, we propose an\ninnovative framework for multi-view representation learning, which incorporates\na technique we term 'distilled disentangling'. Our method introduces the\nconcept of masked cross-view prediction, enabling the extraction of compact,\nhigh-quality view-consistent representations from various sources without\nincurring extra computational overhead. Additionally, we develop a distilled\ndisentangling module that efficiently filters out consistency-related\ninformation from multi-view representations, resulting in purer view-specific\nrepresentations. This approach significantly reduces redundancy between\nview-consistent and view-specific representations, enhancing the overall\nefficiency of the learning process. Our empirical evaluations reveal that\nhigher mask ratios substantially improve the quality of view-consistent\nrepresentations. Moreover, we find that reducing the dimensionality of\nview-consistent representations relative to that of view-specific\nrepresentations further refines the quality of the combined representations.\nOur code is accessible at: https://github.com/Guanzhou-Ke/MRDD.\n","authors":["Guanzhou Ke","Bo Wang","Xiaoli Wang","Shengfeng He"],"pdf_url":"https://arxiv.org/pdf/2403.10897v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.20194v1","updated":"2024-03-29T14:15:12Z","published":"2024-03-29T14:15:12Z","title":"ConvBench: A Multi-Turn Conversation Evaluation Benchmark with\n  Hierarchical Capability for Large Vision-Language Models","summary":"  This paper presents ConvBench, a novel multi-turn conversation evaluation\nbenchmark tailored for Large Vision-Language Models (LVLMs). Unlike existing\nbenchmarks that assess individual capabilities in single-turn dialogues,\nConvBench adopts a three-level multimodal capability hierarchy, mimicking human\ncognitive processes by stacking up perception, reasoning, and creativity. Each\nlevel focuses on a distinct capability, mirroring the cognitive progression\nfrom basic perception to logical reasoning and ultimately to advanced\ncreativity. ConvBench comprises 577 meticulously curated multi-turn\nconversations encompassing 215 tasks reflective of real-world demands.\nAutomatic evaluations quantify response performance at each turn and overall\nconversation level. Leveraging the capability hierarchy, ConvBench enables\nprecise attribution of conversation mistakes to specific levels. Experimental\nresults reveal a performance gap between multi-modal models, including GPT4-V,\nand human performance in multi-turn conversations. Additionally, weak\nfine-grained perception in multi-modal models contributes to reasoning and\ncreation failures. ConvBench serves as a catalyst for further research aimed at\nenhancing visual dialogues.\n","authors":["Shuo Liu","Kaining Ying","Hao Zhang","Yue Yang","Yuqi Lin","Tianle Zhang","Chuanhao Li","Yu Qiao","Ping Luo","Wenqi Shao","Kaipeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.20194v1.pdf","comment":null}]},"2024-04-01T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2402.04616v2","updated":"2024-04-01T01:28:48Z","published":"2024-02-07T06:48:24Z","title":"TinyLLM: Learning a Small Student from Multiple Large Language Models","summary":"  Transferring the reasoning capability from stronger large language models\n(LLMs) to smaller ones has been quite appealing, as smaller LLMs are more\nflexible to deploy with less expense. Among the existing solutions, knowledge\ndistillation stands out due to its outstanding efficiency and generalization.\nHowever, existing methods suffer from several drawbacks, including limited\nknowledge diversity and the lack of rich contextual information. To solve the\nproblems and facilitate the learning of compact language models, we propose\nTinyLLM, a new knowledge distillation paradigm to learn a small student LLM\nfrom multiple large teacher LLMs. In particular, we encourage the student LLM\nto not only generate the correct answers but also understand the rationales\nbehind these answers. Given that different LLMs possess diverse reasoning\nskills, we guide the student model to assimilate knowledge from various teacher\nLLMs. We further introduce an in-context example generator and a\nteacher-forcing Chain-of-Thought strategy to ensure that the rationales are\naccurate and grounded in contextually appropriate scenarios. Extensive\nexperiments on six datasets across two reasoning tasks demonstrate the\nsuperiority of our method. Results show that TinyLLM can outperform large\nteacher LLMs significantly, despite a considerably smaller model size.\n","authors":["Yijun Tian","Yikun Han","Xiusi Chen","Wei Wang","Nitesh V. Chawla"],"pdf_url":"https://arxiv.org/pdf/2402.04616v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19928v2","updated":"2024-04-01T09:17:01Z","published":"2024-03-29T02:32:15Z","title":"DiJiang: Efficient Large Language Models through Compact Kernelization","summary":"  In an effort to reduce the computational load of Transformers, research on\nlinear attention has gained significant momentum. However, the improvement\nstrategies for attention mechanisms typically necessitate extensive retraining,\nwhich is impractical for large language models with a vast array of parameters.\nIn this paper, we present DiJiang, a novel Frequency Domain Kernelization\napproach that enables the transformation of a pre-trained vanilla Transformer\ninto a linear complexity model with little training costs. By employing a\nweighted Quasi-Monte Carlo method for sampling, the proposed approach\ntheoretically offers superior approximation efficiency. To further reduce the\ntraining computational complexity, our kernelization is based on Discrete\nCosine Transform (DCT) operations. Extensive experiments demonstrate that the\nproposed method achieves comparable performance to the original Transformer,\nbut with significantly reduced training costs and much faster inference speeds.\nOur DiJiang-7B achieves comparable performance with LLaMA2-7B on various\nbenchmark while requires only about 1/50 training cost. Code is available at\nhttps://github.com/YuchuanTian/DiJiang.\n","authors":["Hanting Chen","Zhicheng Liu","Xutao Wang","Yuchuan Tian","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2403.19928v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17076v3","updated":"2024-04-01T03:17:09Z","published":"2023-11-27T22:23:27Z","title":"Compositional Chain-of-Thought Prompting for Large Multimodal Models","summary":"  The combination of strong visual backbones and Large Language Model (LLM)\nreasoning has led to Large Multimodal Models (LMMs) becoming the current\nstandard for a wide range of vision and language (VL) tasks. However, recent\nresearch has shown that even the most advanced LMMs still struggle to capture\naspects of compositional visual reasoning, such as attributes and relationships\nbetween objects. One solution is to utilize scene graphs (SGs)--a formalization\nof objects and their relations and attributes that has been extensively used as\na bridge between the visual and textual domains. Yet, scene graph data requires\nscene graph annotations, which are expensive to collect and thus not easily\nscalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic\nforgetting of the pretraining objective. To overcome this, inspired by\nchain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a\nnovel zero-shot Chain-of-Thought prompting method that utilizes SG\nrepresentations in order to extract compositional knowledge from an LMM.\nSpecifically, we first generate an SG using the LMM, and then use that SG in\nthe prompt to produce a response. Through extensive experiments, we find that\nthe proposed CCoT approach not only improves LMM performance on several vision\nand language VL compositional benchmarks but also improves the performance of\nseveral popular LMMs on general multimodal benchmarks, without the need for\nfine-tuning or annotated ground-truth SGs. Code:\nhttps://github.com/chancharikmitra/CCoT\n","authors":["Chancharik Mitra","Brandon Huang","Trevor Darrell","Roei Herzig"],"pdf_url":"https://arxiv.org/pdf/2311.17076v3.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2403.19898v2","updated":"2024-04-01T01:27:14Z","published":"2024-03-29T00:40:12Z","title":"Structure Matters: Tackling the Semantic Discrepancy in Diffusion Models\n  for Image Inpainting","summary":"  Denoising diffusion probabilistic models for image inpainting aim to add the\nnoise to the texture of image during the forward process and recover masked\nregions with unmasked ones of the texture via the reverse denoising\nprocess.Despite the meaningful semantics generation,the existing arts suffer\nfrom the semantic discrepancy between masked and unmasked regions, since the\nsemantically dense unmasked texture fails to be completely degraded while the\nmasked regions turn to the pure noise in diffusion process,leading to the large\ndiscrepancy between them. In this paper,we aim to answer how unmasked semantics\nguide texture denoising process;together with how to tackle the semantic\ndiscrepancy,to facilitate the consistent and meaningful semantics generation.\nTo this end,we propose a novel structure-guided diffusion model named\nStrDiffusion,to reformulate the conventional texture denoising process under\nstructure guidance to derive a simplified denoising objective for image\ninpainting,while revealing:1)the semantically sparse structure is beneficial to\ntackle semantic discrepancy in early stage, while dense texture generates\nreasonable semantics in late stage;2)the semantics from unmasked regions\nessentially offer the time-dependent structure guidance for the texture\ndenoising process,benefiting from the time-dependent sparsity of the structure\nsemantics.For the denoising process,a structure-guided neural network is\ntrained to estimate the simplified denoising objective by exploiting the\nconsistency of the denoised structure between masked and unmasked\nregions.Besides,we devise an adaptive resampling strategy as a formal criterion\nas whether structure is competent to guide the texture denoising process,while\nregulate their semantic correlations.Extensive experiments validate the merits\nof StrDiffusion over the state-of-the-arts.Our code is available at\nhttps://github.com/htyjers/StrDiffusion.\n","authors":["Haipeng Liu","Yang Wang","Biao Qian","Meng Wang","Yong Rui"],"pdf_url":"https://arxiv.org/pdf/2403.19898v2.pdf","comment":"15 pages, 10 figures, to appear CVPR 2024"},{"id":"http://arxiv.org/abs/2401.13964v3","updated":"2024-04-01T01:26:12Z","published":"2024-01-25T05:55:03Z","title":"An Extensible Framework for Open Heterogeneous Collaborative Perception","summary":"  Collaborative perception aims to mitigate the limitations of single-agent\nperception, such as occlusions, by facilitating data exchange among multiple\nagents. However, most current works consider a homogeneous scenario where all\nagents use identity sensors and perception models. In reality, heterogeneous\nagent types may continually emerge and inevitably face a domain gap when\ncollaborating with existing agents. In this paper, we introduce a new open\nheterogeneous problem: how to accommodate continually emerging new\nheterogeneous agent types into collaborative perception, while ensuring high\nperception performance and low integration cost? To address this problem, we\npropose HEterogeneous ALliance (HEAL), a novel extensible collaborative\nperception framework. HEAL first establishes a unified feature space with\ninitial agents via a novel multi-scale foreground-aware Pyramid Fusion network.\nWhen heterogeneous new agents emerge with previously unseen modalities or\nmodels, we align them to the established unified space with an innovative\nbackward alignment. This step only involves individual training on the new\nagent type, thus presenting extremely low training costs and high\nextensibility. To enrich agents' data heterogeneity, we bring OPV2V-H, a new\nlarge-scale dataset with more diverse sensor types. Extensive experiments on\nOPV2V-H and DAIR-V2X datasets show that HEAL surpasses SOTA methods in\nperformance while reducing the training parameters by 91.5% when integrating 3\nnew agent types. We further implement a comprehensive codebase at:\nhttps://github.com/yifanlu0227/HEAL\n","authors":["Yifan Lu","Yue Hu","Yiqi Zhong","Dequan Wang","Yanfeng Wang","Siheng Chen"],"pdf_url":"https://arxiv.org/pdf/2401.13964v3.pdf","comment":"Accepted by ICLR 2024. The code and data are open-sourced at\n  https://github.com/yifanlu0227/HEAL"},{"id":"http://arxiv.org/abs/2403.12686v2","updated":"2024-04-01T01:23:16Z","published":"2024-03-19T12:45:18Z","title":"WaterVG: Waterway Visual Grounding based on Text-Guided Vision and\n  mmWave Radar","summary":"  The perception of waterways based on human intent is significant for\nautonomous navigation and operations of Unmanned Surface Vehicles (USVs) in\nwater environments. Inspired by visual grounding, we introduce WaterVG, the\nfirst visual grounding dataset designed for USV-based waterway perception based\non human prompts. WaterVG encompasses prompts describing multiple targets, with\nannotations at the instance level including bounding boxes and masks. Notably,\nWaterVG includes 11,568 samples with 34,987 referred targets, whose prompts\nintegrates both visual and radar characteristics. The pattern of text-guided\ntwo sensors equips a finer granularity of text prompts with visual and radar\nfeatures of referred targets. Moreover, we propose a low-power visual grounding\nmodel, Potamoi, which is a multi-task model with a well-designed Phased\nHeterogeneous Modality Fusion (PHMF) mode, including Adaptive Radar Weighting\n(ARW) and Multi-Head Slim Cross Attention (MHSCA). Exactly, ARW extracts\nrequired radar features to fuse with vision for prompt alignment. MHSCA is an\nefficient fusion module with a remarkably small parameter count and FLOPs,\nelegantly fusing scenario context captured by two sensors with linguistic\nfeatures, which performs expressively on visual grounding tasks. Comprehensive\nexperiments and evaluations have been conducted on WaterVG, where our Potamoi\narchives state-of-the-art performances compared with counterparts.\n","authors":["Runwei Guan","Liye Jia","Fengyufan Yang","Shanliang Yao","Erick Purwanto","Xiaohui Zhu","Eng Gee Lim","Jeremy Smith","Ka Lok Man","Xuming Hu","Yutao Yue"],"pdf_url":"https://arxiv.org/pdf/2403.12686v2.pdf","comment":"10 pages, 10 figures"},{"id":"http://arxiv.org/abs/2312.15130v2","updated":"2024-04-01T00:22:18Z","published":"2023-12-23T01:38:41Z","title":"PACE: A Large-Scale Dataset with Pose Annotations in Cluttered\n  Environments","summary":"  Pose estimation is a crucial task in computer vision and robotics, enabling\nthe tracking and manipulation of objects in images or videos. While several\ndatasets exist for pose estimation, there is a lack of large-scale datasets\nspecifically focusing on cluttered scenes with occlusions. We introduce PACE\n(Pose Annotations in Cluttered Environments), a large-scale benchmark designed\nto advance the development and evaluation of pose estimation methods in\ncluttered scenarios. PACE consists of 54,945 frames with 257,673 annotations\nacross 300 videos, covering 576 objects from 44 categories and featuring a mix\nof rigid and articulated items in cluttered scenes. To annotate the real-world\ndata efficiently, we developed an innovative annotation system utilizing a\ncalibrated 3-camera setup. We test state-of-the-art algorithms in PACE along\ntwo tracks: pose estimation, and object pose tracking, revealing the\nbenchmark's challenges and research opportunities. Our code and data is\navailable on https://github.com/qq456cvb/PACE.\n","authors":["Yang You","Kai Xiong","Zhening Yang","Zhengxiang Huang","Junwei Zhou","Ruoxi Shi","Zhou Fang","Adam W. Harley","Leonidas Guibas","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2312.15130v2.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2402.04616v2","updated":"2024-04-01T01:28:48Z","published":"2024-02-07T06:48:24Z","title":"TinyLLM: Learning a Small Student from Multiple Large Language Models","summary":"  Transferring the reasoning capability from stronger large language models\n(LLMs) to smaller ones has been quite appealing, as smaller LLMs are more\nflexible to deploy with less expense. Among the existing solutions, knowledge\ndistillation stands out due to its outstanding efficiency and generalization.\nHowever, existing methods suffer from several drawbacks, including limited\nknowledge diversity and the lack of rich contextual information. To solve the\nproblems and facilitate the learning of compact language models, we propose\nTinyLLM, a new knowledge distillation paradigm to learn a small student LLM\nfrom multiple large teacher LLMs. In particular, we encourage the student LLM\nto not only generate the correct answers but also understand the rationales\nbehind these answers. Given that different LLMs possess diverse reasoning\nskills, we guide the student model to assimilate knowledge from various teacher\nLLMs. We further introduce an in-context example generator and a\nteacher-forcing Chain-of-Thought strategy to ensure that the rationales are\naccurate and grounded in contextually appropriate scenarios. Extensive\nexperiments on six datasets across two reasoning tasks demonstrate the\nsuperiority of our method. Results show that TinyLLM can outperform large\nteacher LLMs significantly, despite a considerably smaller model size.\n","authors":["Yijun Tian","Yikun Han","Xiusi Chen","Wei Wang","Nitesh V. Chawla"],"pdf_url":"https://arxiv.org/pdf/2402.04616v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20261v2","updated":"2024-04-01T05:18:57Z","published":"2024-03-29T16:10:34Z","title":"FABind+: Enhancing Molecular Docking through Improved Pocket Prediction\n  and Pose Generation","summary":"  Molecular docking is a pivotal process in drug discovery. While traditional\ntechniques rely on extensive sampling and simulation governed by physical\nprinciples, these methods are often slow and costly. The advent of deep\nlearning-based approaches has shown significant promise, offering increases in\nboth accuracy and efficiency. Building upon the foundational work of FABind, a\nmodel designed with a focus on speed and accuracy, we present FABind+, an\nenhanced iteration that largely boosts the performance of its predecessor. We\nidentify pocket prediction as a critical bottleneck in molecular docking and\npropose a novel methodology that significantly refines pocket prediction,\nthereby streamlining the docking process. Furthermore, we introduce\nmodifications to the docking module to enhance its pose generation\ncapabilities. In an effort to bridge the gap with conventional\nsampling/generative methods, we incorporate a simple yet effective sampling\ntechnique coupled with a confidence model, requiring only minor adjustments to\nthe regression framework of FABind. Experimental results and analysis reveal\nthat FABind+ remarkably outperforms the original FABind, achieves competitive\nstate-of-the-art performance, and delivers insightful modeling strategies. This\ndemonstrates FABind+ represents a substantial step forward in molecular docking\nand drug discovery. Our code is in https://github.com/QizhiPei/FABind.\n","authors":["Kaiyuan Gao","Qizhi Pei","Jinhua Zhu","Tao Qin","Kun He","Lijun Wu"],"pdf_url":"https://arxiv.org/pdf/2403.20261v2.pdf","comment":"17 pages, 14 figures, 5 tables"}],"Multimedia":[{"id":"http://arxiv.org/abs/2403.12686v2","updated":"2024-04-01T01:23:16Z","published":"2024-03-19T12:45:18Z","title":"WaterVG: Waterway Visual Grounding based on Text-Guided Vision and\n  mmWave Radar","summary":"  The perception of waterways based on human intent is significant for\nautonomous navigation and operations of Unmanned Surface Vehicles (USVs) in\nwater environments. Inspired by visual grounding, we introduce WaterVG, the\nfirst visual grounding dataset designed for USV-based waterway perception based\non human prompts. WaterVG encompasses prompts describing multiple targets, with\nannotations at the instance level including bounding boxes and masks. Notably,\nWaterVG includes 11,568 samples with 34,987 referred targets, whose prompts\nintegrates both visual and radar characteristics. The pattern of text-guided\ntwo sensors equips a finer granularity of text prompts with visual and radar\nfeatures of referred targets. Moreover, we propose a low-power visual grounding\nmodel, Potamoi, which is a multi-task model with a well-designed Phased\nHeterogeneous Modality Fusion (PHMF) mode, including Adaptive Radar Weighting\n(ARW) and Multi-Head Slim Cross Attention (MHSCA). Exactly, ARW extracts\nrequired radar features to fuse with vision for prompt alignment. MHSCA is an\nefficient fusion module with a remarkably small parameter count and FLOPs,\nelegantly fusing scenario context captured by two sensors with linguistic\nfeatures, which performs expressively on visual grounding tasks. Comprehensive\nexperiments and evaluations have been conducted on WaterVG, where our Potamoi\narchives state-of-the-art performances compared with counterparts.\n","authors":["Runwei Guan","Liye Jia","Fengyufan Yang","Shanliang Yao","Erick Purwanto","Xiaohui Zhu","Eng Gee Lim","Jeremy Smith","Ka Lok Man","Xuming Hu","Yutao Yue"],"pdf_url":"https://arxiv.org/pdf/2403.12686v2.pdf","comment":"10 pages, 10 figures"}]},"2024-03-31T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2403.15351v2","updated":"2024-03-31T23:23:46Z","published":"2024-03-22T17:06:05Z","title":"Multi-Review Fusion-in-Context","summary":"  Grounded text generation, encompassing tasks such as long-form\nquestion-answering and summarization, necessitates both content selection and\ncontent consolidation. Current end-to-end methods are difficult to control and\ninterpret due to their opaqueness. Accordingly, recent works have proposed a\nmodular approach, with separate components for each step. Specifically, we\nfocus on the second subtask, of generating coherent text given pre-selected\ncontent in a multi-document setting. Concretely, we formalize Fusion-in-Context\n(FiC) as a standalone task, whose input consists of source texts with\nhighlighted spans of targeted content. A model then needs to generate a\ncoherent passage that includes all and only the target information. Our work\nincludes the development of a curated dataset of 1000 instances in the reviews\ndomain, alongside a novel evaluation framework for assessing the faithfulness\nand coverage of highlights, which strongly correlate to human judgment. Several\nbaseline models exhibit promising outcomes and provide insightful analyses.\nThis study lays the groundwork for further exploration of modular text\ngeneration in the multi-document setting, offering potential improvements in\nthe quality and reliability of generated content. Our benchmark, FuseReviews,\nincluding the dataset, evaluation framework, and designated leaderboard, can be\nfound at https://fusereviews.github.io/.\n","authors":["Aviv Slobodkin","Ori Shapira","Ran Levy","Ido Dagan"],"pdf_url":"https://arxiv.org/pdf/2403.15351v2.pdf","comment":"NAACL 2024, findings"},{"id":"http://arxiv.org/abs/2309.14592v2","updated":"2024-03-31T23:05:53Z","published":"2023-09-26T00:58:36Z","title":"Efficient Post-training Quantization with FP8 Formats","summary":"  Recent advances in deep learning methods such as LLMs and Diffusion models\nhave created a need for improved quantization methods that can meet the\ncomputational demands of these modern architectures while maintaining accuracy.\nTowards this goal, we study the advantages of FP8 data formats for\npost-training quantization across 75 unique network architectures covering a\nwide range of tasks, including machine translation, language modeling, text\ngeneration, image classification, generation, and segmentation. We examine\nthree different FP8 representations (E5M2, E4M3, and E3M4) to study the effects\nof varying degrees of trade-off between dynamic range and precision on model\naccuracy. Based on our extensive study, we developed a quantization workflow\nthat generalizes across different network architectures. Our empirical results\nshow that FP8 formats outperform INT8 in multiple aspects, including workload\ncoverage (92.64% vs. 65.87%), model accuracy and suitability for a broader\nrange of operations. Furthermore, our findings suggest that E4M3 is better\nsuited for NLP models, whereas E3M4 performs marginally better than E4M3 on\ncomputer vision tasks. The code is publicly available on Intel Neural\nCompressor: https://github.com/intel/neural-compressor.\n","authors":["Haihao Shen","Naveen Mellempudi","Xin He","Qun Gao","Chang Wang","Mengni Wang"],"pdf_url":"https://arxiv.org/pdf/2309.14592v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08685v3","updated":"2024-03-31T22:42:03Z","published":"2023-11-15T04:22:22Z","title":"Safer-Instruct: Aligning Language Models with Automated Preference Data","summary":"  Reinforcement learning from human feedback (RLHF) is a vital strategy for\nenhancing model capability in language models. However, annotating preference\ndata for RLHF is a resource-intensive and creativity-demanding process, while\nexisting automatic generation methods face limitations in data diversity and\nquality. In response, we present Safer-Instruct, a novel pipeline for\nautomatically constructing large-scale preference data. Our approach leverages\nreversed instruction tuning, instruction induction, and expert model evaluation\nto efficiently generate high-quality preference data without human annotators.\nTo verify the effectiveness of Safer-Instruct, we apply the pipeline to\nconstruct a safety preference dataset as a case study. Finetuning an Alpaca\nmodel on this synthetic dataset not only demonstrates improved harmlessness but\nalso outperforms models fine-tuned on human-annotated safety preference data,\nall the while maintaining a competitive edge in downstream tasks. Importantly,\nour Safer-Instruct framework is versatile and can be applied to generate\npreference data across various domains, extending its utility beyond safety\npreferences. It addresses the challenges in preference data acquisition and\nadvances the development of more capable and responsible AI systems. For\ndataset and code implementation, see\nhttps://github.com/uscnlp-lime/safer-instruct\n","authors":["Taiwei Shi","Kai Chen","Jieyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2311.08685v3.pdf","comment":"16 pages. NAACL 2024 Camera-ready"},{"id":"http://arxiv.org/abs/2311.09476v2","updated":"2024-03-31T20:58:46Z","published":"2023-11-16T00:39:39Z","title":"ARES: An Automated Evaluation Framework for Retrieval-Augmented\n  Generation Systems","summary":"  Evaluating retrieval-augmented generation (RAG) systems traditionally relies\non hand annotations for input queries, passages to retrieve, and responses to\ngenerate. We introduce ARES, an Automated RAG Evaluation System, for evaluating\nRAG systems along the dimensions of context relevance, answer faithfulness, and\nanswer relevance. By creating its own synthetic training data, ARES finetunes\nlightweight LM judges to assess the quality of individual RAG components. To\nmitigate potential prediction errors, ARES utilizes a small set of\nhuman-annotated datapoints for prediction-powered inference (PPI). Across eight\ndifferent knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES\naccurately evaluates RAG systems while using only a few hundred human\nannotations during evaluation. Furthermore, ARES judges remain effective across\ndomain shifts, proving accurate even after changing the type of queries and/or\ndocuments used in the evaluated RAG systems. We make our code and datasets\npublicly available on Github.\n","authors":["Jon Saad-Falcon","Omar Khattab","Christopher Potts","Matei Zaharia"],"pdf_url":"https://arxiv.org/pdf/2311.09476v2.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2402.16671v3","updated":"2024-03-31T20:14:20Z","published":"2024-02-26T15:47:01Z","title":"StructLM: Towards Building Generalist Models for Structured Knowledge\n  Grounding","summary":"  Structured data sources, such as tables, graphs, and databases, are\nubiquitous knowledge sources. Despite the demonstrated capabilities of large\nlanguage models (LLMs) on plain text, their proficiency in interpreting and\nutilizing structured data remains limited. Our investigation reveals a notable\ndeficiency in LLMs' ability to process structured data, e.g., ChatGPT lags\nbehind state-of-the-art (SoTA) model by an average of 35%. To augment the\nStructured Knowledge Grounding (SKG) capabilities in LLMs, we have developed a\ncomprehensive instruction tuning dataset comprising 1.1 million examples.\nUtilizing this dataset, we train a series of models, referred to as StructLM,\nbased on the Code-LLaMA architecture, ranging from 7B to 34B parameters. Our\nStructLM series surpasses task-specific models on 14 out of 18 evaluated\ndatasets and establishes new SoTA achievements on 7 SKG tasks. Furthermore,\nStructLM demonstrates strong generalization across 6 novel held-out SKG tasks,\noutperforming TableLlama by an average of 35\\% and Flan-UL2 20B by an average\nof 10\\%. Contrary to expectations, we observe that scaling model size offers\nmarginal benefits, with StructLM-34B showing only slight improvements over\nStructLM-7B. This suggests that structured knowledge grounding is still a\nchallenging task and requires more innovative design to push to a new level.\n","authors":["Alex Zhuang","Ge Zhang","Tianyu Zheng","Xinrun Du","Junjie Wang","Weiming Ren","Stephen W. Huang","Jie Fu","Xiang Yue","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2402.16671v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.02185v3","updated":"2024-03-31T19:47:47Z","published":"2023-07-05T10:25:45Z","title":"Citation: A Key to Building Responsible and Accountable Large Language\n  Models","summary":"  Large Language Models (LLMs) bring transformative benefits alongside unique\nchallenges, including intellectual property (IP) and ethical concerns. This\nposition paper explores a novel angle to mitigate these risks, drawing\nparallels between LLMs and established web systems. We identify \"citation\" -\nthe acknowledgement or reference to a source or evidence - as a crucial yet\nmissing component in LLMs. Incorporating citation could enhance content\ntransparency and verifiability, thereby confronting the IP and ethical issues\nin the deployment of LLMs. We further propose that a comprehensive citation\nmechanism for LLMs should account for both non-parametric and parametric\ncontent. Despite the complexity of implementing such a citation mechanism,\nalong with the potential pitfalls, we advocate for its development. Building on\nthis foundation, we outline several research problems in this area, aiming to\nguide future explorations towards building more responsible and accountable\nLLMs.\n","authors":["Jie Huang","Kevin Chen-Chuan Chang"],"pdf_url":"https://arxiv.org/pdf/2307.02185v3.pdf","comment":"NAACL 2024 Findings"},{"id":"http://arxiv.org/abs/2403.05766v2","updated":"2024-03-31T19:45:22Z","published":"2024-03-09T02:27:45Z","title":"FLAP: Flow-Adhering Planning with Constrained Decoding in LLMs","summary":"  Planning is a crucial task for agents in task oriented dialogs (TODs). Human\nagents typically resolve user issues by following predefined workflows,\ndecomposing workflow steps into actionable items, and performing actions by\nexecuting APIs in order; all of which require reasoning and planning. With the\nrecent advances in LLMs, there have been increasing attempts to use them for\ntask planning and API usage. However, the faithfulness of the plans to\npredefined workflows and API dependencies, is not guaranteed with LLMs.\nMoreover, workflows in real life are often custom-defined and prone to changes;\nhence, adaptation is desirable. To study this, we propose the problem of\nfaithful planning in TODs that needs to resolve user intents by following\npredefined flows and preserving API dependencies. To solve this problem, we\npropose FLAP, a Flow-Adhering Planning algorithm based on constrained decoding\nwith lookahead heuristic for LLMs. Our algorithm alleviates the need for\nfinetuning LLMs using domain specific (plan/dependency) data, enables quick\nadaptation to predefined flows, and outperforms other decoding and\nprompting-based baselines. Further, our algorithm empowers smaller LLMs (7B) to\nperform at par larger LLMs (30B-40B).\n","authors":["Shamik Roy","Sailik Sengupta","Daniele Bonadiman","Saab Mansour","Arshit Gupta"],"pdf_url":"https://arxiv.org/pdf/2403.05766v2.pdf","comment":"NAACL 2024 (Camera Ready)"},{"id":"http://arxiv.org/abs/2311.07772v4","updated":"2024-03-31T19:33:50Z","published":"2023-11-13T21:42:38Z","title":"In-context Learning and Gradient Descent Revisited","summary":"  In-context learning (ICL) has shown impressive results in few-shot learning\ntasks, yet its underlying mechanism is still not fully understood. A recent\nline of work suggests that ICL performs gradient descent (GD)-based\noptimization implicitly. While appealing, much of the research focuses on\nsimplified settings, where the parameters of a shallow model are optimized. In\nthis work, we revisit evidence for ICL-GD correspondence on realistic NLP tasks\nand models. We find gaps in evaluation, both in terms of problematic metrics\nand insufficient baselines. We show that surprisingly, even untrained models\nachieve comparable ICL-GD similarity scores despite not exhibiting ICL. Next,\nwe explore a major discrepancy in the flow of information throughout the model\nbetween ICL and GD, which we term Layer Causality. We propose a simple GD-based\noptimization procedure that respects layer causality, and show it improves\nsimilarity scores significantly.\n","authors":["Gilad Deutch","Nadav Magar","Tomer Bar Natan","Guy Dar"],"pdf_url":"https://arxiv.org/pdf/2311.07772v4.pdf","comment":"Accepted to NAACL 2024 main conference"},{"id":"http://arxiv.org/abs/2310.13522v2","updated":"2024-03-31T18:12:16Z","published":"2023-10-20T14:11:04Z","title":"Teaching Language Models to Self-Improve through Interactive\n  Demonstrations","summary":"  The self-improving ability of large language models (LLMs), enabled by\nprompting them to analyze and revise their own outputs, has garnered\nsignificant interest in recent research. However, this ability has been shown\nto be absent and difficult to learn for smaller models, thus widening the\nperformance gap between state-of-the-art LLMs and more cost-effective and\nfaster ones. To reduce this gap, we introduce TriPosT, a training algorithm\nthat endows smaller models with such self-improvement ability, and show that\nour approach can improve a LLaMA-7b's performance on math and reasoning tasks\nby up to 7.13%. In contrast to prior work, we achieve this by using the smaller\nmodel to interact with LLMs to collect feedback and improvements on its own\ngenerations. We then replay this experience to train the small model. Our\nexperiments on four math and reasoning datasets show that the interactive\nexperience of learning from and correcting its own mistakes is crucial for\nsmall models to improve their performance.\n","authors":["Xiao Yu","Baolin Peng","Michel Galley","Jianfeng Gao","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2310.13522v2.pdf","comment":"NAACL 2024 main"},{"id":"http://arxiv.org/abs/2311.08572v2","updated":"2024-03-31T17:01:34Z","published":"2023-11-14T22:32:39Z","title":"Low-Rank Adaptation for Multilingual Summarization: An Empirical Study","summary":"  Although the advancements of pre-trained Large Language Models have\nsignificantly accelerated recent progress in NLP, their ever-increasing size\nposes significant challenges for conventional fine-tuning, especially in\nmemory-intensive tasks. We investigate the potential of Parameter-Efficient\nFine-Tuning, focusing on Low-Rank Adaptation (LoRA), in the domain of\nmultilingual summarization, a task that is both challenging (due to typically\nlong inputs), and relatively unexplored. We conduct an extensive study across\ndifferent data availability scenarios, including high- and low-data settings,\nand cross-lingual transfer, leveraging models of different sizes. Our findings\nreveal that LoRA is competitive with full fine-tuning when trained with high\nquantities of data, and excels in low-data scenarios and cross-lingual\ntransfer. We also study different strategies for few-shot cross-lingual\ntransfer, finding that continued LoRA tuning outperforms full fine-tuning and\nthe dynamic composition of language-specific LoRA modules.\n","authors":["Chenxi Whitehouse","Fantine Huot","Jasmijn Bastings","Mostafa Dehghani","Chu-Cheng Lin","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2311.08572v2.pdf","comment":"Findings of NAACL 2024"},{"id":"http://arxiv.org/abs/2403.19647v2","updated":"2024-03-31T16:54:50Z","published":"2024-03-28T17:56:07Z","title":"Sparse Feature Circuits: Discovering and Editing Interpretable Causal\n  Graphs in Language Models","summary":"  We introduce methods for discovering and applying sparse feature circuits.\nThese are causally implicated subnetworks of human-interpretable features for\nexplaining language model behaviors. Circuits identified in prior work consist\nof polysemantic and difficult-to-interpret units like attention heads or\nneurons, rendering them unsuitable for many downstream applications. In\ncontrast, sparse feature circuits enable detailed understanding of\nunanticipated mechanisms. Because they are based on fine-grained units, sparse\nfeature circuits are useful for downstream tasks: We introduce SHIFT, where we\nimprove the generalization of a classifier by ablating features that a human\njudges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised\nand scalable interpretability pipeline by discovering thousands of sparse\nfeature circuits for automatically discovered model behaviors.\n","authors":["Samuel Marks","Can Rager","Eric J. Michaud","Yonatan Belinkov","David Bau","Aaron Mueller"],"pdf_url":"https://arxiv.org/pdf/2403.19647v2.pdf","comment":"Code and data at https://github.com/saprmarks/feature-circuits.\n  Demonstration at https://feature-circuits.xyz"},{"id":"http://arxiv.org/abs/2402.13249v2","updated":"2024-03-31T15:30:34Z","published":"2024-02-20T18:58:49Z","title":"TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue\n  Summarization","summary":"  Single document news summarization has seen substantial progress on\nfaithfulness in recent years, driven by research on the evaluation of factual\nconsistency, or hallucinations. We ask whether these advances carry over to\nother text summarization domains. We propose a new evaluation benchmark on\ntopic-focused dialogue summarization, generated by LLMs of varying sizes. We\nprovide binary sentence-level human annotations of the factual consistency of\nthese summaries along with detailed explanations of factually inconsistent\nsentences. Our analysis shows that existing LLMs hallucinate significant\namounts of factual errors in the dialogue domain, regardless of the model's\nsize. On the other hand, when LLMs, including GPT-4, serve as binary factual\nevaluators, they perform poorly and can be outperformed by prevailing\nstate-of-the-art specialized factuality evaluation metrics. Finally, we\nconducted an analysis of hallucination types with a curated error taxonomy. We\nfind that there are diverse errors and error distributions in model-generated\nsummaries and that non-LLM based metrics can capture all error types better\nthan LLM-based evaluators.\n","authors":["Liyan Tang","Igor Shalyminov","Amy Wing-mei Wong","Jon Burnsky","Jake W. Vincent","Yu'an Yang","Siffi Singh","Song Feng","Hwanjun Song","Hang Su","Lijia Sun","Yi Zhang","Saab Mansour","Kathleen McKeown"],"pdf_url":"https://arxiv.org/pdf/2402.13249v2.pdf","comment":"NAACL 2024; Linguistic annotations available at\n  https://github.com/amazon-science/tofueval"},{"id":"http://arxiv.org/abs/2309.14771v2","updated":"2024-03-31T13:55:39Z","published":"2023-09-26T09:06:39Z","title":"Knowledgeable In-Context Tuning: Exploring and Exploiting Factual\n  Knowledge for In-Context Learning","summary":"  Large language models (LLMs) enable in-context learning (ICL) by conditioning\non a few labeled training examples as a text-based prompt, eliminating the need\nfor parameter updates and achieving competitive performance. In this paper, we\ndemonstrate that factual knowledge is imperative for the performance of ICL in\nthree core facets: the inherent knowledge learned in LLMs, the factual\nknowledge derived from the selected in-context examples, and the knowledge\nbiases in LLMs for output generation. To unleash the power of LLMs in few-shot\nlearning scenarios, we introduce a novel Knowledgeable In-Context Tuning (KICT)\nframework to further improve the performance of ICL: 1) injecting knowledge\ninto LLMs during continual self-supervised pre-training, 2) judiciously\nselecting the examples for ICL with high knowledge relevance, and 3)\ncalibrating the prediction results based on prior knowledge. We evaluate the\nproposed approaches on autoregressive models (e.g., GPT-style LLMs) over\nmultiple text classification and question-answering tasks. Experimental results\ndemonstrate that KICT substantially outperforms strong baselines and improves\nby more than 13% and 7% on text classification and question-answering tasks,\nrespectively.\n","authors":["Jianing Wang","Chengyu Wang","Chuanqi Tan","Jun Huang","Ming Gao"],"pdf_url":"https://arxiv.org/pdf/2309.14771v2.pdf","comment":"naacl 2024"},{"id":"http://arxiv.org/abs/2310.09518v3","updated":"2024-03-31T13:39:44Z","published":"2023-10-14T07:16:08Z","title":"Instruction Tuning with Human Curriculum","summary":"  In this work, we (1) introduce Curriculum Instruction Tuning, (2) explore the\npotential advantages of employing diverse curriculum strategies, and (3)\ndelineate a synthetic instruction-response generation framework that\ncomplements our theoretical approach. Distinct from the existing instruction\ntuning dataset, our generation pipeline is systematically structured to emulate\nthe sequential and orderly characteristic of human learning. Additionally, we\ndescribe a methodology for generating instruction-response datasets that\nextensively span the various stages of human education, from middle school\nthrough the graduate level, utilizing educational subject catalogs.\n  Before training, we meticulously organize the instruction data to ensure that\nquestions escalate in difficulty regarding (A) the subject matter and (B) the\nintricacy of the instructions. The findings of our study reveal that\nsubstantial improvements in performance can be achieved through the mere\napplication of curriculum ordering to instruction data (achieving gains of\n+4.76 on TruthfulQA, +2.98 on MMLU, +2.8 on OpenbookQA, and +1.28 on ARC-hard)\ncompared to random shuffling. This enhancement is achieved without incurring\nadditional computational expenses. Through comprehensive experimentation, we\nobserve that the advantages of our proposed method are consistently evident\nacross nine benchmarks.\n","authors":["Bruce W. Lee","Hyunsoo Cho","Kang Min Yoo"],"pdf_url":"https://arxiv.org/pdf/2310.09518v3.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.14119v3","updated":"2024-03-31T13:36:54Z","published":"2024-03-21T04:08:29Z","title":"C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via\n  Text Feature Dispersion","summary":"  In deep learning, test-time adaptation has gained attention as a method for\nmodel fine-tuning without the need for labeled data. A prime exemplification is\nthe recently proposed test-time prompt tuning for large-scale vision-language\nmodels such as CLIP. Unfortunately, these prompts have been mainly developed to\nimprove accuracy, overlooking the importance of calibration, which is a crucial\naspect for quantifying prediction uncertainty. However, traditional calibration\nmethods rely on substantial amounts of labeled data, making them impractical\nfor test-time scenarios. To this end, this paper explores calibration during\ntest-time prompt tuning by leveraging the inherent properties of CLIP. Through\na series of observations, we find that the prompt choice significantly affects\nthe calibration in CLIP, where the prompts leading to higher text feature\ndispersion result in better-calibrated predictions. Introducing the Average\nText Feature Dispersion (ATFD), we establish its relationship with calibration\nerror and present a novel method, Calibrated Test-time Prompt Tuning (C-TPT),\nfor optimizing prompts during test-time with enhanced calibration. Through\nextensive experiments on different CLIP architectures and datasets, we show\nthat C-TPT can effectively improve the calibration of test-time prompt tuning\nwithout needing labeled data. The code is publicly accessible at\nhttps://github.com/hee-suk-yoon/C-TPT.\n","authors":["Hee Suk Yoon","Eunseop Yoon","Joshua Tian Jin Tee","Mark Hasegawa-Johnson","Yingzhen Li","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2403.14119v3.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2307.04192v4","updated":"2024-03-31T12:10:24Z","published":"2023-07-09T14:54:30Z","title":"Self-Adaptive Sampling for Efficient Video Question-Answering on\n  Image--Text Models","summary":"  Video question-answering is a fundamental task in the field of video\nunderstanding. Although current vision--language models (VLMs) equipped with\nVideo Transformers have enabled temporal modeling and yielded superior results,\nthey are at the cost of huge computational power and thus too expensive to\ndeploy in real-time application scenarios. An economical workaround only\nsamples a small portion of frames to represent the main content of that video\nand tune an image--text model on these sampled frames. Recent video\nunderstanding models usually randomly sample a set of frames or clips,\nregardless of internal correlations between their visual contents, nor their\nrelevance to the problem. We argue that such kinds of aimless sampling may omit\nthe key frames from which the correct answer can be deduced, and the situation\ngets worse when the sampling sparsity increases, which always happens as the\nvideo lengths increase. To mitigate this issue, we propose two frame sampling\nstrategies, namely the most domain frames (MDF) and most implied frames (MIF),\nto maximally preserve those frames that are most likely vital to the given\nquestions. MDF passively minimizes the risk of key frame omission in a\nbootstrap manner, while MIS actively searches key frames customized for each\nvideo--question pair with the assistance of auxiliary models. The experimental\nresults on three public datasets from three advanced VLMs (CLIP, GIT and\nAll-in-one) demonstrate that our proposed strategies can boost the performance\nfor image-text pretrained models. The source codes pertaining to the method\nproposed in this paper are publicly available at\nhttps://github.com/declare-lab/sas-vqa.\n","authors":["Wei Han","Hui Chen","Min-Yen Kan","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2307.04192v4.pdf","comment":"13 pages, 7 figures, accepted to Findings of NAACL 2024"},{"id":"http://arxiv.org/abs/2310.14159v3","updated":"2024-03-31T10:51:06Z","published":"2023-10-22T03:01:38Z","title":"Can Language Models Laugh at YouTube Short-form Videos?","summary":"  As short-form funny videos on social networks are gaining popularity, it\nbecomes demanding for AI models to understand them for better communication\nwith humans. Unfortunately, previous video humor datasets target specific\ndomains, such as speeches or sitcoms, and mostly focus on verbal cues. We\ncurate a user-generated dataset of 10K multimodal funny videos from YouTube,\ncalled ExFunTube. Using a video filtering pipeline with GPT-3.5, we verify both\nverbal and visual elements contributing to humor. After filtering, we annotate\neach video with timestamps and text explanations for funny moments. Our\nExFunTube is unique over existing datasets in that our videos cover a wide\nrange of domains with various types of humor that necessitate a multimodal\nunderstanding of the content. Also, we develop a zero-shot video-to-text\nprompting to maximize video humor understanding of large language models\n(LLMs). With three different evaluation methods using automatic scores,\nrationale quality experiments, and human evaluations, we show that our\nprompting significantly improves LLMs' ability for humor explanation.\n","authors":["Dayoon Ko","Sangho Lee","Gunhee Kim"],"pdf_url":"https://arxiv.org/pdf/2310.14159v3.pdf","comment":"EMNLP 2023; references added"},{"id":"http://arxiv.org/abs/2401.12987v2","updated":"2024-03-31T09:55:51Z","published":"2024-01-16T07:18:41Z","title":"TelME: Teacher-leading Multimodal Fusion Network for Emotion Recognition\n  in Conversation","summary":"  Emotion Recognition in Conversation (ERC) plays a crucial role in enabling\ndialogue systems to effectively respond to user requests. The emotions in a\nconversation can be identified by the representations from various modalities,\nsuch as audio, visual, and text. However, due to the weak contribution of\nnon-verbal modalities to recognize emotions, multimodal ERC has always been\nconsidered a challenging task. In this paper, we propose Teacher-leading\nMultimodal fusion network for ERC (TelME). TelME incorporates cross-modal\nknowledge distillation to transfer information from a language model acting as\nthe teacher to the non-verbal students, thereby optimizing the efficacy of the\nweak modalities. We then combine multimodal features using a shifting fusion\napproach in which student networks support the teacher. TelME achieves\nstate-of-the-art performance in MELD, a multi-speaker conversation dataset for\nERC. Finally, we demonstrate the effectiveness of our components through\nadditional experiments.\n","authors":["Taeyang Yun","Hyunkuk Lim","Jeonghwan Lee","Min Song"],"pdf_url":"https://arxiv.org/pdf/2401.12987v2.pdf","comment":"NAACL 2024 main conference"},{"id":"http://arxiv.org/abs/2403.19135v2","updated":"2024-03-31T08:16:58Z","published":"2024-03-28T04:12:13Z","title":"Compressing Large Language Models by Streamlining the Unimportant Layer","summary":"  Large language models (LLM) have been extensively applied in various natural\nlanguage tasks and domains, but their applicability is constrained by the large\nnumber of parameters of the models. Consequently, there is an increasing\nemphasis on compact models that exhibit high performance. In this study, we\nobserve that different layers in LLM have varying degrees of perturbation on\nthe hidden states, which allows us to identify less important layers. Based on\nthis phenomenon, we propose LLM-Streamline, which consists of two parts: layer\npruning, where we remove a set of consecutive layers with the lowest importance\nin the model according to the target sparsity; and layer replacement, where we\ntrain a lightweight model to substitute the pruned layers, thereby mitigating\nthe performance degradation caused by pruning. In our experiments, we utilize\nstructures such as a multi-layer perceptron (MLP) and a transformer layer as\nlightweight models and ultimately demonstrate that a single MLP can effectively\nfit the pruned layers. Comprehensive experiments show that our proposed method,\nLLM-Streamline, outperforms previous state-of-the-art (SOTA) model pruning\nmethods.\n","authors":["Xiaodong Chen","Yuxuan Hu","Jing Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.19135v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09989v3","updated":"2024-03-31T07:47:59Z","published":"2024-02-15T14:54:33Z","title":"LLMs as Bridges: Reformulating Grounded Multimodal Named Entity\n  Recognition","summary":"  Grounded Multimodal Named Entity Recognition (GMNER) is a nascent multimodal\ntask that aims to identify named entities, entity types and their corresponding\nvisual regions. GMNER task exhibits two challenging properties: 1) The weak\ncorrelation between image-text pairs in social media results in a significant\nportion of named entities being ungroundable. 2) There exists a distinction\nbetween coarse-grained referring expressions commonly used in similar tasks\n(e.g., phrase localization, referring expression comprehension) and\nfine-grained named entities. In this paper, we propose RiVEG, a unified\nframework that reformulates GMNER into a joint MNER-VE-VG task by leveraging\nlarge language models (LLMs) as a connecting bridge. This reformulation brings\ntwo benefits: 1) It maintains the optimal MNER performance and eliminates the\nneed for employing object detection methods to pre-extract regional features,\nthereby naturally addressing two major limitations of existing GMNER methods.\n2) The introduction of entity expansion expression and Visual Entailment (VE)\nModule unifies Visual Grounding (VG) and Entity Grounding (EG). It enables\nRiVEG to effortlessly inherit the Visual Entailment and Visual Grounding\ncapabilities of any current or prospective multimodal pretraining models.\nExtensive experiments demonstrate that RiVEG outperforms state-of-the-art\nmethods on the existing GMNER dataset and achieves absolute leads of 10.65%,\n6.21%, and 8.83% in all three subtasks.\n","authors":["Jinyuan Li","Han Li","Di Sun","Jiahao Wang","Wenkun Zhang","Zan Wang","Gang Pan"],"pdf_url":"https://arxiv.org/pdf/2402.09989v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00813v2","updated":"2024-03-31T06:32:03Z","published":"2024-02-25T12:37:29Z","title":"UrbanGPT: Spatio-Temporal Large Language Models","summary":"  Spatio-temporal prediction aims to forecast and gain insights into the\never-changing dynamics of urban environments across both time and space. Its\npurpose is to anticipate future patterns, trends, and events in diverse facets\nof urban life, including transportation, population movement, and crime rates.\nAlthough numerous efforts have been dedicated to developing neural network\ntechniques for accurate predictions on spatio-temporal data, it is important to\nnote that many of these methods heavily depend on having sufficient labeled\ndata to generate precise spatio-temporal representations. Unfortunately, the\nissue of data scarcity is pervasive in practical urban sensing scenarios.\nConsequently, it becomes necessary to build a spatio-temporal model with strong\ngeneralization capabilities across diverse spatio-temporal learning scenarios.\nTaking inspiration from the remarkable achievements of large language models\n(LLMs), our objective is to create a spatio-temporal LLM that can exhibit\nexceptional generalization capabilities across a wide range of downstream urban\ntasks. To achieve this objective, we present the UrbanGPT, which seamlessly\nintegrates a spatio-temporal dependency encoder with the instruction-tuning\nparadigm. This integration enables LLMs to comprehend the complex\ninter-dependencies across time and space, facilitating more comprehensive and\naccurate predictions under data scarcity. To validate the effectiveness of our\napproach, we conduct extensive experiments on various public datasets, covering\ndifferent spatio-temporal prediction tasks. The results consistently\ndemonstrate that our UrbanGPT, with its carefully designed architecture,\nconsistently outperforms state-of-the-art baselines. These findings highlight\nthe potential of building large language models for spatio-temporal learning,\nparticularly in zero-shot scenarios where labeled data is scarce.\n","authors":["Zhonghang Li","Lianghao Xia","Jiabin Tang","Yong Xu","Lei Shi","Long Xia","Dawei Yin","Chao Huang"],"pdf_url":"https://arxiv.org/pdf/2403.00813v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2308.08043v3","updated":"2024-03-31T04:39:05Z","published":"2023-08-15T21:14:09Z","title":"DiagGPT: An LLM-based Chatbot with Automatic Topic Management for\n  Task-Oriented Dialogue","summary":"  Large Language Models (LLMs), such as ChatGPT, are increasingly sophisticated\nand exhibit capabilities closely resembling those of humans. A significant\napplication of these LLMs is their use as chat agents, responding to human\ninquiries across various domains. While current LLMs proficiently answer\ngeneral questions, they often fall short in complex diagnostic scenarios such\nas legal, medical, or other specialized consultations. These scenarios\ntypically require Task-Oriented Dialogue (TOD), where an AI chat agent must\nproactively pose questions and guide users toward specific goals or task\ncompletion. Previous fine-tuning models have underperformed in TOD and the full\npotential of this capability in current LLMs has not yet been fully explored.\nIn this paper, we introduce DiagGPT (Dialogue in Diagnosis GPT), an innovative\napproach that extends LLMs to more TOD scenarios. In addition to guiding users\nto complete tasks, DiagGPT can effectively manage the status of all topics\nthroughout the dialogue development. This feature enhances user experience and\noffers a more flexible interaction in TOD. Our experiments demonstrate that\nDiagGPT exhibits outstanding performance in conducting TOD with users, showing\nits potential for practical applications in various fields.\n","authors":["Lang Cao"],"pdf_url":"https://arxiv.org/pdf/2308.08043v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04801v2","updated":"2024-03-31T04:33:56Z","published":"2024-03-05T19:32:01Z","title":"Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs","summary":"  In this paper, we introduce a black-box prompt optimization method that uses\nan attacker LLM agent to uncover higher levels of memorization in a victim\nagent, compared to what is revealed by prompting the target model with the\ntraining data directly, which is the dominant approach of quantifying\nmemorization in LLMs. We use an iterative rejection-sampling optimization\nprocess to find instruction-based prompts with two main characteristics: (1)\nminimal overlap with the training data to avoid presenting the solution\ndirectly to the model, and (2) maximal overlap between the victim model's\noutput and the training data, aiming to induce the victim to spit out training\ndata. We observe that our instruction-based prompts generate outputs with 23.7%\nhigher overlap with training data compared to the baseline prefix-suffix\nmeasurements. Our findings show that (1) instruction-tuned models can expose\npre-training data as much as their base-models, if not more so, (2) contexts\nother than the original training data can lead to leakage, and (3) using\ninstructions proposed by other LLMs can open a new avenue of automated attacks\nthat we should further study and explore. The code can be found at\nhttps://github.com/Alymostafa/Instruction_based_attack .\n","authors":["Aly M. Kassem","Omar Mahmoud","Niloofar Mireshghallah","Hyunwoo Kim","Yulia Tsvetkov","Yejin Choi","Sherif Saad","Santu Rana"],"pdf_url":"https://arxiv.org/pdf/2403.04801v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08461v2","updated":"2024-03-31T03:06:51Z","published":"2023-10-12T16:21:04Z","title":"DistillSpec: Improving Speculative Decoding via Knowledge Distillation","summary":"  Speculative decoding (SD) accelerates large language model inference by\nemploying a faster draft model for generating multiple tokens, which are then\nverified in parallel by the larger target model, resulting in the text\ngenerated according to the target model distribution. However, identifying a\ncompact draft model that is well-aligned with the target model is challenging.\nTo tackle this issue, we propose DistillSpec that uses knowledge distillation\nto better align the draft model with the target model, before applying SD.\nDistillSpec makes two key design choices, which we demonstrate via systematic\nstudy to be crucial to improving the draft and target alignment: utilizing\non-policy data generation from the draft model, and tailoring the divergence\nfunction to the task and decoding strategy. Notably, DistillSpec yields\nimpressive 10 - 45% speedups over standard SD on a range of standard\nbenchmarks, using both greedy and non-greedy sampling. Furthermore, we combine\nDistillSpec with lossy SD to achieve fine-grained control over the latency vs.\ntask performance trade-off. Finally, in practical scenarios with models of\nvarying sizes, first using distillation to boost the performance of the target\nmodel and then applying DistillSpec to train a well-aligned draft model can\nreduce decoding latency by 6-10x with minimal performance drop, compared to\nstandard decoding without distillation.\n","authors":["Yongchao Zhou","Kaifeng Lyu","Ankit Singh Rawat","Aditya Krishna Menon","Afshin Rostamizadeh","Sanjiv Kumar","Jean-François Kagy","Rishabh Agarwal"],"pdf_url":"https://arxiv.org/pdf/2310.08461v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06415v4","updated":"2024-03-31T02:24:39Z","published":"2023-09-08T03:59:02Z","title":"Down the Toxicity Rabbit Hole: A Novel Framework to Bias Audit Large\n  Language Models","summary":"  This paper makes three contributions. First, it presents a generalizable,\nnovel framework dubbed \\textit{toxicity rabbit hole} that iteratively elicits\ntoxic content from a wide suite of large language models. Spanning a set of\n1,266 identity groups, we first conduct a bias audit of \\texttt{PaLM 2}\nguardrails presenting key insights. Next, we report generalizability across\nseveral other models. Through the elicited toxic content, we present a broad\nanalysis with a key emphasis on racism, antisemitism, misogyny, Islamophobia,\nhomophobia, and transphobia. Finally, driven by concrete examples, we discuss\npotential ramifications.\n","authors":["Arka Dutta","Adel Khorramrouz","Sujan Dutta","Ashiqur R. KhudaBukhsh"],"pdf_url":"https://arxiv.org/pdf/2309.06415v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00836v3","updated":"2024-03-31T01:02:55Z","published":"2023-10-02T01:00:50Z","title":"Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical\n  Reasoning Capabilities of Language Models","summary":"  Logical reasoning is fundamental for humans yet presents a substantial\nchallenge in the domain of Artificial Intelligence. Initially, researchers used\nKnowledge Representation and Reasoning (KR) systems that did not scale and\nrequired non-trivial manual effort. Recently, the emergence of large language\nmodels (LLMs) has demonstrated the ability to overcome various limitations of\nformal Knowledge Representation (KR) systems. Consequently, there's a growing\ninterest in using LLMs for logical reasoning via natural language. This work\nstrives to understand the proficiency of LLMs in logical reasoning by offering\na brief review of the latest progress in this area; with a focus on the logical\nreasoning datasets, tasks, and the methods adopted to utilize LLMs for\nreasoning. To offer a thorough analysis, we have compiled a benchmark titled\nLogiGLUE. This includes 24 varied datasets encompassing deductive, abductive,\nand inductive reasoning. Utilizing LogiGLUE as a foundation, we have trained an\ninstruction fine-tuned language model, resulting in LogiT5. We study\nsingle-task training, multi-task training, and \"chain-of-thought\" knowledge\ndistillation fine-tuning technique to assess the performance of model across\nthe different logical reasoning categories. We also assess various LLMs using\nLogiGLUE, and the findings indicate that LLMs excel most in abductive\nreasoning, followed by deductive reasoning, while they are least effective at\ninductive reasoning. We aim to shed light on the capabilities and potential\npathways for enhancing logical reasoning proficiency in LLMs, paving the way\nfor more advanced and nuanced developments in this critical field.\n","authors":["Man Luo","Shrinidhi Kumbhar","Ming shen","Mihir Parmar","Neeraj Varshney","Pratyay Banerjee","Somak Aditya","Chitta Baral"],"pdf_url":"https://arxiv.org/pdf/2310.00836v3.pdf","comment":"Work in progress"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2312.01998v2","updated":"2024-03-31T22:58:09Z","published":"2023-12-04T16:22:06Z","title":"Language-only Efficient Training of Zero-shot Composed Image Retrieval","summary":"  Composed image retrieval (CIR) task takes a composed query of image and text,\naiming to search relative images for both conditions. Conventional CIR\napproaches need a training dataset composed of triplets of query image, query\ntext, and target image, which is very expensive to collect. Several recent\nworks have worked on the zero-shot (ZS) CIR paradigm to tackle the issue\nwithout using pre-collected triplets. However, the existing ZS-CIR methods show\nlimited backbone scalability and generalizability due to the lack of diversity\nof the input texts during training. We propose a novel CIR framework, only\nusing language for its training. Our LinCIR (Language-only training for CIR)\ncan be trained only with text datasets by a novel self-supervision named\nself-masking projection (SMP). We project the text latent embedding to the\ntoken embedding space and construct a new text by replacing the keyword tokens\nof the original text. Then, we let the new and original texts have the same\nlatent embedding vector. With this simple strategy, LinCIR is surprisingly\nefficient and highly effective; LinCIR with CLIP ViT-G backbone is trained in\n48 minutes and shows the best ZS-CIR performances on four different CIR\nbenchmarks, CIRCO, GeneCIS, FashionIQ, and CIRR, even outperforming supervised\nmethod on FashionIQ. Code is available at https://github.com/navervision/lincir\n","authors":["Geonmo Gu","Sanghyuk Chun","Wonjae Kim","Yoohoon Kang","Sangdoo Yun"],"pdf_url":"https://arxiv.org/pdf/2312.01998v2.pdf","comment":"CVPR 2024 camera-ready; First two authors contributed equally; 17\n  pages, 3.1MB"},{"id":"http://arxiv.org/abs/2311.13958v2","updated":"2024-03-31T22:39:12Z","published":"2023-11-23T12:16:33Z","title":"Handling The Non-Smooth Challenge in Tensor SVD: A Multi-Objective\n  Tensor Recovery Framework","summary":"  Recently, numerous tensor singular value decomposition (t-SVD)-based tensor\nrecovery methods have shown promise in processing visual data, such as color\nimages and videos. However, these methods often suffer from severe performance\ndegradation when confronted with tensor data exhibiting non-smooth changes. It\nhas been commonly observed in real-world scenarios but ignored by the\ntraditional t-SVD-based methods. In this work, we introduce a novel tensor\nrecovery model with a learnable tensor nuclear norm to address such a\nchallenge. We develop a new optimization algorithm named the Alternating\nProximal Multiplier Method (APMM) to iteratively solve the proposed tensor\ncompletion model. Theoretical analysis demonstrates the convergence of the\nproposed APMM to the Karush-Kuhn-Tucker (KKT) point of the optimization\nproblem. In addition, we propose a multi-objective tensor recovery framework\nbased on APMM to efficiently explore the correlations of tensor data across its\nvarious dimensions, providing a new perspective on extending the t-SVD-based\nmethod to higher-order tensor cases. Numerical experiments demonstrated the\neffectiveness of the proposed method in tensor completion.\n","authors":["Jingjing Zheng","Wanglong Lu","Wenzhe Wang","Yankai Cao","Xiaoqin Zhang","Xianta Jiang"],"pdf_url":"https://arxiv.org/pdf/2311.13958v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.18822v2","updated":"2024-03-31T21:11:59Z","published":"2023-11-30T18:58:17Z","title":"ElasticDiffusion: Training-free Arbitrary Size Image Generation through\n  Global-Local Content Separation","summary":"  Diffusion models have revolutionized image generation in recent years, yet\nthey are still limited to a few sizes and aspect ratios. We propose\nElasticDiffusion, a novel training-free decoding method that enables pretrained\ntext-to-image diffusion models to generate images with various sizes.\nElasticDiffusion attempts to decouple the generation trajectory of a pretrained\nmodel into local and global signals. The local signal controls low-level pixel\ninformation and can be estimated on local patches, while the global signal is\nused to maintain overall structural consistency and is estimated with a\nreference image. We test our method on CelebA-HQ (faces) and LAION-COCO\n(objects/indoor/outdoor scenes). Our experiments and qualitative results show\nsuperior image coherence quality across aspect ratios compared to\nMultiDiffusion and the standard decoding strategy of Stable Diffusion. Project\npage: https://elasticdiffusion.github.io/\n","authors":["Moayed Haji-Ali","Guha Balakrishnan","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2311.18822v2.pdf","comment":"Accepted at CVPR 2024. Project Page:\n  https://elasticdiffusion.github.io/"},{"id":"http://arxiv.org/abs/2309.15204v2","updated":"2024-03-31T20:59:03Z","published":"2023-09-26T19:05:18Z","title":"CLRmatchNet: Enhancing Curved Lane Detection with Deep Matching Process","summary":"  Lane detection plays a crucial role in autonomous driving by providing vital\ndata to ensure safe navigation. Modern algorithms rely on anchor-based\ndetectors, which are then followed by a label-assignment process to categorize\ntraining detections as positive or negative instances based on learned\ngeometric attributes. Accurate label assignment has great impact on the model\nperformance, that is usually relying on a pre-defined classical cost function\nevaluating GT-prediction alignment. However, classical label assignment methods\nface limitations due to their reliance on predefined cost functions derived\nfrom low-dimensional models, potentially impacting their optimality. Our\nresearch introduces MatchNet, a deep learning submodule-based approach aimed at\nimproving the label assignment process. Integrated into a state-of-the-art lane\ndetection network such as the Cross Layer Refinement Network for Lane Detection\n(CLRNet), MatchNet replaces the conventional label assignment process with a\nsubmodule network. The integrated model, CLRmatchNet, surpasses CLRNet, showing\nsubstantial improvements in scenarios involving curved lanes, with remarkable\nimprovement across all backbones of +2.8% for ResNet34, +2.3% for ResNet101,\nand +2.96% for DLA34. In addition, it maintains or even improves comparable\nresults in other sections. Our method boosts the confidence level in lane\ndetection, allowing an increase in the confidence threshold. Our code is\navailable at: https://github.com/sapirkontente/CLRmatchNet.git\n","authors":["Sapir Kontente","Roy Orfaig","Ben-Zion Bobrovsky"],"pdf_url":"https://arxiv.org/pdf/2309.15204v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13716v2","updated":"2024-03-31T19:23:55Z","published":"2023-11-22T22:20:10Z","title":"DiverseNet: Decision Diversified Semi-supervised Semantic Segmentation\n  Networks for Remote Sensing Imagery","summary":"  Semi-supervised learning aims to help reduce the cost of the manual labelling\nprocess by leveraging valuable features extracted from a substantial pool of\nunlabeled data alongside a limited set of labelled data during the training\nphase. Since pixel-level manual labelling in large-scale remote sensing imagery\nis expensive, semi-supervised learning becomes an appropriate solution to this.\nHowever, most of the existing consistency learning frameworks based on network\nperturbation are very bulky. There is still a lack of lightweight and efficient\nperturbation methods to promote the diversity of features and the precision of\npseudo labels during training. In order to fill this gap, we propose DiverseNet\nwhich explores multi-head and multi-model semi-supervised learning algorithms\nby simultaneously enhancing precision and diversity during training. The two\nproposed methods in the DiverseNet family, namely DiverseHead and DiverseModel,\nboth achieve the better semantic segmentation performance in four widely\nutilised remote sensing imagery data sets compared to state-of-the-art\nsemi-supervised learning methods. Meanwhile, the proposed DiverseHead\narchitecture is simple and relatively lightweight in terms of parameter space\ncompared to the state-of-the-art methods whilst reaching high-performance\nresults for all the tested data sets.\n","authors":["Wanli Ma","Oktay Karakus","Paul L. Rosin"],"pdf_url":"https://arxiv.org/pdf/2311.13716v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08409v2","updated":"2024-03-31T19:01:07Z","published":"2024-01-16T14:49:26Z","title":"Faster ISNet for Background Bias Mitigation on Deep Neural Networks","summary":"  Bias or spurious correlations in image backgrounds can impact neural\nnetworks, causing shortcut learning (Clever Hans Effect) and hampering\ngeneralization to real-world data. ISNet, a recently introduced architecture,\nproposed the optimization of Layer-Wise Relevance Propagation (LRP, an\nexplanation technique) heatmaps, to mitigate the influence of backgrounds on\ndeep classifiers. However, ISNet's training time scales linearly with the\nnumber of classes in an application. Here, we propose reformulated\narchitectures whose training time becomes independent from this number.\nAdditionally, we introduce a concise and model-agnostic LRP implementation. We\nchallenge the proposed architectures using synthetic background bias, and\nCOVID-19 detection in chest X-rays, an application that commonly presents\nbackground bias. The networks hindered background attention and shortcut\nlearning, surpassing multiple state-of-the-art models on out-of-distribution\ntest datasets. Representing a potentially massive training speed improvement\nover ISNet, the proposed architectures introduce LRP optimization into a gamut\nof applications that the original model cannot feasibly handle.\n","authors":["Pedro R. A. S. Bassi","Sergio Decherchi","Andrea Cavalli"],"pdf_url":"https://arxiv.org/pdf/2401.08409v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10671v2","updated":"2024-03-31T18:37:10Z","published":"2023-12-17T10:07:03Z","title":"Open3DIS: Open-Vocabulary 3D Instance Segmentation with 2D Mask Guidance","summary":"  We introduce Open3DIS, a novel solution designed to tackle the problem of\nOpen-Vocabulary Instance Segmentation within 3D scenes. Objects within 3D\nenvironments exhibit diverse shapes, scales, and colors, making precise\ninstance-level identification a challenging task. Recent advancements in\nOpen-Vocabulary scene understanding have made significant strides in this area\nby employing class-agnostic 3D instance proposal networks for object\nlocalization and learning queryable features for each 3D mask. While these\nmethods produce high-quality instance proposals, they struggle with identifying\nsmall-scale and geometrically ambiguous objects. The key idea of our method is\na new module that aggregates 2D instance masks across frames and maps them to\ngeometrically coherent point cloud regions as high-quality object proposals\naddressing the above limitations. These are then combined with 3D\nclass-agnostic instance proposals to include a wide range of objects in the\nreal world. To validate our approach, we conducted experiments on three\nprominent datasets, including ScanNet200, S3DIS, and Replica, demonstrating\nsignificant performance gains in segmenting objects with diverse categories\nover the state-of-the-art approaches.\n","authors":["Phuc D. A. Nguyen","Tuan Duc Ngo","Chuang Gan","Evangelos Kalogerakis","Anh Tran","Cuong Pham","Khoi Nguyen"],"pdf_url":"https://arxiv.org/pdf/2312.10671v2.pdf","comment":"CVPR 2024. Project page: https://open3dis.github.io/"},{"id":"http://arxiv.org/abs/2312.02142v4","updated":"2024-03-31T18:11:18Z","published":"2023-12-04T18:58:40Z","title":"Object Recognition as Next Token Prediction","summary":"  We present an approach to pose object recognition as next token prediction.\nThe idea is to apply a language decoder that auto-regressively predicts the\ntext tokens from image embeddings to form labels. To ground this prediction\nprocess in auto-regression, we customize a non-causal attention mask for the\ndecoder, incorporating two key features: modeling tokens from different labels\nto be independent, and treating image tokens as a prefix. This masking\nmechanism inspires an efficient method - one-shot sampling - to simultaneously\nsample tokens of multiple labels in parallel and rank generated labels by their\nprobabilities during inference. To further enhance the efficiency, we propose a\nsimple strategy to construct a compact decoder by simply discarding the\nintermediate blocks of a pretrained language model. This approach yields a\ndecoder that matches the full model's performance while being notably more\nefficient. The code is available at https://github.com/kaiyuyue/nxtp\n","authors":["Kaiyu Yue","Bor-Chun Chen","Jonas Geiping","Hengduo Li","Tom Goldstein","Ser-Nam Lim"],"pdf_url":"https://arxiv.org/pdf/2312.02142v4.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2402.12289v3","updated":"2024-03-31T17:08:00Z","published":"2024-02-19T17:04:04Z","title":"DriveVLM: The Convergence of Autonomous Driving and Large\n  Vision-Language Models","summary":"  A primary hurdle of autonomous driving in urban environments is understanding\ncomplex and long-tail scenarios, such as challenging road conditions and\ndelicate human behaviors. We introduce DriveVLM, an autonomous driving system\nleveraging Vision-Language Models (VLMs) for enhanced scene understanding and\nplanning capabilities. DriveVLM integrates a unique combination of\nchain-of-thought (CoT) modules for scene description, scene analysis, and\nhierarchical planning. Furthermore, recognizing the limitations of VLMs in\nspatial reasoning and heavy computational requirements, we propose\nDriveVLM-Dual, a hybrid system that synergizes the strengths of DriveVLM with\nthe traditional autonomous driving pipeline. DriveVLM-Dual achieves robust\nspatial understanding and real-time inference speed. Extensive experiments on\nboth the nuScenes dataset and our SUP-AD dataset demonstrate the effectiveness\nof DriveVLM and the enhanced performance of DriveVLM-Dual, surpassing existing\nmethods in complex and unpredictable driving conditions.\n","authors":["Xiaoyu Tian","Junru Gu","Bailin Li","Yicheng Liu","Chenxu Hu","Yang Wang","Kun Zhan","Peng Jia","Xianpeng Lang","Hang Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.12289v3.pdf","comment":"Project Page: https://tsinghua-mars-lab.github.io/DriveVLM/"},{"id":"http://arxiv.org/abs/2311.14218v2","updated":"2024-03-31T17:05:15Z","published":"2023-11-23T22:27:31Z","title":"A New Benchmark and Model for Challenging Image Manipulation Detection","summary":"  The ability to detect manipulation in multimedia data is vital in digital\nforensics. Existing Image Manipulation Detection (IMD) methods are mainly based\non detecting anomalous features arisen from image editing or double compression\nartifacts. All existing IMD techniques encounter challenges when it comes to\ndetecting small tampered regions from a large image. Moreover,\ncompression-based IMD approaches face difficulties in cases of double\ncompression of identical quality factors. To investigate the State-of-The-Art\n(SoTA) IMD methods in those challenging conditions, we introduce a new\nChallenging Image Manipulation Detection (CIMD) benchmark dataset, which\nconsists of two subsets, for evaluating editing-based and compression-based IMD\nmethods, respectively. The dataset images were manually taken and tampered with\nhigh-quality annotations. In addition, we propose a new two-branch network\nmodel based on HRNet that can better detect both the image-editing and\ncompression artifacts in those challenging conditions. Extensive experiments on\nthe CIMD benchmark show that our model significantly outperforms SoTA IMD\nmethods on CIMD.\n","authors":["Zhenfei Zhang","Mingyang Li","Ming-Ching Chang"],"pdf_url":"https://arxiv.org/pdf/2311.14218v2.pdf","comment":"9 pages, 6 figures, 3 tabels. AAAI-24"},{"id":"http://arxiv.org/abs/2309.11281v3","updated":"2024-03-31T16:59:45Z","published":"2023-09-20T13:05:42Z","title":"Language-driven Object Fusion into Neural Radiance Fields with\n  Pose-Conditioned Dataset Updates","summary":"  Neural radiance field is an emerging rendering method that generates\nhigh-quality multi-view consistent images from a neural scene representation\nand volume rendering. Although neural radiance field-based techniques are\nrobust for scene reconstruction, their ability to add or remove objects remains\nlimited. This paper proposes a new language-driven approach for object\nmanipulation with neural radiance fields through dataset updates. Specifically,\nto insert a new foreground object represented by a set of multi-view images\ninto a background radiance field, we use a text-to-image diffusion model to\nlearn and generate combined images that fuse the object of interest into the\ngiven background across views. These combined images are then used for refining\nthe background radiance field so that we can render view-consistent images\ncontaining both the object and the background. To ensure view consistency, we\npropose a dataset updates strategy that prioritizes radiance field training\nwith camera views close to the already-trained views prior to propagating the\ntraining to remaining views. We show that under the same dataset updates\nstrategy, we can easily adapt our method for object insertion using data from\ntext-to-3D models as well as object removal. Experimental results show that our\nmethod generates photorealistic images of the edited scenes, and outperforms\nstate-of-the-art methods in 3D reconstruction and neural radiance field\nblending.\n","authors":["Ka Chun Shum","Jaeyeon Kim","Binh-Son Hua","Duc Thanh Nguyen","Sai-Kit Yeung"],"pdf_url":"https://arxiv.org/pdf/2309.11281v3.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2402.19276v4","updated":"2024-03-31T15:19:30Z","published":"2024-02-29T15:44:00Z","title":"Modular Blind Video Quality Assessment","summary":"  Blind video quality assessment (BVQA) plays a pivotal role in evaluating and\nimproving the viewing experience of end-users across a wide range of\nvideo-based platforms and services. Contemporary deep learning-based models\nprimarily analyze video content in its aggressively subsampled format, while\nbeing blind to the impact of the actual spatial resolution and frame rate on\nvideo quality. In this paper, we propose a modular BVQA model and a method of\ntraining it to improve its modularity. Our model comprises a base quality\npredictor, a spatial rectifier, and a temporal rectifier, responding to the\nvisual content and distortion, spatial resolution, and frame rate changes on\nvideo quality, respectively. During training, spatial and temporal rectifiers\nare dropped out with some probabilities to render the base quality predictor a\nstandalone BVQA model, which should work better with the rectifiers. Extensive\nexperiments on both professionally-generated content and user-generated content\nvideo databases show that our quality model achieves superior or comparable\nperformance to current methods. Additionally, the modularity of our model\noffers an opportunity to analyze existing video quality databases in terms of\ntheir spatial and temporal complexity.\n","authors":["Wen Wen","Mu Li","Yabin Zhang","Yiting Liao","Junlin Li","Li Zhang","Kede Ma"],"pdf_url":"https://arxiv.org/pdf/2402.19276v4.pdf","comment":"Accepted by CVPR 2024; Camera-ready version"},{"id":"http://arxiv.org/abs/2303.06797v2","updated":"2024-03-31T14:35:18Z","published":"2023-03-13T01:07:32Z","title":"Multi-Channel Orthogonal Transform-Based Perceptron Layers for Efficient\n  ResNets","summary":"  In this paper, we propose a set of transform-based neural network layers as\nan alternative to the $3\\times3$ Conv2D layers in Convolutional Neural Networks\n(CNNs). The proposed layers can be implemented based on orthogonal transforms\nsuch as the Discrete Cosine Transform (DCT), Hadamard transform (HT), and\nbiorthogonal Block Wavelet Transform (BWT). Furthermore, by taking advantage of\nthe convolution theorems, convolutional filtering operations are performed in\nthe transform domain using element-wise multiplications. Trainable\nsoft-thresholding layers, that remove noise in the transform domain, bring\nnonlinearity to the transform domain layers. Compared to the Conv2D layer,\nwhich is spatial-agnostic and channel-specific, the proposed layers are\nlocation-specific and channel-specific. Moreover, these proposed layers reduce\nthe number of parameters and multiplications significantly while improving the\naccuracy results of regular ResNets on the ImageNet-1K classification task.\nFurthermore, they can be inserted with a batch normalization layer before the\nglobal average pooling layer in the conventional ResNets as an additional layer\nto improve classification accuracy.\n","authors":["Hongyi Pan","Emadeldeen Hamdan","Xin Zhu","Salih Atici","Ahmet Enis Cetin"],"pdf_url":"https://arxiv.org/pdf/2303.06797v2.pdf","comment":"This work is accepted to IEEE Transactions on Neural Networks and\n  Learning Systems. The initial title is \"Orthogonal Transform Domain\n  Approaches for the Convolutional Layer\". We changed it to \"Multi-Channel\n  Orthogonal Transform-Based Perceptron Layers for Efficient ResNets\" based on\n  reviewer's comment. arXiv admin note: text overlap with arXiv:2211.08577"},{"id":"http://arxiv.org/abs/2403.14119v3","updated":"2024-03-31T13:36:54Z","published":"2024-03-21T04:08:29Z","title":"C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via\n  Text Feature Dispersion","summary":"  In deep learning, test-time adaptation has gained attention as a method for\nmodel fine-tuning without the need for labeled data. A prime exemplification is\nthe recently proposed test-time prompt tuning for large-scale vision-language\nmodels such as CLIP. Unfortunately, these prompts have been mainly developed to\nimprove accuracy, overlooking the importance of calibration, which is a crucial\naspect for quantifying prediction uncertainty. However, traditional calibration\nmethods rely on substantial amounts of labeled data, making them impractical\nfor test-time scenarios. To this end, this paper explores calibration during\ntest-time prompt tuning by leveraging the inherent properties of CLIP. Through\na series of observations, we find that the prompt choice significantly affects\nthe calibration in CLIP, where the prompts leading to higher text feature\ndispersion result in better-calibrated predictions. Introducing the Average\nText Feature Dispersion (ATFD), we establish its relationship with calibration\nerror and present a novel method, Calibrated Test-time Prompt Tuning (C-TPT),\nfor optimizing prompts during test-time with enhanced calibration. Through\nextensive experiments on different CLIP architectures and datasets, we show\nthat C-TPT can effectively improve the calibration of test-time prompt tuning\nwithout needing labeled data. The code is publicly accessible at\nhttps://github.com/hee-suk-yoon/C-TPT.\n","authors":["Hee Suk Yoon","Eunseop Yoon","Joshua Tian Jin Tee","Mark Hasegawa-Johnson","Yingzhen Li","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2403.14119v3.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.19456v2","updated":"2024-03-31T13:26:11Z","published":"2024-03-28T14:27:36Z","title":"Break-for-Make: Modular Low-Rank Adaptations for Composable\n  Content-Style Customization","summary":"  Personalized generation paradigms empower designers to customize visual\nintellectual properties with the help of textual descriptions by tuning or\nadapting pre-trained text-to-image models on a few images. Recent works explore\napproaches for concurrently customizing both content and detailed visual style\nappearance. However, these existing approaches often generate images where the\ncontent and style are entangled. In this study, we reconsider the customization\nof content and style concepts from the perspective of parameter space\nconstruction. Unlike existing methods that utilize a shared parameter space for\ncontent and style, we propose a learning framework that separates the parameter\nspace to facilitate individual learning of content and style, thereby enabling\ndisentangled content and style. To achieve this goal, we introduce \"partly\nlearnable projection\" (PLP) matrices to separate the original adapters into\ndivided sub-parameter spaces. We propose \"break-for-make\" customization\nlearning pipeline based on PLP, which is simple yet effective. We break the\noriginal adapters into \"up projection\" and \"down projection\", train content and\nstyle PLPs individually with the guidance of corresponding textual prompts in\nthe separate adapters, and maintain generalization by employing a\nmulti-correspondence projection learning strategy. Based on the adapters broken\napart for separate training content and style, we then make the entity\nparameter space by reconstructing the content and style PLPs matrices, followed\nby fine-tuning the combined adapter to generate the target object with the\ndesired appearance. Experiments on various styles, including textures,\nmaterials, and artistic style, show that our method outperforms\nstate-of-the-art single/multiple concept learning pipelines in terms of\ncontent-style-prompt alignment.\n","authors":["Yu Xu","Fan Tang","Juan Cao","Yuxin Zhang","Oliver Deussen","Weiming Dong","Jintao Li","Tong-Yee Lee"],"pdf_url":"https://arxiv.org/pdf/2403.19456v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.18254v2","updated":"2024-03-31T13:13:37Z","published":"2023-11-30T05:05:38Z","title":"Sketch Input Method Editor: A Comprehensive Dataset and Methodology for\n  Systematic Input Recognition","summary":"  With the recent surge in the use of touchscreen devices, free-hand sketching\nhas emerged as a promising modality for human-computer interaction. While\nprevious research has focused on tasks such as recognition, retrieval, and\ngeneration of familiar everyday objects, this study aims to create a Sketch\nInput Method Editor (SketchIME) specifically designed for a professional C4I\nsystem. Within this system, sketches are utilized as low-fidelity prototypes\nfor recommending standardized symbols in the creation of comprehensive\nsituation maps. This paper also presents a systematic dataset comprising 374\nspecialized sketch types, and proposes a simultaneous recognition and\nsegmentation architecture with multilevel supervision between recognition and\nsegmentation to improve performance and enhance interpretability. By\nincorporating few-shot domain adaptation and class-incremental learning, the\nnetwork's ability to adapt to new users and extend to new task-specific classes\nis significantly enhanced. Results from experiments conducted on both the\nproposed dataset and the SPG dataset illustrate the superior performance of the\nproposed architecture. Our dataset and code are publicly available at\nhttps://github.com/GuangmingZhu/SketchIME.\n","authors":["Guangming Zhu","Siyuan Wang","Qing Cheng","Kelong Wu","Hao Li","Liang Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.18254v2.pdf","comment":"The paper has been accepted by ACM Multimedia 2023"},{"id":"http://arxiv.org/abs/2308.08443v2","updated":"2024-03-31T12:39:48Z","published":"2023-08-16T15:51:05Z","title":"High-Fidelity Lake Extraction via Two-Stage Prompt Enhancement:\n  Establishing a Novel Baseline and Benchmark","summary":"  Lake extraction from remote sensing imagery is a complex challenge due to the\nvaried lake shapes and data noise. Current methods rely on multispectral image\ndatasets, making it challenging to learn lake features accurately from pixel\narrangements. This, in turn, affects model learning and the creation of\naccurate segmentation masks. This paper introduces a prompt-based dataset\nconstruction approach that provides approximate lake locations using point,\nbox, and mask prompts. We also propose a two-stage prompt enhancement\nframework, LEPrompter, with prompt-based and prompt-free stages during\ntraining. The prompt-based stage employs a prompt encoder to extract prior\ninformation, integrating prompt tokens and image embedding through self- and\ncross-attention in the prompt decoder. Prompts are deactivated to ensure\nindependence during inference, enabling automated lake extraction without\nintroducing additional parameters and GFlops. Extensive experiments showcase\nperformance improvements of our proposed approach compared to the previous\nstate-of-the-art method. The source code is available at\nhttps://github.com/BastianChen/LEPrompter.\n","authors":["Ben Chen","Xuechao Zou","Kai Li","Yu Zhang","Junliang Xing","Pin Tao"],"pdf_url":"https://arxiv.org/pdf/2308.08443v2.pdf","comment":"Accepted by ICME 2024"},{"id":"http://arxiv.org/abs/2308.01813v2","updated":"2024-03-31T12:27:16Z","published":"2023-08-03T15:21:08Z","title":"Deep Neural Networks Fused with Textures for Image Classification","summary":"  Fine-grained image classification (FGIC) is a challenging task in computer\nvision for due to small visual differences among inter-subcategories, but,\nlarge intra-class variations. Deep learning methods have achieved remarkable\nsuccess in solving FGIC. In this paper, we propose a fusion approach to address\nFGIC by combining global texture with local patch-based information. The first\npipeline extracts deep features from various fixed-size non-overlapping patches\nand encodes features by sequential modelling using the long short-term memory\n(LSTM). Another path computes image-level textures at multiple scales using the\nlocal binary patterns (LBP). The advantages of both streams are integrated to\nrepresent an efficient feature vector for image classification. The method is\ntested on eight datasets representing the human faces, skin lesions, food\ndishes, marine lives, etc. using four standard backbone CNNs. Our method has\nattained better classification accuracy over existing methods with notable\nmargins.\n","authors":["Asish Bera","Debotosh Bhattacharjee","Mita Nasipuri"],"pdf_url":"https://arxiv.org/pdf/2308.01813v2.pdf","comment":"14 pages, 6 figures, 4 tables, conference"},{"id":"http://arxiv.org/abs/2402.12677v2","updated":"2024-03-31T12:18:51Z","published":"2024-02-20T02:54:03Z","title":"Object-level Geometric Structure Preserving for Natural Image Stitching","summary":"  The topic of stitching images with globally natural structures holds\nparamount significance. Current methodologies exhibit the ability to preserve\nlocal geometric structures, yet fall short in maintaining relationships between\nthese geometric structures. In this paper, we endeavor to safeguard the\noverall, OBJect-level structures within images based on Global Similarity\nPrior, while concurrently mitigating distortion and ghosting artifacts with\nOBJ-GSP. Our approach leverages the Segment Anything Model to extract geometric\nstructures with semantic information, enhancing the algorithm's ability to\npreserve objects in a manner that aligns more intuitively with human\nperception. We seek to identify spatial constraints that govern the\nrelationships between various geometric boundaries. Recognizing that multiple\ngeometric boundaries collectively define complete objects, we employ triangular\nmeshes to safeguard not only individual geometric structures but also the\noverall shapes of objects within the images. Empirical evaluations across\nmultiple image stitching datasets demonstrate that our method establishes a new\nstate-of-the-art benchmark in image stitching. Our implementation and dataset\nis publicly available at https://github.com/RussRobin/OBJ-GSP .\n","authors":["Wenxiao Cai","Wankou Yang"],"pdf_url":"https://arxiv.org/pdf/2402.12677v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.04192v4","updated":"2024-03-31T12:10:24Z","published":"2023-07-09T14:54:30Z","title":"Self-Adaptive Sampling for Efficient Video Question-Answering on\n  Image--Text Models","summary":"  Video question-answering is a fundamental task in the field of video\nunderstanding. Although current vision--language models (VLMs) equipped with\nVideo Transformers have enabled temporal modeling and yielded superior results,\nthey are at the cost of huge computational power and thus too expensive to\ndeploy in real-time application scenarios. An economical workaround only\nsamples a small portion of frames to represent the main content of that video\nand tune an image--text model on these sampled frames. Recent video\nunderstanding models usually randomly sample a set of frames or clips,\nregardless of internal correlations between their visual contents, nor their\nrelevance to the problem. We argue that such kinds of aimless sampling may omit\nthe key frames from which the correct answer can be deduced, and the situation\ngets worse when the sampling sparsity increases, which always happens as the\nvideo lengths increase. To mitigate this issue, we propose two frame sampling\nstrategies, namely the most domain frames (MDF) and most implied frames (MIF),\nto maximally preserve those frames that are most likely vital to the given\nquestions. MDF passively minimizes the risk of key frame omission in a\nbootstrap manner, while MIS actively searches key frames customized for each\nvideo--question pair with the assistance of auxiliary models. The experimental\nresults on three public datasets from three advanced VLMs (CLIP, GIT and\nAll-in-one) demonstrate that our proposed strategies can boost the performance\nfor image-text pretrained models. The source codes pertaining to the method\nproposed in this paper are publicly available at\nhttps://github.com/declare-lab/sas-vqa.\n","authors":["Wei Han","Hui Chen","Min-Yen Kan","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2307.04192v4.pdf","comment":"13 pages, 7 figures, accepted to Findings of NAACL 2024"},{"id":"http://arxiv.org/abs/2311.12588v2","updated":"2024-03-31T12:06:55Z","published":"2023-11-21T13:21:22Z","title":"HiPose: Hierarchical Binary Surface Encoding and Correspondence Pruning\n  for RGB-D 6DoF Object Pose Estimation","summary":"  In this work, we present a novel dense-correspondence method for 6DoF object\npose estimation from a single RGB-D image. While many existing data-driven\nmethods achieve impressive performance, they tend to be time-consuming due to\ntheir reliance on rendering-based refinement approaches. To circumvent this\nlimitation, we present HiPose, which establishes 3D-3D correspondences in a\ncoarse-to-fine manner with a hierarchical binary surface encoding. Unlike\nprevious dense-correspondence methods, we estimate the correspondence surface\nby employing point-to-surface matching and iteratively constricting the surface\nuntil it becomes a correspondence point while gradually removing outliers.\nExtensive experiments on public benchmarks LM-O, YCB-V, and T-Less demonstrate\nthat our method surpasses all refinement-free methods and is even on par with\nexpensive refinement-based approaches. Crucially, our approach is\ncomputationally efficient and enables real-time critical applications with high\naccuracy requirements.\n","authors":["Yongliang Lin","Yongzhi Su","Praveen Nathan","Sandeep Inuganti","Yan Di","Martin Sundermeyer","Fabian Manhardt","Didier Stricke","Jason Rambach","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.12588v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2302.02314v4","updated":"2024-03-31T11:58:28Z","published":"2023-02-05T06:27:45Z","title":"CECT: Controllable Ensemble CNN and Transformer for COVID-19 Image\n  Classification","summary":"  The COVID-19 pandemic has resulted in hundreds of million cases and numerous\ndeaths worldwide. Here, we develop a novel classification network CECT by\ncontrollable ensemble convolutional neural network and transformer to provide a\ntimely and accurate COVID-19 diagnosis. The CECT is composed of a parallel\nconvolutional encoder block, an aggregate transposed-convolutional decoder\nblock, and a windowed attention classification block. Each block captures\nfeatures at different scales from 28 $\\times$ 28 to 224 $\\times$ 224 from the\ninput, composing enriched and comprehensive information. Different from\nexisting methods, our CECT can capture features at both multi-local and global\nscales without any sophisticated module design. Moreover, the contribution of\nlocal features at different scales can be controlled with the proposed ensemble\ncoefficients. We evaluate CECT on two public COVID-19 datasets and it reaches\nthe highest accuracy of 98.1% in the intra-dataset evaluation, outperforming\nexisting state-of-the-art methods. Moreover, the developed CECT achieves an\naccuracy of 90.9% on the unseen dataset in the inter-dataset evaluation,\nshowing extraordinary generalization ability. With remarkable feature capture\nability and generalization ability, we believe CECT can be extended to other\nmedical scenarios as a powerful diagnosis tool. Code is available at\nhttps://github.com/NUS-Tim/CECT.\n","authors":["Zhaoshan Liu","Lei Shen"],"pdf_url":"https://arxiv.org/pdf/2302.02314v4.pdf","comment":"Computers in Biology and Medicine Accepted"},{"id":"http://arxiv.org/abs/2303.11797v2","updated":"2024-03-31T11:53:55Z","published":"2023-03-21T12:28:21Z","title":"CAT-Seg: Cost Aggregation for Open-Vocabulary Semantic Segmentation","summary":"  Open-vocabulary semantic segmentation presents the challenge of labeling each\npixel within an image based on a wide range of text descriptions. In this work,\nwe introduce a novel cost-based approach to adapt vision-language foundation\nmodels, notably CLIP, for the intricate task of semantic segmentation. Through\naggregating the cosine similarity score, i.e., the cost volume between image\nand text embeddings, our method potently adapts CLIP for segmenting seen and\nunseen classes by fine-tuning its encoders, addressing the challenges faced by\nexisting methods in handling unseen classes. Building upon this, we explore\nmethods to effectively aggregate the cost volume considering its multi-modal\nnature of being established between image and text embeddings. Furthermore, we\nexamine various methods for efficiently fine-tuning CLIP.\n","authors":["Seokju Cho","Heeseong Shin","Sunghwan Hong","Anurag Arnab","Paul Hongsuck Seo","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2303.11797v2.pdf","comment":"Accepted to CVPR 2024. Project page:\n  https://ku-cvlab.github.io/CAT-Seg/"},{"id":"http://arxiv.org/abs/2310.14159v3","updated":"2024-03-31T10:51:06Z","published":"2023-10-22T03:01:38Z","title":"Can Language Models Laugh at YouTube Short-form Videos?","summary":"  As short-form funny videos on social networks are gaining popularity, it\nbecomes demanding for AI models to understand them for better communication\nwith humans. Unfortunately, previous video humor datasets target specific\ndomains, such as speeches or sitcoms, and mostly focus on verbal cues. We\ncurate a user-generated dataset of 10K multimodal funny videos from YouTube,\ncalled ExFunTube. Using a video filtering pipeline with GPT-3.5, we verify both\nverbal and visual elements contributing to humor. After filtering, we annotate\neach video with timestamps and text explanations for funny moments. Our\nExFunTube is unique over existing datasets in that our videos cover a wide\nrange of domains with various types of humor that necessitate a multimodal\nunderstanding of the content. Also, we develop a zero-shot video-to-text\nprompting to maximize video humor understanding of large language models\n(LLMs). With three different evaluation methods using automatic scores,\nrationale quality experiments, and human evaluations, we show that our\nprompting significantly improves LLMs' ability for humor explanation.\n","authors":["Dayoon Ko","Sangho Lee","Gunhee Kim"],"pdf_url":"https://arxiv.org/pdf/2310.14159v3.pdf","comment":"EMNLP 2023; references added"},{"id":"http://arxiv.org/abs/2312.04553v2","updated":"2024-03-31T10:27:03Z","published":"2023-12-07T18:59:21Z","title":"SPIDeRS: Structured Polarization for Invisible Depth and Reflectance\n  Sensing","summary":"  Can we capture shape and reflectance in stealth? Such capability would be\nvaluable for many application domains in vision, xR, robotics, and HCI. We\nintroduce structured polarization for invisible depth and reflectance sensing\n(SPIDeRS), the first depth and reflectance sensing method using patterns of\npolarized light. The key idea is to modulate the angle of linear polarization\n(AoLP) of projected light at each pixel. The use of polarization makes it\ninvisible and lets us recover not only depth but also directly surface normals\nand even reflectance. We implement SPIDeRS with a liquid crystal spatial light\nmodulator (SLM) and a polarimetric camera. We derive a novel method for\nrobustly extracting the projected structured polarization pattern from the\npolarimetric object appearance. We evaluate the effectiveness of SPIDeRS by\napplying it to a number of real-world objects. The results show that our method\nsuccessfully reconstructs object shapes of various materials and is robust to\ndiffuse reflection and ambient light. We also demonstrate relighting using\nrecovered surface normals and reflectance. We believe SPIDeRS opens a new\navenue of polarization use in visual sensing.\n","authors":["Tomoki Ichikawa","Shohei Nobuhara","Ko Nishino"],"pdf_url":"https://arxiv.org/pdf/2312.04553v2.pdf","comment":"to be published in CVPR 2024"},{"id":"http://arxiv.org/abs/2312.01196v2","updated":"2024-03-31T10:20:37Z","published":"2023-12-02T18:06:24Z","title":"Neural Parametric Gaussians for Monocular Non-Rigid Object\n  Reconstruction","summary":"  Reconstructing dynamic objects from monocular videos is a severely\nunderconstrained and challenging problem, and recent work has approached it in\nvarious directions. However, owing to the ill-posed nature of this problem,\nthere has been no solution that can provide consistent, high-quality novel\nviews from camera positions that are significantly different from the training\nviews. In this work, we introduce Neural Parametric Gaussians (NPGs) to take on\nthis challenge by imposing a two-stage approach: first, we fit a low-rank\nneural deformation model, which then is used as regularization for non-rigid\nreconstruction in the second stage. The first stage learns the object's\ndeformations such that it preserves consistency in novel views. The second\nstage obtains high reconstruction quality by optimizing 3D Gaussians that are\ndriven by the coarse model. To this end, we introduce a local 3D Gaussian\nrepresentation, where temporally shared Gaussians are anchored in and deformed\nby local oriented volumes. The resulting combined model can be rendered as\nradiance fields, resulting in high-quality photo-realistic reconstructions of\nthe non-rigidly deforming objects. We demonstrate that NPGs achieve superior\nresults compared to previous works, especially in challenging scenarios with\nfew multi-view cues.\n","authors":["Devikalyan Das","Christopher Wewer","Raza Yunus","Eddy Ilg","Jan Eric Lenssen"],"pdf_url":"https://arxiv.org/pdf/2312.01196v2.pdf","comment":"Accepted at CVPR 2024 | Project Website:\n  https://geometric-rl.mpi-inf.mpg.de/npg"},{"id":"http://arxiv.org/abs/2302.10306v2","updated":"2024-03-31T10:01:44Z","published":"2023-01-25T11:00:32Z","title":"Deep Convolutional Framelet Denoising for Panoramic by Mixed Wavelet\n  Integration","summary":"  Enhancing quality and removing noise during preprocessing is one of the most\ncritical steps in image processing. X-ray images are created by photons\ncolliding with atoms and the variation in scattered noise absorption. This\nnoise leads to a deterioration in the graph's medical quality and, at times,\nresults in repetition, thereby increasing the patient's effective dose. One of\nthe most critical challenges in this area has consistently been lowering the\nimage noise. Techniques like BM3d, low-pass filters, and Autoencoder have taken\nthis step. Owing to their structural design and high rate of repetition, neural\nnetworks employing diverse architectures have, over the past decade, achieved\nnoise reduction with satisfactory outcomes, surpassing the traditional BM3D and\nlow-pass filters. The combination of the Hankel matrix with neural networks\nrepresents one of these configurations. The Hankel matrix aims to identify a\nlocal circle by separating individual values into local and non-local\ncomponents, utilizing a non-local matrix. A non-local matrix can be created\nusing the wave or DCT. This paper suggests integrating the waveform with the\nDaubechies (D4) wavelet due to its higher energy concentration and employs the\nu-Net neural network architecture, which incorporates the waveform exclusively\nat each stage. The outcomes were evaluated using the PSNR and SSIM criteria,\nand the outcomes were verified by using various waves. The effectiveness of a\none-wave network has increased from 0.5% to 1.2%, according to studies done on\nother datasets.\n","authors":["Masoud Shahraki Mohammadi","Seyed Javad Seyed Mahdavi Chabok"],"pdf_url":"https://arxiv.org/pdf/2302.10306v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08255v3","updated":"2024-03-31T09:33:50Z","published":"2023-12-13T16:18:40Z","title":"OCTDL: Optical Coherence Tomography Dataset for Image-Based Deep\n  Learning Methods","summary":"  Optical coherence tomography (OCT) is a non-invasive imaging technique with\nextensive clinical applications in ophthalmology. OCT enables the visualization\nof the retinal layers, playing a vital role in the early detection and\nmonitoring of retinal diseases. OCT uses the principle of light wave\ninterference to create detailed images of the retinal microstructures, making\nit a valuable tool for diagnosing ocular conditions. This work presents an\nopen-access OCT dataset (OCTDL) comprising over 2000 OCT images labeled\naccording to disease group and retinal pathology. The dataset consists of OCT\nrecords of patients with Age-related Macular Degeneration (AMD), Diabetic\nMacular Edema (DME), Epiretinal Membrane (ERM), Retinal Artery Occlusion (RAO),\nRetinal Vein Occlusion (RVO), and Vitreomacular Interface Disease (VID). The\nimages were acquired with an Optovue Avanti RTVue XR using raster scanning\nprotocols with dynamic scan length and image resolution. Each retinal b-scan\nwas acquired by centering on the fovea and interpreted and cataloged by an\nexperienced retinal specialist. In this work, we applied Deep Learning\nclassification techniques to this new open-access dataset.\n","authors":["Mikhail Kulyabin","Aleksei Zhdanov","Anastasia Nikiforova","Andrey Stepichev","Anna Kuznetsova","Mikhail Ronkin","Vasilii Borisov","Alexander Bogachev","Sergey Korotkich","Paul A Constable","Andreas Maier"],"pdf_url":"https://arxiv.org/pdf/2312.08255v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05869v2","updated":"2024-03-31T09:31:56Z","published":"2024-02-08T17:57:59Z","title":"Adaptive Surface Normal Constraint for Geometric Estimation from\n  Monocular Images","summary":"  We introduce a novel approach to learn geometries such as depth and surface\nnormal from images while incorporating geometric context. The difficulty of\nreliably capturing geometric context in existing methods impedes their ability\nto accurately enforce the consistency between the different geometric\nproperties, thereby leading to a bottleneck of geometric estimation quality. We\ntherefore propose the Adaptive Surface Normal (ASN) constraint, a simple yet\nefficient method. Our approach extracts geometric context that encodes the\ngeometric variations present in the input image and correlates depth estimation\nwith geometric constraints. By dynamically determining reliable local geometry\nfrom randomly sampled candidates, we establish a surface normal constraint,\nwhere the validity of these candidates is evaluated using the geometric\ncontext. Furthermore, our normal estimation leverages the geometric context to\nprioritize regions that exhibit significant geometric variations, which makes\nthe predicted normals accurately capture intricate and detailed geometric\ninformation. Through the integration of geometric context, our method unifies\ndepth and surface normal estimations within a cohesive framework, which enables\nthe generation of high-quality 3D geometry from images. We validate the\nsuperiority of our approach over state-of-the-art methods through extensive\nevaluations and comparisons on diverse indoor and outdoor datasets, showcasing\nits efficiency and robustness.\n","authors":["Xiaoxiao Long","Yuhang Zheng","Yupeng Zheng","Beiwen Tian","Cheng Lin","Lingjie Liu","Hao Zhao","Guyue Zhou","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2402.05869v2.pdf","comment":"Accepted by TPAMI. arXiv admin note: substantial text overlap with\n  arXiv:2103.15483"},{"id":"http://arxiv.org/abs/2311.15672v2","updated":"2024-03-31T09:10:24Z","published":"2023-11-27T10:01:31Z","title":"HAVE-FUN: Human Avatar Reconstruction from Few-Shot Unconstrained Images","summary":"  As for human avatar reconstruction, contemporary techniques commonly\nnecessitate the acquisition of costly data and struggle to achieve satisfactory\nresults from a small number of casual images. In this paper, we investigate\nthis task from a few-shot unconstrained photo album. The reconstruction of\nhuman avatars from such data sources is challenging because of limited data\namount and dynamic articulated poses. For handling dynamic data, we integrate a\nskinning mechanism with deep marching tetrahedra (DMTet) to form a drivable\ntetrahedral representation, which drives arbitrary mesh topologies generated by\nthe DMTet for the adaptation of unconstrained images. To effectively mine\ninstructive information from few-shot data, we devise a two-phase optimization\nmethod with few-shot reference and few-shot guidance. The former focuses on\naligning avatar identity with reference images, while the latter aims to\ngenerate plausible appearances for unseen regions. Overall, our framework,\ncalled HaveFun, can undertake avatar reconstruction, rendering, and animation.\nExtensive experiments on our developed benchmarks demonstrate that HaveFun\nexhibits substantially superior performance in reconstructing the human body\nand hand. Project website: https://seanchenxy.github.io/HaveFunWeb/.\n","authors":["Xihe Yang","Xingyu Chen","Daiheng Gao","Shaohui Wang","Xiaoguang Han","Baoyuan Wang"],"pdf_url":"https://arxiv.org/pdf/2311.15672v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15070v2","updated":"2024-03-31T08:36:57Z","published":"2023-08-29T07:11:52Z","title":"DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior","summary":"  We present DiffBIR, a general restoration pipeline that could handle\ndifferent blind image restoration tasks in a unified framework. DiffBIR\ndecouples blind image restoration problem into two stages: 1) degradation\nremoval: removing image-independent content; 2) information regeneration:\ngenerating the lost image content. Each stage is developed independently but\nthey work seamlessly in a cascaded manner. In the first stage, we use\nrestoration modules to remove degradations and obtain high-fidelity restored\nresults. For the second stage, we propose IRControlNet that leverages the\ngenerative ability of latent diffusion models to generate realistic details.\nSpecifically, IRControlNet is trained based on specially produced condition\nimages without distracting noisy content for stable generation performance.\nMoreover, we design a region-adaptive restoration guidance that can modify the\ndenoising process during inference without model re-training, allowing users to\nbalance realness and fidelity through a tunable guidance scale. Extensive\nexperiments have demonstrated DiffBIR's superiority over state-of-the-art\napproaches for blind image super-resolution, blind face restoration and blind\nimage denoising tasks on both synthetic and real-world datasets. The code is\navailable at https://github.com/XPixelGroup/DiffBIR.\n","authors":["Xinqi Lin","Jingwen He","Ziyan Chen","Zhaoyang Lyu","Bo Dai","Fanghua Yu","Wanli Ouyang","Yu Qiao","Chao Dong"],"pdf_url":"https://arxiv.org/pdf/2308.15070v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.13474v2","updated":"2024-03-31T08:12:21Z","published":"2022-08-29T10:19:10Z","title":"Prompt Tuning with Soft Context Sharing for Vision-Language Models","summary":"  Vision-language models have recently shown great potential on many tasks in\ncomputer vision. Meanwhile, prior work demonstrates prompt tuning designed for\nvision-language models could acquire superior performance on few-shot image\nrecognition compared to linear probe, a strong baseline. In practice, many\nfew-shot tasks are inherently correlated, particularly within specialized\ndomains. However, such information is overlooked previously. Inspired by the\nfact that modeling task relationship by multi-task learning can usually boost\nperformance, we propose a novel method SoftCPT (Soft Context Sharing for Prompt\nTuning) to tune pre-trained vision-language models on multiple target few-shot\ntasks jointly. Specifically, we design a task-shared meta network to generate\nprompt context for each task using task name together with a learnable task\ncontext as input. The parameters of this meta network as well as the task\ncontext are tuned on the joint training set of all tasks. As such, the prompt\ncontext of all tasks will be shared in a soft manner. Extensive experiments\nacross four multi-task few-shot datasets covering 44 tasks and 1593 categories\ndemonstrate that SoftCPT significantly outperforms single-task prompt tuning\nmethods, highlighting the effectiveness of multi-task learning for\nvision-language prompt tuning. Code is available at\nhttps://github.com/kding1225/softcpt.\n","authors":["Kun Ding","Ying Wang","Pengzhang Liu","Qiang Yu","Haojian Zhang","Shiming Xiang","Chunhong Pan"],"pdf_url":"https://arxiv.org/pdf/2208.13474v2.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2311.16096v3","updated":"2024-03-31T08:06:12Z","published":"2023-11-27T18:59:04Z","title":"Animatable Gaussians: Learning Pose-dependent Gaussian Maps for\n  High-fidelity Human Avatar Modeling","summary":"  Modeling animatable human avatars from RGB videos is a long-standing and\nchallenging problem. Recent works usually adopt MLP-based neural radiance\nfields (NeRF) to represent 3D humans, but it remains difficult for pure MLPs to\nregress pose-dependent garment details. To this end, we introduce Animatable\nGaussians, a new avatar representation that leverages powerful 2D CNNs and 3D\nGaussian splatting to create high-fidelity avatars. To associate 3D Gaussians\nwith the animatable avatar, we learn a parametric template from the input\nvideos, and then parameterize the template on two front \\& back canonical\nGaussian maps where each pixel represents a 3D Gaussian. The learned template\nis adaptive to the wearing garments for modeling looser clothes like dresses.\nSuch template-guided 2D parameterization enables us to employ a powerful\nStyleGAN-based CNN to learn the pose-dependent Gaussian maps for modeling\ndetailed dynamic appearances. Furthermore, we introduce a pose projection\nstrategy for better generalization given novel poses. Overall, our method can\ncreate lifelike avatars with dynamic, realistic and generalized appearances.\nExperiments show that our method outperforms other state-of-the-art approaches.\nCode: https://github.com/lizhe00/AnimatableGaussians\n","authors":["Zhe Li","Zerong Zheng","Lizhen Wang","Yebin Liu"],"pdf_url":"https://arxiv.org/pdf/2311.16096v3.pdf","comment":"Accepted by CVPR 2024, Projectpage:\n  https://animatable-gaussians.github.io/, Code:\n  https://github.com/lizhe00/AnimatableGaussians"},{"id":"http://arxiv.org/abs/2210.09846v3","updated":"2024-03-31T07:50:22Z","published":"2022-10-15T11:00:54Z","title":"G-PECNet: Towards a Generalizable Pedestrian Trajectory Prediction\n  System","summary":"  Navigating dynamic physical environments without obstructing or damaging\nhuman assets is of quintessential importance for social robots. In this work,\nwe solve autonomous drone navigation's sub-problem of predicting out-of-domain\nhuman and agent trajectories using a deep generative model. Our method:\nGeneral-PECNet or G-PECNet observes an improvement of 9.5\\% on the Final\nDisplacement Error (FDE) on 2020's benchmark: PECNet through a combination of\narchitectural improvements inspired by periodic activation functions and\nsynthetic trajectory (data) augmentations using Hidden Markov Models (HMMs) and\nReinforcement Learning (RL). Additionally, we propose a simple\ngeometry-inspired metric for trajectory non-linearity and outlier detection,\nhelpful for the task. Code available at\nhttps://github.com/Aryan-Garg/PECNet-Pedestrian-Trajectory-Prediction.git\n","authors":["Aryan Garg","Renu M. Rameshan"],"pdf_url":"https://arxiv.org/pdf/2210.09846v3.pdf","comment":"Notable ICLR Tiny Paper 2024"},{"id":"http://arxiv.org/abs/2402.09989v3","updated":"2024-03-31T07:47:59Z","published":"2024-02-15T14:54:33Z","title":"LLMs as Bridges: Reformulating Grounded Multimodal Named Entity\n  Recognition","summary":"  Grounded Multimodal Named Entity Recognition (GMNER) is a nascent multimodal\ntask that aims to identify named entities, entity types and their corresponding\nvisual regions. GMNER task exhibits two challenging properties: 1) The weak\ncorrelation between image-text pairs in social media results in a significant\nportion of named entities being ungroundable. 2) There exists a distinction\nbetween coarse-grained referring expressions commonly used in similar tasks\n(e.g., phrase localization, referring expression comprehension) and\nfine-grained named entities. In this paper, we propose RiVEG, a unified\nframework that reformulates GMNER into a joint MNER-VE-VG task by leveraging\nlarge language models (LLMs) as a connecting bridge. This reformulation brings\ntwo benefits: 1) It maintains the optimal MNER performance and eliminates the\nneed for employing object detection methods to pre-extract regional features,\nthereby naturally addressing two major limitations of existing GMNER methods.\n2) The introduction of entity expansion expression and Visual Entailment (VE)\nModule unifies Visual Grounding (VG) and Entity Grounding (EG). It enables\nRiVEG to effortlessly inherit the Visual Entailment and Visual Grounding\ncapabilities of any current or prospective multimodal pretraining models.\nExtensive experiments demonstrate that RiVEG outperforms state-of-the-art\nmethods on the existing GMNER dataset and achieves absolute leads of 10.65%,\n6.21%, and 8.83% in all three subtasks.\n","authors":["Jinyuan Li","Han Li","Di Sun","Jiahao Wang","Wenkun Zhang","Zan Wang","Gang Pan"],"pdf_url":"https://arxiv.org/pdf/2402.09989v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07636v4","updated":"2024-03-31T07:42:17Z","published":"2024-03-12T13:18:22Z","title":"Decomposing Disease Descriptions for Enhanced Pathology Detection: A\n  Multi-Aspect Vision-Language Pre-training Framework","summary":"  Medical vision language pre-training (VLP) has emerged as a frontier of\nresearch, enabling zero-shot pathological recognition by comparing the query\nimage with the textual descriptions for each disease. Due to the complex\nsemantics of biomedical texts, current methods struggle to align medical images\nwith key pathological findings in unstructured reports. This leads to the\nmisalignment with the target disease's textual representation. In this paper,\nwe introduce a novel VLP framework designed to dissect disease descriptions\ninto their fundamental aspects, leveraging prior knowledge about the visual\nmanifestations of pathologies. This is achieved by consulting a large language\nmodel and medical experts. Integrating a Transformer module, our approach\naligns an input image with the diverse elements of a disease, generating\naspect-centric image representations. By consolidating the matches from each\naspect, we improve the compatibility between an image and its associated\ndisease. Additionally, capitalizing on the aspect-oriented representations, we\npresent a dual-head Transformer tailored to process known and unknown diseases,\noptimizing the comprehensive detection efficacy. Conducting experiments on\nseven downstream datasets, ours improves the accuracy of recent methods by up\nto 8.56% and 17.26% for seen and unseen categories, respectively. Our code is\nreleased at https://github.com/HieuPhan33/MAVL.\n","authors":["Vu Minh Hieu Phan","Yutong Xie","Yuankai Qi","Lingqiao Liu","Liyang Liu","Bowen Zhang","Zhibin Liao","Qi Wu","Minh-Son To","Johan W. Verjans"],"pdf_url":"https://arxiv.org/pdf/2403.07636v4.pdf","comment":"Accepted at CVPR2024. Pre-print before final camera-ready version"},{"id":"http://arxiv.org/abs/2312.01616v3","updated":"2024-03-31T05:57:57Z","published":"2023-12-04T04:14:09Z","title":"SchurVINS: Schur Complement-Based Lightweight Visual Inertial Navigation\n  System","summary":"  Accuracy and computational efficiency are the most important metrics to\nVisual Inertial Navigation System (VINS). The existing VINS algorithms with\neither high accuracy or low computational complexity, are difficult to provide\nthe high precision localization in resource-constrained devices. To this end,\nwe propose a novel filter-based VINS framework named SchurVINS, which could\nguarantee both high accuracy by building a complete residual model and low\ncomputational complexity with Schur complement. Technically, we first formulate\nthe full residual model where Gradient, Hessian and observation covariance are\nexplicitly modeled. Then Schur complement is employed to decompose the full\nmodel into ego-motion residual model and landmark residual model. Finally,\nExtended Kalman Filter (EKF) update is implemented in these two models with\nhigh efficiency. Experiments on EuRoC and TUM-VI datasets show that our method\nnotably outperforms state-of-the-art (SOTA) methods in both accuracy and\ncomputational complexity. The experimental code of SchurVINS is available at\nhttps://github.com/bytedance/SchurVINS.\n","authors":["Yunfei Fan","Tianyu Zhao","Guidong Wang"],"pdf_url":"https://arxiv.org/pdf/2312.01616v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16051v2","updated":"2024-03-31T05:51:58Z","published":"2024-03-24T07:36:38Z","title":"Segment Anything Model for Road Network Graph Extraction","summary":"  We propose SAM-Road, an adaptation of the Segment Anything Model (SAM) for\nextracting large-scale, vectorized road network graphs from satellite imagery.\nTo predict graph geometry, we formulate it as a dense semantic segmentation\ntask, leveraging the inherent strengths of SAM. The image encoder of SAM is\nfine-tuned to produce probability masks for roads and intersections, from which\nthe graph vertices are extracted via simple non-maximum suppression. To predict\ngraph topology, we designed a lightweight transformer-based graph neural\nnetwork, which leverages the SAM image embeddings to estimate the edge\nexistence probabilities between vertices. Our approach directly predicts the\ngraph vertices and edges for large regions without expensive and complex\npost-processing heuristics, and is capable of building complete road network\ngraphs spanning multiple square kilometers in a matter of seconds. With its\nsimple, straightforward, and minimalist design, SAM-Road achieves comparable\naccuracy with the state-of-the-art method RNGDet++, while being 40 times faster\non the City-scale dataset. We thus demonstrate the power of a foundational\nvision model when applied to a graph learning task. The code is available at\nhttps://github.com/htcr/sam_road.\n","authors":["Congrui Hetang","Haoru Xue","Cindy Le","Tianwei Yue","Wenping Wang","Yihui He"],"pdf_url":"https://arxiv.org/pdf/2403.16051v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10145v2","updated":"2024-03-31T05:24:35Z","published":"2024-03-15T09:44:02Z","title":"RCooper: A Real-world Large-scale Dataset for Roadside Cooperative\n  Perception","summary":"  The value of roadside perception, which could extend the boundaries of\nautonomous driving and traffic management, has gradually become more prominent\nand acknowledged in recent years. However, existing roadside perception\napproaches only focus on the single-infrastructure sensor system, which cannot\nrealize a comprehensive understanding of a traffic area because of the limited\nsensing range and blind spots. Orienting high-quality roadside perception, we\nneed Roadside Cooperative Perception (RCooper) to achieve practical\narea-coverage roadside perception for restricted traffic areas. Rcooper has its\nown domain-specific challenges, but further exploration is hindered due to the\nlack of datasets. We hence release the first real-world, large-scale RCooper\ndataset to bloom the research on practical roadside cooperative perception,\nincluding detection and tracking. The manually annotated dataset comprises 50k\nimages and 30k point clouds, including two representative traffic scenes (i.e.,\nintersection and corridor). The constructed benchmarks prove the effectiveness\nof roadside cooperation perception and demonstrate the direction of further\nresearch. Codes and dataset can be accessed at:\nhttps://github.com/AIR-THU/DAIR-RCooper.\n","authors":["Ruiyang Hao","Siqi Fan","Yingru Dai","Zhenlin Zhang","Chenxi Li","Yuntian Wang","Haibao Yu","Wenxian Yang","Jirui Yuan","Zaiqing Nie"],"pdf_url":"https://arxiv.org/pdf/2403.10145v2.pdf","comment":"Accepted by CVPR2024. 10 pages with 6 figures"},{"id":"http://arxiv.org/abs/2401.06415v2","updated":"2024-03-31T05:22:00Z","published":"2024-01-12T07:23:02Z","title":"3D Reconstruction of Interacting Multi-Person in Clothing from a Single\n  Image","summary":"  This paper introduces a novel pipeline to reconstruct the geometry of\ninteracting multi-person in clothing on a globally coherent scene space from a\nsingle image. The main challenge arises from the occlusion: a part of a human\nbody is not visible from a single view due to the occlusion by others or the\nself, which introduces missing geometry and physical implausibility (e.g.,\npenetration). We overcome this challenge by utilizing two human priors for\ncomplete 3D geometry and surface contacts. For the geometry prior, an encoder\nlearns to regress the image of a person with missing body parts to the latent\nvectors; a decoder decodes these vectors to produce 3D features of the\nassociated geometry; and an implicit network combines these features with a\nsurface normal map to reconstruct a complete and detailed 3D humans. For the\ncontact prior, we develop an image-space contact detector that outputs a\nprobability distribution of surface contacts between people in 3D. We use these\npriors to globally refine the body poses, enabling the penetration-free and\naccurate reconstruction of interacting multi-person in clothing on the scene\nspace. The results demonstrate that our method is complete, globally coherent,\nand physically plausible compared to existing methods.\n","authors":["Junuk Cha","Hansol Lee","Jaewon Kim","Nhat Nguyen Bao Truong","Jae Shin Yoon","Seungryul Baek"],"pdf_url":"https://arxiv.org/pdf/2401.06415v2.pdf","comment":"Accepted to WACV 2024"},{"id":"http://arxiv.org/abs/2312.16084v2","updated":"2024-03-31T04:45:58Z","published":"2023-12-26T15:14:37Z","title":"LangSplat: 3D Language Gaussian Splatting","summary":"  Humans live in a 3D world and commonly use natural language to interact with\na 3D scene. Modeling a 3D language field to support open-ended language queries\nin 3D has gained increasing attention recently. This paper introduces\nLangSplat, which constructs a 3D language field that enables precise and\nefficient open-vocabulary querying within 3D spaces. Unlike existing methods\nthat ground CLIP language embeddings in a NeRF model, LangSplat advances the\nfield by utilizing a collection of 3D Gaussians, each encoding language\nfeatures distilled from CLIP, to represent the language field. By employing a\ntile-based splatting technique for rendering language features, we circumvent\nthe costly rendering process inherent in NeRF. Instead of directly learning\nCLIP embeddings, LangSplat first trains a scene-wise language autoencoder and\nthen learns language features on the scene-specific latent space, thereby\nalleviating substantial memory demands imposed by explicit modeling. Existing\nmethods struggle with imprecise and vague 3D language fields, which fail to\ndiscern clear boundaries between objects. We delve into this issue and propose\nto learn hierarchical semantics using SAM, thereby eliminating the need for\nextensively querying the language field across various scales and the\nregularization of DINO features. Extensive experimental results show that\nLangSplat significantly outperforms the previous state-of-the-art method LERF\nby a large margin. Notably, LangSplat is extremely efficient, achieving a 199\n$\\times$ speedup compared to LERF at the resolution of 1440 $\\times$ 1080. We\nstrongly recommend readers to check out our video results at\nhttps://langsplat.github.io/\n","authors":["Minghan Qin","Wanhua Li","Jiawei Zhou","Haoqian Wang","Hanspeter Pfister"],"pdf_url":"https://arxiv.org/pdf/2312.16084v2.pdf","comment":"CVPR 2024. Project Page: https://langsplat.github.io"},{"id":"http://arxiv.org/abs/2303.08314v3","updated":"2024-03-31T04:11:30Z","published":"2023-03-15T02:08:20Z","title":"Guided Slot Attention for Unsupervised Video Object Segmentation","summary":"  Unsupervised video object segmentation aims to segment the most prominent\nobject in a video sequence. However, the existence of complex backgrounds and\nmultiple foreground objects make this task challenging. To address this issue,\nwe propose a guided slot attention network to reinforce spatial structural\ninformation and obtain better foreground--background separation. The foreground\nand background slots, which are initialized with query guidance, are\niteratively refined based on interactions with template information.\nFurthermore, to improve slot--template interaction and effectively fuse global\nand local features in the target and reference frames, K-nearest neighbors\nfiltering and a feature aggregation transformer are introduced. The proposed\nmodel achieves state-of-the-art performance on two popular datasets.\nAdditionally, we demonstrate the robustness of the proposed model in\nchallenging scenes through various comparative experiments.\n","authors":["Minhyeok Lee","Suhwan Cho","Dogyoon Lee","Chaewon Park","Jungho Lee","Sangyoun Lee"],"pdf_url":"https://arxiv.org/pdf/2303.08314v3.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2312.08963v2","updated":"2024-03-31T02:18:23Z","published":"2023-12-14T14:10:57Z","title":"LEMON: Learning 3D Human-Object Interaction Relation from 2D Images","summary":"  Learning 3D human-object interaction relation is pivotal to embodied AI and\ninteraction modeling. Most existing methods approach the goal by learning to\npredict isolated interaction elements, e.g., human contact, object affordance,\nand human-object spatial relation, primarily from the perspective of either the\nhuman or the object. Which underexploit certain correlations between the\ninteraction counterparts (human and object), and struggle to address the\nuncertainty in interactions. Actually, objects' functionalities potentially\naffect humans' interaction intentions, which reveals what the interaction is.\nMeanwhile, the interacting humans and objects exhibit matching geometric\nstructures, which presents how to interact. In light of this, we propose\nharnessing these inherent correlations between interaction counterparts to\nmitigate the uncertainty and jointly anticipate the above interaction elements\nin 3D space. To achieve this, we present LEMON (LEarning 3D huMan-Object\niNteraction relation), a unified model that mines interaction intentions of the\ncounterparts and employs curvatures to guide the extraction of geometric\ncorrelations, combining them to anticipate the interaction elements. Besides,\nthe 3D Interaction Relation dataset (3DIR) is collected to serve as the test\nbed for training and evaluation. Extensive experiments demonstrate the\nsuperiority of LEMON over methods estimating each element in isolation.\n","authors":["Yuhang Yang","Wei Zhai","Hongchen Luo","Yang Cao","Zheng-Jun Zha"],"pdf_url":"https://arxiv.org/pdf/2312.08963v2.pdf","comment":"accept by CVPR2024"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2312.01998v2","updated":"2024-03-31T22:58:09Z","published":"2023-12-04T16:22:06Z","title":"Language-only Efficient Training of Zero-shot Composed Image Retrieval","summary":"  Composed image retrieval (CIR) task takes a composed query of image and text,\naiming to search relative images for both conditions. Conventional CIR\napproaches need a training dataset composed of triplets of query image, query\ntext, and target image, which is very expensive to collect. Several recent\nworks have worked on the zero-shot (ZS) CIR paradigm to tackle the issue\nwithout using pre-collected triplets. However, the existing ZS-CIR methods show\nlimited backbone scalability and generalizability due to the lack of diversity\nof the input texts during training. We propose a novel CIR framework, only\nusing language for its training. Our LinCIR (Language-only training for CIR)\ncan be trained only with text datasets by a novel self-supervision named\nself-masking projection (SMP). We project the text latent embedding to the\ntoken embedding space and construct a new text by replacing the keyword tokens\nof the original text. Then, we let the new and original texts have the same\nlatent embedding vector. With this simple strategy, LinCIR is surprisingly\nefficient and highly effective; LinCIR with CLIP ViT-G backbone is trained in\n48 minutes and shows the best ZS-CIR performances on four different CIR\nbenchmarks, CIRCO, GeneCIS, FashionIQ, and CIRR, even outperforming supervised\nmethod on FashionIQ. Code is available at https://github.com/navervision/lincir\n","authors":["Geonmo Gu","Sanghyuk Chun","Wonjae Kim","Yoohoon Kang","Sangdoo Yun"],"pdf_url":"https://arxiv.org/pdf/2312.01998v2.pdf","comment":"CVPR 2024 camera-ready; First two authors contributed equally; 17\n  pages, 3.1MB"},{"id":"http://arxiv.org/abs/2311.09476v2","updated":"2024-03-31T20:58:46Z","published":"2023-11-16T00:39:39Z","title":"ARES: An Automated Evaluation Framework for Retrieval-Augmented\n  Generation Systems","summary":"  Evaluating retrieval-augmented generation (RAG) systems traditionally relies\non hand annotations for input queries, passages to retrieve, and responses to\ngenerate. We introduce ARES, an Automated RAG Evaluation System, for evaluating\nRAG systems along the dimensions of context relevance, answer faithfulness, and\nanswer relevance. By creating its own synthetic training data, ARES finetunes\nlightweight LM judges to assess the quality of individual RAG components. To\nmitigate potential prediction errors, ARES utilizes a small set of\nhuman-annotated datapoints for prediction-powered inference (PPI). Across eight\ndifferent knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES\naccurately evaluates RAG systems while using only a few hundred human\nannotations during evaluation. Furthermore, ARES judges remain effective across\ndomain shifts, proving accurate even after changing the type of queries and/or\ndocuments used in the evaluated RAG systems. We make our code and datasets\npublicly available on Github.\n","authors":["Jon Saad-Falcon","Omar Khattab","Christopher Potts","Matei Zaharia"],"pdf_url":"https://arxiv.org/pdf/2311.09476v2.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2305.01157v3","updated":"2024-03-31T19:56:37Z","published":"2023-05-02T02:21:49Z","title":"Complex Logical Reasoning over Knowledge Graphs using Large Language\n  Models","summary":"  Reasoning over knowledge graphs (KGs) is a challenging task that requires a\ndeep understanding of the complex relationships between entities and the\nunderlying logic of their relations. Current approaches rely on learning\ngeometries to embed entities in vector space for logical query operations, but\nthey suffer from subpar performance on complex queries and dataset-specific\nrepresentations. In this paper, we propose a novel decoupled approach,\nLanguage-guided Abstract Reasoning over Knowledge graphs (LARK), that\nformulates complex KG reasoning as a combination of contextual KG search and\nlogical query reasoning, to leverage the strengths of graph extraction\nalgorithms and large language models (LLM), respectively. Our experiments\ndemonstrate that the proposed approach outperforms state-of-the-art KG\nreasoning methods on standard benchmark datasets across several logical query\nconstructs, with significant performance gain for queries of higher complexity.\nFurthermore, we show that the performance of our approach improves\nproportionally to the increase in size of the underlying LLM, enabling the\nintegration of the latest advancements in LLMs for logical reasoning over KGs.\nOur work presents a new direction for addressing the challenges of complex KG\nreasoning and paves the way for future research in this area.\n","authors":["Nurendra Choudhary","Chandan K. Reddy"],"pdf_url":"https://arxiv.org/pdf/2305.01157v3.pdf","comment":"Code available at https://github.com/Akirato/LLM-KG-Reasoning"},{"id":"http://arxiv.org/abs/2312.16018v3","updated":"2024-03-31T11:06:34Z","published":"2023-12-26T12:12:58Z","title":"RecRanker: Instruction Tuning Large Language Model as Ranker for Top-k\n  Recommendation","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities and\nhave been extensively deployed across various domains, including recommender\nsystems. Prior research has employed specialized \\textit{prompts} to leverage\nthe in-context learning capabilities of LLMs for recommendation purposes. More\nrecent studies have utilized instruction tuning techniques to align LLMs with\nhuman preferences, promising more effective recommendations. However, existing\nmethods suffer from several limitations. The full potential of LLMs is not\nfully elicited due to low-quality tuning data and the overlooked integration of\nconventional recommender signals. Furthermore, LLMs may generate inconsistent\nresponses for different ranking tasks in the recommendation, potentially\nleading to unreliable results.\n  In this paper, we introduce \\textbf{RecRanker}, tailored for instruction\ntuning LLMs to serve as the \\textbf{Ranker} for top-\\textit{k}\n\\textbf{Rec}ommendations. Specifically, we introduce an adaptive sampling\nmodule for sampling high-quality, representative, and diverse training data. To\nenhance the prompt, we introduce a position shifting strategy to mitigate\nposition bias and augment the prompt with auxiliary information from\nconventional recommendation models, thereby enriching the contextual\nunderstanding of the LLM. Subsequently, we utilize the sampled data to assemble\nan instruction-tuning dataset with the augmented prompts comprising three\ndistinct ranking tasks: pointwise, pairwise, and listwise rankings. We further\npropose a hybrid ranking method to enhance the model performance by ensembling\nthese ranking tasks. Our empirical evaluations demonstrate the effectiveness of\nour proposed RecRanker in both direct and sequential recommendation scenarios.\n","authors":["Sichun Luo","Bowei He","Haohan Zhao","Wei Shao","Yanlin Qi","Yinya Huang","Aojun Zhou","Yuxuan Yao","Zongpeng Li","Yuanzhang Xiao","Mingjie Zhan","Linqi Song"],"pdf_url":"https://arxiv.org/pdf/2312.16018v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15651v2","updated":"2024-03-31T04:20:36Z","published":"2023-08-29T22:03:17Z","title":"Ensuring User-side Fairness in Dynamic Recommender Systems","summary":"  User-side group fairness is crucial for modern recommender systems, aiming to\nalleviate performance disparities among user groups defined by sensitive\nattributes like gender, race, or age. In the ever-evolving landscape of\nuser-item interactions, continual adaptation to newly collected data is crucial\nfor recommender systems to stay aligned with the latest user preferences.\nHowever, we observe that such continual adaptation often exacerbates\nperformance disparities. This necessitates a thorough investigation into\nuser-side fairness in dynamic recommender systems, an area that has been\nunexplored in the literature. This problem is challenging due to distribution\nshifts, frequent model updates, and non-differentiability of ranking metrics.\nTo our knowledge, this paper presents the first principled study on ensuring\nuser-side fairness in dynamic recommender systems. We start with theoretical\nanalyses on fine-tuning v.s. retraining, showing that the best practice is\nincremental fine-tuning with restart. Guided by our theoretical analyses, we\npropose FAir Dynamic rEcommender (FADE), an end-to-end fine-tuning framework to\ndynamically ensure user-side fairness over time. To overcome the\nnon-differentiability of recommendation metrics in the fairness loss, we\nfurther introduce Differentiable Hit (DH) as an improvement over the recent\nNeuralNDCG method, not only alleviating its gradient vanishing issue but also\nachieving higher efficiency. Besides that, we also address the instability\nissue of the fairness loss by leveraging the competing nature between the\nrecommendation loss and the fairness loss. Through extensive experiments on\nreal-world datasets, we demonstrate that FADE effectively and efficiently\nreduces performance disparities with little sacrifice in the overall\nrecommendation performance.\n","authors":["Hyunsik Yoo","Zhichen Zeng","Jian Kang","Ruizhong Qiu","David Zhou","Zhining Liu","Fei Wang","Charlie Xu","Eunice Chan","Hanghang Tong"],"pdf_url":"https://arxiv.org/pdf/2308.15651v2.pdf","comment":"19 pages, 20 figures, 2 tables, ACM Web Conference 2024"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2403.07953v2","updated":"2024-03-31T23:47:47Z","published":"2024-03-12T06:25:47Z","title":"Abstracting Sparse DNN Acceleration via Structured Sparse Tensor\n  Decomposition","summary":"  Exploiting sparsity in deep neural networks (DNNs) has been a promising area\nto meet the growing computation need of modern DNNs. However, in practice,\nsparse DNN acceleration still faces a key challenge. To minimize the overhead\nof sparse acceleration, hardware designers have proposed structured sparse\nhardware support recently, which provides limited flexibility and requires\nextra model fine-tuning. Moreover, any sparse model fine-tuned for certain\nstructured sparse hardware cannot be accelerated by other structured hardware.\nTo bridge the gap between sparse DNN models and hardware, this paper proposes\ntensor approximation via structured decomposition (TASD), which leverages the\ndistributive property in linear algebra to turn any sparse tensor into a series\nof structured sparse tensors. Next, we develop a software framework, TASDER, to\naccelerate DNNs by searching layer-wise, high-quality structured decomposition\nfor both weight and activation tensors so that they can be accelerated by any\nsystems with structured sparse hardware support. Evaluation results show that,\nby exploiting prior structured sparse hardware baselines, our method can\naccelerate off-the-shelf dense and sparse DNNs without fine-tuning and improves\nenergy-delay-product by up to 83% and 74% on average.\n","authors":["Geonhwa Jeong","Po-An Tsai","Abhimanyu R. Bambhaniya","Stephen W. Keckler","Tushar Krishna"],"pdf_url":"https://arxiv.org/pdf/2403.07953v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02766v4","updated":"2024-03-31T23:15:10Z","published":"2023-11-05T20:51:03Z","title":"Riemannian Laplace Approximation with the Fisher Metric","summary":"  Laplace's method approximates a target density with a Gaussian distribution\nat its mode. It is computationally efficient and asymptotically exact for\nBayesian inference due to the Bernstein-von Mises theorem, but for complex\ntargets and finite-data posteriors it is often too crude an approximation. A\nrecent generalization of the Laplace Approximation transforms the Gaussian\napproximation according to a chosen Riemannian geometry providing a richer\napproximation family, while still retaining computational efficiency. However,\nas shown here, its properties depend heavily on the chosen metric, indeed the\nmetric adopted in previous work results in approximations that are overly\nnarrow as well as being biased even at the limit of infinite data. We correct\nthis shortcoming by developing the approximation family further, deriving two\nalternative variants that are exact at the limit of infinite data, extending\nthe theoretical analysis of the method, and demonstrating practical\nimprovements in a range of experiments.\n","authors":["Hanlin Yu","Marcelo Hartmann","Bernardo Williams","Mark Girolami","Arto Klami"],"pdf_url":"https://arxiv.org/pdf/2311.02766v4.pdf","comment":"AISTATS 2024, with additional fixes and improvements"},{"id":"http://arxiv.org/abs/2402.00976v2","updated":"2024-03-31T23:06:53Z","published":"2024-02-01T19:47:31Z","title":"Investigating Recurrent Transformers with Dynamic Halt","summary":"  In this paper, we study the inductive biases of two major approaches to\naugmenting Transformers with a recurrent mechanism - (1) the approach of\nincorporating a depth-wise recurrence similar to Universal Transformers; and\n(2) the approach of incorporating a chunk-wise temporal recurrence like\nTemporal Latent Bottleneck. Furthermore, we propose and investigate novel ways\nto extend and combine the above methods - for example, we propose a global\nmean-based dynamic halting mechanism for Universal Transformer and an\naugmentation of Temporal Latent Bottleneck with elements from Universal\nTransformer. We compare the models and probe their inductive biases in several\ndiagnostic tasks such as Long Range Arena (LRA), flip-flop language modeling,\nListOps, and Logical Inference.\n","authors":["Jishnu Ray Chowdhury","Cornelia Caragea"],"pdf_url":"https://arxiv.org/pdf/2402.00976v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14592v2","updated":"2024-03-31T23:05:53Z","published":"2023-09-26T00:58:36Z","title":"Efficient Post-training Quantization with FP8 Formats","summary":"  Recent advances in deep learning methods such as LLMs and Diffusion models\nhave created a need for improved quantization methods that can meet the\ncomputational demands of these modern architectures while maintaining accuracy.\nTowards this goal, we study the advantages of FP8 data formats for\npost-training quantization across 75 unique network architectures covering a\nwide range of tasks, including machine translation, language modeling, text\ngeneration, image classification, generation, and segmentation. We examine\nthree different FP8 representations (E5M2, E4M3, and E3M4) to study the effects\nof varying degrees of trade-off between dynamic range and precision on model\naccuracy. Based on our extensive study, we developed a quantization workflow\nthat generalizes across different network architectures. Our empirical results\nshow that FP8 formats outperform INT8 in multiple aspects, including workload\ncoverage (92.64% vs. 65.87%), model accuracy and suitability for a broader\nrange of operations. Furthermore, our findings suggest that E4M3 is better\nsuited for NLP models, whereas E3M4 performs marginally better than E4M3 on\ncomputer vision tasks. The code is publicly available on Intel Neural\nCompressor: https://github.com/intel/neural-compressor.\n","authors":["Haihao Shen","Naveen Mellempudi","Xin He","Qun Gao","Chang Wang","Mengni Wang"],"pdf_url":"https://arxiv.org/pdf/2309.14592v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13958v2","updated":"2024-03-31T22:39:12Z","published":"2023-11-23T12:16:33Z","title":"Handling The Non-Smooth Challenge in Tensor SVD: A Multi-Objective\n  Tensor Recovery Framework","summary":"  Recently, numerous tensor singular value decomposition (t-SVD)-based tensor\nrecovery methods have shown promise in processing visual data, such as color\nimages and videos. However, these methods often suffer from severe performance\ndegradation when confronted with tensor data exhibiting non-smooth changes. It\nhas been commonly observed in real-world scenarios but ignored by the\ntraditional t-SVD-based methods. In this work, we introduce a novel tensor\nrecovery model with a learnable tensor nuclear norm to address such a\nchallenge. We develop a new optimization algorithm named the Alternating\nProximal Multiplier Method (APMM) to iteratively solve the proposed tensor\ncompletion model. Theoretical analysis demonstrates the convergence of the\nproposed APMM to the Karush-Kuhn-Tucker (KKT) point of the optimization\nproblem. In addition, we propose a multi-objective tensor recovery framework\nbased on APMM to efficiently explore the correlations of tensor data across its\nvarious dimensions, providing a new perspective on extending the t-SVD-based\nmethod to higher-order tensor cases. Numerical experiments demonstrated the\neffectiveness of the proposed method in tensor completion.\n","authors":["Jingjing Zheng","Wanglong Lu","Wenzhe Wang","Yankai Cao","Xiaoqin Zhang","Xianta Jiang"],"pdf_url":"https://arxiv.org/pdf/2311.13958v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03294v2","updated":"2024-03-31T21:11:08Z","published":"2023-10-05T03:47:57Z","title":"DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context\n  LLMs Training","summary":"  FlashAttention (Dao, 2023) effectively reduces the quadratic peak memory\nusage to linear in training transformer-based large language models (LLMs) on a\nsingle GPU. In this paper, we introduce DISTFLASHATTN, a distributed\nmemory-efficient attention mechanism optimized for long-context LLMs training.\nWe propose three key techniques: token-level workload balancing, overlapping\nkey-value communication, and a rematerialization-aware gradient checkpointing\nalgorithm. We evaluate DISTFLASHATTN on Llama-7B and variants with sequence\nlengths from 32K to 512K. DISTFLASHATTN achieves 8x longer sequences, 4.45 -\n5.64x speedup compared to Ring Self-Attention, 2 - 8x longer sequences, 1.24 -\n2.01x speedup compared to Megatron-LM with FlashAttention. It achieves 1.67x\nand 1.26 - 1.88x speedup compared to recent Ring Attention and\nDeepSpeed-Ulysses. Code is available at https://github.com/RulinShao/LightSeq.\n","authors":["Dacheng Li","Rulin Shao","Anze Xie","Eric P. Xing","Xuezhe Ma","Ion Stoica","Joseph E. Gonzalez","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.03294v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07772v4","updated":"2024-03-31T19:33:50Z","published":"2023-11-13T21:42:38Z","title":"In-context Learning and Gradient Descent Revisited","summary":"  In-context learning (ICL) has shown impressive results in few-shot learning\ntasks, yet its underlying mechanism is still not fully understood. A recent\nline of work suggests that ICL performs gradient descent (GD)-based\noptimization implicitly. While appealing, much of the research focuses on\nsimplified settings, where the parameters of a shallow model are optimized. In\nthis work, we revisit evidence for ICL-GD correspondence on realistic NLP tasks\nand models. We find gaps in evaluation, both in terms of problematic metrics\nand insufficient baselines. We show that surprisingly, even untrained models\nachieve comparable ICL-GD similarity scores despite not exhibiting ICL. Next,\nwe explore a major discrepancy in the flow of information throughout the model\nbetween ICL and GD, which we term Layer Causality. We propose a simple GD-based\noptimization procedure that respects layer causality, and show it improves\nsimilarity scores significantly.\n","authors":["Gilad Deutch","Nadav Magar","Tomer Bar Natan","Guy Dar"],"pdf_url":"https://arxiv.org/pdf/2311.07772v4.pdf","comment":"Accepted to NAACL 2024 main conference"},{"id":"http://arxiv.org/abs/2308.09730v2","updated":"2024-03-31T19:28:25Z","published":"2023-08-17T19:12:32Z","title":"Virtual imaging trials improved the transparency and reliability of AI\n  systems in COVID-19 imaging","summary":"  The credibility of AI models in medical imaging is often challenged by\nreproducibility issues and obscured clinical insights, a reality highlighted\nduring the COVID-19 pandemic by many reports of near-perfect artificial\nintelligence (AI) models that all failed to generalize. To address these\nconcerns, we propose a virtual imaging trial framework, employing a diverse\ncollection of medical images that are both clinical and simulated. In this\nstudy, COVID-19 serves as a case example to unveil the intrinsic and extrinsic\nfactors influencing AI performance. Our findings underscore a significant\nimpact of dataset characteristics on AI efficacy. Even when trained on large,\ndiverse clinical datasets with thousands of patients, AI performance plummeted\nby up to 20% in generalization. However, virtual imaging trials offer a robust\nplatform for objective assessment, unveiling nuanced insights into the\nrelationships between patient- and physics-based factors and AI performance.\nFor instance, disease extent markedly influenced AI efficacy, computed\ntomography (CT) out-performed chest radiography (CXR), while imaging dose\nexhibited minimal impact. Using COVID-19 as a case study, this virtual imaging\ntrial study verified that radiology AI models often suffer from a\nreproducibility crisis. Virtual imaging trials not only offered a solution for\nobjective performance assessment but also extracted several clinical insights.\nThis study illuminates the path for leveraging virtual imaging to augment the\nreliability, transparency, and clinical relevance of AI in medical imaging.\n","authors":["Fakrul Islam Tushar","Lavsen Dahal","Saman Sotoudeh-Paima","Ehsan Abadi","W. Paul Segars","Ehsan Samei","Joseph Y. Lo"],"pdf_url":"https://arxiv.org/pdf/2308.09730v2.pdf","comment":"3 tables, 4 figures, 1 Supplement"},{"id":"http://arxiv.org/abs/2401.08409v2","updated":"2024-03-31T19:01:07Z","published":"2024-01-16T14:49:26Z","title":"Faster ISNet for Background Bias Mitigation on Deep Neural Networks","summary":"  Bias or spurious correlations in image backgrounds can impact neural\nnetworks, causing shortcut learning (Clever Hans Effect) and hampering\ngeneralization to real-world data. ISNet, a recently introduced architecture,\nproposed the optimization of Layer-Wise Relevance Propagation (LRP, an\nexplanation technique) heatmaps, to mitigate the influence of backgrounds on\ndeep classifiers. However, ISNet's training time scales linearly with the\nnumber of classes in an application. Here, we propose reformulated\narchitectures whose training time becomes independent from this number.\nAdditionally, we introduce a concise and model-agnostic LRP implementation. We\nchallenge the proposed architectures using synthetic background bias, and\nCOVID-19 detection in chest X-rays, an application that commonly presents\nbackground bias. The networks hindered background attention and shortcut\nlearning, surpassing multiple state-of-the-art models on out-of-distribution\ntest datasets. Representing a potentially massive training speed improvement\nover ISNet, the proposed architectures introduce LRP optimization into a gamut\nof applications that the original model cannot feasibly handle.\n","authors":["Pedro R. A. S. Bassi","Sergio Decherchi","Andrea Cavalli"],"pdf_url":"https://arxiv.org/pdf/2401.08409v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06234v2","updated":"2024-03-31T18:18:05Z","published":"2023-11-10T18:49:53Z","title":"EVORA: Deep Evidential Traversability Learning for Risk-Aware Off-Road\n  Autonomy","summary":"  Traversing terrain with good traction is crucial for achieving fast off-road\nnavigation. Instead of manually designing costs based on terrain features,\nexisting methods learn terrain properties directly from data via\nself-supervision to automatically penalize trajectories moving through\nundesirable terrain, but challenges remain to properly quantify and mitigate\nthe risk due to uncertainty in learned models. To this end, this work proposes\na unified framework to learn uncertainty-aware traction model and plan\nrisk-aware trajectories. For uncertainty quantification, we efficiently model\nboth aleatoric and epistemic uncertainty by learning discrete traction\ndistributions and probability densities of the traction predictor's latent\nfeatures. Leveraging evidential deep learning, we parameterize Dirichlet\ndistributions with the network outputs and propose a novel uncertainty-aware\nsquared Earth Mover's distance loss with a closed-form expression that improves\nlearning accuracy and navigation performance. For risk-aware navigation, the\nproposed planner simulates state trajectories with the worst-case expected\ntraction to handle aleatoric uncertainty, and penalizes trajectories moving\nthrough terrain with high epistemic uncertainty. Our approach is extensively\nvalidated in simulation and on wheeled and quadruped robots, showing improved\nnavigation performance compared to methods that assume no slip, assume the\nexpected traction, or optimize for the worst-case expected cost.\n","authors":["Xiaoyi Cai","Siddharth Ancha","Lakshay Sharma","Philip R. Osteen","Bernadette Bucher","Stephen Phillips","Jiuguang Wang","Michael Everett","Nicholas Roy","Jonathan P. How"],"pdf_url":"https://arxiv.org/pdf/2311.06234v2.pdf","comment":"Under review. Journal extension for arXiv:2210.00153. Project\n  website: https://xiaoyi-cai.github.io/evora/"},{"id":"http://arxiv.org/abs/2307.05775v3","updated":"2024-03-31T17:03:00Z","published":"2023-07-11T20:06:12Z","title":"Weisfeiler and Leman Go Measurement Modeling: Probing the Validity of\n  the WL Test","summary":"  The expressive power of graph neural networks is usually measured by\ncomparing how many pairs of graphs or nodes an architecture can possibly\ndistinguish as non-isomorphic to those distinguishable by the $k$-dimensional\nWeisfeiler-Leman ($k$-WL) test. In this paper, we uncover misalignments between\ngraph machine learning practitioners' conceptualizations of expressive power\nand $k$-WL through a systematic analysis of the reliability and validity of\n$k$-WL. We conduct a survey ($n = 18$) of practitioners to surface their\nconceptualizations of expressive power and their assumptions about $k$-WL. In\ncontrast to practitioners' beliefs, our analysis (which draws from graph theory\nand benchmark auditing) reveals that $k$-WL does not guarantee isometry, can be\nirrelevant to real-world graph tasks, and may not promote generalization or\ntrustworthiness. We argue for extensional definitions and measurement of\nexpressive power based on benchmarks. We further contribute guiding questions\nfor constructing such benchmarks, which is critical for graph machine learning\npractitioners to develop and transparently communicate our understandings of\nexpressive power.\n","authors":["Arjun Subramonian","Adina Williams","Maximilian Nickel","Yizhou Sun","Levent Sagun"],"pdf_url":"https://arxiv.org/pdf/2307.05775v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08572v2","updated":"2024-03-31T17:01:34Z","published":"2023-11-14T22:32:39Z","title":"Low-Rank Adaptation for Multilingual Summarization: An Empirical Study","summary":"  Although the advancements of pre-trained Large Language Models have\nsignificantly accelerated recent progress in NLP, their ever-increasing size\nposes significant challenges for conventional fine-tuning, especially in\nmemory-intensive tasks. We investigate the potential of Parameter-Efficient\nFine-Tuning, focusing on Low-Rank Adaptation (LoRA), in the domain of\nmultilingual summarization, a task that is both challenging (due to typically\nlong inputs), and relatively unexplored. We conduct an extensive study across\ndifferent data availability scenarios, including high- and low-data settings,\nand cross-lingual transfer, leveraging models of different sizes. Our findings\nreveal that LoRA is competitive with full fine-tuning when trained with high\nquantities of data, and excels in low-data scenarios and cross-lingual\ntransfer. We also study different strategies for few-shot cross-lingual\ntransfer, finding that continued LoRA tuning outperforms full fine-tuning and\nthe dynamic composition of language-specific LoRA modules.\n","authors":["Chenxi Whitehouse","Fantine Huot","Jasmijn Bastings","Mostafa Dehghani","Chu-Cheng Lin","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2311.08572v2.pdf","comment":"Findings of NAACL 2024"},{"id":"http://arxiv.org/abs/2403.19647v2","updated":"2024-03-31T16:54:50Z","published":"2024-03-28T17:56:07Z","title":"Sparse Feature Circuits: Discovering and Editing Interpretable Causal\n  Graphs in Language Models","summary":"  We introduce methods for discovering and applying sparse feature circuits.\nThese are causally implicated subnetworks of human-interpretable features for\nexplaining language model behaviors. Circuits identified in prior work consist\nof polysemantic and difficult-to-interpret units like attention heads or\nneurons, rendering them unsuitable for many downstream applications. In\ncontrast, sparse feature circuits enable detailed understanding of\nunanticipated mechanisms. Because they are based on fine-grained units, sparse\nfeature circuits are useful for downstream tasks: We introduce SHIFT, where we\nimprove the generalization of a classifier by ablating features that a human\njudges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised\nand scalable interpretability pipeline by discovering thousands of sparse\nfeature circuits for automatically discovered model behaviors.\n","authors":["Samuel Marks","Can Rager","Eric J. Michaud","Yonatan Belinkov","David Bau","Aaron Mueller"],"pdf_url":"https://arxiv.org/pdf/2403.19647v2.pdf","comment":"Code and data at https://github.com/saprmarks/feature-circuits.\n  Demonstration at https://feature-circuits.xyz"},{"id":"http://arxiv.org/abs/2310.02391v3","updated":"2024-03-31T16:38:57Z","published":"2023-10-03T19:24:24Z","title":"SE(3)-Stochastic Flow Matching for Protein Backbone Generation","summary":"  The computational design of novel protein structures has the potential to\nimpact numerous scientific disciplines greatly. Toward this goal, we introduce\nFoldFlow, a series of novel generative models of increasing modeling power\nbased on the flow-matching paradigm over $3\\mathrm{D}$ rigid motions -- i.e.\nthe group $\\text{SE}(3)$ -- enabling accurate modeling of protein backbones. We\nfirst introduce FoldFlow-Base, a simulation-free approach to learning\ndeterministic continuous-time dynamics and matching invariant target\ndistributions on $\\text{SE}(3)$. We next accelerate training by incorporating\nRiemannian optimal transport to create FoldFlow-OT, leading to the construction\nof both more simple and stable flows. Finally, we design FoldFlow-SFM, coupling\nboth Riemannian OT and simulation-free training to learn stochastic\ncontinuous-time dynamics over $\\text{SE}(3). Our family of FoldFlow, generative\nmodels offers several key advantages over previous approaches to the generative\nmodeling of proteins: they are more stable and faster to train than\ndiffusion-based approaches, and our models enjoy the ability to map any\ninvariant source distribution to any invariant target distribution over\n$\\text{SE}(3)$. Empirically, we validate FoldFlow, on protein backbone\ngeneration of up to $300$ amino acids leading to high-quality designable,\ndiverse, and novel samples.\n","authors":["Avishek Joey Bose","Tara Akhound-Sadegh","Guillaume Huguet","Kilian Fatras","Jarrid Rector-Brooks","Cheng-Hao Liu","Andrei Cristian Nica","Maksym Korablyov","Michael Bronstein","Alexander Tong"],"pdf_url":"https://arxiv.org/pdf/2310.02391v3.pdf","comment":"ICLR 2024 Spotlight"},{"id":"http://arxiv.org/abs/2403.18998v2","updated":"2024-03-31T16:15:58Z","published":"2024-03-27T20:38:04Z","title":"Few-Shot Cross-System Anomaly Trace Classification for\n  Microservice-based systems","summary":"  Microservice-based systems (MSS) may experience failures in various fault\ncategories due to their complex and dynamic nature. To effectively handle\nfailures, AIOps tools utilize trace-based anomaly detection and root cause\nanalysis. In this paper, we propose a novel framework for few-shot abnormal\ntrace classification for MSS. Our framework comprises two main components: (1)\nMulti-Head Attention Autoencoder for constructing system-specific trace\nrepresentations, which enables (2) Transformer Encoder-based Model-Agnostic\nMeta-Learning to perform effective and efficient few-shot learning for abnormal\ntrace classification. The proposed framework is evaluated on two representative\nMSS, Trainticket and OnlineBoutique, with open datasets. The results show that\nour framework can adapt the learned knowledge to classify new, unseen abnormal\ntraces of novel fault categories both within the same system it was initially\ntrained on and even in the different MSS. Within the same MSS, our framework\nachieves an average accuracy of 93.26\\% and 85.2\\% across 50 meta-testing tasks\nfor Trainticket and OnlineBoutique, respectively, when provided with 10\ninstances for each task. In a cross-system context, our framework gets an\naverage accuracy of 92.19\\% and 84.77\\% for the same meta-testing tasks of the\nrespective system, also with 10 instances provided for each task. Our work\ndemonstrates the applicability of achieving few-shot abnormal trace\nclassification for MSS and shows how it can enable cross-system adaptability.\nThis opens an avenue for building more generalized AIOps tools that require\nless system-specific data labeling for anomaly detection and root cause\nanalysis.\n","authors":["Yuqing Wang","Mika V. Mantylä","Serge Demeyer","Mutlu Beyazit","Joanna Kisaakye","Jesse Nyyssölä"],"pdf_url":"https://arxiv.org/pdf/2403.18998v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2308.16859v2","updated":"2024-03-31T16:03:24Z","published":"2023-08-31T17:03:34Z","title":"Information Theoretically Optimal Sample Complexity of Learning\n  Dynamical Directed Acyclic Graphs","summary":"  In this article, the optimal sample complexity of learning the underlying\ninteractions or dependencies of a Linear Dynamical System (LDS) over a Directed\nAcyclic Graph (DAG) is studied. We call such a DAG underlying an LDS as\ndynamical DAG (DDAG). In particular, we consider a DDAG where the nodal\ndynamics are driven by unobserved exogenous noise sources that are wide-sense\nstationary (WSS) in time but are mutually uncorrelated, and have the same\n{power spectral density (PSD)}. Inspired by the static DAG setting, a metric\nand an algorithm based on the PSD matrix of the observed time series are\nproposed to reconstruct the DDAG. It is shown that the optimal sample\ncomplexity (or length of state trajectory) needed to learn the DDAG is\n$n=\\Theta(q\\log(p/q))$, where $p$ is the number of nodes and $q$ is the maximum\nnumber of parents per node. To prove the sample complexity upper bound, a\nconcentration bound for the PSD estimation is derived, under two different\nsampling strategies. A matching min-max lower bound using generalized Fano's\ninequality also is provided, thus showing the order optimality of the proposed\nalgorithm.\n","authors":["Mishfad Shaikh Veedu","Deepjyoti Deka","Murti V. Salapaka"],"pdf_url":"https://arxiv.org/pdf/2308.16859v2.pdf","comment":"21 pages. Accepted for publication in AISTATS 2024"},{"id":"http://arxiv.org/abs/2403.01590v2","updated":"2024-03-31T14:31:14Z","published":"2024-03-03T18:58:21Z","title":"The Hidden Attention of Mamba Models","summary":"  The Mamba layer offers an efficient selective state space model (SSM) that is\nhighly effective in modeling multiple domains, including NLP, long-range\nsequence processing, and computer vision. Selective SSMs are viewed as dual\nmodels, in which one trains in parallel on the entire sequence via an IO-aware\nparallel scan, and deploys in an autoregressive manner. We add a third view and\nshow that such models can be viewed as attention-driven models. This new\nperspective enables us to empirically and theoretically compare the underlying\nmechanisms to that of the self-attention layers in transformers and allows us\nto peer inside the inner workings of the Mamba model with explainability\nmethods. Our code is publicly available.\n","authors":["Ameen Ali","Itamar Zimerman","Lior Wolf"],"pdf_url":"https://arxiv.org/pdf/2403.01590v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01276v3","updated":"2024-03-31T14:28:51Z","published":"2023-11-02T14:44:50Z","title":"Neural Atoms: Propagating Long-range Interaction in Molecular Graphs\n  through Efficient Communication Channel","summary":"  Graph Neural Networks (GNNs) have been widely adopted for drug discovery with\nmolecular graphs. Nevertheless, current GNNs mainly excel in leveraging\nshort-range interactions (SRI) but struggle to capture long-range interactions\n(LRI), both of which are crucial for determining molecular properties. To\ntackle this issue, we propose a method to abstract the collective information\nof atomic groups into a few $\\textit{Neural Atoms}$ by implicitly projecting\nthe atoms of a molecular. Specifically, we explicitly exchange the information\namong neural atoms and project them back to the atoms' representations as an\nenhancement. With this mechanism, neural atoms establish the communication\nchannels among distant nodes, effectively reducing the interaction scope of\narbitrary node pairs into a single hop. To provide an inspection of our method\nfrom a physical perspective, we reveal its connection to the traditional LRI\ncalculation method, Ewald Summation. The Neural Atom can enhance GNNs to\ncapture LRI by approximating the potential LRI of the molecular. We conduct\nextensive experiments on four long-range graph benchmarks, covering graph-level\nand link-level tasks on molecular graphs. We achieve up to a 27.32% and 38.27%\nimprovement in the 2D and 3D scenarios, respectively. Empirically, our method\ncan be equipped with an arbitrary GNN to help capture LRI. Code and datasets\nare publicly available in https://github.com/tmlr-group/NeuralAtom.\n","authors":["Xuan Li","Zhanke Zhou","Jiangchao Yao","Yu Rong","Lu Zhang","Bo Han"],"pdf_url":"https://arxiv.org/pdf/2311.01276v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18824v2","updated":"2024-03-31T14:12:15Z","published":"2023-10-28T21:17:36Z","title":"Intrinsic Gaussian Vector Fields on Manifolds","summary":"  Various applications ranging from robotics to climate science require\nmodeling signals on non-Euclidean domains, such as the sphere. Gaussian process\nmodels on manifolds have recently been proposed for such tasks, in particular\nwhen uncertainty quantification is needed. In the manifold setting,\nvector-valued signals can behave very differently from scalar-valued ones, with\nmuch of the progress so far focused on modeling the latter. The former,\nhowever, are crucial for many applications, such as modeling wind speeds or\nforce fields of unknown dynamical systems. In this paper, we propose novel\nGaussian process models for vector-valued signals on manifolds that are\nintrinsically defined and account for the geometry of the space in\nconsideration. We provide computational primitives needed to deploy the\nresulting Hodge-Mat\\'ern Gaussian vector fields on the two-dimensional sphere\nand the hypertori. Further, we highlight two generalization directions:\ndiscrete two-dimensional meshes and \"ideal\" manifolds like hyperspheres, Lie\ngroups, and homogeneous spaces. Finally, we show that our Gaussian vector\nfields constitute considerably more refined inductive biases than the extrinsic\nfields proposed before.\n","authors":["Daniel Robert-Nicoud","Andreas Krause","Viacheslav Borovitskiy"],"pdf_url":"https://arxiv.org/pdf/2310.18824v2.pdf","comment":"Version accepted at AISTATS 2024"},{"id":"http://arxiv.org/abs/2310.09518v3","updated":"2024-03-31T13:39:44Z","published":"2023-10-14T07:16:08Z","title":"Instruction Tuning with Human Curriculum","summary":"  In this work, we (1) introduce Curriculum Instruction Tuning, (2) explore the\npotential advantages of employing diverse curriculum strategies, and (3)\ndelineate a synthetic instruction-response generation framework that\ncomplements our theoretical approach. Distinct from the existing instruction\ntuning dataset, our generation pipeline is systematically structured to emulate\nthe sequential and orderly characteristic of human learning. Additionally, we\ndescribe a methodology for generating instruction-response datasets that\nextensively span the various stages of human education, from middle school\nthrough the graduate level, utilizing educational subject catalogs.\n  Before training, we meticulously organize the instruction data to ensure that\nquestions escalate in difficulty regarding (A) the subject matter and (B) the\nintricacy of the instructions. The findings of our study reveal that\nsubstantial improvements in performance can be achieved through the mere\napplication of curriculum ordering to instruction data (achieving gains of\n+4.76 on TruthfulQA, +2.98 on MMLU, +2.8 on OpenbookQA, and +1.28 on ARC-hard)\ncompared to random shuffling. This enhancement is achieved without incurring\nadditional computational expenses. Through comprehensive experimentation, we\nobserve that the advantages of our proposed method are consistently evident\nacross nine benchmarks.\n","authors":["Bruce W. Lee","Hyunsoo Cho","Kang Min Yoo"],"pdf_url":"https://arxiv.org/pdf/2310.09518v3.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.14119v3","updated":"2024-03-31T13:36:54Z","published":"2024-03-21T04:08:29Z","title":"C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via\n  Text Feature Dispersion","summary":"  In deep learning, test-time adaptation has gained attention as a method for\nmodel fine-tuning without the need for labeled data. A prime exemplification is\nthe recently proposed test-time prompt tuning for large-scale vision-language\nmodels such as CLIP. Unfortunately, these prompts have been mainly developed to\nimprove accuracy, overlooking the importance of calibration, which is a crucial\naspect for quantifying prediction uncertainty. However, traditional calibration\nmethods rely on substantial amounts of labeled data, making them impractical\nfor test-time scenarios. To this end, this paper explores calibration during\ntest-time prompt tuning by leveraging the inherent properties of CLIP. Through\na series of observations, we find that the prompt choice significantly affects\nthe calibration in CLIP, where the prompts leading to higher text feature\ndispersion result in better-calibrated predictions. Introducing the Average\nText Feature Dispersion (ATFD), we establish its relationship with calibration\nerror and present a novel method, Calibrated Test-time Prompt Tuning (C-TPT),\nfor optimizing prompts during test-time with enhanced calibration. Through\nextensive experiments on different CLIP architectures and datasets, we show\nthat C-TPT can effectively improve the calibration of test-time prompt tuning\nwithout needing labeled data. The code is publicly accessible at\nhttps://github.com/hee-suk-yoon/C-TPT.\n","authors":["Hee Suk Yoon","Eunseop Yoon","Joshua Tian Jin Tee","Mark Hasegawa-Johnson","Yingzhen Li","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2403.14119v3.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.12418v2","updated":"2024-03-31T13:28:19Z","published":"2024-03-19T04:02:57Z","title":"STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space\n  Model","summary":"  Spatial-Temporal Graph (STG) data is characterized as dynamic, heterogenous,\nand non-stationary, leading to the continuous challenge of spatial-temporal\ngraph learning. In the past few years, various GNN-based methods have been\nproposed to solely focus on mimicking the relationships among node individuals\nof the STG network, ignoring the significance of modeling the intrinsic\nfeatures that exist in STG system over time. In contrast, modern Selective\nState Space Models (SSSMs) present a new approach which treat STG Network as a\nsystem, and meticulously explore the STG system's dynamic state evolution\nacross temporal dimension. In this work, we introduce Spatial-Temporal Graph\nMamba (STG-Mamba) as the first exploration of leveraging the powerful selective\nstate space models for STG learning by treating STG Network as a system, and\nemploying the Graph Selective State Space Block (GS3B) to precisely\ncharacterize the dynamic evolution of STG networks. STG-Mamba is formulated as\nan Encoder-Decoder architecture, which takes GS3B as the basic module, for\nefficient sequential data modeling. Furthermore, to strengthen GNN's ability of\nmodeling STG data under the setting of SSSMs, we propose Kalman Filtering Graph\nNeural Networks (KFGN) for adaptive graph structure upgrading. KFGN smoothly\nfits in the context of selective state space evolution, and at the same time\nkeeps linear complexity. Extensive empirical studies are conducted on three\nbenchmark STG forecasting datasets, demonstrating the performance superiority\nand computational efficiency of STG-Mamba. It not only surpasses existing\nstate-of-the-art methods in terms of STG forecasting performance, but also\neffectively alleviate the computational bottleneck of large-scale graph\nnetworks in reducing the computational cost of FLOPs and test inference time.\n","authors":["Lincan Li","Hanchen Wang","Wenjie Zhang","Adelle Coster"],"pdf_url":"https://arxiv.org/pdf/2403.12418v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00600v2","updated":"2024-03-31T12:45:09Z","published":"2023-12-01T14:06:28Z","title":"Improving Plasticity in Online Continual Learning via Collaborative\n  Learning","summary":"  Online Continual Learning (CL) solves the problem of learning the\never-emerging new classification tasks from a continuous data stream. Unlike\nits offline counterpart, in online CL, the training data can only be seen once.\nMost existing online CL research regards catastrophic forgetting (i.e., model\nstability) as almost the only challenge. In this paper, we argue that the\nmodel's capability to acquire new knowledge (i.e., model plasticity) is another\nchallenge in online CL. While replay-based strategies have been shown to be\neffective in alleviating catastrophic forgetting, there is a notable gap in\nresearch attention toward improving model plasticity. To this end, we propose\nCollaborative Continual Learning (CCL), a collaborative learning based strategy\nto improve the model's capability in acquiring new concepts. Additionally, we\nintroduce Distillation Chain (DC), a collaborative learning scheme to boost the\ntraining of the models. We adapt CCL-DC to existing representative online CL\nworks. Extensive experiments demonstrate that even if the learners are\nwell-trained with state-of-the-art online CL methods, our strategy can still\nimprove model plasticity dramatically, and thereby improve the overall\nperformance by a large margin. The source code of our work is available at\nhttps://github.com/maorong-wang/CCL-DC.\n","authors":["Maorong Wang","Nicolas Michel","Ling Xiao","Toshihiko Yamasaki"],"pdf_url":"https://arxiv.org/pdf/2312.00600v2.pdf","comment":"Update Camera-ready revision for CVPR'24"},{"id":"http://arxiv.org/abs/2308.00225v2","updated":"2024-03-31T12:20:25Z","published":"2023-08-01T01:39:25Z","title":"Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent\n  Cognitive Bias","summary":"  Recent studies show that instruction tuning (IT) and reinforcement learning\nfrom human feedback (RLHF) improve the abilities of large language models (LMs)\ndramatically. While these tuning methods can help align models with human\nobjectives and generate high-quality text, not much is known about their\npotential adverse effects. In this work, we investigate the effect of IT and\nRLHF on decision making and reasoning in LMs, focusing on three cognitive\nbiases - the decoy effect, the certainty effect, and the belief bias - all of\nwhich are known to influence human decision-making and reasoning. Our findings\nhighlight the presence of these biases in various models from the GPT-3,\nMistral, and T5 families. Notably, we find a stronger presence of biases in\nmodels that have undergone instruction tuning, such as Flan-T5,\nMistral-Instruct, GPT3.5, and GPT4. Our work constitutes a step toward\ncomprehending cognitive biases in instruction-tuned LMs, which is crucial for\nthe development of more reliable and unbiased language models.\n","authors":["Itay Itzhak","Gabriel Stanovsky","Nir Rosenfeld","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2308.00225v2.pdf","comment":"TACL 2024. Presented at ACL 2024. 12 pages"},{"id":"http://arxiv.org/abs/2302.02314v4","updated":"2024-03-31T11:58:28Z","published":"2023-02-05T06:27:45Z","title":"CECT: Controllable Ensemble CNN and Transformer for COVID-19 Image\n  Classification","summary":"  The COVID-19 pandemic has resulted in hundreds of million cases and numerous\ndeaths worldwide. Here, we develop a novel classification network CECT by\ncontrollable ensemble convolutional neural network and transformer to provide a\ntimely and accurate COVID-19 diagnosis. The CECT is composed of a parallel\nconvolutional encoder block, an aggregate transposed-convolutional decoder\nblock, and a windowed attention classification block. Each block captures\nfeatures at different scales from 28 $\\times$ 28 to 224 $\\times$ 224 from the\ninput, composing enriched and comprehensive information. Different from\nexisting methods, our CECT can capture features at both multi-local and global\nscales without any sophisticated module design. Moreover, the contribution of\nlocal features at different scales can be controlled with the proposed ensemble\ncoefficients. We evaluate CECT on two public COVID-19 datasets and it reaches\nthe highest accuracy of 98.1% in the intra-dataset evaluation, outperforming\nexisting state-of-the-art methods. Moreover, the developed CECT achieves an\naccuracy of 90.9% on the unseen dataset in the inter-dataset evaluation,\nshowing extraordinary generalization ability. With remarkable feature capture\nability and generalization ability, we believe CECT can be extended to other\nmedical scenarios as a powerful diagnosis tool. Code is available at\nhttps://github.com/NUS-Tim/CECT.\n","authors":["Zhaoshan Liu","Lei Shen"],"pdf_url":"https://arxiv.org/pdf/2302.02314v4.pdf","comment":"Computers in Biology and Medicine Accepted"},{"id":"http://arxiv.org/abs/2305.14749v4","updated":"2024-03-31T10:03:17Z","published":"2023-05-24T05:46:56Z","title":"gRNAde: Geometric Deep Learning for 3D RNA inverse design","summary":"  Computational RNA design tasks are often posed as inverse problems, where\nsequences are designed based on adopting a single desired secondary structure\nwithout considering 3D geometry and conformational diversity. We introduce\ngRNAde, a geometric RNA design pipeline operating on 3D RNA backbones to design\nsequences that explicitly account for structure and dynamics. Under the hood,\ngRNAde is a multi-state Graph Neural Network that generates candidate RNA\nsequences conditioned on one or more 3D backbone structures where the\nidentities of the bases are unknown. On a single-state fixed backbone re-design\nbenchmark of 14 RNA structures from the PDB identified by Das et al. [2010],\ngRNAde obtains higher native sequence recovery rates (56% on average) compared\nto Rosetta (45% on average), taking under a second to produce designs compared\nto the reported hours for Rosetta. We further demonstrate the utility of gRNAde\non a new benchmark of multi-state design for structurally flexible RNAs, as\nwell as zero-shot ranking of mutational fitness landscapes in a retrospective\nanalysis of a recent RNA polymerase ribozyme structure. Open source code:\nhttps://github.com/chaitjo/geometric-rna-design\n","authors":["Chaitanya K. Joshi","Arian R. Jamasb","Ramon Viñas","Charles Harris","Simon Mathis","Alex Morehead","Pietro Liò"],"pdf_url":"https://arxiv.org/pdf/2305.14749v4.pdf","comment":"Previously titled 'Multi-State RNA Design with Geometric Multi-Graph\n  Neural Networks', presented at ICML 2023 Computational Biology Workshop"},{"id":"http://arxiv.org/abs/2401.12987v2","updated":"2024-03-31T09:55:51Z","published":"2024-01-16T07:18:41Z","title":"TelME: Teacher-leading Multimodal Fusion Network for Emotion Recognition\n  in Conversation","summary":"  Emotion Recognition in Conversation (ERC) plays a crucial role in enabling\ndialogue systems to effectively respond to user requests. The emotions in a\nconversation can be identified by the representations from various modalities,\nsuch as audio, visual, and text. However, due to the weak contribution of\nnon-verbal modalities to recognize emotions, multimodal ERC has always been\nconsidered a challenging task. In this paper, we propose Teacher-leading\nMultimodal fusion network for ERC (TelME). TelME incorporates cross-modal\nknowledge distillation to transfer information from a language model acting as\nthe teacher to the non-verbal students, thereby optimizing the efficacy of the\nweak modalities. We then combine multimodal features using a shifting fusion\napproach in which student networks support the teacher. TelME achieves\nstate-of-the-art performance in MELD, a multi-speaker conversation dataset for\nERC. Finally, we demonstrate the effectiveness of our components through\nadditional experiments.\n","authors":["Taeyang Yun","Hyunkuk Lim","Jeonghwan Lee","Min Song"],"pdf_url":"https://arxiv.org/pdf/2401.12987v2.pdf","comment":"NAACL 2024 main conference"},{"id":"http://arxiv.org/abs/2310.14770v2","updated":"2024-03-31T09:37:14Z","published":"2023-10-23T10:13:35Z","title":"Theoretically Grounded Loss Functions and Algorithms for Score-Based\n  Multi-Class Abstention","summary":"  Learning with abstention is a key scenario where the learner can abstain from\nmaking a prediction at some cost. In this paper, we analyze the score-based\nformulation of learning with abstention in the multi-class classification\nsetting. We introduce new families of surrogate losses for the abstention loss\nfunction, which include the state-of-the-art surrogate losses in the\nsingle-stage setting and a novel family of loss functions in the two-stage\nsetting. We prove strong non-asymptotic and hypothesis set-specific consistency\nguarantees for these surrogate losses, which upper-bound the estimation error\nof the abstention loss function in terms of the estimation error of the\nsurrogate loss. Our bounds can help compare different score-based surrogates\nand guide the design of novel abstention algorithms by minimizing the proposed\nsurrogate losses. We experimentally evaluate our new algorithms on CIFAR-10,\nCIFAR-100, and SVHN datasets and the practical significance of our new\nsurrogate losses and two-stage abstention algorithms. Our results also show\nthat the relative performance of the state-of-the-art score-based surrogate\nlosses can vary across datasets.\n","authors":["Anqi Mao","Mehryar Mohri","Yutao Zhong"],"pdf_url":"https://arxiv.org/pdf/2310.14770v2.pdf","comment":"AISTATS 2024"},{"id":"http://arxiv.org/abs/2312.08255v3","updated":"2024-03-31T09:33:50Z","published":"2023-12-13T16:18:40Z","title":"OCTDL: Optical Coherence Tomography Dataset for Image-Based Deep\n  Learning Methods","summary":"  Optical coherence tomography (OCT) is a non-invasive imaging technique with\nextensive clinical applications in ophthalmology. OCT enables the visualization\nof the retinal layers, playing a vital role in the early detection and\nmonitoring of retinal diseases. OCT uses the principle of light wave\ninterference to create detailed images of the retinal microstructures, making\nit a valuable tool for diagnosing ocular conditions. This work presents an\nopen-access OCT dataset (OCTDL) comprising over 2000 OCT images labeled\naccording to disease group and retinal pathology. The dataset consists of OCT\nrecords of patients with Age-related Macular Degeneration (AMD), Diabetic\nMacular Edema (DME), Epiretinal Membrane (ERM), Retinal Artery Occlusion (RAO),\nRetinal Vein Occlusion (RVO), and Vitreomacular Interface Disease (VID). The\nimages were acquired with an Optovue Avanti RTVue XR using raster scanning\nprotocols with dynamic scan length and image resolution. Each retinal b-scan\nwas acquired by centering on the fovea and interpreted and cataloged by an\nexperienced retinal specialist. In this work, we applied Deep Learning\nclassification techniques to this new open-access dataset.\n","authors":["Mikhail Kulyabin","Aleksei Zhdanov","Anastasia Nikiforova","Andrey Stepichev","Anna Kuznetsova","Mikhail Ronkin","Vasilii Borisov","Alexander Bogachev","Sergey Korotkich","Paul A Constable","Andreas Maier"],"pdf_url":"https://arxiv.org/pdf/2312.08255v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14774v2","updated":"2024-03-31T09:15:36Z","published":"2023-10-23T10:19:09Z","title":"Principled Approaches for Learning to Defer with Multiple Experts","summary":"  We present a study of surrogate losses and algorithms for the general problem\nof learning to defer with multiple experts. We first introduce a new family of\nsurrogate losses specifically tailored for the multiple-expert setting, where\nthe prediction and deferral functions are learned simultaneously. We then prove\nthat these surrogate losses benefit from strong $H$-consistency bounds. We\nillustrate the application of our analysis through several examples of\npractical surrogate losses, for which we give explicit guarantees. These loss\nfunctions readily lead to the design of new learning to defer algorithms based\non their minimization. While the main focus of this work is a theoretical\nanalysis, we also report the results of several experiments on SVHN and\nCIFAR-10 datasets.\n","authors":["Anqi Mao","Mehryar Mohri","Yutao Zhong"],"pdf_url":"https://arxiv.org/pdf/2310.14774v2.pdf","comment":"ISAIM 2024"},{"id":"http://arxiv.org/abs/2302.11774v2","updated":"2024-03-31T09:09:16Z","published":"2023-02-23T04:26:34Z","title":"Semantic-Fused Multi-Granularity Cross-City Traffic Prediction","summary":"  Accurate traffic prediction is essential for effective urban management and\nthe improvement of transportation efficiency. Recently, data-driven traffic\nprediction methods have been widely adopted, with better performance than\ntraditional approaches. However, they often require large amounts of data for\neffective training, which becomes challenging given the prevalence of data\nscarcity in regions with inadequate sensing infrastructures. To address this\nissue, we propose a Semantic-Fused Multi-Granularity Transfer Learning (SFMGTL)\nmodel to achieve knowledge transfer across cities with fused semantics at\ndifferent granularities. In detail, we design a semantic fusion module to fuse\nvarious semantics while conserving static spatial dependencies via\nreconstruction losses. Then, a fused graph is constructed based on node\nfeatures through graph structure learning. Afterwards, we implement\nhierarchical node clustering to generate graphs with different granularity. To\nextract feasible meta-knowledge, we further introduce common and private\nmemories and obtain domain-invariant features via adversarial training. It is\nworth noting that our work jointly addresses semantic fusion and\nmulti-granularity issues in transfer learning. We conduct extensive experiments\non six real-world datasets to verify the effectiveness of our SFMGTL model by\ncomparing it with other state-of-the-art baselines. Afterwards, we also perform\nablation and case studies, demonstrating that our model possesses substantially\nfewer parameters compared to baseline models. Moreover, we illustrate how\nknowledge transfer aids the model in accurately predicting demands, especially\nduring peak hours. The codes can be found at\nhttps://github.com/zeonchen/SFMGTL.\n","authors":["Kehua Chen","Yuxuan Liang","Jindong Han","Siyuan Feng","Meixin Zhu","Hai Yang"],"pdf_url":"https://arxiv.org/pdf/2302.11774v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14772v2","updated":"2024-03-31T09:05:24Z","published":"2023-10-23T10:16:27Z","title":"Predictor-Rejector Multi-Class Abstention: Theoretical Analysis and\n  Algorithms","summary":"  We study the key framework of learning with abstention in the multi-class\nclassification setting. In this setting, the learner can choose to abstain from\nmaking a prediction with some pre-defined cost. We present a series of new\ntheoretical and algorithmic results for this learning problem in the\npredictor-rejector framework. We introduce several new families of surrogate\nlosses for which we prove strong non-asymptotic and hypothesis set-specific\nconsistency guarantees, thereby resolving positively two existing open\nquestions. These guarantees provide upper bounds on the estimation error of the\nabstention loss function in terms of that of the surrogate loss. We analyze\nboth a single-stage setting where the predictor and rejector are learned\nsimultaneously and a two-stage setting crucial in applications, where the\npredictor is learned in a first stage using a standard surrogate loss such as\ncross-entropy. These guarantees suggest new multi-class abstention algorithms\nbased on minimizing these surrogate losses. We also report the results of\nextensive experiments comparing these algorithms to the current\nstate-of-the-art algorithms on CIFAR-10, CIFAR-100 and SVHN datasets. Our\nresults demonstrate empirically the benefit of our new surrogate losses and\nshow the remarkable performance of our broadly applicable two-stage abstention\nalgorithm.\n","authors":["Anqi Mao","Mehryar Mohri","Yutao Zhong"],"pdf_url":"https://arxiv.org/pdf/2310.14772v2.pdf","comment":"ALT 2024"},{"id":"http://arxiv.org/abs/2402.16326v3","updated":"2024-03-31T08:45:51Z","published":"2024-02-26T06:20:28Z","title":"A Provably Accurate Randomized Sampling Algorithm for Logistic\n  Regression","summary":"  In statistics and machine learning, logistic regression is a widely-used\nsupervised learning technique primarily employed for binary classification\ntasks. When the number of observations greatly exceeds the number of predictor\nvariables, we present a simple, randomized sampling-based algorithm for\nlogistic regression problem that guarantees high-quality approximations to both\nthe estimated probabilities and the overall discrepancy of the model. Our\nanalysis builds upon two simple structural conditions that boil down to\nrandomized matrix multiplication, a fundamental and well-understood primitive\nof randomized numerical linear algebra. We analyze the properties of estimated\nprobabilities of logistic regression when leverage scores are used to sample\nobservations, and prove that accurate approximations can be achieved with a\nsample whose size is much smaller than the total number of observations. To\nfurther validate our theoretical findings, we conduct comprehensive empirical\nevaluations. Overall, our work sheds light on the potential of using randomized\nsampling approaches to efficiently approximate the estimated probabilities in\nlogistic regression, offering a practical and computationally efficient\nsolution for large-scale datasets.\n","authors":["Agniva Chowdhury","Pradeep Ramuhalli"],"pdf_url":"https://arxiv.org/pdf/2402.16326v3.pdf","comment":"Published in the proceedings of AAAI 2024"},{"id":"http://arxiv.org/abs/2307.16120v4","updated":"2024-03-31T08:40:15Z","published":"2023-07-30T03:59:47Z","title":"Deep Unrolling Networks with Recurrent Momentum Acceleration for\n  Nonlinear Inverse Problems","summary":"  Combining the strengths of model-based iterative algorithms and data-driven\ndeep learning solutions, deep unrolling networks (DuNets) have become a popular\ntool to solve inverse imaging problems. While DuNets have been successfully\napplied to many linear inverse problems, nonlinear problems tend to impair the\nperformance of the method. Inspired by momentum acceleration techniques that\nare often used in optimization algorithms, we propose a recurrent momentum\nacceleration (RMA) framework that uses a long short-term memory recurrent\nneural network (LSTM-RNN) to simulate the momentum acceleration process. The\nRMA module leverages the ability of the LSTM-RNN to learn and retain knowledge\nfrom the previous gradients. We apply RMA to two popular DuNets -- the learned\nproximal gradient descent (LPGD) and the learned primal-dual (LPD) methods,\nresulting in LPGD-RMA and LPD-RMA respectively. We provide experimental results\non two nonlinear inverse problems: a nonlinear deconvolution problem, and an\nelectrical impedance tomography problem with limited boundary measurements. In\nthe first experiment we have observed that the improvement due to RMA largely\nincreases with respect to the nonlinearity of the problem. The results of the\nsecond example further demonstrate that the RMA schemes can significantly\nimprove the performance of DuNets in strongly ill-posed problems.\n","authors":["Qingping Zhou","Jiayu Qian","Junqi Tang","Jinglai Li"],"pdf_url":"https://arxiv.org/pdf/2307.16120v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12835v2","updated":"2024-03-31T08:19:12Z","published":"2023-12-20T08:36:55Z","title":"Near-Optimal Resilient Aggregation Rules for Distributed Learning Using\n  1-Center and 1-Mean Clustering with Outliers","summary":"  Byzantine machine learning has garnered considerable attention in light of\nthe unpredictable faults that can occur in large-scale distributed learning\nsystems. The key to secure resilience against Byzantine machines in distributed\nlearning is resilient aggregation mechanisms. Although abundant resilient\naggregation rules have been proposed, they are designed in ad-hoc manners,\nimposing extra barriers on comparing, analyzing, and improving the rules across\nperformance criteria. This paper studies near-optimal aggregation rules using\nclustering in the presence of outliers. Our outlier-robust clustering approach\nutilizes geometric properties of the update vectors provided by workers. Our\nanalysis show that constant approximations to the 1-center and 1-mean\nclustering problems with outliers provide near-optimal resilient aggregators\nfor metric-based criteria, which have been proven to be crucial in the\nhomogeneous and heterogeneous cases respectively. In addition, we discuss two\ncontradicting types of attacks under which no single aggregation rule is\nguaranteed to improve upon the naive average. Based on the discussion, we\npropose a two-phase resilient aggregation framework. We run experiments for\nimage classification using a non-convex loss function. The proposed algorithms\noutperform previously known aggregation rules by a large margin with both\nhomogeneous and heterogeneous data distributions among non-faulty workers. Code\nand appendix are available at https://github.com/jerry907/AAAI24-RASHB.\n","authors":["Yuhao Yi","Ronghui You","Hong Liu","Changxin Liu","Yuan Wang","Jiancheng Lv"],"pdf_url":"https://arxiv.org/pdf/2312.12835v2.pdf","comment":"17 pages, 4 figures. Accepted by the 38th Annual AAAI Conference on\n  Artificial Intelligence (AAAI'24)"},{"id":"http://arxiv.org/abs/2309.03835v3","updated":"2024-03-31T07:53:19Z","published":"2023-09-07T16:49:38Z","title":"Instructing Robots by Sketching: Learning from Demonstration via\n  Probabilistic Diagrammatic Teaching","summary":"  Learning for Demonstration (LfD) enables robots to acquire new skills by\nimitating expert demonstrations, allowing users to communicate their\ninstructions in an intuitive manner. Recent progress in LfD often relies on\nkinesthetic teaching or teleoperation as the medium for users to specify the\ndemonstrations. Kinesthetic teaching requires physical handling of the robot,\nwhile teleoperation demands proficiency with additional hardware. This paper\nintroduces an alternative paradigm for LfD called Diagrammatic Teaching.\nDiagrammatic Teaching aims to teach robots novel skills by prompting the user\nto sketch out demonstration trajectories on 2D images of the scene, these are\nthen synthesised as a generative model of motion trajectories in 3D task space.\nAdditionally, we present the Ray-tracing Probabilistic Trajectory Learning\n(RPTL) framework for Diagrammatic Teaching. RPTL extracts time-varying\nprobability densities from the 2D sketches, applies ray-tracing to find\ncorresponding regions in 3D Cartesian space, and fits a probabilistic model of\nmotion trajectories to these regions. New motion trajectories, which mimic\nthose sketched by the user, can then be generated from the probabilistic model.\nWe empirically validate our framework both in simulation and on real robots,\nwhich include a fixed-base manipulator and a quadruped-mounted manipulator.\n","authors":["Weiming Zhi","Tianyi Zhang","Matthew Johnson-Roberson"],"pdf_url":"https://arxiv.org/pdf/2309.03835v3.pdf","comment":"To appear in ICRA 2024"},{"id":"http://arxiv.org/abs/2210.09846v3","updated":"2024-03-31T07:50:22Z","published":"2022-10-15T11:00:54Z","title":"G-PECNet: Towards a Generalizable Pedestrian Trajectory Prediction\n  System","summary":"  Navigating dynamic physical environments without obstructing or damaging\nhuman assets is of quintessential importance for social robots. In this work,\nwe solve autonomous drone navigation's sub-problem of predicting out-of-domain\nhuman and agent trajectories using a deep generative model. Our method:\nGeneral-PECNet or G-PECNet observes an improvement of 9.5\\% on the Final\nDisplacement Error (FDE) on 2020's benchmark: PECNet through a combination of\narchitectural improvements inspired by periodic activation functions and\nsynthetic trajectory (data) augmentations using Hidden Markov Models (HMMs) and\nReinforcement Learning (RL). Additionally, we propose a simple\ngeometry-inspired metric for trajectory non-linearity and outlier detection,\nhelpful for the task. Code available at\nhttps://github.com/Aryan-Garg/PECNet-Pedestrian-Trajectory-Prediction.git\n","authors":["Aryan Garg","Renu M. Rameshan"],"pdf_url":"https://arxiv.org/pdf/2210.09846v3.pdf","comment":"Notable ICLR Tiny Paper 2024"},{"id":"http://arxiv.org/abs/2308.04185v2","updated":"2024-03-31T05:11:35Z","published":"2023-08-08T11:10:42Z","title":"Iterative Sketching for Secure Coded Regression","summary":"  Linear regression is a fundamental and primitive problem in supervised\nmachine learning, with applications ranging from epidemiology to finance. In\nthis work, we propose methods for speeding up distributed linear regression. We\ndo so by leveraging randomized techniques, while also ensuring security and\nstraggler resiliency in asynchronous distributed computing systems.\nSpecifically, we randomly rotate the basis of the system of equations and then\nsubsample blocks, to simultaneously secure the information and reduce the\ndimension of the regression problem. In our setup, the basis rotation\ncorresponds to an encoded encryption in an approximate gradient coding scheme,\nand the subsampling corresponds to the responses of the non-straggling servers\nin the centralized coded computing framework. This results in a distributive\niterative stochastic approach for matrix compression and steepest descent.\n","authors":["Neophytos Charalambides","Hessam Mahdavifar","Mert Pilanci","Alfred O. Hero III"],"pdf_url":"https://arxiv.org/pdf/2308.04185v2.pdf","comment":"29 pages, 8 figures. arXiv admin note: substantial text overlap with\n  arXiv:2201.08522"},{"id":"http://arxiv.org/abs/2312.03041v2","updated":"2024-03-31T04:21:08Z","published":"2023-12-05T15:54:13Z","title":"Transformer-Based Deep Learning Model for Bored Pile Load-Deformation\n  Prediction in Bangkok Subsoil","summary":"  This paper presents a novel deep learning model based on the transformer\narchitecture to predict the load-deformation behavior of large bored piles in\nBangkok subsoil. The model encodes the soil profile and pile features as\ntokenization input, and generates the load-deformation curve as output. The\nmodel also incorporates the previous sequential data of load-deformation curve\ninto the decoder to improve the prediction accuracy. The model also\nincorporates the previous sequential data of load-deformation curve into the\ndecoder. The model shows a satisfactory accuracy and generalization ability for\nthe load-deformation curve prediction, with a mean absolute error of 5.72% for\nthe test data. The model could also be used for parametric analysis and design\noptimization of piles under different soil and pile conditions, pile cross\nsection, pile length and type of pile.\n","authors":["Sompote Youwai","Chissanupong Thongnoo"],"pdf_url":"https://arxiv.org/pdf/2312.03041v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15651v2","updated":"2024-03-31T04:20:36Z","published":"2023-08-29T22:03:17Z","title":"Ensuring User-side Fairness in Dynamic Recommender Systems","summary":"  User-side group fairness is crucial for modern recommender systems, aiming to\nalleviate performance disparities among user groups defined by sensitive\nattributes like gender, race, or age. In the ever-evolving landscape of\nuser-item interactions, continual adaptation to newly collected data is crucial\nfor recommender systems to stay aligned with the latest user preferences.\nHowever, we observe that such continual adaptation often exacerbates\nperformance disparities. This necessitates a thorough investigation into\nuser-side fairness in dynamic recommender systems, an area that has been\nunexplored in the literature. This problem is challenging due to distribution\nshifts, frequent model updates, and non-differentiability of ranking metrics.\nTo our knowledge, this paper presents the first principled study on ensuring\nuser-side fairness in dynamic recommender systems. We start with theoretical\nanalyses on fine-tuning v.s. retraining, showing that the best practice is\nincremental fine-tuning with restart. Guided by our theoretical analyses, we\npropose FAir Dynamic rEcommender (FADE), an end-to-end fine-tuning framework to\ndynamically ensure user-side fairness over time. To overcome the\nnon-differentiability of recommendation metrics in the fairness loss, we\nfurther introduce Differentiable Hit (DH) as an improvement over the recent\nNeuralNDCG method, not only alleviating its gradient vanishing issue but also\nachieving higher efficiency. Besides that, we also address the instability\nissue of the fairness loss by leveraging the competing nature between the\nrecommendation loss and the fairness loss. Through extensive experiments on\nreal-world datasets, we demonstrate that FADE effectively and efficiently\nreduces performance disparities with little sacrifice in the overall\nrecommendation performance.\n","authors":["Hyunsik Yoo","Zhichen Zeng","Jian Kang","Ruizhong Qiu","David Zhou","Zhining Liu","Fei Wang","Charlie Xu","Eunice Chan","Hanghang Tong"],"pdf_url":"https://arxiv.org/pdf/2308.15651v2.pdf","comment":"19 pages, 20 figures, 2 tables, ACM Web Conference 2024"},{"id":"http://arxiv.org/abs/2310.08461v2","updated":"2024-03-31T03:06:51Z","published":"2023-10-12T16:21:04Z","title":"DistillSpec: Improving Speculative Decoding via Knowledge Distillation","summary":"  Speculative decoding (SD) accelerates large language model inference by\nemploying a faster draft model for generating multiple tokens, which are then\nverified in parallel by the larger target model, resulting in the text\ngenerated according to the target model distribution. However, identifying a\ncompact draft model that is well-aligned with the target model is challenging.\nTo tackle this issue, we propose DistillSpec that uses knowledge distillation\nto better align the draft model with the target model, before applying SD.\nDistillSpec makes two key design choices, which we demonstrate via systematic\nstudy to be crucial to improving the draft and target alignment: utilizing\non-policy data generation from the draft model, and tailoring the divergence\nfunction to the task and decoding strategy. Notably, DistillSpec yields\nimpressive 10 - 45% speedups over standard SD on a range of standard\nbenchmarks, using both greedy and non-greedy sampling. Furthermore, we combine\nDistillSpec with lossy SD to achieve fine-grained control over the latency vs.\ntask performance trade-off. Finally, in practical scenarios with models of\nvarying sizes, first using distillation to boost the performance of the target\nmodel and then applying DistillSpec to train a well-aligned draft model can\nreduce decoding latency by 6-10x with minimal performance drop, compared to\nstandard decoding without distillation.\n","authors":["Yongchao Zhou","Kaifeng Lyu","Ankit Singh Rawat","Aditya Krishna Menon","Afshin Rostamizadeh","Sanjiv Kumar","Jean-François Kagy","Rishabh Agarwal"],"pdf_url":"https://arxiv.org/pdf/2310.08461v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17561v2","updated":"2024-03-31T03:02:10Z","published":"2024-03-26T10:10:53Z","title":"A Survey on Deep Learning and State-of-the-art Applications","summary":"  Deep learning, a branch of artificial intelligence, is a computational model\nthat uses multiple layers of interconnected units (neurons) to learn intricate\npatterns and representations directly from raw input data. Empowered by this\nlearning capability, it has become a powerful tool for solving complex problems\nand is the core driver of many groundbreaking technologies and innovations.\nBuilding a deep learning model is a challenging task due to the algorithm`s\ncomplexity and the dynamic nature of real-world problems. Several studies have\nreviewed deep learning concepts and applications. However, the studies mostly\nfocused on the types of deep learning models and convolutional neural network\narchitectures, offering limited coverage of the state-of-the-art of deep\nlearning models and their applications in solving complex problems across\ndifferent domains. Therefore, motivated by the limitations, this study aims to\ncomprehensively review the state-of-the-art deep learning models in computer\nvision, natural language processing, time series analysis and pervasive\ncomputing. We highlight the key features of the models and their effectiveness\nin solving the problems within each domain. Furthermore, this study presents\nthe fundamentals of deep learning, various deep learning model types and\nprominent convolutional neural network architectures. Finally, challenges and\nfuture directions in deep learning research are discussed to offer a broader\nperspective for future researchers.\n","authors":["Mohd Halim Mohd Noor","Ayokunle Olalekan Ige"],"pdf_url":"https://arxiv.org/pdf/2403.17561v2.pdf","comment":"Submitted to Elsevier Neural Networks"},{"id":"http://arxiv.org/abs/2401.11351v2","updated":"2024-03-31T00:32:13Z","published":"2024-01-21T00:19:16Z","title":"A comprehensive review of Quantum Machine Learning: from NISQ to Fault\n  Tolerance","summary":"  Quantum machine learning, which involves running machine learning algorithms\non quantum devices, has garnered significant attention in both academic and\nbusiness circles. In this paper, we offer a comprehensive and unbiased review\nof the various concepts that have emerged in the field of quantum machine\nlearning. This includes techniques used in Noisy Intermediate-Scale Quantum\n(NISQ) technologies and approaches for algorithms compatible with\nfault-tolerant quantum computing hardware. Our review covers fundamental\nconcepts, algorithms, and the statistical learning theory pertinent to quantum\nmachine learning.\n","authors":["Yunfei Wang","Junyu Liu"],"pdf_url":"https://arxiv.org/pdf/2401.11351v2.pdf","comment":"28 pages. Invited review"}],"Multimedia":[{"id":"http://arxiv.org/abs/2403.19456v2","updated":"2024-03-31T13:26:11Z","published":"2024-03-28T14:27:36Z","title":"Break-for-Make: Modular Low-Rank Adaptations for Composable\n  Content-Style Customization","summary":"  Personalized generation paradigms empower designers to customize visual\nintellectual properties with the help of textual descriptions by tuning or\nadapting pre-trained text-to-image models on a few images. Recent works explore\napproaches for concurrently customizing both content and detailed visual style\nappearance. However, these existing approaches often generate images where the\ncontent and style are entangled. In this study, we reconsider the customization\nof content and style concepts from the perspective of parameter space\nconstruction. Unlike existing methods that utilize a shared parameter space for\ncontent and style, we propose a learning framework that separates the parameter\nspace to facilitate individual learning of content and style, thereby enabling\ndisentangled content and style. To achieve this goal, we introduce \"partly\nlearnable projection\" (PLP) matrices to separate the original adapters into\ndivided sub-parameter spaces. We propose \"break-for-make\" customization\nlearning pipeline based on PLP, which is simple yet effective. We break the\noriginal adapters into \"up projection\" and \"down projection\", train content and\nstyle PLPs individually with the guidance of corresponding textual prompts in\nthe separate adapters, and maintain generalization by employing a\nmulti-correspondence projection learning strategy. Based on the adapters broken\napart for separate training content and style, we then make the entity\nparameter space by reconstructing the content and style PLPs matrices, followed\nby fine-tuning the combined adapter to generate the target object with the\ndesired appearance. Experiments on various styles, including textures,\nmaterials, and artistic style, show that our method outperforms\nstate-of-the-art single/multiple concept learning pipelines in terms of\ncontent-style-prompt alignment.\n","authors":["Yu Xu","Fan Tang","Juan Cao","Yuxin Zhang","Oliver Deussen","Weiming Dong","Jintao Li","Tong-Yee Lee"],"pdf_url":"https://arxiv.org/pdf/2403.19456v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.04192v4","updated":"2024-03-31T12:10:24Z","published":"2023-07-09T14:54:30Z","title":"Self-Adaptive Sampling for Efficient Video Question-Answering on\n  Image--Text Models","summary":"  Video question-answering is a fundamental task in the field of video\nunderstanding. Although current vision--language models (VLMs) equipped with\nVideo Transformers have enabled temporal modeling and yielded superior results,\nthey are at the cost of huge computational power and thus too expensive to\ndeploy in real-time application scenarios. An economical workaround only\nsamples a small portion of frames to represent the main content of that video\nand tune an image--text model on these sampled frames. Recent video\nunderstanding models usually randomly sample a set of frames or clips,\nregardless of internal correlations between their visual contents, nor their\nrelevance to the problem. We argue that such kinds of aimless sampling may omit\nthe key frames from which the correct answer can be deduced, and the situation\ngets worse when the sampling sparsity increases, which always happens as the\nvideo lengths increase. To mitigate this issue, we propose two frame sampling\nstrategies, namely the most domain frames (MDF) and most implied frames (MIF),\nto maximally preserve those frames that are most likely vital to the given\nquestions. MDF passively minimizes the risk of key frame omission in a\nbootstrap manner, while MIS actively searches key frames customized for each\nvideo--question pair with the assistance of auxiliary models. The experimental\nresults on three public datasets from three advanced VLMs (CLIP, GIT and\nAll-in-one) demonstrate that our proposed strategies can boost the performance\nfor image-text pretrained models. The source codes pertaining to the method\nproposed in this paper are publicly available at\nhttps://github.com/declare-lab/sas-vqa.\n","authors":["Wei Han","Hui Chen","Min-Yen Kan","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2307.04192v4.pdf","comment":"13 pages, 7 figures, accepted to Findings of NAACL 2024"}]},"2024-03-30T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2402.11271v2","updated":"2024-03-30T22:05:59Z","published":"2024-02-17T13:02:54Z","title":"MONAL: Model Autophagy Analysis for Modeling Human-AI Interactions","summary":"  The increasing significance of large models and their multi-modal variants in\nsocietal information processing has ignited debates on social safety and\nethics. However, there exists a paucity of comprehensive analysis for: (i) the\ninteractions between human and artificial intelligence systems, and (ii)\nunderstanding and addressing the associated limitations. To bridge this gap, we\npropose Model Autophagy Analysis (MONAL) for large models' self-consumption\nexplanation. MONAL employs two distinct autophagous loops (referred to as\n``self-consumption loops'') to elucidate the suppression of human-generated\ninformation in the exchange between human and AI systems. Through comprehensive\nexperiments on diverse datasets, we evaluate the capacities of generated models\nas both creators and disseminators of information. Our key findings reveal (i)\nA progressive prevalence of model-generated synthetic information over time\nwithin training datasets compared to human-generated information; (ii) The\ndiscernible tendency of large models, when acting as information transmitters\nacross multiple iterations, to selectively modify or prioritize specific\ncontents; and (iii) The potential for a reduction in the diversity of socially\nor human-generated information, leading to bottlenecks in the performance\nenhancement of large models and confining them to local optima.\n","authors":["Shu Yang","Muhammad Asif Ali","Lu Yu","Lijie Hu","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2402.11271v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07889v2","updated":"2024-03-30T22:00:22Z","published":"2023-10-11T20:52:30Z","title":"LangNav: Language as a Perceptual Representation for Navigation","summary":"  We explore the use of language as a perceptual representation for\nvision-and-language navigation (VLN), with a focus on low-data settings. Our\napproach uses off-the-shelf vision systems for image captioning and object\ndetection to convert an agent's egocentric panoramic view at each time step\ninto natural language descriptions. We then finetune a pretrained language\nmodel to select an action, based on the current view and the trajectory\nhistory, that would best fulfill the navigation instructions. In contrast to\nthe standard setup which adapts a pretrained language model to work directly\nwith continuous visual features from pretrained vision models, our approach\ninstead uses (discrete) language as the perceptual representation. We explore\nseveral use cases of our language-based navigation (LangNav) approach on the\nR2R VLN benchmark: generating synthetic trajectories from a prompted language\nmodel (GPT-4) with which to finetune a smaller language model; domain transfer\nwhere we transfer a policy learned on one simulated environment (ALFRED) to\nanother (more realistic) environment (R2R); and combining both vision- and\nlanguage-based representations for VLN. Our approach is found to improve upon\nbaselines that rely on visual features in settings where only a few expert\ntrajectories (10-100) are available, demonstrating the potential of language as\na perceptual representation for navigation.\n","authors":["Bowen Pan","Rameswar Panda","SouYoung Jin","Rogerio Feris","Aude Oliva","Phillip Isola","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2310.07889v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11517v3","updated":"2024-03-30T21:14:37Z","published":"2023-12-12T19:34:23Z","title":"A Natural Language Processing-Based Classification and Mode-Based\n  Ranking of Musculoskeletal Disorder Risk Factors","summary":"  This research delves into Musculoskeletal Disorder (MSD) risk factors, using\na blend of Natural Language Processing (NLP) and mode-based ranking. The aim is\nto refine understanding, classification, and prioritization for focused\nprevention and treatment. Eight NLP models are evaluated, combining pre-trained\ntransformers, cosine similarity, and distance metrics to categorize factors\ninto personal, biomechanical, workplace, psychological, and organizational\nclasses. BERT with cosine similarity achieves 28% accuracy; sentence\ntransformer with Euclidean, Bray-Curtis, and Minkowski distances scores 100%.\nWith 10-fold cross-validation, statistical tests ensure robust results. Survey\ndata and mode-based ranking determine severity hierarchy, aligning with the\nliterature. \"Working posture\" is the most severe, highlighting posture's role.\nSurvey insights emphasize \"Job insecurity,\" \"Effort reward imbalance,\" and\n\"Poor employee facility\" as significant contributors. Rankings offer actionable\ninsights for MSD prevention. The study suggests targeted interventions,\nworkplace improvements, and future research directions. This integrated NLP and\nranking approach enhances MSD comprehension and informs occupational health\nstrategies.\n","authors":["Md Abrar Jahin","Subrata Talapatra"],"pdf_url":"https://arxiv.org/pdf/2312.11517v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07148v2","updated":"2024-03-30T20:18:54Z","published":"2024-02-11T10:23:34Z","title":"X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for\n  Large Language Models with Applications in Protein Mechanics and Molecular\n  Design","summary":"  We report a mixture of expert strategy to create fine-tuned large language\nmodels using a deep layer-wise token-level approach based on low-rank\nadaptation (LoRA). Starting with a set of pre-trained LoRA adapters, our gating\nstrategy uses the hidden states to dynamically mix adapted layers, allowing the\nresulting X-LoRA model to draw upon different capabilities and create\nnever-before-used deep layer-wise combinations to solve tasks. The design is\ninspired by the biological principles of universality and diversity, where\nneural network building blocks are reused in different hierarchical\nmanifestations. Hence, the X-LoRA model can be easily implemented for any\nexisting large language model (LLM) without a need for modifications of the\nunderlying structure. We develop a tailored X-LoRA model that offers scientific\ncapabilities including forward/inverse analysis tasks and enhanced reasoning\ncapability, focused on biomaterial analysis, protein mechanics and design. The\nimpact of this work include access to readily expandable and adaptable models\nwith strong domain knowledge and the capability to integrate across areas of\nknowledge. Featuring experts in biology, mathematics, reasoning, bio-inspired\nmaterials, mechanics and materials, chemistry, protein biophysics, mechanics\nand quantum-mechanics based molecular properties, we conduct a series of\nphysics-focused case studies. We examine knowledge recall, protein mechanics\nforward/inverse tasks, protein design, adversarial agentic modeling including\nontological knowledge graph construction, as well as molecular design. The\nmodel is capable not only of making quantitative predictions of nanomechanical\nproperties of proteins or quantum mechanical molecular properties, but also\nreasons over the results and correctly predicts likely mechanisms that explain\ndistinct molecular behaviors.\n","authors":["Eric L. Buehler","Markus J. Buehler"],"pdf_url":"https://arxiv.org/pdf/2402.07148v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.00739v4","updated":"2024-03-30T17:22:44Z","published":"2023-05-26T21:39:05Z","title":"SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL\n  (extended)","summary":"  Text-to-SQL, the process of translating natural language into Structured\nQuery Language (SQL), represents a transformative application of large language\nmodels (LLMs), potentially revolutionizing how humans interact with data. This\npaper introduces the SQL-PaLM framework, a comprehensive solution for\nunderstanding and enhancing Text-to-SQL using LLMs, using in the learning\nregimes of few-shot prompting and instruction fine-tuning. With few-shot\nprompting, we explore the effectiveness of consistency decoding with\nexecution-based error filtering. With instruction fine-tuning, we delve deep in\nunderstanding the critical paradigms that influence the performance of tuned\nLLMs. In particular, we investigate how performance can be improved through\nexpanded training data coverage and diversity, synthetic data augmentation, and\nintegrating query-specific database content. We propose a test-time selection\nmethod to further refine accuracy by integrating SQL outputs from multiple\nparadigms with execution feedback as guidance. Additionally, we tackle the\npractical challenge of navigating intricate databases with a significant number\nof tables and columns, proposing efficient techniques for accurately selecting\nrelevant database elements to enhance Text-to-SQL performance. Our holistic\napproach yields substantial advancements in Text-to-SQL, as demonstrated on two\nkey public benchmarks, Spider and BIRD. Through comprehensive ablations and\nerror analyses, we shed light on the strengths and weaknesses of our framework,\noffering valuable insights into Text-to-SQL's future work.\n","authors":["Ruoxi Sun","Sercan Ö. Arik","Alex Muzio","Lesly Miculicich","Satya Gundabathula","Pengcheng Yin","Hanjun Dai","Hootan Nakhost","Rajarishi Sinha","Zifeng Wang","Tomas Pfister"],"pdf_url":"https://arxiv.org/pdf/2306.00739v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.08591v2","updated":"2024-03-30T17:13:58Z","published":"2023-09-15T17:45:28Z","title":"Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation\n  into Multicultural Proverbs and Sayings","summary":"  Large language models (LLMs) are highly adept at question answering and\nreasoning tasks, but when reasoning in a situational context, human\nexpectations vary depending on the relevant cultural common ground. As\nlanguages are associated with diverse cultures, LLMs should also be\nculturally-diverse reasoners. In this paper, we study the ability of a wide\nrange of state-of-the-art multilingual LLMs (mLLMs) to reason with proverbs and\nsayings in a conversational context. Our experiments reveal that: (1) mLLMs\n\"know\" limited proverbs and memorizing proverbs does not mean understanding\nthem within a conversational context; (2) mLLMs struggle to reason with\nfigurative proverbs and sayings, and when asked to select the wrong answer\n(instead of asking it to select the correct answer); and (3) there is a\n\"culture gap\" in mLLMs when reasoning about proverbs and sayings translated\nfrom other languages. We construct and release our evaluation dataset MAPS\n(MulticultrAl Proverbs and Sayings) for proverb understanding with\nconversational context for six different languages.\n","authors":["Chen Cecilia Liu","Fajri Koto","Timothy Baldwin","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2309.08591v2.pdf","comment":"NAACL"},{"id":"http://arxiv.org/abs/2403.11124v2","updated":"2024-03-30T16:48:16Z","published":"2024-03-17T07:08:55Z","title":"Scaling Data Diversity for Fine-Tuning Language Models in Human\n  Alignment","summary":"  Alignment with human preference prevents large language models (LLMs) from\ngenerating misleading or toxic content while requiring high-cost human\nfeedback. Assuming resources of human annotation are limited, there are two\ndifferent ways of allocating considered: more diverse PROMPTS or more diverse\nRESPONSES to be labeled. Nonetheless, a straightforward comparison between\ntheir impact is absent. In this work, we first control the diversity of both\nsides according to the number of samples for fine-tuning, which can directly\nreflect their influence. We find that instead of numerous prompts, more\nresponses but fewer prompts better trigger LLMs for human alignment.\nAdditionally, the concept of diversity for prompts can be more complex than\nresponses that are typically quantified by single digits. Consequently, a new\nformulation of prompt diversity is proposed, further implying a linear\ncorrelation with the final performance of LLMs after fine-tuning. We also\nleverage it on data augmentation and conduct experiments to show its effect on\ndifferent algorithms.\n","authors":["Feifan Song","Bowen Yu","Hao Lang","Haiyang Yu","Fei Huang","Houfeng Wang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2403.11124v2.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2401.14295v2","updated":"2024-03-30T16:33:36Z","published":"2024-01-25T16:34:00Z","title":"Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of\n  Thoughts","summary":"  The field of natural language processing (NLP) has witnessed significant\nprogress in recent years, with a notable focus on improving large language\nmodels' (LLM) performance through innovative prompting techniques. Among these,\nprompt engineering coupled with structures has emerged as a promising paradigm,\nwith designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts,\nin which the overall LLM reasoning is guided by a structure such as a graph. As\nillustrated with numerous examples, this paradigm significantly enhances the\nLLM's capability to solve numerous tasks, ranging from logical or mathematical\nreasoning to planning or creative writing. To facilitate the understanding of\nthis growing field and pave the way for future developments, we devise a\ngeneral blueprint for effective and efficient LLM reasoning schemes. For this,\nwe conduct an in-depth analysis of the prompt execution pipeline, clarifying\nand clearly defining different concepts. We then build the first taxonomy of\nstructure-enhanced LLM reasoning schemes. We focus on identifying fundamental\nclasses of harnessed structures, and we analyze the representations of these\nstructures, algorithms executed with these structures, and many others. We\nrefer to these structures as reasoning topologies, because their representation\nbecomes to a degree spatial, as they are contained within the LLM context. Our\nstudy compares existing prompting schemes using the proposed taxonomy,\ndiscussing how certain design choices lead to different patterns in performance\nand cost. We also outline theoretical underpinnings, relationships between\nprompting and other parts of the LLM ecosystem such as knowledge bases, and the\nassociated research challenges. Our work will help to advance future prompt\nengineering techniques.\n","authors":["Maciej Besta","Florim Memedi","Zhenyu Zhang","Robert Gerstenberger","Guangyuan Piao","Nils Blach","Piotr Nyczyk","Marcin Copik","Grzegorz Kwaśniewski","Jürgen Müller","Lukas Gianinazzi","Ales Kubicek","Hubert Niewiadomski","Aidan O'Mahony","Onur Mutlu","Torsten Hoefler"],"pdf_url":"https://arxiv.org/pdf/2401.14295v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10038v2","updated":"2024-03-30T16:10:47Z","published":"2024-02-15T16:00:58Z","title":"RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization\n  Method for Alignment of Large Language Models","summary":"  Reinforcement learning from human feedback (RLHF) has been extensively\nemployed to align large language models with user intent. However, proximal\npolicy optimization (PPO) based RLHF is occasionally unstable requiring\nsignificant hyperparameter finetuning, and computationally expensive to\nmaximize the estimated reward during alignment. Recently, direct preference\noptimization (DPO) is proposed to address those challenges. However, DPO relies\non contrastive responses generated from human annotator and alternative LLM,\ninstead of the policy model, limiting the effectiveness of the RLHF. In this\npaper, we addresses both challenges by systematically combining rejection\nsampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the\ndevelopment of a supervised fine-tuned policy model (SFT). A varied set of k\nresponses per prompt are sampled directly from the SFT model. RS-DPO identifies\npairs of contrastive samples based on their reward distribution. Finally, we\napply DPO with the contrastive samples to align the model to human preference.\nOur experiments indicate that our proposed method effectively fine-tunes LLMs\nwith limited resource environments, leading to improved alignment with user\nintent. Furthermore, it outperforms existing methods, including RS, PPO, and\nDPO.\n","authors":["Saeed Khaki","JinJin Li","Lan Ma","Liu Yang","Prathap Ramachandra"],"pdf_url":"https://arxiv.org/pdf/2402.10038v2.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.07676v2","updated":"2024-03-30T16:09:34Z","published":"2023-10-11T17:21:03Z","title":"Composite Backdoor Attacks Against Large Language Models","summary":"  Large language models (LLMs) have demonstrated superior performance compared\nto previous methods on various tasks, and often serve as the foundation models\nfor many researches and services. However, the untrustworthy third-party LLMs\nmay covertly introduce vulnerabilities for downstream tasks. In this paper, we\nexplore the vulnerability of LLMs through the lens of backdoor attacks.\nDifferent from existing backdoor attacks against LLMs, ours scatters multiple\ntrigger keys in different prompt components. Such a Composite Backdoor Attack\n(CBA) is shown to be stealthier than implanting the same multiple trigger keys\nin only a single component. CBA ensures that the backdoor is activated only\nwhen all trigger keys appear. Our experiments demonstrate that CBA is effective\nin both natural language processing (NLP) and multimodal tasks. For instance,\nwith $3\\%$ poisoning samples against the LLaMA-7B model on the Emotion dataset,\nour attack achieves a $100\\%$ Attack Success Rate (ASR) with a False Triggered\nRate (FTR) below $2.06\\%$ and negligible model accuracy degradation. Our work\nhighlights the necessity of increased security research on the trustworthiness\nof foundation LLMs.\n","authors":["Hai Huang","Zhengyu Zhao","Michael Backes","Yun Shen","Yang Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.07676v2.pdf","comment":"To Appear in Findings of the Association for Computational\n  Linguistics: NAACL 2024, June 2024"},{"id":"http://arxiv.org/abs/2308.10509v2","updated":"2024-03-30T16:09:14Z","published":"2023-08-21T06:50:29Z","title":"An Examination of the Compositionality of Large Generative\n  Vision-Language Models","summary":"  With the success of Large Language Models (LLMs), many Generative\nVision-Language Models (GVLMs) have been constructed via multimodal instruction\ntuning. However, the performance of GVLMs in multimodal compositional reasoning\nremains under-explored. In this paper, we examine both the evaluation metrics\n(VisualGPTScore, etc.) and current benchmarks for evaluating the\ncompositionality of GVLMs. We identify the syntactical bias in current\nbenchmarks, which is exploited by the linguistic capability of GVLMs. The bias\nrenders VisualGPTScore an insufficient metric for assessing GVLMs. To combat\nthis, we first introduce a SyntaxBias Score, leveraging LLMs to quantify such\nbias for mitigation. A challenging new task is subsequently added to evaluate\nthe robustness of GVLMs against inherent inclination toward syntactical\ncorrectness. Using the bias-mitigated datasets and the new task, we propose a\nnovel benchmark, namely SyntActically DE-biased benchmark (SADE). Our study\nprovides an unbiased benchmark for the compositionality of GVLMs, facilitating\nfuture research in this direction (Code and dataset are available at\nhttps://github.com/TeleeMa/SADE).\n","authors":["Teli Ma","Rong Li","Junwei Liang"],"pdf_url":"https://arxiv.org/pdf/2308.10509v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01139v3","updated":"2024-03-30T15:40:03Z","published":"2024-03-02T08:53:40Z","title":"ParallelPARC: A Scalable Pipeline for Generating Natural-Language\n  Analogies","summary":"  Analogy-making is central to human cognition, allowing us to adapt to novel\nsituations -- an ability that current AI systems still lack. Most analogy\ndatasets today focus on simple analogies (e.g., word analogies); datasets\nincluding complex types of analogies are typically manually curated and very\nsmall. We believe that this holds back progress in computational analogy. In\nthis work, we design a data generation pipeline, ParallelPARC (Parallel\nParagraph Creator) leveraging state-of-the-art Large Language Models (LLMs) to\ncreate complex, paragraph-based analogies, as well as distractors, both simple\nand challenging. We demonstrate our pipeline and create ProPara-Logy, a dataset\nof analogies between scientific processes. We publish a gold-set, validated by\nhumans, and a silver-set, generated automatically. We test LLMs' and humans'\nanalogy recognition in binary and multiple-choice settings, and found that\nhumans outperform the best models (~13% gap) after a light supervision. We\ndemonstrate that our silver-set is useful for training models. Lastly, we show\nchallenging distractors confuse LLMs, but not humans. We hope our pipeline will\nencourage research in this emerging field.\n","authors":["Oren Sultan","Yonatan Bitton","Ron Yosef","Dafna Shahaf"],"pdf_url":"https://arxiv.org/pdf/2403.01139v3.pdf","comment":"NAACL 2024 main"},{"id":"http://arxiv.org/abs/2312.09238v2","updated":"2024-03-30T15:35:16Z","published":"2023-12-14T18:58:12Z","title":"Auto MC-Reward: Automated Dense Reward Design with Large Language Models\n  for Minecraft","summary":"  Many reinforcement learning environments (e.g., Minecraft) provide only\nsparse rewards that indicate task completion or failure with binary values. The\nchallenge in exploration efficiency in such environments makes it difficult for\nreinforcement-learning-based agents to learn complex tasks. To address this,\nthis paper introduces an advanced learning system, named Auto MC-Reward, that\nleverages Large Language Models (LLMs) to automatically design dense reward\nfunctions, thereby enhancing the learning efficiency. Auto MC-Reward consists\nof three important components: Reward Designer, Reward Critic, and Trajectory\nAnalyzer. Given the environment information and task descriptions, the Reward\nDesigner first design the reward function by coding an executable Python\nfunction with predefined observation inputs. Then, our Reward Critic will be\nresponsible for verifying the code, checking whether the code is\nself-consistent and free of syntax and semantic errors. Further, the Trajectory\nAnalyzer summarizes possible failure causes and provides refinement suggestions\naccording to collected trajectories. In the next round, Reward Designer will\nfurther refine and iterate the dense reward function based on feedback.\nExperiments demonstrate a significant improvement in the success rate and\nlearning efficiency of our agents in complex tasks in Minecraft, such as\nobtaining diamond with the efficient ability to avoid lava, and efficiently\nexplore trees and animals that are sparse in the plains biome.\n","authors":["Hao Li","Xue Yang","Zhaokai Wang","Xizhou Zhu","Jie Zhou","Yu Qiao","Xiaogang Wang","Hongsheng Li","Lewei Lu","Jifeng Dai"],"pdf_url":"https://arxiv.org/pdf/2312.09238v2.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2402.08498v3","updated":"2024-03-30T15:08:39Z","published":"2024-02-13T14:53:12Z","title":"Auditing Counterfire: Evaluating Advanced Counterargument Generation\n  with Evidence and Style","summary":"  We audited counter-arguments generated by large language models (LLMs),\nfocusing on their ability to generate evidence-based and stylistic\ncounter-arguments to posts from the Reddit ChangeMyView dataset. Our evaluation\nis based on Counterfire: a new dataset of 32,000 counter-arguments generated\nfrom large language models (LLMs): GPT-3.5 Turbo and Koala and their fine-tuned\nvariants, and PaLM 2, with varying prompts for evidence use and argumentative\nstyle. GPT-3.5 Turbo ranked highest in argument quality with strong\nparaphrasing and style adherence, particularly in `reciprocity' style\narguments. However, the `No Style' counter-arguments proved most persuasive on\naverage. The findings suggest that a balance between evidentiality and\nstylistic elements is vital to a compelling counter-argument. We close with a\ndiscussion of future research directions and implications for fine-tuning LLMs.\n","authors":["Preetika Verma","Kokil Jaidka","Svetlana Churina"],"pdf_url":"https://arxiv.org/pdf/2402.08498v3.pdf","comment":"19 pages, 10 figures, 11 tables"},{"id":"http://arxiv.org/abs/2310.18964v2","updated":"2024-03-30T15:01:08Z","published":"2023-10-29T10:07:32Z","title":"LLMs and Finetuning: Benchmarking cross-domain performance for hate\n  speech detection","summary":"  In the evolving landscape of online communication, hate speech detection\nremains a formidable challenge, further compounded by the diversity of digital\nplatforms. This study investigates the effectiveness and adaptability of\npre-trained and fine-tuned Large Language Models (LLMs) in identifying hate\nspeech, to address two central questions: (1) To what extent does the model\nperformance depend on the fine-tuning and training parameters?, (2) To what\nextent do models generalize to cross-domain hate speech detection? and (3) What\nare the specific features of the datasets or models that influence the\ngeneralization potential? The experiment shows that LLMs offer a huge advantage\nover the state-of-the-art even without pretraining. To answer (1) we analyze 36\nin-domain classifiers comprising LLaMA, Vicuna, and their variations in\npre-trained and fine-tuned states across nine publicly available datasets that\nspan a wide range of platforms and discussion forums. To answer (2), we\nassessed the performance of 288 out-of-domain classifiers for a given\nend-domain dataset. In answer to (3), ordinary least squares analyses suggest\nthat the advantage of training with fine-grained hate speech labels is greater\nfor smaller training datasets but washed away with the increase in dataset\nsize. We conclude with a vision for the future of hate speech detection,\nemphasizing cross-domain generalizability and appropriate benchmarking\npractices.\n","authors":["Ahmad Nasir","Aadish Sharma","Kokil Jaidka"],"pdf_url":"https://arxiv.org/pdf/2310.18964v2.pdf","comment":"9 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.15952v2","updated":"2024-03-30T13:21:42Z","published":"2024-03-23T23:06:32Z","title":"IllusionVQA: A Challenging Optical Illusion Dataset for Vision Language\n  Models","summary":"  The advent of Vision Language Models (VLM) has allowed researchers to\ninvestigate the visual understanding of a neural network using natural\nlanguage. Beyond object classification and detection, VLMs are capable of\nvisual comprehension and common-sense reasoning. This naturally led to the\nquestion: How do VLMs respond when the image itself is inherently unreasonable?\nTo this end, we present IllusionVQA: a diverse dataset of challenging optical\nillusions and hard-to-interpret scenes to test the capability of VLMs in two\ndistinct multiple-choice VQA tasks - comprehension and soft localization.\nGPT4V, the best-performing VLM, achieves 62.99% accuracy (4-shot) on the\ncomprehension task and 49.7% on the localization task (4-shot and\nChain-of-Thought). Human evaluation reveals that humans achieve 91.03% and 100%\naccuracy in comprehension and localization. We discover that In-Context\nLearning (ICL) and Chain-of-Thought reasoning substantially degrade the\nperformance of GeminiPro on the localization task. Tangentially, we discover a\npotential weakness in the ICL capabilities of VLMs: they fail to locate optical\nillusions even when the correct answer is in the context window as a few-shot\nexample.\n","authors":["Haz Sameen Shahgir","Khondker Salman Sayeed","Abhik Bhattacharjee","Wasi Uddin Ahmad","Yue Dong","Rifat Shahriyar"],"pdf_url":"https://arxiv.org/pdf/2403.15952v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.12599v4","updated":"2024-03-30T11:14:55Z","published":"2023-05-21T23:16:26Z","title":"Abstract Meaning Representation-Based Logic-Driven Data Augmentation for\n  Logical Reasoning","summary":"  Combining large language models with logical reasoning enhances their\ncapacity to address problems in a robust and reliable manner. Nevertheless, the\nintricate nature of logical reasoning poses challenges to gathering reliable\ndata from the web for building comprehensive training datasets, subsequently\naffecting the performance on downstream tasks. To address this, we introduce a\nnovel logic-driven data augmentation approach, AMR-LDA. AMR-LDA converts the\noriginal text into an Abstract Meaning Representation (AMR) graph, a structured\nsemantic representation that encapsulates the logic structure of the sentence,\nupon which operations are performed to generate logically modified AMR graphs.\nThe modified AMR graphs are subsequently converted back into text to create\naugmented data. Notably, our methodology is architecture-agnostic and enhances\nboth generative large language models, such as GPT-3.5 and GPT-4, through\nprompt augmentation, and discriminative large language models through\ncontrastive learning with logic-driven data augmentation. Empirical evidence\nunderscores the efficacy of our proposed method with improvement in performance\nacross seven downstream tasks, such as reading comprehension requiring logical\nreasoning, textual entailment, and natural language inference. Furthermore, our\nmethod leads on the ReClor leaderboard\n(https://eval.ai/web/challenges/challenge-page/503/leaderboard/1347). The\nsource code and data are publicly available https://bit.ly/3OWKe8r.\n","authors":["Qiming Bao","Alex Yuxuan Peng","Zhenyun Deng","Wanjun Zhong","Gael Gendron","Timothy Pistotti","Neset Tan","Nathan Young","Yang Chen","Yonghua Zhu","Paul Denny","Michael Witbrock","Jiamou Liu"],"pdf_url":"https://arxiv.org/pdf/2305.12599v4.pdf","comment":"The short version (v2) was accepted for oral presentation at the\n  first LLM@IJCAI 2023 non-archival symposium; the full version is under\n  review. Update the mistake in Figure 1"},{"id":"http://arxiv.org/abs/2403.15796v2","updated":"2024-03-30T09:55:12Z","published":"2024-03-23T11:03:31Z","title":"Understanding Emergent Abilities of Language Models from the Loss\n  Perspective","summary":"  Recent studies have put into question the belief that emergent abilities in\nlanguage models are exclusive to large models. This skepticism arises from two\nobservations: 1) smaller models can also exhibit high performance on emergent\nabilities and 2) there is doubt on the discontinuous metrics used to measure\nthese abilities. In this paper, we propose to study emergent abilities in the\nlens of pre-training loss, instead of model size or training compute. We\ndemonstrate that the models with the same pre-training loss, but different\nmodel and data sizes, generate the same performance on various downstream\ntasks. We also discover that a model exhibits emergent abilities on certain\ntasks -- regardless of the continuity of metrics -- when its pre-training loss\nfalls below a specific threshold. Before reaching this threshold, its\nperformance remains at the level of random guessing. This inspires us to\nredefine emergent abilities as those that manifest in models with lower\npre-training losses, highlighting that these abilities cannot be predicted by\nmerely extrapolating the performance trends of models with higher pre-training\nlosses.\n","authors":["Zhengxiao Du","Aohan Zeng","Yuxiao Dong","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2403.15796v2.pdf","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2310.09430v4","updated":"2024-03-30T09:49:19Z","published":"2023-10-13T22:29:15Z","title":"Assessing and Enhancing the Robustness of Large Language Models with\n  Task Structure Variations for Logical Reasoning","summary":"  Large language models (LLMs), such as LLaMA, Alpaca, Vicuna, GPT-3.5 and\nGPT-4, have advanced the performance of AI systems on various natural language\nprocessing tasks to human-like levels. However, their generalisation and\nrobustness when performing logical reasoning has not been sufficiently\nassessed. To comprehensively evaluate this ability, we develop three new\nlogical reasoning datasets named \"ReClor-plus\", \"LogiQA-plus\" and\n\"LogiQAv2-plus\" that extend standard logical reasoning datasets to evaluate the\nrobustness of the LLM's reasoning. For each, we create three subsets: the first\nwith randomly shuffled options, the second with the correct choices replaced by\n\"none of the other options is correct\", and the third with a combination of\nshuffling and substitution. Experiments on these datasets show that these\nsimple augmentations greatly hinder the models' performance. Despite their high\nperformance on the original publicly available datasets, we find that all\nmodels perform poorly on these newly constructed datasets. We also demonstrate\nthat introducing task variations into the training set can markedly improve the\nmodel's performance on both the original and our developed datasets. Finally,\nwe show that applying logic-driven data augmentation for fine-tuning and\nprompting can enhance generalisation in both discriminative and generative\nmodels, offering a path to improving their robustness for tasks involving\nlogical reasoning. Source code and data are made publicly available at\nhttps://github.com/Strong-AI-Lab/Logical-and-abstract-reasoning.\n","authors":["Qiming Bao","Gael Gendron","Alex Yuxuan Peng","Wanjun Zhong","Neset Tan","Yang Chen","Michael Witbrock","Jiamou Liu"],"pdf_url":"https://arxiv.org/pdf/2310.09430v4.pdf","comment":"The short version (v3) was accepted for oral presentation at the\n  first LLM@IJCAI 2023 non-archival symposium; the full version is under review"},{"id":"http://arxiv.org/abs/2401.05952v2","updated":"2024-03-30T09:15:50Z","published":"2024-01-11T14:44:08Z","title":"LLM-as-a-Coauthor: Can Mixed Human-Written and Machine-Generated Text Be\n  Detected?","summary":"  With the rapid development and widespread application of Large Language\nModels (LLMs), the use of Machine-Generated Text (MGT) has become increasingly\ncommon, bringing with it potential risks, especially in terms of quality and\nintegrity in fields like news, education, and science. Current research mainly\nfocuses on purely MGT detection without adequately addressing mixed scenarios,\nincluding AI-revised Human-Written Text (HWT) or human-revised MGT. To tackle\nthis challenge, we define mixtext, a form of mixed text involving both AI and\nhuman-generated content. Then, we introduce MixSet, the first dataset dedicated\nto studying these mixtext scenarios. Leveraging MixSet, we executed\ncomprehensive experiments to assess the efficacy of prevalent MGT detectors in\nhandling mixtext situations, evaluating their performance in terms of\neffectiveness, robustness, and generalization. Our findings reveal that\nexisting detectors struggle to identify mixtext, particularly in dealing with\nsubtle modifications and style adaptability. This research underscores the\nurgent need for more fine-grain detectors tailored for mixtext, offering\nvaluable insights for future research. Code and Models are available at\nhttps://github.com/Dongping-Chen/MixSet.\n","authors":["Qihui Zhang","Chujie Gao","Dongping Chen","Yue Huang","Yixin Huang","Zhenyang Sun","Shilin Zhang","Weiye Li","Zhengyan Fu","Yao Wan","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2401.05952v2.pdf","comment":"Accepted by NAACL 2024"},{"id":"http://arxiv.org/abs/2305.14124v3","updated":"2024-03-30T08:49:04Z","published":"2023-05-23T14:48:42Z","title":"When Does Monolingual Data Help Multilingual Translation: The Role of\n  Domain and Model Scale","summary":"  Multilingual machine translation (MMT), trained on a mixture of parallel and\nmonolingual data, is key for improving translation in low-resource language\npairs. However, the literature offers conflicting results on the performance of\ndifferent methods of including monolingual data. To resolve this, we examine\nhow denoising autoencoding (DAE) and backtranslation (BT) impact MMT under\ndifferent data conditions and model scales. Unlike prior studies, we use a\nrealistic dataset of 100 translation directions and consider many domain\ncombinations of monolingual and test data. We find that monolingual data\ngenerally helps MMT, but models are surprisingly brittle to domain mismatches,\nespecially at smaller model scales. BT is beneficial when the parallel,\nmonolingual, and test data sources are similar but can be detrimental\notherwise, while DAE is less effective than previously reported. Next, we\nanalyze the impact of scale (from 90M to 1.6B parameters) and find it is\nimportant for both methods, particularly DAE. As scale increases, DAE\ntransitions from underperforming the parallel-only baseline at 90M to\nconverging with BT performance at 1.6B, and even surpassing it in low-resource.\nThese results offer new insights into how to best use monolingual data in MMT.\n","authors":["Christos Baziotis","Biao Zhang","Alexandra Birch","Barry Haddow"],"pdf_url":"https://arxiv.org/pdf/2305.14124v3.pdf","comment":"Accepted to NAACL 2024 (Main conference)"},{"id":"http://arxiv.org/abs/2207.14000v3","updated":"2024-03-30T08:18:15Z","published":"2022-07-28T10:44:46Z","title":"Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study\n  on Out-of-Distribution Generalisation","summary":"  Combining deep learning with symbolic logic reasoning aims to capitalize on\nthe success of both fields and is drawing increasing attention. Inspired by\nDeepLogic, an end-to-end model trained to perform inference on logic programs,\nwe introduce IMA-GloVe-GA, an iterative neural inference network for multi-step\nreasoning expressed in natural language. In our model, reasoning is performed\nusing an iterative memory neural network based on RNN with a gated attention\nmechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES\nV1 and CONCEPTRULES V2. Experimental results show DeepLogic with gated\nattention can achieve higher test accuracy than DeepLogic and other RNN\nbaseline models. Our model achieves better out-of-distribution generalisation\nthan RoBERTa-Large when the rules have been shuffled. Furthermore, to address\nthe issue of unbalanced distribution of reasoning depths in the current\nmulti-step reasoning datasets, we develop PARARULE-Plus, a large dataset with\nmore examples that require deeper reasoning steps. Experimental results show\nthat the addition of PARARULE-Plus can increase the model's performance on\nexamples requiring deeper reasoning depths. The source code and data are\navailable at\nhttps://github.com/Strong-AI-Lab/Multi-Step-Deductive-Reasoning-Over-Natural-Language.\n","authors":["Qiming Bao","Alex Yuxuan Peng","Tim Hartill","Neset Tan","Zhenyun Deng","Michael Witbrock","Jiamou Liu"],"pdf_url":"https://arxiv.org/pdf/2207.14000v3.pdf","comment":"10 pages, 3 figures, The 2nd International Joint Conference on\n  Learning & Reasoning and 16th International Workshop on Neural-Symbolic\n  Learning and Reasoning (IJCLR-NeSy 2022)"},{"id":"http://arxiv.org/abs/2306.02796v2","updated":"2024-03-30T07:55:41Z","published":"2023-06-05T11:46:36Z","title":"MCTS: A Multi-Reference Chinese Text Simplification Dataset","summary":"  Text simplification aims to make the text easier to understand by applying\nrewriting transformations. There has been very little research on Chinese text\nsimplification for a long time. The lack of generic evaluation data is an\nessential reason for this phenomenon. In this paper, we introduce MCTS, a\nmulti-reference Chinese text simplification dataset. We describe the annotation\nprocess of the dataset and provide a detailed analysis. Furthermore, we\nevaluate the performance of several unsupervised methods and advanced large\nlanguage models. We additionally provide Chinese text simplification parallel\ndata that can be used for training, acquired by utilizing machine translation\nand English text simplification. We hope to build a basic understanding of\nChinese text simplification through the foundational work and provide\nreferences for future research. All of the code and data are released at\nhttps://github.com/blcuicall/mcts/.\n","authors":["Ruining Chong","Luming Lu","Liner Yang","Jinran Nie","Zhenghao Liu","Shuo Wang","Shuhan Zhou","Yaoxin Li","Erhong Yang"],"pdf_url":"https://arxiv.org/pdf/2306.02796v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04476v2","updated":"2024-03-30T05:18:05Z","published":"2024-02-06T23:52:10Z","title":"Dual-View Visual Contextualization for Web Navigation","summary":"  Automatic web navigation aims to build a web agent that can follow language\ninstructions to execute complex and diverse tasks on real-world websites.\nExisting work primarily takes HTML documents as input, which define the\ncontents and action spaces (i.e., actionable elements and operations) of\nwebpages. Nevertheless, HTML documents may not provide a clear task-related\ncontext for each element, making it hard to select the right (sequence of)\nactions. In this paper, we propose to contextualize HTML elements through their\n\"dual views\" in webpage screenshots: each HTML element has its corresponding\nbounding box and visual content in the screenshot. We build upon the insight --\nweb developers tend to arrange task-related elements nearby on webpages to\nenhance user experiences -- and propose to contextualize each element with its\nneighbor elements, using both textual and visual features. The resulting\nrepresentations of HTML elements are more informative for the agent to take\naction. We validate our method on the recently released Mind2Web dataset, which\nfeatures diverse navigation domains and tasks on real-world websites. Our\nmethod consistently outperforms the baseline in all the scenarios, including\ncross-task, cross-website, and cross-domain ones.\n","authors":["Jihyung Kil","Chan Hee Song","Boyuan Zheng","Xiang Deng","Yu Su","Wei-Lun Chao"],"pdf_url":"https://arxiv.org/pdf/2402.04476v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2309.04372v2","updated":"2024-03-30T05:05:52Z","published":"2023-09-08T15:06:05Z","title":"MoEController: Instruction-based Arbitrary Image Manipulation with\n  Mixture-of-Expert Controllers","summary":"  Diffusion-model-based text-guided image generation has recently made\nastounding progress, producing fascinating results in open-domain image\nmanipulation tasks. Few models, however, currently have complete zero-shot\ncapabilities for both global and local image editing due to the complexity and\ndiversity of image manipulation tasks. In this work, we propose a method with a\nmixture-of-expert (MOE) controllers to align the text-guided capacity of\ndiffusion models with different kinds of human instructions, enabling our model\nto handle various open-domain image manipulation tasks with natural language\ninstructions. First, we use large language models (ChatGPT) and conditional\nimage synthesis models (ControlNet) to generate a large number of global image\ntransfer dataset in addition to the instruction-based local image editing\ndataset. Then, using an MOE technique and task-specific adaptation training on\na large-scale dataset, our conditional diffusion model can edit images globally\nand locally. Extensive experiments demonstrate that our approach performs\nsurprisingly well on various image manipulation tasks when dealing with\nopen-domain images and arbitrary human instructions. Please refer to our\nproject page: [https://oppo-mente-lab.github.io/moe_controller/]\n","authors":["Sijia Li","Chen Chen","Haonan Lu"],"pdf_url":"https://arxiv.org/pdf/2309.04372v2.pdf","comment":"6 pages,6 figures"},{"id":"http://arxiv.org/abs/2403.07440v3","updated":"2024-03-30T04:36:54Z","published":"2024-03-12T09:32:25Z","title":"Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A\n  Brain-Inspired Method for Parameter-Efficient Fine-Tuning","summary":"  Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have\nbeen proven to significantly enhance model performance on a variety of\ndownstream tasks and effectively control the output behaviors of LPLMs. Recent\nstudies have proposed numerous methods for fine-tuning a small number of\nparameters based on open-source LPLMs, reducing the demand for computational\nand storage resources. Among these, reparameterization fine-tuning methods\nrepresented by LoRA (Low-Rank Adaptation) have gained popularity. We find that\nalthough these methods perform well in many aspects, there is still\nconsiderable room for improvement in terms of complex task adaptability,\nperformance, stability, and algorithm complexity. In response to this, inspired\nby the idea that the functions of the brain are shaped by its geometric\nstructure, this paper integrates this idea into LoRA technology and proposes a\nnew matrix transformation-based reparameterization method for efficient\nfine-tuning, named Matrix-Transformation based Low-Rank Adaptation (MTLoRA).\nMTLoRA aims to dynamically alter its spatial geometric structure by applying a\ntransformation-matrix T to perform linear transformations, such as rotation,\nscaling, and translation, on the task-specific parameter matrix, generating new\nmatrix feature patterns (eigenvectors) to mimic the fundamental influence of\ncomplex geometric structure feature patterns in the brain on functions, thereby\nenhancing the model's performance in downstream tasks. In Natural Language\nUnderstanding (NLU) tasks, it is evaluated using the GLUE benchmark test, and\nthe results reveal that MTLoRA achieves an overall performance increase of\nabout 1.0% across eight tasks; in Natural Language Generation (NLG) tasks,\nMTLoRA improves performance by an average of 0.95% and 0.56% in the DART and\nWebNLG tasks, respectively.\n","authors":["Yao Liang","Yuwei Wang","Yang Li","Yi Zeng"],"pdf_url":"https://arxiv.org/pdf/2403.07440v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17916v2","updated":"2024-03-30T04:16:20Z","published":"2024-02-27T22:07:52Z","title":"LLM-Resistant Math Word Problem Generation via Adversarial Attacks","summary":"  Large language models (LLMs) have significantly transformed the educational\nlandscape. As current plagiarism detection tools struggle to keep pace with\nLLMs' rapid advancements, the educational community faces the challenge of\nassessing students' true problem-solving abilities in the presence of LLMs. In\nthis work, we explore a new paradigm for ensuring fair evaluation -- generating\nadversarial examples which preserve the structure and difficulty of the\noriginal questions aimed for assessment, but are unsolvable by LLMs. Focusing\non the domain of math word problems, we leverage abstract syntax trees to\nstructurally generate adversarial examples that cause LLMs to produce incorrect\nanswers by simply editing the numeric values in the problems. We conduct\nexperiments on various open- and closed-source LLMs, quantitatively and\nqualitatively demonstrating that our method significantly degrades their math\nproblem-solving ability. We identify shared vulnerabilities among LLMs and\npropose a cost-effective approach to attack high-cost models. Additionally, we\nconduct automatic analysis on math problems and investigate the cause of\nfailure, offering a nuanced view into model's limitation.\n","authors":["Roy Xie","Chengxuan Huang","Junlin Wang","Bhuwan Dhingra"],"pdf_url":"https://arxiv.org/pdf/2402.17916v2.pdf","comment":"Code/data: https://github.com/ruoyuxie/adversarial_mwps_generation"},{"id":"http://arxiv.org/abs/2311.07884v2","updated":"2024-03-30T03:54:06Z","published":"2023-11-14T03:38:55Z","title":"Fair Abstractive Summarization of Diverse Perspectives","summary":"  People from different social and demographic groups express diverse\nperspectives and conflicting opinions on a broad set of topics such as product\nreviews, healthcare, law, and politics. A fair summary should provide a\ncomprehensive coverage of diverse perspectives without underrepresenting\ncertain groups. However, current work in summarization metrics and Large\nLanguage Models (LLMs) evaluation has not explored fair abstractive\nsummarization. In this paper, we systematically investigate fair abstractive\nsummarization for user-generated data. We first formally define fairness in\nabstractive summarization as not underrepresenting perspectives of any groups\nof people, and we propose four reference-free automatic metrics by measuring\nthe differences between target and source perspectives. We evaluate nine LLMs,\nincluding three GPT models, four LLaMA models, PaLM 2, and Claude, on six\ndatasets collected from social media, online reviews, and recorded transcripts.\nExperiments show that both the model-generated and the human-written reference\nsummaries suffer from low fairness. We conduct a comprehensive analysis of the\ncommon factors influencing fairness and propose three simple but effective\nmethods to alleviate unfair summarization. Our dataset and code are available\nat https://github.com/psunlpgroup/FairSumm.\n","authors":["Yusen Zhang","Nan Zhang","Yixin Liu","Alexander Fabbri","Junru Liu","Ryo Kamoi","Xiaoxin Lu","Caiming Xiong","Jieyu Zhao","Dragomir Radev","Kathleen McKeown","Rui Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.07884v2.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.13362v3","updated":"2024-03-30T03:10:48Z","published":"2024-03-20T07:44:06Z","title":"Incentivizing News Consumption on Social Media Platforms Using Large\n  Language Models and Realistic Bot Accounts","summary":"  Polarization, declining trust, and wavering support for democratic norms are\npressing threats to U.S. democracy. Exposure to verified and quality news may\nlower individual susceptibility to these threats and make citizens more\nresilient to misinformation, populism, and hyperpartisan rhetoric. This project\nexamines how to enhance users' exposure to and engagement with verified and\nideologically balanced news in an ecologically valid setting. We rely on a\nlarge-scale two-week long field experiment (from 1/19/2023 to 2/3/2023) on\n28,457 Twitter users. We created 28 bots utilizing GPT-2 that replied to users\ntweeting about sports, entertainment, or lifestyle with a contextual reply\ncontaining two hardcoded elements: a URL to the topic-relevant section of\nquality news organization and an encouragement to follow its Twitter account.\nTo further test differential effects by gender of the bots, treated users were\nrandomly assigned to receive responses by bots presented as female or male. We\nexamine whether our over-time intervention enhances the following of news media\norganization, the sharing and the liking of news content and the tweeting about\npolitics and the liking of political content. We find that the treated users\nfollowed more news accounts and the users in the female bot treatment were more\nlikely to like news content than the control. Most of these results, however,\nwere small in magnitude and confined to the already politically interested\nTwitter users, as indicated by their pre-treatment tweeting about politics.\nThese findings have implications for social media and news organizations, and\nalso offer direction for future work on how Large Language Models and other\ncomputational interventions can effectively enhance individual on-platform\nengagement with quality news and public affairs.\n","authors":["Hadi Askari","Anshuman Chhabra","Bernhard Clemm von Hohenberg","Michael Heseltine","Magdalena Wojcieszak"],"pdf_url":"https://arxiv.org/pdf/2403.13362v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11511v2","updated":"2024-03-30T03:01:42Z","published":"2023-12-12T05:38:55Z","title":"ComplexityNet: Increasing LLM Inference Efficiency by Learning Task\n  Complexity","summary":"  We present ComplexityNet, a streamlined language model designed for assessing\ntask complexity. This model predicts the likelihood of accurate output by\nvarious language models, each with different capabilities. Our initial\napplication of ComplexityNet involves the Mostly Basic Python Problems (MBPP)\ndataset. We pioneered the creation of the first set of labels to define task\ncomplexity. ComplexityNet achieved a notable 79% accuracy in determining task\ncomplexity, a significant improvement over the 34% accuracy of the original,\nnon fine-tuned model. Furthermore, ComplexityNet effectively reduces\ncomputational resource usage by 90% compared to using the highest complexity\nmodel, while maintaining a high code generation accuracy of 86.7%. This study\ndemonstrates that fine-tuning smaller models to categorize tasks based on their\ncomplexity can lead to a more balanced trade-off between accuracy and\nefficiency in the use of Large Language Models. Our findings suggest a\npromising direction for optimizing LLM applications, especially in\nresource-constrained environments.\n","authors":["Henry Bae","Aghyad Deeb","Alex Fleury","Kehang Zhu"],"pdf_url":"https://arxiv.org/pdf/2312.11511v2.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2402.03769v2","updated":"2024-03-30T23:04:10Z","published":"2024-02-06T07:22:50Z","title":"AttackNet: Enhancing Biometric Security via Tailored Convolutional\n  Neural Network Architectures for Liveness Detection","summary":"  Biometric security is the cornerstone of modern identity verification and\nauthentication systems, where the integrity and reliability of biometric\nsamples is of paramount importance. This paper introduces AttackNet, a bespoke\nConvolutional Neural Network architecture, meticulously designed to combat\nspoofing threats in biometric systems. Rooted in deep learning methodologies,\nthis model offers a layered defense mechanism, seamlessly transitioning from\nlow-level feature extraction to high-level pattern discernment. Three\ndistinctive architectural phases form the crux of the model, each underpinned\nby judiciously chosen activation functions, normalization techniques, and\ndropout layers to ensure robustness and resilience against adversarial attacks.\nBenchmarking our model across diverse datasets affirms its prowess, showcasing\nsuperior performance metrics in comparison to contemporary models. Furthermore,\na detailed comparative analysis accentuates the model's efficacy, drawing\nparallels with prevailing state-of-the-art methodologies. Through iterative\nrefinement and an informed architectural strategy, AttackNet underscores the\npotential of deep learning in safeguarding the future of biometric security.\n","authors":["Oleksandr Kuznetsov","Dmytro Zakharov","Emanuele Frontoni","Andrea Maranesi"],"pdf_url":"https://arxiv.org/pdf/2402.03769v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09373v3","updated":"2024-03-30T22:10:36Z","published":"2023-03-16T15:01:50Z","title":"MAPSeg: Unified Unsupervised Domain Adaptation for Heterogeneous Medical\n  Image Segmentation Based on 3D Masked Autoencoding and Pseudo-Labeling","summary":"  Robust segmentation is critical for deriving quantitative measures from\nlarge-scale, multi-center, and longitudinal medical scans. Manually annotating\nmedical scans, however, is expensive and labor-intensive and may not always be\navailable in every domain. Unsupervised domain adaptation (UDA) is a\nwell-studied technique that alleviates this label-scarcity problem by\nleveraging available labels from another domain. In this study, we introduce\nMasked Autoencoding and Pseudo-Labeling Segmentation (MAPSeg), a\n$\\textbf{unified}$ UDA framework with great versatility and superior\nperformance for heterogeneous and volumetric medical image segmentation. To the\nbest of our knowledge, this is the first study that systematically reviews and\ndevelops a framework to tackle four different domain shifts in medical image\nsegmentation. More importantly, MAPSeg is the first framework that can be\napplied to $\\textbf{centralized}$, $\\textbf{federated}$, and\n$\\textbf{test-time}$ UDA while maintaining comparable performance. We compare\nMAPSeg with previous state-of-the-art methods on a private infant brain MRI\ndataset and a public cardiac CT-MRI dataset, and MAPSeg outperforms others by a\nlarge margin (10.5 Dice improvement on the private MRI dataset and 5.7 on the\npublic CT-MRI dataset). MAPSeg poses great practical value and can be applied\nto real-world problems. GitHub: https://github.com/XuzheZ/MAPSeg/.\n","authors":["Xuzhe Zhang","Yuhao Wu","Elsa Angelini","Ang Li","Jia Guo","Jerod M. Rasmussen","Thomas G. O'Connor","Pathik D. Wadhwa","Andrea Parolin Jackowski","Hai Li","Jonathan Posner","Andrew F. Laine","Yun Wang"],"pdf_url":"https://arxiv.org/pdf/2303.09373v3.pdf","comment":"CVPR 2024 camera-ready (8 pages, 3 figures) with the supplemental\n  materials (5 pages, 4 figures). Xuzhe Zhang and Yuhao Wu are co-first\n  authors. Andrew F. Laine and Yun Wang are co-senior supervising authors"},{"id":"http://arxiv.org/abs/2310.07889v2","updated":"2024-03-30T22:00:22Z","published":"2023-10-11T20:52:30Z","title":"LangNav: Language as a Perceptual Representation for Navigation","summary":"  We explore the use of language as a perceptual representation for\nvision-and-language navigation (VLN), with a focus on low-data settings. Our\napproach uses off-the-shelf vision systems for image captioning and object\ndetection to convert an agent's egocentric panoramic view at each time step\ninto natural language descriptions. We then finetune a pretrained language\nmodel to select an action, based on the current view and the trajectory\nhistory, that would best fulfill the navigation instructions. In contrast to\nthe standard setup which adapts a pretrained language model to work directly\nwith continuous visual features from pretrained vision models, our approach\ninstead uses (discrete) language as the perceptual representation. We explore\nseveral use cases of our language-based navigation (LangNav) approach on the\nR2R VLN benchmark: generating synthetic trajectories from a prompted language\nmodel (GPT-4) with which to finetune a smaller language model; domain transfer\nwhere we transfer a policy learned on one simulated environment (ALFRED) to\nanother (more realistic) environment (R2R); and combining both vision- and\nlanguage-based representations for VLN. Our approach is found to improve upon\nbaselines that rely on visual features in settings where only a few expert\ntrajectories (10-100) are available, demonstrating the potential of language as\na perceptual representation for navigation.\n","authors":["Bowen Pan","Rameswar Panda","SouYoung Jin","Rogerio Feris","Aude Oliva","Phillip Isola","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2310.07889v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17791v2","updated":"2024-03-30T20:51:33Z","published":"2023-11-29T16:35:24Z","title":"U-Net v2: Rethinking the Skip Connections of U-Net for Medical Image\n  Segmentation","summary":"  In this paper, we introduce U-Net v2, a new robust and efficient U-Net\nvariant for medical image segmentation. It aims to augment the infusion of\nsemantic information into low-level features while simultaneously refining\nhigh-level features with finer details. For an input image, we begin by\nextracting multi-level features with a deep neural network encoder. Next, we\nenhance the feature map of each level by infusing semantic information from\nhigher-level features and integrating finer details from lower-level features\nthrough Hadamard product. Our novel skip connections empower features of all\nthe levels with enriched semantic characteristics and intricate details. The\nimproved features are subsequently transmitted to the decoder for further\nprocessing and segmentation. Our method can be seamlessly integrated into any\nEncoder-Decoder network. We evaluate our method on several public medical image\nsegmentation datasets for skin lesion segmentation and polyp segmentation, and\nthe experimental results demonstrate the segmentation accuracy of our new\nmethod over state-of-the-art methods, while preserving memory and computational\nefficiency. Code is available at: https://github.com/yaoppeng/U-Net_v2\n","authors":["Yaopeng Peng","Milan Sonka","Danny Z. Chen"],"pdf_url":"https://arxiv.org/pdf/2311.17791v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13659v2","updated":"2024-03-30T20:14:03Z","published":"2024-03-20T15:08:43Z","title":"Recursive Joint Cross-Modal Attention for Multimodal Fusion in\n  Dimensional Emotion Recognition","summary":"  Though multimodal emotion recognition has achieved significant progress over\nrecent years, the potential of rich synergic relationships across the\nmodalities is not fully exploited. In this paper, we introduce Recursive Joint\nCross-Modal Attention (RJCMA) to effectively capture both intra-and inter-modal\nrelationships across audio, visual and text modalities for dimensional emotion\nrecognition. In particular, we compute the attention weights based on\ncross-correlation between the joint audio-visual-text feature representations\nand the feature representations of individual modalities to simultaneously\ncapture intra- and inter-modal relationships across the modalities. The\nattended features of the individual modalities are again fed as input to the\nfusion model in a recursive mechanism to obtain more refined feature\nrepresentations. We have also explored Temporal Convolutional Networks (TCNs)\nto improve the temporal modeling of the feature representations of individual\nmodalities. Extensive experiments are conducted to evaluate the performance of\nthe proposed fusion model on the challenging Affwild2 dataset. By effectively\ncapturing the synergic intra- and inter-modal relationships across audio,\nvisual and text modalities, the proposed fusion model achieves a Concordance\nCorrelation Coefficient (CCC) of 0.585 (0.542) and 0.659 (0.619) for valence\nand arousal respectively on the validation set (test set). This shows a\nsignificant improvement over the baseline of 0.24 (0.211) and 0.20 (0.191) for\nvalence and arousal respectively on the validation set (test set) of the\nvalence-arousal challenge of 6th Affective Behavior Analysis in-the-Wild (ABAW)\ncompetition.\n","authors":["R. Gnana Praveen","Jahangir Alam"],"pdf_url":"https://arxiv.org/pdf/2403.13659v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09383v3","updated":"2024-03-30T18:22:34Z","published":"2023-03-16T15:13:09Z","title":"Unifying Top-down and Bottom-up Scanpath Prediction Using Transformers","summary":"  Most models of visual attention aim at predicting either top-down or\nbottom-up control, as studied using different visual search and free-viewing\ntasks. In this paper we propose the Human Attention Transformer (HAT), a single\nmodel that predicts both forms of attention control. HAT uses a novel\ntransformer-based architecture and a simplified foveated retina that\ncollectively create a spatio-temporal awareness akin to the dynamic visual\nworking memory of humans. HAT not only establishes a new state-of-the-art in\npredicting the scanpath of fixations made during target-present and\ntarget-absent visual search and ``taskless'' free viewing, but also makes human\ngaze behavior interpretable. Unlike previous methods that rely on a coarse grid\nof fixation cells and experience information loss due to fixation\ndiscretization, HAT features a sequential dense prediction architecture and\noutputs a dense heatmap for each fixation, thus avoiding discretizing\nfixations. HAT sets a new standard in computational attention, which emphasizes\neffectiveness, generality, and interpretability. HAT's demonstrated scope and\napplicability will likely inspire the development of new attention models that\ncan better predict human behavior in various attention-demanding scenarios.\nCode is available at https://github.com/cvlab-stonybrook/HAT.\n","authors":["Zhibo Yang","Sounak Mondal","Seoyoung Ahn","Ruoyu Xue","Gregory Zelinsky","Minh Hoai","Dimitris Samaras"],"pdf_url":"https://arxiv.org/pdf/2303.09383v3.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2402.05079v2","updated":"2024-03-30T17:51:35Z","published":"2024-02-07T18:33:04Z","title":"Mamba-UNet: UNet-Like Pure Visual Mamba for Medical Image Segmentation","summary":"  In recent advancements in medical image analysis, Convolutional Neural\nNetworks (CNN) and Vision Transformers (ViT) have set significant benchmarks.\nWhile the former excels in capturing local features through its convolution\noperations, the latter achieves remarkable global context understanding by\nleveraging self-attention mechanisms. However, both architectures exhibit\nlimitations in efficiently modeling long-range dependencies within medical\nimages, which is a critical aspect for precise segmentation. Inspired by the\nMamba architecture, known for its proficiency in handling long sequences and\nglobal contextual information with enhanced computational efficiency as a State\nSpace Model (SSM), we propose Mamba-UNet, a novel architecture that synergizes\nthe U-Net in medical image segmentation with Mamba's capability. Mamba-UNet\nadopts a pure Visual Mamba (VMamba)-based encoder-decoder structure, infused\nwith skip connections to preserve spatial information across different scales\nof the network. This design facilitates a comprehensive feature learning\nprocess, capturing intricate details and broader semantic contexts within\nmedical images. We introduce a novel integration mechanism within the VMamba\nblocks to ensure seamless connectivity and information flow between the encoder\nand decoder paths, enhancing the segmentation performance. We conducted\nexperiments on publicly available ACDC MRI Cardiac segmentation dataset, and\nSynapse CT Abdomen segmentation dataset. The results show that Mamba-UNet\noutperforms several types of UNet in medical image segmentation under the same\nhyper-parameter setting. The source code and baseline implementations are\navailable.\n","authors":["Ziyang Wang","Jian-Qing Zheng","Yichi Zhang","Ge Cui","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2402.05079v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18216v2","updated":"2024-03-30T17:00:18Z","published":"2023-05-29T17:00:40Z","title":"Towards minimizing efforts for Morphing Attacks -- Deep embeddings for\n  morphing pair selection and improved Morphing Attack Detection","summary":"  Face Morphing Attacks pose a threat to the security of identity documents,\nespecially with respect to a subsequent access control process, because it\nenables both individuals involved to exploit the same document. In this study,\nface embeddings serve two purposes: pre-selecting images for large-scale\nMorphing Attack generation and detecting potential Morphing Attacks. We build\nupon previous embedding studies in both use cases using the MagFace model. For\nthe first objective, we employ an pre-selection algorithm that pairs\nindividuals based on face embedding similarity. We quantify the attack\npotential of differently morphed face images to compare the usability of\npre-selection in automatically generating numerous successful Morphing Attacks.\nRegarding the second objective, we compare embeddings from two state-of-the-art\nface recognition systems in terms of their ability to detect Morphing Attacks.\nOur findings demonstrate that ArcFace and MagFace provide valuable face\nembeddings for image pre-selection. Both open-source and COTS face recognition\nsystems are susceptible to generated attacks, particularly when pre-selection\nis based on embeddings rather than random pairing which was only constrained by\nsoft biometrics. More accurate face recognition systems exhibit greater\nvulnerability to attacks, with COTS systems being the most susceptible.\nAdditionally, MagFace embeddings serve as a robust alternative for detecting\nmorphed face images compared to the previously used ArcFace embeddings. The\nresults endorse the advantages of face embeddings in more effective image\npre-selection for face morphing and accurate detection of morphed face images.\nThis is supported by extensive analysis of various designed attacks. The\nMagFace model proves to be a powerful alternative to the commonly used ArcFace\nmodel for both objectives, pre-selection and attack detection.\n","authors":["Roman Kessler","Kiran Raja","Juan Tapia","Christoph Busch"],"pdf_url":"https://arxiv.org/pdf/2305.18216v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19314v2","updated":"2024-03-30T16:36:17Z","published":"2024-03-28T11:12:33Z","title":"Total-Decom: Decomposed 3D Scene Reconstruction with Minimal Interaction","summary":"  Scene reconstruction from multi-view images is a fundamental problem in\ncomputer vision and graphics. Recent neural implicit surface reconstruction\nmethods have achieved high-quality results; however, editing and manipulating\nthe 3D geometry of reconstructed scenes remains challenging due to the absence\nof naturally decomposed object entities and complex object/background\ncompositions. In this paper, we present Total-Decom, a novel method for\ndecomposed 3D reconstruction with minimal human interaction. Our approach\nseamlessly integrates the Segment Anything Model (SAM) with hybrid\nimplicit-explicit neural surface representations and a mesh-based\nregion-growing technique for accurate 3D object decomposition. Total-Decom\nrequires minimal human annotations while providing users with real-time control\nover the granularity and quality of decomposition. We extensively evaluate our\nmethod on benchmark datasets and demonstrate its potential for downstream\napplications, such as animation and scene editing. The code is available at\nhttps://github.com/CVMI-Lab/Total-Decom.git.\n","authors":["Xiaoyang Lyu","Chirui Chang","Peng Dai","Yang-Tian Sun","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2403.19314v2.pdf","comment":"8 pages, 7 figures, accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2402.10038v2","updated":"2024-03-30T16:10:47Z","published":"2024-02-15T16:00:58Z","title":"RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization\n  Method for Alignment of Large Language Models","summary":"  Reinforcement learning from human feedback (RLHF) has been extensively\nemployed to align large language models with user intent. However, proximal\npolicy optimization (PPO) based RLHF is occasionally unstable requiring\nsignificant hyperparameter finetuning, and computationally expensive to\nmaximize the estimated reward during alignment. Recently, direct preference\noptimization (DPO) is proposed to address those challenges. However, DPO relies\non contrastive responses generated from human annotator and alternative LLM,\ninstead of the policy model, limiting the effectiveness of the RLHF. In this\npaper, we addresses both challenges by systematically combining rejection\nsampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the\ndevelopment of a supervised fine-tuned policy model (SFT). A varied set of k\nresponses per prompt are sampled directly from the SFT model. RS-DPO identifies\npairs of contrastive samples based on their reward distribution. Finally, we\napply DPO with the contrastive samples to align the model to human preference.\nOur experiments indicate that our proposed method effectively fine-tunes LLMs\nwith limited resource environments, leading to improved alignment with user\nintent. Furthermore, it outperforms existing methods, including RS, PPO, and\nDPO.\n","authors":["Saeed Khaki","JinJin Li","Lan Ma","Liu Yang","Prathap Ramachandra"],"pdf_url":"https://arxiv.org/pdf/2402.10038v2.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2308.10509v2","updated":"2024-03-30T16:09:14Z","published":"2023-08-21T06:50:29Z","title":"An Examination of the Compositionality of Large Generative\n  Vision-Language Models","summary":"  With the success of Large Language Models (LLMs), many Generative\nVision-Language Models (GVLMs) have been constructed via multimodal instruction\ntuning. However, the performance of GVLMs in multimodal compositional reasoning\nremains under-explored. In this paper, we examine both the evaluation metrics\n(VisualGPTScore, etc.) and current benchmarks for evaluating the\ncompositionality of GVLMs. We identify the syntactical bias in current\nbenchmarks, which is exploited by the linguistic capability of GVLMs. The bias\nrenders VisualGPTScore an insufficient metric for assessing GVLMs. To combat\nthis, we first introduce a SyntaxBias Score, leveraging LLMs to quantify such\nbias for mitigation. A challenging new task is subsequently added to evaluate\nthe robustness of GVLMs against inherent inclination toward syntactical\ncorrectness. Using the bias-mitigated datasets and the new task, we propose a\nnovel benchmark, namely SyntActically DE-biased benchmark (SADE). Our study\nprovides an unbiased benchmark for the compositionality of GVLMs, facilitating\nfuture research in this direction (Code and dataset are available at\nhttps://github.com/TeleeMa/SADE).\n","authors":["Teli Ma","Rong Li","Junwei Liang"],"pdf_url":"https://arxiv.org/pdf/2308.10509v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05471v2","updated":"2024-03-30T16:06:36Z","published":"2023-07-11T17:56:22Z","title":"Scale Alone Does not Improve Mechanistic Interpretability in Vision\n  Models","summary":"  In light of the recent widespread adoption of AI systems, understanding the\ninternal information processing of neural networks has become increasingly\ncritical. Most recently, machine vision has seen remarkable progress by scaling\nneural networks to unprecedented levels in dataset and model size. We here ask\nwhether this extraordinary increase in scale also positively impacts the field\nof mechanistic interpretability. In other words, has our understanding of the\ninner workings of scaled neural networks improved as well? We use a\npsychophysical paradigm to quantify one form of mechanistic interpretability\nfor a diverse suite of nine models and find no scaling effect for\ninterpretability - neither for model nor dataset size. Specifically, none of\nthe investigated state-of-the-art models are easier to interpret than the\nGoogLeNet model from almost a decade ago. Latest-generation vision models\nappear even less interpretable than older architectures, hinting at a\nregression rather than improvement, with modern models sacrificing\ninterpretability for accuracy. These results highlight the need for models\nexplicitly designed to be mechanistically interpretable and the need for more\nhelpful interpretability methods to increase our understanding of networks at\nan atomic level. We release a dataset containing more than 130'000 human\nresponses from our psychophysical evaluation of 767 units across nine models.\nThis dataset facilitates research on automated instead of human-based\ninterpretability evaluations, which can ultimately be leveraged to directly\noptimize the mechanistic interpretability of models.\n","authors":["Roland S. Zimmermann","Thomas Klein","Wieland Brendel"],"pdf_url":"https://arxiv.org/pdf/2307.05471v2.pdf","comment":"Spotlight at NeurIPS 2023. The first two authors contributed equally.\n  Code available at https://brendel-group.github.io/imi/"},{"id":"http://arxiv.org/abs/2403.13589v2","updated":"2024-03-30T15:53:31Z","published":"2024-03-20T13:37:29Z","title":"ReGround: Improving Textual and Spatial Grounding at No Cost","summary":"  When an image generation process is guided by both a text prompt and spatial\ncues, such as a set of bounding boxes, do these elements work in harmony, or\ndoes one dominate the other? Our analysis of a pretrained image diffusion model\nthat integrates gated self-attention into the U-Net reveals that spatial\ngrounding often outweighs textual grounding due to the sequential flow from\ngated self-attention to cross-attention. We demonstrate that such bias can be\nsignificantly mitigated without sacrificing accuracy in either grounding by\nsimply rewiring the network architecture, changing from sequential to parallel\nfor gated self-attention and cross-attention. This surprisingly simple yet\neffective solution does not require any fine-tuning of the network but\nsignificantly reduces the trade-off between the two groundings. Our experiments\ndemonstrate significant improvements from the original GLIGEN to the rewired\nversion in the trade-off between textual grounding and spatial grounding.\n","authors":["Yuseung Lee","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2403.13589v2.pdf","comment":"Project page: https://re-ground.github.io/"},{"id":"http://arxiv.org/abs/2401.17759v3","updated":"2024-03-30T15:46:35Z","published":"2024-01-31T11:36:12Z","title":"Rapid post-disaster infrastructure damage characterisation enabled by\n  remote sensing and deep learning technologies -- a tiered approach","summary":"  Critical infrastructure are systematically targeted during wars and extensive\nnatural disasters because critical infrastructure is vital for enabling\nconnectivity and transportation of people and goods, and hence, underpins\nnational and international economic growth. Mass destruction of transport\nassets, in conjunction with minimal or no accessibility in the wake of natural\nand anthropogenic disasters, prevents us from delivering rapid recovery and\nadaptation. A solution to this challenge is to use technology that enables\nstand-off observations. Nevertheless, no methods exist for the integrated\ncharacterisation of damage at multiple scales, i.e. regional, asset, and\nstructural scales, while there is no systematic correlation between\ninfrastructure damage assessments across these scales. We propose a methodology\nbased on an integrated multi-scale tiered approach to fill this capability gap.\nIn doing so, we demonstrate how damage characterisation can be enabled by\nfit-for-purpose digital technologies. Next, the methodology is applied and\nvalidated to a case study in Ukraine that includes 17 bridges all damages by\nhuman targeted interventions. From macro to micro, we deploy technology to\nintegrate assessments at scale, using from Sentinel-1 SAR images, crowdsourced\ninformation, and high-resolution images to deep learning to characterise\ninfrastructure damage. For the first time, the interferometric coherence\ndifference and semantic segmentation of images were deployed to improve the\nreliability of damage characterisations at different scales, i.e. regional,\ninfrastructure asset and component, with the aim of enhancing the damage\ncharacterisation accuracy. This integrated approach accelerates\ndecision-making, and therefore, facilitates more efficient restoration and\nadaptation efforts, ultimately fostering resilience into our infrastructure.\n","authors":["Nadiia Kopiika","Andreas Karavias","Pavlos Krassakis","Zehao Ye","Jelena Ninic","Nataliya Shakhovska","Nikolaos Koukouzas","Sotirios Argyroudis","Stergios-Aristoteles Mitoulis"],"pdf_url":"https://arxiv.org/pdf/2401.17759v3.pdf","comment":"Main text (33 pages,15 figures); Supplementary materials (19 pages)"},{"id":"http://arxiv.org/abs/2312.09238v2","updated":"2024-03-30T15:35:16Z","published":"2023-12-14T18:58:12Z","title":"Auto MC-Reward: Automated Dense Reward Design with Large Language Models\n  for Minecraft","summary":"  Many reinforcement learning environments (e.g., Minecraft) provide only\nsparse rewards that indicate task completion or failure with binary values. The\nchallenge in exploration efficiency in such environments makes it difficult for\nreinforcement-learning-based agents to learn complex tasks. To address this,\nthis paper introduces an advanced learning system, named Auto MC-Reward, that\nleverages Large Language Models (LLMs) to automatically design dense reward\nfunctions, thereby enhancing the learning efficiency. Auto MC-Reward consists\nof three important components: Reward Designer, Reward Critic, and Trajectory\nAnalyzer. Given the environment information and task descriptions, the Reward\nDesigner first design the reward function by coding an executable Python\nfunction with predefined observation inputs. Then, our Reward Critic will be\nresponsible for verifying the code, checking whether the code is\nself-consistent and free of syntax and semantic errors. Further, the Trajectory\nAnalyzer summarizes possible failure causes and provides refinement suggestions\naccording to collected trajectories. In the next round, Reward Designer will\nfurther refine and iterate the dense reward function based on feedback.\nExperiments demonstrate a significant improvement in the success rate and\nlearning efficiency of our agents in complex tasks in Minecraft, such as\nobtaining diamond with the efficient ability to avoid lava, and efficiently\nexplore trees and animals that are sparse in the plains biome.\n","authors":["Hao Li","Xue Yang","Zhaokai Wang","Xizhou Zhu","Jie Zhou","Yu Qiao","Xiaogang Wang","Hongsheng Li","Lewei Lu","Jifeng Dai"],"pdf_url":"https://arxiv.org/pdf/2312.09238v2.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2402.16774v2","updated":"2024-03-30T14:38:44Z","published":"2024-02-26T17:45:00Z","title":"Video-Based Autism Detection with Deep Learning","summary":"  Individuals with Autism Spectrum Disorder (ASD) often experience challenges\nin health, communication, and sensory processing; therefore, early diagnosis is\nnecessary for proper treatment and care. In this work, we consider the problem\nof detecting or classifying ASD children to aid medical professionals in early\ndiagnosis. We develop a deep learning model that analyzes video clips of\nchildren reacting to sensory stimuli, with the intent of capturing key\ndifferences in reactions and behavior between ASD and non-ASD participants.\nUnlike many recent studies in ASD classification with MRI data, which require\nexpensive specialized equipment, our method utilizes a powerful but relatively\naffordable GPU, a standard computer setup, and a video camera for inference.\nResults show that our model effectively generalizes and understands key\ndifferences in the distinct movements of the children. It is noteworthy that\nour model exhibits successful classification performance despite the limited\namount of data for a deep learning problem and limited temporal information\navailable for learning, even with the motion artifacts.\n","authors":["M. Serna-Aguilera","X. B. Nguyen","A. Singh","L. Rockers","S. Park","L. Neely","H. Seo","K. Luu"],"pdf_url":"https://arxiv.org/pdf/2402.16774v2.pdf","comment":"Poster Abstract. Accepted into 2024 IEEE Green Technologies\n  Conference"},{"id":"http://arxiv.org/abs/2311.15855v2","updated":"2024-03-30T14:21:40Z","published":"2023-11-27T14:22:07Z","title":"SiTH: Single-view Textured Human Reconstruction with Image-Conditioned\n  Diffusion","summary":"  A long-standing goal of 3D human reconstruction is to create lifelike and\nfully detailed 3D humans from single-view images. The main challenge lies in\ninferring unknown body shapes, appearances, and clothing details in areas not\nvisible in the images. To address this, we propose SiTH, a novel pipeline that\nuniquely integrates an image-conditioned diffusion model into a 3D mesh\nreconstruction workflow. At the core of our method lies the decomposition of\nthe challenging single-view reconstruction problem into generative\nhallucination and reconstruction subproblems. For the former, we employ a\npowerful generative diffusion model to hallucinate unseen back-view appearance\nbased on the input images. For the latter, we leverage skinned body meshes as\nguidance to recover full-body texture meshes from the input and back-view\nimages. SiTH requires as few as 500 3D human scans for training while\nmaintaining its generality and robustness to diverse images. Extensive\nevaluations on two 3D human benchmarks, including our newly created one,\nhighlighted our method's superior accuracy and perceptual quality in 3D\ntextured human reconstruction. Our code and evaluation benchmark are available\nat https://ait.ethz.ch/sith\n","authors":["Hsuan-I Ho","Jie Song","Otmar Hilliges"],"pdf_url":"https://arxiv.org/pdf/2311.15855v2.pdf","comment":"23 pages, 23 figures, CVPR 2024"},{"id":"http://arxiv.org/abs/2312.03029v2","updated":"2024-03-30T14:19:10Z","published":"2023-12-05T11:01:44Z","title":"Gaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic\n  Gaussians","summary":"  Creating high-fidelity 3D head avatars has always been a research hotspot,\nbut there remains a great challenge under lightweight sparse view setups. In\nthis paper, we propose Gaussian Head Avatar represented by controllable 3D\nGaussians for high-fidelity head avatar modeling. We optimize the neutral 3D\nGaussians and a fully learned MLP-based deformation field to capture complex\nexpressions. The two parts benefit each other, thereby our method can model\nfine-grained dynamic details while ensuring expression accuracy. Furthermore,\nwe devise a well-designed geometry-guided initialization strategy based on\nimplicit SDF and Deep Marching Tetrahedra for the stability and convergence of\nthe training procedure. Experiments show our approach outperforms other\nstate-of-the-art sparse-view methods, achieving ultra high-fidelity rendering\nquality at 2K resolution even under exaggerated expressions.\n","authors":["Yuelang Xu","Benwang Chen","Zhe Li","Hongwen Zhang","Lizhen Wang","Zerong Zheng","Yebin Liu"],"pdf_url":"https://arxiv.org/pdf/2312.03029v2.pdf","comment":"Projectpage: https://yuelangx.github.io/gaussianheadavatar, Code:\n  https://github.com/YuelangX/Gaussian-Head-Avatar"},{"id":"http://arxiv.org/abs/2312.09313v3","updated":"2024-03-30T14:01:27Z","published":"2023-12-14T19:38:06Z","title":"LatentEditor: Text Driven Local Editing of 3D Scenes","summary":"  While neural fields have made significant strides in view synthesis and scene\nreconstruction, editing them poses a formidable challenge due to their implicit\nencoding of geometry and texture information from multi-view inputs. In this\npaper, we introduce \\textsc{LatentEditor}, an innovative framework designed to\nempower users with the ability to perform precise and locally controlled\nediting of neural fields using text prompts. Leveraging denoising diffusion\nmodels, we successfully embed real-world scenes into the latent space,\nresulting in a faster and more adaptable NeRF backbone for editing compared to\ntraditional methods. To enhance editing precision, we introduce a delta score\nto calculate the 2D mask in the latent space that serves as a guide for local\nmodifications while preserving irrelevant regions. Our novel pixel-level\nscoring approach harnesses the power of InstructPix2Pix (IP2P) to discern the\ndisparity between IP2P conditional and unconditional noise predictions in the\nlatent space. The edited latents conditioned on the 2D masks are then\niteratively updated in the training set to achieve 3D local editing. Our\napproach achieves faster editing speeds and superior output quality compared to\nexisting 3D editing models, bridging the gap between textual instructions and\nhigh-quality 3D scene editing in latent space. We show the superiority of our\napproach on four benchmark 3D datasets, LLFF, IN2N, NeRFStudio and NeRF-Art.\n","authors":["Umar Khalid","Hasan Iqbal","Nazmul Karim","Jing Hua","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2312.09313v3.pdf","comment":"Project Page: https://latenteditor.github.io/"},{"id":"http://arxiv.org/abs/2403.19002v2","updated":"2024-03-30T14:00:27Z","published":"2024-03-27T20:52:30Z","title":"Robust Active Speaker Detection in Noisy Environments","summary":"  This paper addresses the issue of active speaker detection (ASD) in noisy\nenvironments and formulates a robust active speaker detection (rASD) problem.\nExisting ASD approaches leverage both audio and visual modalities, but\nnon-speech sounds in the surrounding environment can negatively impact\nperformance. To overcome this, we propose a novel framework that utilizes\naudio-visual speech separation as guidance to learn noise-free audio features.\nThese features are then utilized in an ASD model, and both tasks are jointly\noptimized in an end-to-end framework. Our proposed framework mitigates residual\nnoise and audio quality reduction issues that can occur in a naive cascaded\ntwo-stage framework that directly uses separated speech for ASD, and enables\nthe two tasks to be optimized simultaneously. To further enhance the robustness\nof the audio features and handle inherent speech noises, we propose a dynamic\nweighted loss approach to train the speech separator. We also collected a\nreal-world noise audio dataset to facilitate investigations. Experiments\ndemonstrate that non-speech audio noises significantly impact ASD models, and\nour proposed approach improves ASD performance in noisy environments. The\nframework is general and can be applied to different ASD approaches to improve\ntheir robustness. Our code, models, and data will be released.\n","authors":["Siva Sai Nagender Vasireddy","Chenxu Zhang","Xiaohu Guo","Yapeng Tian"],"pdf_url":"https://arxiv.org/pdf/2403.19002v2.pdf","comment":"15 pages, 5 figures"},{"id":"http://arxiv.org/abs/2205.14375v5","updated":"2024-03-30T13:49:58Z","published":"2022-05-28T09:08:50Z","title":"WaveMix: A Resource-efficient Neural Network for Image Analysis","summary":"  We propose a novel neural architecture for computer vision -- WaveMix -- that\nis resource-efficient and yet generalizable and scalable. While using fewer\ntrainable parameters, GPU RAM, and computations, WaveMix networks achieve\ncomparable or better accuracy than the state-of-the-art convolutional neural\nnetworks, vision transformers, and token mixers for several tasks. This\nefficiency can translate to savings in time, cost, and energy. To achieve these\ngains we used multi-level two-dimensional discrete wavelet transform (2D-DWT)\nin WaveMix blocks, which has the following advantages: (1) It reorganizes\nspatial information based on three strong image priors -- scale-invariance,\nshift-invariance, and sparseness of edges -- (2) in a lossless manner without\nadding parameters, (3) while also reducing the spatial sizes of feature maps,\nwhich reduces the memory and time required for forward and backward passes, and\n(4) expanding the receptive field faster than convolutions do. The whole\narchitecture is a stack of self-similar and resolution-preserving WaveMix\nblocks, which allows architectural flexibility for various tasks and levels of\nresource availability. WaveMix establishes new benchmarks for segmentation on\nCityscapes; and for classification on Galaxy 10 DECals, Places-365, five EMNIST\ndatasets, and iNAT-mini and performs competitively on other benchmarks. Our\ncode and trained models are publicly available.\n","authors":["Pranav Jeevan","Kavitha Viswanathan","Anandu A S","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2205.14375v5.pdf","comment":"20 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.18035v2","updated":"2024-03-30T13:28:54Z","published":"2024-03-26T18:40:36Z","title":"Bidirectional Consistency Models","summary":"  Diffusion models (DMs) are capable of generating remarkably high-quality\nsamples by iteratively denoising a random vector, a process that corresponds to\nmoving along the probability flow ordinary differential equation (PF ODE).\nInterestingly, DMs can also invert an input image to noise by moving backward\nalong the PF ODE, a key operation for downstream tasks such as interpolation\nand image editing. However, the iterative nature of this process restricts its\nspeed, hindering its broader application. Recently, Consistency Models (CMs)\nhave emerged to address this challenge by approximating the integral of the PF\nODE, largely reducing the number of iterations. Yet, the absence of an explicit\nODE solver complicates the inversion process. To resolve this, we introduce the\nBidirectional Consistency Model (BCM), which learns a single neural network\nthat enables both forward and backward traversal along the PF ODE, efficiently\nunifying generation and inversion tasks within one framework. Notably, our\nproposed method enables one-step generation and inversion while also allowing\nthe use of additional steps to enhance generation quality or reduce\nreconstruction error. Furthermore, by leveraging our model's bidirectional\nconsistency, we introduce a sampling strategy that can enhance FID while\npreserving the generated image content. We further showcase our model's\ncapabilities in several downstream tasks, such as interpolation and inpainting,\nand present demonstrations of potential applications, including blind\nrestoration of compressed images and defending black-box adversarial attacks.\n","authors":["Liangchen Li","Jiajun He"],"pdf_url":"https://arxiv.org/pdf/2403.18035v2.pdf","comment":"40 pages, 25 figures"},{"id":"http://arxiv.org/abs/2403.15952v2","updated":"2024-03-30T13:21:42Z","published":"2024-03-23T23:06:32Z","title":"IllusionVQA: A Challenging Optical Illusion Dataset for Vision Language\n  Models","summary":"  The advent of Vision Language Models (VLM) has allowed researchers to\ninvestigate the visual understanding of a neural network using natural\nlanguage. Beyond object classification and detection, VLMs are capable of\nvisual comprehension and common-sense reasoning. This naturally led to the\nquestion: How do VLMs respond when the image itself is inherently unreasonable?\nTo this end, we present IllusionVQA: a diverse dataset of challenging optical\nillusions and hard-to-interpret scenes to test the capability of VLMs in two\ndistinct multiple-choice VQA tasks - comprehension and soft localization.\nGPT4V, the best-performing VLM, achieves 62.99% accuracy (4-shot) on the\ncomprehension task and 49.7% on the localization task (4-shot and\nChain-of-Thought). Human evaluation reveals that humans achieve 91.03% and 100%\naccuracy in comprehension and localization. We discover that In-Context\nLearning (ICL) and Chain-of-Thought reasoning substantially degrade the\nperformance of GeminiPro on the localization task. Tangentially, we discover a\npotential weakness in the ICL capabilities of VLMs: they fail to locate optical\nillusions even when the correct answer is in the context window as a few-shot\nexample.\n","authors":["Haz Sameen Shahgir","Khondker Salman Sayeed","Abhik Bhattacharjee","Wasi Uddin Ahmad","Yue Dong","Rifat Shahriyar"],"pdf_url":"https://arxiv.org/pdf/2403.15952v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00081v2","updated":"2024-03-30T12:45:08Z","published":"2023-11-30T03:20:37Z","title":"Synthesize, Diagnose, and Optimize: Towards Fine-Grained Vision-Language\n  Understanding","summary":"  Vision language models (VLM) have demonstrated remarkable performance across\nvarious downstream tasks. However, understanding fine-grained visual-linguistic\nconcepts, such as attributes and inter-object relationships, remains a\nsignificant challenge. While several benchmarks aim to evaluate VLMs in finer\ngranularity, their primary focus remains on the linguistic aspect, neglecting\nthe visual dimension. Here, we highlight the importance of evaluating VLMs from\nboth a textual and visual perspective. We introduce a progressive pipeline to\nsynthesize images that vary in a specific attribute while ensuring consistency\nin all other aspects. Utilizing this data engine, we carefully design a\nbenchmark, SPEC, to diagnose the comprehension of object size, position,\nexistence, and count. Subsequently, we conduct a thorough evaluation of four\nleading VLMs on SPEC. Surprisingly, their performance is close to random guess,\nrevealing significant limitations. With this in mind, we propose a simple yet\neffective approach to optimize VLMs in fine-grained understanding, achieving\nsignificant improvements on SPEC without compromising the zero-shot\nperformance. Results on two additional fine-grained benchmarks also show\nconsistent improvements, further validating the transferability of our\napproach. Code and data are available at https://github.com/wjpoom/SPEC.\n","authors":["Wujian Peng","Sicheng Xie","Zuyao You","Shiyi Lan","Zuxuan Wu"],"pdf_url":"https://arxiv.org/pdf/2312.00081v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2312.14024v2","updated":"2024-03-30T12:39:06Z","published":"2023-12-21T16:54:09Z","title":"NICP: Neural ICP for 3D Human Registration at Scale","summary":"  Aligning a template to 3D human point clouds is a long-standing problem\ncrucial for tasks like animation, reconstruction, and enabling supervised\nlearning pipelines. Recent data-driven methods leverage predicted surface\ncorrespondences; however, they are not robust to varied poses, identities, or\nnoise. In contrast, industrial solutions often rely on expensive manual\nannotations or multi-view capturing systems. Recently, neural fields have shown\npromising results. Still, their purely data-driven and extrinsic nature does\nnot incorporate any guidance toward the target surface, often resulting in a\ntrivial misalignment of the template registration. Currently, no method can be\nconsidered the standard for 3D Human registration, limiting the scalability of\ndownstream applications. In this work, we propose NSR, a pipeline that, for the\nfirst time, generalizes and scales across thousands of shapes and more than ten\ndifferent data sources. Our essential contribution is NICP, an ICP-style\nself-supervised task tailored to neural fields. NICP takes a few seconds, is\nself-supervised, and works out of the box on pre-trained neural fields. We\ncombine it with a localized Neural Field trained on a large MoCap dataset. NSR\nachieves the state of the art over public benchmarks, and the release of its\ncode and checkpoints will provide the community with a powerful tool useful for\nmany downstream tasks like dataset alignments, cleaning, or asset animation.\n","authors":["Riccardo Marin","Enric Corona","Gerard Pons-Moll"],"pdf_url":"https://arxiv.org/pdf/2312.14024v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16739v2","updated":"2024-03-30T12:08:08Z","published":"2023-11-28T12:35:13Z","title":"As-Plausible-As-Possible: Plausibility-Aware Mesh Deformation Using 2D\n  Diffusion Priors","summary":"  We present As-Plausible-as-Possible (APAP) mesh deformation technique that\nleverages 2D diffusion priors to preserve the plausibility of a mesh under\nuser-controlled deformation. Our framework uses per-face Jacobians to represent\nmesh deformations, where mesh vertex coordinates are computed via a\ndifferentiable Poisson Solve. The deformed mesh is rendered, and the resulting\n2D image is used in the Score Distillation Sampling (SDS) process, which\nenables extracting meaningful plausibility priors from a pretrained 2D\ndiffusion model. To better preserve the identity of the edited mesh, we\nfine-tune our 2D diffusion model with LoRA. Gradients extracted by SDS and a\nuser-prescribed handle displacement are then backpropagated to the per-face\nJacobians, and we use iterative gradient descent to compute the final\ndeformation that balances between the user edit and the output plausibility. We\nevaluate our method with 2D and 3D meshes and demonstrate qualitative and\nquantitative improvements when using plausibility priors over\ngeometry-preservation or distortion-minimization priors used by previous\ntechniques. Our project page is at: https://as-plausible-aspossible.github.io/\n","authors":["Seungwoo Yoo","Kunho Kim","Vladimir G. Kim","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2311.16739v2.pdf","comment":"Project page: https://as-plausible-as-possible.github.io/"},{"id":"http://arxiv.org/abs/2401.04071v2","updated":"2024-03-30T12:05:52Z","published":"2024-01-08T18:18:02Z","title":"Fun with Flags: Robust Principal Directions via Flag Manifolds","summary":"  Principal component analysis (PCA), along with its extensions to manifolds\nand outlier contaminated data, have been indispensable in computer vision and\nmachine learning. In this work, we present a unifying formalism for PCA and its\nvariants, and introduce a framework based on the flags of linear subspaces, ie\na hierarchy of nested linear subspaces of increasing dimension, which not only\nallows for a common implementation but also yields novel variants, not explored\npreviously. We begin by generalizing traditional PCA methods that either\nmaximize variance or minimize reconstruction error. We expand these\ninterpretations to develop a wide array of new dimensionality reduction\nalgorithms by accounting for outliers and the data manifold. To devise a common\ncomputational approach, we recast robust and dual forms of PCA as optimization\nproblems on flag manifolds. We then integrate tangent space approximations of\nprincipal geodesic analysis (tangent-PCA) into this flag-based framework,\ncreating novel robust and dual geodesic PCA variations. The remarkable\nflexibility offered by the 'flagification' introduced here enables even more\nalgorithmic variants identified by specific flag types. Last but not least, we\npropose an effective convergent solver for these flag-formulations employing\nthe Stiefel manifold. Our empirical results on both real-world and synthetic\nscenarios, demonstrate the superiority of our novel algorithms, especially in\nterms of robustness to outliers on manifolds.\n","authors":["Nathan Mankovich","Gustau Camps-Valls","Tolga Birdal"],"pdf_url":"https://arxiv.org/pdf/2401.04071v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12463v2","updated":"2024-03-30T11:35:52Z","published":"2023-12-18T19:02:07Z","title":"Open Vocabulary Semantic Scene Sketch Understanding","summary":"  We study the underexplored but fundamental vision problem of machine\nunderstanding of abstract freehand scene sketches. We introduce a sketch\nencoder that results in semantically-aware feature space, which we evaluate by\ntesting its performance on a semantic sketch segmentation task. To train our\nmodel we rely only on the availability of bitmap sketches with their brief\ncaptions and do not require any pixel-level annotations. To obtain\ngeneralization to a large set of sketches and categories, we build on a vision\ntransformer encoder pretrained with the CLIP model. We freeze the text encoder\nand perform visual-prompt tuning of the visual encoder branch while introducing\na set of critical modifications. Firstly, we augment the classical key-query\n(k-q) self-attention blocks with value-value (v-v) self-attention blocks.\nCentral to our model is a two-level hierarchical network design that enables\nefficient semantic disentanglement: The first level ensures holistic scene\nsketch encoding, and the second level focuses on individual categories. We,\nthen, in the second level of the hierarchy, introduce a cross-attention between\ntextual and visual branches. Our method outperforms zero-shot CLIP pixel\naccuracy of segmentation results by 37 points, reaching an accuracy of $85.5\\%$\non the FS-COCO sketch dataset. Finally, we conduct a user study that allows us\nto identify further improvements needed over our method to reconcile machine\nand human understanding of scene sketches.\n","authors":["Ahmed Bourouis","Judith Ellen Fan","Yulia Gryaditskaya"],"pdf_url":"https://arxiv.org/pdf/2312.12463v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08835v3","updated":"2024-03-30T11:01:17Z","published":"2023-11-15T10:22:35Z","title":"Correlation-guided Query-Dependency Calibration in Video Representation\n  Learning for Temporal Grounding","summary":"  Video Temporal Grounding is to identify specific moments or highlights from a\nvideo corresponding to textual descriptions. Typical approaches in temporal\ngrounding treat all video clips equally during the encoding process regardless\nof their semantic relevance with the text query. Therefore, we propose\nCorrelation-Guided DEtection TRansformer(CG-DETR), exploring to provide clues\nfor query-associated video clips within the cross-modal attention. First, we\ndesign an adaptive cross-attention with dummy tokens. Dummy tokens conditioned\nby text query take portions of the attention weights, preventing irrelevant\nvideo clips from being represented by the text query. Yet, not all words\nequally inherit the text query's correlation to video clips. Thus, we further\nguide the cross-attention map by inferring the fine-grained correlation between\nvideo clips and words. We enable this by learning a joint embedding space for\nhigh-level concepts, i.e., moment and sentence level, and inferring the\nclip-word correlation. Lastly, we exploit the moment-specific characteristics\nand combine them with the context of each video to form a moment-adaptive\nsaliency detector. By exploiting the degrees of text engagement in each video\nclip, it precisely measures the highlightness of each clip. CG-DETR achieves\nstate-of-the-art results on various benchmarks for temporal grounding.\n","authors":["WonJun Moon","Sangeek Hyun","SuBeen Lee","Jae-Pil Heo"],"pdf_url":"https://arxiv.org/pdf/2311.08835v3.pdf","comment":"34 pages, 16 figures, 13 tables, Code is available at\n  https://github.com/wjun0830/CGDETR"},{"id":"http://arxiv.org/abs/2312.02244v2","updated":"2024-03-30T10:49:41Z","published":"2023-12-04T12:30:07Z","title":"Geometrically-driven Aggregation for Zero-shot 3D Point Cloud\n  Understanding","summary":"  Zero-shot 3D point cloud understanding can be achieved via 2D Vision-Language\nModels (VLMs). Existing strategies directly map Vision-Language Models from 2D\npixels of rendered or captured views to 3D points, overlooking the inherent and\nexpressible point cloud geometric structure. Geometrically similar or close\nregions can be exploited for bolstering point cloud understanding as they are\nlikely to share semantic information. To this end, we introduce the first\ntraining-free aggregation technique that leverages the point cloud's 3D\ngeometric structure to improve the quality of the transferred Vision-Language\nModels. Our approach operates iteratively, performing local-to-global\naggregation based on geometric and semantic point-level reasoning. We benchmark\nour approach on three downstream tasks, including classification, part\nsegmentation, and semantic segmentation, with a variety of datasets\nrepresenting both synthetic/real-world, and indoor/outdoor scenarios. Our\napproach achieves new state-of-the-art results in all benchmarks. We will\nrelease the source code publicly.\n","authors":["Guofeng Mei","Luigi Riz","Yiming Wang","Fabio Poiesi"],"pdf_url":"https://arxiv.org/pdf/2312.02244v2.pdf","comment":"Zero-shot, point cloud, 2D Vision-Language Models, geometric\n  structure, training-free"},{"id":"http://arxiv.org/abs/2312.01307v2","updated":"2024-03-30T10:46:34Z","published":"2023-12-03T07:22:42Z","title":"SAGE: Bridging Semantic and Actionable Parts for GEneralizable\n  Manipulation of Articulated Objects","summary":"  To interact with daily-life articulated objects of diverse structures and\nfunctionalities, understanding the object parts plays a central role in both\nuser instruction comprehension and task execution. However, the possible\ndiscordance between the semantic meaning and physics functionalities of the\nparts poses a challenge for designing a general system. To address this\nproblem, we propose SAGE, a novel framework that bridges semantic and\nactionable parts of articulated objects to achieve generalizable manipulation\nunder natural language instructions. More concretely, given an articulated\nobject, we first observe all the semantic parts on it, conditioned on which an\ninstruction interpreter proposes possible action programs that concretize the\nnatural language instruction. Then, a part-grounding module maps the semantic\nparts into so-called Generalizable Actionable Parts (GAParts), which inherently\ncarry information about part motion. End-effector trajectories are predicted on\nthe GAParts, which, together with the action program, form an executable\npolicy. Additionally, an interactive feedback module is incorporated to respond\nto failures, which closes the loop and increases the robustness of the overall\nframework. Key to the success of our framework is the joint proposal and\nknowledge fusion between a large vision-language model (VLM) and a small\ndomain-specific model for both context comprehension and part perception, with\nthe former providing general intuitions and the latter serving as expert facts.\nBoth simulation and real-robot experiments show our effectiveness in handling a\nlarge variety of articulated objects with diverse language-instructed goals.\n","authors":["Haoran Geng","Songlin Wei","Congyue Deng","Bokui Shen","He Wang","Leonidas Guibas"],"pdf_url":"https://arxiv.org/pdf/2312.01307v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13347v2","updated":"2024-03-30T09:45:38Z","published":"2024-03-20T07:15:22Z","title":"vid-TLDR: Training Free Token merging for Light-weight Video Transformer","summary":"  Video Transformers have become the prevalent solution for various video\ndownstream tasks with superior expressive power and flexibility. However, these\nvideo transformers suffer from heavy computational costs induced by the massive\nnumber of tokens across the entire video frames, which has been the major\nbarrier to training the model. Further, the patches irrelevant to the main\ncontents, e.g., backgrounds, degrade the generalization performance of models.\nTo tackle these issues, we propose training free token merging for lightweight\nvideo Transformer (vid-TLDR) that aims to enhance the efficiency of video\nTransformers by merging the background tokens without additional training. For\nvid-TLDR, we introduce a novel approach to capture the salient regions in\nvideos only with the attention map. Further, we introduce the saliency-aware\ntoken merging strategy by dropping the background tokens and sharpening the\nobject scores. Our experiments show that vid-TLDR significantly mitigates the\ncomputational complexity of video Transformers while achieving competitive\nperformance compared to the base model without vid-TLDR. Code is available at\nhttps://github.com/mlvlab/vid-TLDR.\n","authors":["Joonmyung Choi","Sanghyeok Lee","Jaewon Chu","Minhyuk Choi","Hyunwoo J. Kim"],"pdf_url":"https://arxiv.org/pdf/2403.13347v2.pdf","comment":"Conference on Computer Vision and Pattern Recognition (CVPR), 2024"},{"id":"http://arxiv.org/abs/2402.17726v3","updated":"2024-03-30T09:35:47Z","published":"2024-02-27T17:58:09Z","title":"VRP-SAM: SAM with Visual Reference Prompt","summary":"  In this paper, we propose a novel Visual Reference Prompt (VRP) encoder that\nempowers the Segment Anything Model (SAM) to utilize annotated reference images\nas prompts for segmentation, creating the VRP-SAM model. In essence, VRP-SAM\ncan utilize annotated reference images to comprehend specific objects and\nperform segmentation of specific objects in target image. It is note that the\nVRP encoder can support a variety of annotation formats for reference images,\nincluding \\textbf{point}, \\textbf{box}, \\textbf{scribble}, and \\textbf{mask}.\nVRP-SAM achieves a breakthrough within the SAM framework by extending its\nversatility and applicability while preserving SAM's inherent strengths, thus\nenhancing user-friendliness. To enhance the generalization ability of VRP-SAM,\nthe VRP encoder adopts a meta-learning strategy. To validate the effectiveness\nof VRP-SAM, we conducted extensive empirical studies on the Pascal and COCO\ndatasets. Remarkably, VRP-SAM achieved state-of-the-art performance in visual\nreference segmentation with minimal learnable parameters. Furthermore, VRP-SAM\ndemonstrates strong generalization capabilities, allowing it to perform\nsegmentation of unseen objects and enabling cross-domain segmentation. The\nsource code and models will be available at\n\\url{https://github.com/syp2ysy/VRP-SAM}\n","authors":["Yanpeng Sun","Jiahui Chen","Shan Zhang","Xinyu Zhang","Qiang Chen","Gang Zhang","Errui Ding","Jingdong Wang","Zechao Li"],"pdf_url":"https://arxiv.org/pdf/2402.17726v3.pdf","comment":"Accepted by CVPR 2024; The camera-ready version"},{"id":"http://arxiv.org/abs/2011.14598v4","updated":"2024-03-30T09:24:13Z","published":"2020-11-30T07:44:52Z","title":"Video Self-Stitching Graph Network for Temporal Action Localization","summary":"  Temporal action localization (TAL) in videos is a challenging task,\nespecially due to the large variation in action temporal scales. Short actions\nusually occupy a major proportion in the datasets, but tend to have the lowest\nperformance. In this paper, we confront the challenge of short actions and\npropose a multi-level cross-scale solution dubbed as video self-stitching graph\nnetwork (VSGN). We have two key components in VSGN: video self-stitching (VSS)\nand cross-scale graph pyramid network (xGPN). In VSS, we focus on a short\nperiod of a video and magnify it along the temporal dimension to obtain a\nlarger scale. We stitch the original clip and its magnified counterpart in one\ninput sequence to take advantage of the complementary properties of both\nscales. The xGPN component further exploits the cross-scale correlations by a\npyramid of cross-scale graph networks, each containing a hybrid module to\naggregate features from across scales as well as within the same scale. Our\nVSGN not only enhances the feature representations, but also generates more\npositive anchors for short actions and more short training samples. Experiments\ndemonstrate that VSGN obviously improves the localization performance of short\nactions as well as achieving the state-of-the-art overall performance on\nTHUMOS-14 and ActivityNet-v1.3.\n","authors":["Chen Zhao","Ali Thabet","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2011.14598v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07710v2","updated":"2024-03-30T09:20:36Z","published":"2024-02-12T15:23:19Z","title":"Optimizing Sparse Convolution on GPUs with CUDA for 3D Point Cloud\n  Processing in Embedded Systems","summary":"  In recent years, there has been a significant increase in the utilization of\ndeep learning methods, particularly convolutional neural networks (CNNs), which\nhave emerged as the dominant approach in various domains that involve\nstructured grid data, such as picture analysis and processing. Nevertheless,\nthe exponential growth in the utilization of LiDAR and 3D sensors across many\ndomains has resulted in an increased need for the analysis of 3D point clouds.\nThe utilization of 3D point clouds is crucial in various applications,\nincluding object recognition and segmentation, as they offer a spatial\ndepiction of things within a three-dimensional environment. In contrast to\nphotos, point clouds exhibit sparsity and lack a regular grid, hence posing\ndistinct processing and computational issues.\n","authors":["Chester Luo","Kevin Lai"],"pdf_url":"https://arxiv.org/pdf/2402.07710v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2205.10490v2","updated":"2024-03-30T08:52:40Z","published":"2022-05-21T02:38:16Z","title":"Aligning Logits Generatively for Principled Black-Box Knowledge\n  Distillation","summary":"  Black-Box Knowledge Distillation (B2KD) is a formulated problem for\ncloud-to-edge model compression with invisible data and models hosted on the\nserver. B2KD faces challenges such as limited Internet exchange and edge-cloud\ndisparity of data distributions. In this paper, we formalize a two-step\nworkflow consisting of deprivatization and distillation, and theoretically\nprovide a new optimization direction from logits to cell boundary different\nfrom direct logits alignment. With its guidance, we propose a new method\nMapping-Emulation KD (MEKD) that distills a black-box cumbersome model into a\nlightweight one. Our method does not differentiate between treating soft or\nhard responses, and consists of: 1) deprivatization: emulating the inverse\nmapping of the teacher function with a generator, and 2) distillation: aligning\nlow-dimensional logits of the teacher and student models by reducing the\ndistance of high-dimensional image points. For different teacher-student pairs,\nour method yields inspiring distillation performance on various benchmarks, and\noutperforms the previous state-of-the-art approaches.\n","authors":["Jing Ma","Xiang Xiang","Ke Wang","Yuchuan Wu","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2205.10490v2.pdf","comment":"To appear at CVPR 2024; significantly rewritten with extra\n  experiments since the preliminary report"},{"id":"http://arxiv.org/abs/2311.17516v4","updated":"2024-03-30T08:35:17Z","published":"2023-11-29T10:39:53Z","title":"MMA-Diffusion: MultiModal Attack on Diffusion Models","summary":"  In recent years, Text-to-Image (T2I) models have seen remarkable\nadvancements, gaining widespread adoption. However, this progress has\ninadvertently opened avenues for potential misuse, particularly in generating\ninappropriate or Not-Safe-For-Work (NSFW) content. Our work introduces\nMMA-Diffusion, a framework that presents a significant and realistic threat to\nthe security of T2I models by effectively circumventing current defensive\nmeasures in both open-source models and commercial online services. Unlike\nprevious approaches, MMA-Diffusion leverages both textual and visual modalities\nto bypass safeguards like prompt filters and post-hoc safety checkers, thus\nexposing and highlighting the vulnerabilities in existing defense mechanisms.\n","authors":["Yijun Yang","Ruiyuan Gao","Xiaosen Wang","Tsung-Yi Ho","Nan Xu","Qiang Xu"],"pdf_url":"https://arxiv.org/pdf/2311.17516v4.pdf","comment":"CVPR 2024. Our codes and benchmarks are available at\n  https://github.com/cure-lab/MMA-Diffusion"},{"id":"http://arxiv.org/abs/2401.04105v2","updated":"2024-03-30T08:06:01Z","published":"2024-01-08T18:59:31Z","title":"Dr$^2$Net: Dynamic Reversible Dual-Residual Networks for\n  Memory-Efficient Finetuning","summary":"  Large pretrained models are increasingly crucial in modern computer vision\ntasks. These models are typically used in downstream tasks by end-to-end\nfinetuning, which is highly memory-intensive for tasks with high-resolution\ndata, e.g., video understanding, small object detection, and point cloud\nanalysis. In this paper, we propose Dynamic Reversible Dual-Residual Networks,\nor Dr$^2$Net, a novel family of network architectures that acts as a surrogate\nnetwork to finetune a pretrained model with substantially reduced memory\nconsumption. Dr$^2$Net contains two types of residual connections, one\nmaintaining the residual structure in the pretrained models, and the other\nmaking the network reversible. Due to its reversibility, intermediate\nactivations, which can be reconstructed from output, are cleared from memory\nduring training. We use two coefficients on either type of residual connections\nrespectively, and introduce a dynamic training strategy that seamlessly\ntransitions the pretrained model to a reversible network with much higher\nnumerical precision. We evaluate Dr$^2$Net on various pretrained models and\nvarious tasks, and show that it can reach comparable performance to\nconventional finetuning but with significantly less memory usage.\n","authors":["Chen Zhao","Shuming Liu","Karttikeya Mangalam","Guocheng Qian","Fatimah Zohra","Abdulmohsen Alghannam","Jitendra Malik","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2401.04105v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08869v2","updated":"2024-03-30T07:23:20Z","published":"2023-12-10T08:25:41Z","title":"I'M HOI: Inertia-aware Monocular Capture of 3D Human-Object Interactions","summary":"  We are living in a world surrounded by diverse and \"smart\" devices with rich\nmodalities of sensing ability. Conveniently capturing the interactions between\nus humans and these objects remains far-reaching. In this paper, we present\nI'm-HOI, a monocular scheme to faithfully capture the 3D motions of both the\nhuman and object in a novel setting: using a minimal amount of RGB camera and\nobject-mounted Inertial Measurement Unit (IMU). It combines general motion\ninference and category-aware refinement. For the former, we introduce a\nholistic human-object tracking method to fuse the IMU signals and the RGB\nstream and progressively recover the human motions and subsequently the\ncompanion object motions. For the latter, we tailor a category-aware motion\ndiffusion model, which is conditioned on both the raw IMU observations and the\nresults from the previous stage under over-parameterization representation. It\nsignificantly refines the initial results and generates vivid body, hand, and\nobject motions. Moreover, we contribute a large dataset with ground truth human\nand object motions, dense RGB inputs, and rich object-mounted IMU measurements.\nExtensive experiments demonstrate the effectiveness of I'm-HOI under a hybrid\ncapture setting. Our dataset and code will be released to the community.\n","authors":["Chengfeng Zhao","Juze Zhang","Jiashen Du","Ziwei Shan","Junye Wang","Jingyi Yu","Jingya Wang","Lan Xu"],"pdf_url":"https://arxiv.org/pdf/2312.08869v2.pdf","comment":"Accepted to CVPR 2024. Project page:\n  https://afterjourney00.github.io/IM-HOI.github.io/"},{"id":"http://arxiv.org/abs/2312.05716v2","updated":"2024-03-30T07:04:15Z","published":"2023-12-10T00:51:05Z","title":"Initialization Matters for Adversarial Transfer Learning","summary":"  With the prevalence of the Pretraining-Finetuning paradigm in transfer\nlearning, the robustness of downstream tasks has become a critical concern. In\nthis work, we delve into adversarial robustness in transfer learning and reveal\nthe critical role of initialization, including both the pretrained model and\nthe linear head. First, we discover the necessity of an adversarially robust\npretrained model. Specifically, we reveal that with a standard pretrained\nmodel, Parameter-Efficient Finetuning (PEFT) methods either fail to be\nadversarially robust or continue to exhibit significantly degraded adversarial\nrobustness on downstream tasks, even with adversarial training during\nfinetuning. Leveraging a robust pretrained model, surprisingly, we observe that\na simple linear probing can outperform full finetuning and other PEFT methods\nwith random initialization on certain datasets. We further identify that linear\nprobing excels in preserving robustness from the robust pretraining. Based on\nthis, we propose Robust Linear Initialization (RoLI) for adversarial\nfinetuning, which initializes the linear head with the weights obtained by\nadversarial linear probing to maximally inherit the robustness from\npretraining. Across five different image classification datasets, we\ndemonstrate the effectiveness of RoLI and achieve new state-of-the-art results.\nOur code is available at \\url{https://github.com/DongXzz/RoLI}.\n","authors":["Andong Hua","Jindong Gu","Zhiyu Xue","Nicholas Carlini","Eric Wong","Yao Qin"],"pdf_url":"https://arxiv.org/pdf/2312.05716v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2307.12872v2","updated":"2024-03-30T06:59:35Z","published":"2023-07-24T15:10:22Z","title":"Latent Code Augmentation Based on Stable Diffusion for Data-free\n  Substitute Attacks","summary":"  Since the training data of the target model is not available in the black-box\nsubstitute attack, most recent schemes utilize GANs to generate data for\ntraining the substitute model. However, these GANs-based schemes suffer from\nlow training efficiency as the generator needs to be retrained for each target\nmodel during the substitute training process, as well as low generation\nquality. To overcome these limitations, we consider utilizing the diffusion\nmodel to generate data, and propose a novel data-free substitute attack scheme\nbased on the Stable Diffusion (SD) to improve the efficiency and accuracy of\nsubstitute training. Despite the data generated by the SD exhibiting high\nquality, it presents a different distribution of domains and a large variation\nof positive and negative samples for the target model. For this problem, we\npropose Latent Code Augmentation (LCA) to facilitate SD in generating data that\naligns with the data distribution of the target model. Specifically, we augment\nthe latent codes of the inferred member data with LCA and use them as guidance\nfor SD. With the guidance of LCA, the data generated by the SD not only meets\nthe discriminative criteria of the target model but also exhibits high\ndiversity. By utilizing this data, it is possible to train the substitute model\nthat closely resembles the target model more efficiently. Extensive experiments\ndemonstrate that our LCA achieves higher attack success rates and requires\nfewer query budgets compared to GANs-based schemes for different target models.\nOur codes are available at \\url{https://github.com/LzhMeng/LCA}.\n","authors":["Mingwen Shao","Lingzhuang Meng","Yuanjian Qiao","Lixu Zhang","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2307.12872v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2309.01327v2","updated":"2024-03-30T06:50:28Z","published":"2023-09-04T03:06:04Z","title":"Can I Trust Your Answer? Visually Grounded Video Question Answering","summary":"  We study visually grounded VideoQA in response to the emerging trends of\nutilizing pretraining techniques for video-language understanding.\nSpecifically, by forcing vision-language models (VLMs) to answer questions and\nsimultaneously provide visual evidence, we seek to ascertain the extent to\nwhich the predictions of such techniques are genuinely anchored in relevant\nvideo content, versus spurious correlations from language or irrelevant visual\ncontext. Towards this, we construct NExT-GQA -- an extension of NExT-QA with\n10.5$K$ temporal grounding (or location) labels tied to the original QA pairs.\nWith NExT-GQA, we scrutinize a series of state-of-the-art VLMs. Through\npost-hoc attention analysis, we find that these models are extremely weak in\nsubstantiating the answers despite their strong QA performance. This exposes\nthe limitation of current VLMs in making reliable predictions. As a remedy, we\nfurther explore and propose a grounded-QA method via Gaussian mask optimization\nand cross-modal learning. Experiments with different backbones demonstrate that\nthis grounding mechanism improves both grounding and QA. With these efforts, we\naim to push towards trustworthy VLMs in VQA systems. Our dataset and code are\navailable at https://github.com/doc-doc/NExT-GQA.\n","authors":["Junbin Xiao","Angela Yao","Yicong Li","Tat Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2309.01327v2.pdf","comment":"Accepted to CVPR'24. (Compared with preprint version, we mainly\n  improve the presentation, discuss more related works, and extend experiments\n  in Appendix.)"},{"id":"http://arxiv.org/abs/2312.02224v2","updated":"2024-03-30T06:50:25Z","published":"2023-12-03T22:05:05Z","title":"Tracing Hyperparameter Dependencies for Model Parsing via Learnable\n  Graph Pooling Network","summary":"  Model Parsing defines the research task of predicting hyperparameters of the\ngenerative model (GM), given a generated image as input. Since a diverse set of\nhyperparameters is jointly employed by the generative model, and dependencies\noften exist among them, it is crucial to learn these hyperparameter\ndependencies for the improved model parsing performance. To explore such\nimportant dependencies, we propose a novel model parsing method called\nLearnable Graph Pooling Network (LGPN). Specifically, we transform model\nparsing into a graph node classification task, using graph nodes and edges to\nrepresent hyperparameters and their dependencies, respectively. Furthermore,\nLGPN incorporates a learnable pooling-unpooling mechanism tailored to model\nparsing, which adaptively learns hyperparameter dependencies of GMs used to\ngenerate the input image. We also extend our proposed method to CNN-generated\nimage detection and coordinate attacks detection. Empirically, we achieve\nstate-of-the-art results in model parsing and its extended applications,\nshowing the effectiveness of our method. Our source code are available.\n","authors":["Xiao Guo","Vishal Asnani","Sijia Liu","Xiaoming Liu"],"pdf_url":"https://arxiv.org/pdf/2312.02224v2.pdf","comment":"24 pages, 15 figures, 17 tables"},{"id":"http://arxiv.org/abs/2403.15955v3","updated":"2024-03-30T06:42:02Z","published":"2024-03-23T23:22:54Z","title":"Finding needles in a haystack: A Black-Box Approach to Invisible\n  Watermark Detection","summary":"  In this paper, we propose WaterMark Detection (WMD), the first invisible\nwatermark detection method under a black-box and annotation-free setting. WMD\nis capable of detecting arbitrary watermarks within a given reference dataset\nusing a clean non-watermarked dataset as a reference, without relying on\nspecific decoding methods or prior knowledge of the watermarking techniques. We\ndevelop WMD using foundations of offset learning, where a clean non-watermarked\ndataset enables us to isolate the influence of only watermarked samples in the\nreference dataset. Our comprehensive evaluations demonstrate the effectiveness\nof WMD, significantly outperforming naive detection methods, which only yield\nAUC scores around 0.5. In contrast, WMD consistently achieves impressive\ndetection AUC scores, surpassing 0.9 in most single-watermark datasets and\nexceeding 0.7 in more challenging multi-watermark scenarios across diverse\ndatasets and watermarking methods. As invisible watermarks become increasingly\nprevalent, while specific decoding techniques remain undisclosed, our approach\nprovides a versatile solution and establishes a path toward increasing\naccountability, transparency, and trust in our digital visual content.\n","authors":["Minzhou Pan","Zhenting Wang","Xin Dong","Vikash Sehwag","Lingjuan Lyu","Xue Lin"],"pdf_url":"https://arxiv.org/pdf/2403.15955v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12790v2","updated":"2024-03-30T06:36:06Z","published":"2023-09-22T11:02:57Z","title":"NTO3D: Neural Target Object 3D Reconstruction with Segment Anything","summary":"  Neural 3D reconstruction from multi-view images has recently attracted\nincreasing attention from the community. Existing methods normally learn a\nneural field for the whole scene, while it is still under-explored how to\nreconstruct a target object indicated by users. Considering the Segment\nAnything Model (SAM) has shown effectiveness in segmenting any 2D images, in\nthis paper, we propose NTO3D, a novel high-quality Neural Target Object 3D\n(NTO3D) reconstruction method, which leverages the benefits of both neural\nfield and SAM. We first propose a novel strategy to lift the multi-view 2D\nsegmentation masks of SAM into a unified 3D occupancy field. The 3D occupancy\nfield is then projected into 2D space and generates the new prompts for SAM.\nThis process is iterative until convergence to separate the target object from\nthe scene. After this, we then lift the 2D features of the SAM encoder into a\n3D feature field in order to improve the reconstruction quality of the target\nobject. NTO3D lifts the 2D masks and features of SAM into the 3D neural field\nfor high-quality neural target object 3D reconstruction. We conduct detailed\nexperiments on several benchmark datasets to demonstrate the advantages of our\nmethod. The code will be available at: https://github.com/ucwxb/NTO3D.\n","authors":["Xiaobao Wei","Renrui Zhang","Jiarui Wu","Jiaming Liu","Ming Lu","Yandong Guo","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2309.12790v2.pdf","comment":"accepted by CVPR24"},{"id":"http://arxiv.org/abs/2310.02279v3","updated":"2024-03-30T06:29:48Z","published":"2023-10-01T05:07:17Z","title":"Consistency Trajectory Models: Learning Probability Flow ODE Trajectory\n  of Diffusion","summary":"  Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion\nmodel sampling at the cost of sample quality but lack a natural way to\ntrade-off quality for speed. To address this limitation, we propose Consistency\nTrajectory Model (CTM), a generalization encompassing CM and score-based models\nas special cases. CTM trains a single neural network that can -- in a single\nforward pass -- output scores (i.e., gradients of log-density) and enables\nunrestricted traversal between any initial and final time along the Probability\nFlow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables\nthe efficient combination of adversarial training and denoising score matching\nloss to enhance performance and achieves new state-of-the-art FIDs for\nsingle-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at\n64x64 resolution (FID 1.92). CTM also enables a new family of sampling schemes,\nboth deterministic and stochastic, involving long jumps along the ODE solution\ntrajectories. It consistently improves sample quality as computational budgets\nincrease, avoiding the degradation seen in CM. Furthermore, unlike CM, CTM's\naccess to the score function can streamline the adoption of established\ncontrollable/conditional generation methods from the diffusion community. This\naccess also enables the computation of likelihood. The code is available at\nhttps://github.com/sony/ctm.\n","authors":["Dongjun Kim","Chieh-Hsin Lai","Wei-Hsiang Liao","Naoki Murata","Yuhta Takida","Toshimitsu Uesaka","Yutong He","Yuki Mitsufuji","Stefano Ermon"],"pdf_url":"https://arxiv.org/pdf/2310.02279v3.pdf","comment":"International Conference on Learning Representations"},{"id":"http://arxiv.org/abs/2403.19026v2","updated":"2024-03-30T06:15:36Z","published":"2024-03-27T21:43:12Z","title":"Egocentric Scene-aware Human Trajectory Prediction","summary":"  Wearable collaborative robots stand to assist human wearers who need fall\nprevention assistance or wear exoskeletons. Such a robot needs to be able to\npredict the ego motion of the wearer based on egocentric vision and the\nsurrounding scene. In this work, we leveraged body-mounted cameras and sensors\nto anticipate the trajectory of human wearers through complex surroundings. To\nfacilitate research in ego-motion prediction, we have collected a comprehensive\nwalking scene navigation dataset centered on the user's perspective. We present\na method to predict human motion conditioning on the surrounding static scene.\nOur method leverages a diffusion model to produce a distribution of potential\nfuture trajectories, taking into account the user's observation of the\nenvironment. We introduce a compact representation to encode the user's visual\nmemory of the surroundings, as well as an efficient sample-generating technique\nto speed up real-time inference of a diffusion model. We ablate our model and\ncompare it to baselines, and results show that our model outperforms existing\nmethods on key metrics of collision avoidance and trajectory mode coverage.\n","authors":["Weizhuo Wang","C. Karen Liu","Monroe Kennedy III"],"pdf_url":"https://arxiv.org/pdf/2403.19026v2.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2402.18102v2","updated":"2024-03-30T06:06:38Z","published":"2024-02-28T06:45:47Z","title":"Passive Snapshot Coded Aperture Dual-Pixel RGB-D Imaging","summary":"  Passive, compact, single-shot 3D sensing is useful in many application areas\nsuch as microscopy, medical imaging, surgical navigation, and autonomous\ndriving where form factor, time, and power constraints can exist. Obtaining\nRGB-D scene information over a short imaging distance, in an ultra-compact form\nfactor, and in a passive, snapshot manner is challenging. Dual-pixel (DP)\nsensors are a potential solution to achieve the same. DP sensors collect light\nrays from two different halves of the lens in two interleaved pixel arrays,\nthus capturing two slightly different views of the scene, like a stereo camera\nsystem. However, imaging with a DP sensor implies that the defocus blur size is\ndirectly proportional to the disparity seen between the views. This creates a\ntrade-off between disparity estimation vs. deblurring accuracy. To improve this\ntrade-off effect, we propose CADS (Coded Aperture Dual-Pixel Sensing), in which\nwe use a coded aperture in the imaging lens along with a DP sensor. In our\napproach, we jointly learn an optimal coded pattern and the reconstruction\nalgorithm in an end-to-end optimization setting. Our resulting CADS imaging\nsystem demonstrates improvement of >1.5dB PSNR in all-in-focus (AIF) estimates\nand 5-6% in depth estimation quality over naive DP sensing for a wide range of\naperture settings. Furthermore, we build the proposed CADS prototypes for DSLR\nphotography settings and in an endoscope and a dermoscope form factor. Our\nnovel coded dual-pixel sensing approach demonstrates accurate RGB-D\nreconstruction results in simulations and real-world experiments in a passive,\nsnapshot, and compact manner.\n","authors":["Bhargav Ghanekar","Salman Siddique Khan","Pranav Sharma","Shreyas Singh","Vivek Boominathan","Kaushik Mitra","Ashok Veeraraghavan"],"pdf_url":"https://arxiv.org/pdf/2402.18102v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08653v2","updated":"2024-03-30T06:05:40Z","published":"2023-12-14T04:47:20Z","title":"SKDF: A Simple Knowledge Distillation Framework for Distilling\n  Open-Vocabulary Knowledge to Open-world Object Detector","summary":"  In this paper, we attempt to specialize the VLM model for OWOD tasks by\ndistilling its open-world knowledge into a language-agnostic detector.\nSurprisingly, we observe that the combination of a simple \\textbf{knowledge\ndistillation} approach and the automatic pseudo-labeling mechanism in OWOD can\nachieve better performance for unknown object detection, even with a small\namount of data. Unfortunately, knowledge distillation for unknown objects\nseverely affects the learning of detectors with conventional structures for\nknown objects, leading to catastrophic forgetting. To alleviate these problems,\nwe propose the \\textbf{down-weight loss function} for knowledge distillation\nfrom vision-language to single vision modality. Meanwhile, we propose the\n\\textbf{cascade decouple decoding structure} that decouples the learning of\nlocalization and recognition to reduce the impact of category interactions of\nknown and unknown objects on the localization learning process. Ablation\nexperiments demonstrate that both of them are effective in mitigating the\nimpact of open-world knowledge distillation on the learning of known objects.\nAdditionally, to alleviate the current lack of comprehensive benchmarks for\nevaluating the ability of the open-world detector to detect unknown objects in\nthe open world, we propose two benchmarks, which we name\n\"\\textbf{StandardSet}$\\heartsuit$\" and \"\\textbf{IntensiveSet}$\\spadesuit$\"\nrespectively, based on the complexity of their testing scenarios. Comprehensive\nexperiments performed on OWOD, MS-COCO, and our proposed benchmarks demonstrate\nthe effectiveness of our methods. The code and proposed dataset are available\nat \\url{https://github.com/xiaomabufei/SKDF}.\n","authors":["Shuailei Ma","Yuefeng Wang","Ying Wei","Jiaqi Fan","Enming Zhang","Xinyu Sun","Peihao Chen"],"pdf_url":"https://arxiv.org/pdf/2312.08653v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2303.11623"},{"id":"http://arxiv.org/abs/2312.04551v2","updated":"2024-03-30T06:02:45Z","published":"2023-12-07T18:59:18Z","title":"Free3D: Consistent Novel View Synthesis without 3D Representation","summary":"  We introduce Free3D, a simple accurate method for monocular open-set novel\nview synthesis (NVS). Similar to Zero-1-to-3, we start from a pre-trained 2D\nimage generator for generalization, and fine-tune it for NVS. Compared to other\nworks that took a similar approach, we obtain significant improvements without\nresorting to an explicit 3D representation, which is slow and memory-consuming,\nand without training an additional network for 3D reconstruction. Our key\ncontribution is to improve the way the target camera pose is encoded in the\nnetwork, which we do by introducing a new ray conditioning normalization (RCN)\nlayer. The latter injects pose information in the underlying 2D image generator\nby telling each pixel its viewing direction. We further improve multi-view\nconsistency by using light-weight multi-view attention layers and by sharing\ngeneration noise between the different views. We train Free3D on the Objaverse\ndataset and demonstrate excellent generalization to new categories in new\ndatasets, including OmniObject3D and GSO. The project page is available at\nhttps://chuanxiaz.com/free3d/.\n","authors":["Chuanxia Zheng","Andrea Vedaldi"],"pdf_url":"https://arxiv.org/pdf/2312.04551v2.pdf","comment":"webpage: https://chuanxiaz.com/free3d/, code:\n  https://github.com/lyndonzheng/Free3D"},{"id":"http://arxiv.org/abs/2402.04476v2","updated":"2024-03-30T05:18:05Z","published":"2024-02-06T23:52:10Z","title":"Dual-View Visual Contextualization for Web Navigation","summary":"  Automatic web navigation aims to build a web agent that can follow language\ninstructions to execute complex and diverse tasks on real-world websites.\nExisting work primarily takes HTML documents as input, which define the\ncontents and action spaces (i.e., actionable elements and operations) of\nwebpages. Nevertheless, HTML documents may not provide a clear task-related\ncontext for each element, making it hard to select the right (sequence of)\nactions. In this paper, we propose to contextualize HTML elements through their\n\"dual views\" in webpage screenshots: each HTML element has its corresponding\nbounding box and visual content in the screenshot. We build upon the insight --\nweb developers tend to arrange task-related elements nearby on webpages to\nenhance user experiences -- and propose to contextualize each element with its\nneighbor elements, using both textual and visual features. The resulting\nrepresentations of HTML elements are more informative for the agent to take\naction. We validate our method on the recently released Mind2Web dataset, which\nfeatures diverse navigation domains and tasks on real-world websites. Our\nmethod consistently outperforms the baseline in all the scenarios, including\ncross-task, cross-website, and cross-domain ones.\n","authors":["Jihyung Kil","Chan Hee Song","Boyuan Zheng","Xiang Deng","Yu Su","Wei-Lun Chao"],"pdf_url":"https://arxiv.org/pdf/2402.04476v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2309.04372v2","updated":"2024-03-30T05:05:52Z","published":"2023-09-08T15:06:05Z","title":"MoEController: Instruction-based Arbitrary Image Manipulation with\n  Mixture-of-Expert Controllers","summary":"  Diffusion-model-based text-guided image generation has recently made\nastounding progress, producing fascinating results in open-domain image\nmanipulation tasks. Few models, however, currently have complete zero-shot\ncapabilities for both global and local image editing due to the complexity and\ndiversity of image manipulation tasks. In this work, we propose a method with a\nmixture-of-expert (MOE) controllers to align the text-guided capacity of\ndiffusion models with different kinds of human instructions, enabling our model\nto handle various open-domain image manipulation tasks with natural language\ninstructions. First, we use large language models (ChatGPT) and conditional\nimage synthesis models (ControlNet) to generate a large number of global image\ntransfer dataset in addition to the instruction-based local image editing\ndataset. Then, using an MOE technique and task-specific adaptation training on\na large-scale dataset, our conditional diffusion model can edit images globally\nand locally. Extensive experiments demonstrate that our approach performs\nsurprisingly well on various image manipulation tasks when dealing with\nopen-domain images and arbitrary human instructions. Please refer to our\nproject page: [https://oppo-mente-lab.github.io/moe_controller/]\n","authors":["Sijia Li","Chen Chen","Haonan Lu"],"pdf_url":"https://arxiv.org/pdf/2309.04372v2.pdf","comment":"6 pages,6 figures"},{"id":"http://arxiv.org/abs/2403.10988v2","updated":"2024-03-30T04:56:05Z","published":"2024-03-16T18:04:12Z","title":"Boosting Flow-based Generative Super-Resolution Models via Learned Prior","summary":"  Flow-based super-resolution (SR) models have demonstrated astonishing\ncapabilities in generating high-quality images. However, these methods\nencounter several challenges during image generation, such as grid artifacts,\nexploding inverses, and suboptimal results due to a fixed sampling temperature.\nTo overcome these issues, this work introduces a conditional learned prior to\nthe inference phase of a flow-based SR model. This prior is a latent code\npredicted by our proposed latent module conditioned on the low-resolution\nimage, which is then transformed by the flow model into an SR image. Our\nframework is designed to seamlessly integrate with any contemporary flow-based\nSR model without modifying its architecture or pre-trained weights. We evaluate\nthe effectiveness of our proposed framework through extensive experiments and\nablation analyses. The proposed framework successfully addresses all the\ninherent issues in flow-based SR models and enhances their performance in\nvarious SR scenarios. Our code is available at:\nhttps://github.com/liyuantsao/FlowSR-LP\n","authors":["Li-Yuan Tsao","Yi-Chen Lo","Chia-Che Chang","Hao-Wei Chen","Roy Tseng","Chien Feng","Chun-Yi Lee"],"pdf_url":"https://arxiv.org/pdf/2403.10988v2.pdf","comment":"Accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2312.02134v3","updated":"2024-03-30T04:22:34Z","published":"2023-12-04T18:55:45Z","title":"GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single\n  Video via Animatable 3D Gaussians","summary":"  We present GaussianAvatar, an efficient approach to creating realistic human\navatars with dynamic 3D appearances from a single video. We start by\nintroducing animatable 3D Gaussians to explicitly represent humans in various\nposes and clothing styles. Such an explicit and animatable representation can\nfuse 3D appearances more efficiently and consistently from 2D observations. Our\nrepresentation is further augmented with dynamic properties to support\npose-dependent appearance modeling, where a dynamic appearance network along\nwith an optimizable feature tensor is designed to learn the\nmotion-to-appearance mapping. Moreover, by leveraging the differentiable motion\ncondition, our method enables a joint optimization of motions and appearances\nduring avatar modeling, which helps to tackle the long-standing issue of\ninaccurate motion estimation in monocular settings. The efficacy of\nGaussianAvatar is validated on both the public dataset and our collected\ndataset, demonstrating its superior performances in terms of appearance quality\nand rendering efficiency.\n","authors":["Liangxiao Hu","Hongwen Zhang","Yuxiang Zhang","Boyao Zhou","Boning Liu","Shengping Zhang","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2312.02134v3.pdf","comment":"Project Page: https://huliangxiao.github.io/GaussianAvatar"},{"id":"http://arxiv.org/abs/2401.00374v5","updated":"2024-03-30T04:15:34Z","published":"2023-12-31T02:25:41Z","title":"EMAGE: Towards Unified Holistic Co-Speech Gesture Generation via\n  Expressive Masked Audio Gesture Modeling","summary":"  We propose EMAGE, a framework to generate full-body human gestures from audio\nand masked gestures, encompassing facial, local body, hands, and global\nmovements. To achieve this, we first introduce BEAT2 (BEAT-SMPLX-FLAME), a new\nmesh-level holistic co-speech dataset. BEAT2 combines a MoShed SMPL-X body with\nFLAME head parameters and further refines the modeling of head, neck, and\nfinger movements, offering a community-standardized, high-quality 3D motion\ncaptured dataset. EMAGE leverages masked body gesture priors during training to\nboost inference performance. It involves a Masked Audio Gesture Transformer,\nfacilitating joint training on audio-to-gesture generation and masked gesture\nreconstruction to effectively encode audio and body gesture hints. Encoded body\nhints from masked gestures are then separately employed to generate facial and\nbody movements. Moreover, EMAGE adaptively merges speech features from the\naudio's rhythm and content and utilizes four compositional VQ-VAEs to enhance\nthe results' fidelity and diversity. Experiments demonstrate that EMAGE\ngenerates holistic gestures with state-of-the-art performance and is flexible\nin accepting predefined spatial-temporal gesture inputs, generating complete,\naudio-synchronized results. Our code and dataset are available\nhttps://pantomatrix.github.io/EMAGE/\n","authors":["Haiyang Liu","Zihao Zhu","Giorgio Becherini","Yichen Peng","Mingyang Su","You Zhou","Xuefei Zhe","Naoya Iwamoto","Bo Zheng","Michael J. Black"],"pdf_url":"https://arxiv.org/pdf/2401.00374v5.pdf","comment":"Fix typos; Conflict of Interest Disclosure; CVPR Camera Ready;\n  Project Page: https://pantomatrix.github.io/EMAGE/"},{"id":"http://arxiv.org/abs/2302.05043v2","updated":"2024-03-30T03:50:28Z","published":"2023-02-10T04:12:11Z","title":"A Review of Predictive and Contrastive Self-supervised Learning for\n  Medical Images","summary":"  Over the last decade, supervised deep learning on manually annotated big data\nhas been progressing significantly on computer vision tasks. But the\napplication of deep learning in medical image analysis was limited by the\nscarcity of high-quality annotated medical imaging data. An emerging solution\nis self-supervised learning (SSL), among which contrastive SSL is the most\nsuccessful approach to rivalling or outperforming supervised learning. This\nreview investigates several state-of-the-art contrastive SSL algorithms\noriginally on natural images as well as their adaptations for medical images,\nand concludes by discussing recent advances, current limitations, and future\ndirections in applying contrastive SSL in the medical domain.\n","authors":["Wei-Chien Wang","Euijoon Ahn","Dagan Feng","Jinman Kim"],"pdf_url":"https://arxiv.org/pdf/2302.05043v2.pdf","comment":"Article links:\n  https://link.springer.com/article/10.1007/s11633-022-1406-4"},{"id":"http://arxiv.org/abs/2403.17610v2","updated":"2024-03-30T03:46:10Z","published":"2024-03-26T11:43:05Z","title":"MMVP: A Multimodal MoCap Dataset with Vision and Pressure Sensors","summary":"  Foot contact is an important cue for human motion capture, understanding, and\ngeneration. Existing datasets tend to annotate dense foot contact using visual\nmatching with thresholding or incorporating pressure signals. However, these\napproaches either suffer from low accuracy or are only designed for small-range\nand slow motion. There is still a lack of a vision-pressure multimodal dataset\nwith large-range and fast human motion, as well as accurate and dense\nfoot-contact annotation. To fill this gap, we propose a Multimodal MoCap\nDataset with Vision and Pressure sensors, named MMVP. MMVP provides accurate\nand dense plantar pressure signals synchronized with RGBD observations, which\nis especially useful for both plausible shape estimation, robust pose fitting\nwithout foot drifting, and accurate global translation tracking. To validate\nthe dataset, we propose an RGBD-P SMPL fitting method and also a\nmonocular-video-based baseline framework, VP-MoCap, for human motion capture.\nExperiments demonstrate that our RGBD-P SMPL Fitting results significantly\noutperform pure visual motion capture. Moreover, VP-MoCap outperforms SOTA\nmethods in foot-contact and global translation estimation accuracy. We believe\nthe configuration of the dataset and the baseline frameworks will stimulate the\nresearch in this direction and also provide a good reference for MoCap\napplications in various domains. Project page:\nhttps://metaverse-ai-lab-thu.github.io/MMVP-Dataset/.\n","authors":["He Zhang","Shenghao Ren","Haolei Yuan","Jianhui Zhao","Fan Li","Shuangpeng Sun","Zhenghao Liang","Tao Yu","Qiu Shen","Xun Cao"],"pdf_url":"https://arxiv.org/pdf/2403.17610v2.pdf","comment":"CVPR2024"},{"id":"http://arxiv.org/abs/2312.01280v2","updated":"2024-03-30T03:30:18Z","published":"2023-12-03T04:36:04Z","title":"Brain Decodes Deep Nets","summary":"  We developed a tool for visualizing and analyzing large pre-trained vision\nmodels by mapping them onto the brain, thus exposing their hidden inside. Our\ninnovation arises from a surprising usage of brain encoding: predicting brain\nfMRI measurements in response to images. We report two findings. First,\nexplicit mapping between the brain and deep-network features across dimensions\nof space, layers, scales, and channels is crucial. This mapping method,\nFactorTopy, is plug-and-play for any deep-network; with it, one can paint a\npicture of the network onto the brain (literally!). Second, our visualization\nshows how different training methods matter: they lead to remarkable\ndifferences in hierarchical organization and scaling behavior, growing with\nmore data or network capacity. It also provides insight into fine-tuning: how\npre-trained models change when adapting to small datasets. We found brain-like\nhierarchically organized network suffer less from catastrophic forgetting after\nfine-tuned.\n","authors":["Huzheng Yang","James Gee","Jianbo Shi"],"pdf_url":"https://arxiv.org/pdf/2312.01280v2.pdf","comment":"Website: see https://huzeyann.github.io/brain-decodes-deep-nets .\n  Code: see https://github.com/huzeyann/BrainDecodesDeepNets"},{"id":"http://arxiv.org/abs/2312.09243v2","updated":"2024-03-30T03:08:43Z","published":"2023-12-14T18:58:52Z","title":"OccNeRF: Advancing 3D Occupancy Prediction in LiDAR-Free Environments","summary":"  As a fundamental task of vision-based perception, 3D occupancy prediction\nreconstructs 3D structures of surrounding environments. It provides detailed\ninformation for autonomous driving planning and navigation. However, most\nexisting methods heavily rely on the LiDAR point clouds to generate occupancy\nground truth, which is not available in the vision-based system. In this paper,\nwe propose an OccNeRF method for training occupancy networks without 3D\nsupervision. Different from previous works which consider a bounded scene, we\nparameterize the reconstructed occupancy fields and reorganize the sampling\nstrategy to align with the cameras' infinite perceptive range. The neural\nrendering is adopted to convert occupancy fields to multi-camera depth maps,\nsupervised by multi-frame photometric consistency. Moreover, for semantic\noccupancy prediction, we design several strategies to polish the prompts and\nfilter the outputs of a pretrained open-vocabulary 2D segmentation model.\nExtensive experiments for both self-supervised depth estimation and 3D\noccupancy prediction tasks on nuScenes and SemanticKITTI datasets demonstrate\nthe effectiveness of our method.\n","authors":["Chubin Zhang","Juncheng Yan","Yi Wei","Jiaxin Li","Li Liu","Yansong Tang","Yueqi Duan","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2312.09243v2.pdf","comment":"Code: https://github.com/LinShan-Bin/OccNeRF"},{"id":"http://arxiv.org/abs/2403.19001v2","updated":"2024-03-30T02:42:08Z","published":"2024-03-27T20:51:02Z","title":"Cross-domain Fiber Cluster Shape Analysis for Language Performance\n  Cognitive Score Prediction","summary":"  Shape plays an important role in computer graphics, offering informative\nfeatures to convey an object's morphology and functionality. Shape analysis in\nbrain imaging can help interpret structural and functionality correlations of\nthe human brain. In this work, we investigate the shape of the brain's 3D white\nmatter connections and its potential predictive relationship to human cognitive\nfunction. We reconstruct brain connections as sequences of 3D points using\ndiffusion magnetic resonance imaging (dMRI) tractography. To describe each\nconnection, we extract 12 shape descriptors in addition to traditional dMRI\nconnectivity and tissue microstructure features. We introduce a novel\nframework, Shape--fused Fiber Cluster Transformer (SFFormer), that leverages a\nmulti-head cross-attention feature fusion module to predict subject-specific\nlanguage performance based on dMRI tractography. We assess the performance of\nthe method on a large dataset including 1065 healthy young adults. The results\ndemonstrate that both the transformer-based SFFormer model and its inter/intra\nfeature fusion with shape, microstructure, and connectivity are informative,\nand together, they improve the prediction of subject-specific language\nperformance scores. Overall, our results indicate that the shape of the brain's\nconnections is predictive of human language function.\n","authors":["Yui Lo","Yuqian Chen","Dongnan Liu","Wan Liu","Leo Zekelman","Fan Zhang","Yogesh Rathi","Nikos Makris","Alexandra J. Golby","Weidong Cai","Lauren J. O'Donnell"],"pdf_url":"https://arxiv.org/pdf/2403.19001v2.pdf","comment":"2 figures, 11 pages"},{"id":"http://arxiv.org/abs/2401.00901v2","updated":"2024-03-30T02:30:14Z","published":"2023-12-31T13:53:37Z","title":"Video-GroundingDINO: Towards Open-Vocabulary Spatio-Temporal Video\n  Grounding","summary":"  Video grounding aims to localize a spatio-temporal section in a video\ncorresponding to an input text query. This paper addresses a critical\nlimitation in current video grounding methodologies by introducing an\nOpen-Vocabulary Spatio-Temporal Video Grounding task. Unlike prevalent\nclosed-set approaches that struggle with open-vocabulary scenarios due to\nlimited training data and predefined vocabularies, our model leverages\npre-trained representations from foundational spatial grounding models. This\nempowers it to effectively bridge the semantic gap between natural language and\ndiverse visual content, achieving strong performance in closed-set and\nopen-vocabulary settings. Our contributions include a novel spatio-temporal\nvideo grounding model, surpassing state-of-the-art results in closed-set\nevaluations on multiple datasets and demonstrating superior performance in\nopen-vocabulary scenarios. Notably, the proposed model outperforms\nstate-of-the-art methods in closed-set settings on VidSTG (Declarative and\nInterrogative) and HC-STVG (V1 and V2) datasets. Furthermore, in\nopen-vocabulary evaluations on HC-STVG V1 and YouCook-Interactions, our model\nsurpasses the recent best-performing models by $4.88$ m_vIoU and $1.83\\%$\naccuracy, demonstrating its efficacy in handling diverse linguistic and visual\nconcepts for improved video understanding. Our codes will be publicly released.\n","authors":["Syed Talal Wasim","Muzammal Naseer","Salman Khan","Ming-Hsuan Yang","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2401.00901v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09788v2","updated":"2024-03-30T01:21:42Z","published":"2023-12-15T13:43:24Z","title":"Collaborating Foundation Models for Domain Generalized Semantic\n  Segmentation","summary":"  Domain Generalized Semantic Segmentation (DGSS) deals with training a model\non a labeled source domain with the aim of generalizing to unseen domains\nduring inference. Existing DGSS methods typically effectuate robust features by\nmeans of Domain Randomization (DR). Such an approach is often limited as it can\nonly account for style diversification and not content. In this work, we take\nan orthogonal approach to DGSS and propose to use an assembly of CoLlaborative\nFOUndation models for Domain Generalized Semantic Segmentation (CLOUDS). In\ndetail, CLOUDS is a framework that integrates FMs of various kinds: (i) CLIP\nbackbone for its robust feature representation, (ii) generative models to\ndiversify the content, thereby covering various modes of the possible target\ndistribution, and (iii) Segment Anything Model (SAM) for iteratively refining\nthe predictions of the segmentation model. Extensive experiments show that our\nCLOUDS excels in adapting from synthetic to real DGSS benchmarks and under\nvarying weather conditions, notably outperforming prior methods by 5.6% and\n6.7% on averaged miou, respectively. The code is available at :\nhttps://github.com/yasserben/CLOUDS\n","authors":["Yasser Benigmim","Subhankar Roy","Slim Essid","Vicky Kalogeiton","Stéphane Lathuilière"],"pdf_url":"https://arxiv.org/pdf/2312.09788v2.pdf","comment":"https://github.com/yasserben/CLOUDS ; Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2303.02835v2","updated":"2024-03-30T01:21:22Z","published":"2023-03-06T02:05:14Z","title":"Traffic Scene Parsing through the TSP6K Dataset","summary":"  Traffic scene perception in computer vision is a critically important task to\nachieve intelligent cities. To date, most existing datasets focus on autonomous\ndriving scenes. We observe that the models trained on those driving datasets\noften yield unsatisfactory results on traffic monitoring scenes. However,\nlittle effort has been put into improving the traffic monitoring scene\nunderstanding, mainly due to the lack of specific datasets. To fill this gap,\nwe introduce a specialized traffic monitoring dataset, termed TSP6K, containing\nimages from the traffic monitoring scenario, with high-quality pixel-level and\ninstance-level annotations. The TSP6K dataset captures more crowded traffic\nscenes with several times more traffic participants than the existing driving\nscenes. We perform a detailed analysis of the dataset and comprehensively\nevaluate previous popular scene parsing methods, instance segmentation methods\nand unsupervised domain adaption methods. Furthermore, considering the vast\ndifference in instance sizes, we propose a detail refining decoder for scene\nparsing, which recovers the details of different semantic regions in traffic\nscenes owing to the proposed TSP6K dataset. Experiments show its effectiveness\nin parsing the traffic monitoring scenes. Code and dataset are available at\nhttps://github.com/PengtaoJiang/TSP6K.\n","authors":["Peng-Tao Jiang","Yuqi Yang","Yang Cao","Qibin Hou","Ming-Ming Cheng","Chunhua Shen"],"pdf_url":"https://arxiv.org/pdf/2303.02835v2.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2311.12981v2","updated":"2024-03-30T01:18:21Z","published":"2023-11-21T20:33:17Z","title":"SD-NAE: Generating Natural Adversarial Examples with Stable Diffusion","summary":"  Natural Adversarial Examples (NAEs), images arising naturally from the\nenvironment and capable of deceiving classifiers, are instrumental in robustly\nevaluating and identifying vulnerabilities in trained models. In this work,\nunlike prior works that passively collect NAEs from real images, we propose to\nactively synthesize NAEs using the state-of-the-art Stable Diffusion.\nSpecifically, our method formulates a controlled optimization process, where we\nperturb the token embedding that corresponds to a specified class to generate\nNAEs. This generation process is guided by the gradient of loss from the target\nclassifier, ensuring that the created image closely mimics the ground-truth\nclass yet fools the classifier. Named SD-NAE (Stable Diffusion for Natural\nAdversarial Examples), our innovative method is effective in producing valid\nand useful NAEs, which is demonstrated through a meticulously designed\nexperiment. Code is available at https://github.com/linyueqian/SD-NAE.\n","authors":["Yueqian Lin","Jingyang Zhang","Yiran Chen","Hai Li"],"pdf_url":"https://arxiv.org/pdf/2311.12981v2.pdf","comment":"Accepted by ICLR 2024 TinyPapers"},{"id":"http://arxiv.org/abs/2403.19549v2","updated":"2024-03-30T00:24:44Z","published":"2024-03-28T16:32:06Z","title":"GlORIE-SLAM: Globally Optimized RGB-only Implicit Encoding Point Cloud\n  SLAM","summary":"  Recent advancements in RGB-only dense Simultaneous Localization and Mapping\n(SLAM) have predominantly utilized grid-based neural implicit encodings and/or\nstruggle to efficiently realize global map and pose consistency. To this end,\nwe propose an efficient RGB-only dense SLAM system using a flexible neural\npoint cloud scene representation that adapts to keyframe poses and depth\nupdates, without needing costly backpropagation. Another critical challenge of\nRGB-only SLAM is the lack of geometric priors. To alleviate this issue, with\nthe aid of a monocular depth estimator, we introduce a novel DSPO layer for\nbundle adjustment which optimizes the pose and depth of keyframes along with\nthe scale of the monocular depth. Finally, our system benefits from loop\nclosure and online global bundle adjustment and performs either better or\ncompetitive to existing dense neural RGB SLAM methods in tracking, mapping and\nrendering accuracy on the Replica, TUM-RGBD and ScanNet datasets. The source\ncode will be made available.\n","authors":["Ganlin Zhang","Erik Sandström","Youmin Zhang","Manthan Patel","Luc Van Gool","Martin R. Oswald"],"pdf_url":"https://arxiv.org/pdf/2403.19549v2.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2303.00720v2","updated":"2024-03-30T21:13:14Z","published":"2023-03-01T18:26:14Z","title":"Cross-Modal Entity Matching for Visually Rich Documents","summary":"  Visually rich documents (e.g. leaflets, banners, magazine articles) are\nphysical or digital documents that utilize visual cues to augment their\nsemantics. Information contained in these documents are ad-hoc and often\nincomplete. Existing works that enable structured querying on these documents\ndo not take this into account. This makes it difficult to contextualize the\ninformation retrieved from querying these documents and gather actionable\ninsights from them. We propose Juno -- a cross-modal entity matching framework\nto address this limitation. It augments heterogeneous documents with\nsupplementary information by matching a text span in the document with\nsemantically similar tuples from an external database. Our main contribution in\nthis is a deep neural network with attention that goes beyond traditional\nkeyword-based matching and finds matching tuples by aligning text spans and\nrelational tuples on a multimodal encoding space without any prior knowledge\nabout the document type or the underlying schema. Exhaustive experiments on\nmultiple real-world datasets show that Juno generalizes to heterogeneous\ndocuments with diverse layouts and formats. It outperforms state-of-the-art\nbaselines by more than 6 F1 points with up to 60% less human-labeled samples.\nOur experiments further show that Juno is a computationally robust framework.\nWe can train it only once, and then adapt it dynamically for multiple\nresource-constrained environments without sacrificing its downstream\nperformance. This makes it suitable for on-device deployment in various\nedge-devices. To the best of our knowledge, ours is the first work that\ninvestigates the information incompleteness of visually rich documents and\nproposes a generalizable, performant and computationally robust framework to\naddress it in an end-to-end way.\n","authors":["Ritesh Sarkhel","Arnab Nandi"],"pdf_url":"https://arxiv.org/pdf/2303.00720v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2004.11131v2","updated":"2024-03-30T12:21:59Z","published":"2020-04-23T13:21:00Z","title":"Privacy at Scale: Introducing the PrivaSeer Corpus of Web Privacy\n  Policies","summary":"  Organisations disclose their privacy practices by posting privacy policies on\ntheir website. Even though users often care about their digital privacy, they\noften don't read privacy policies since they require a significant investment\nin time and effort. Although natural language processing can help in privacy\npolicy understanding, there has been a lack of large scale privacy policy\ncorpora that could be used to analyse, understand, and simplify privacy\npolicies. Thus, we create PrivaSeer, a corpus of over one million English\nlanguage website privacy policies, which is significantly larger than any\npreviously available corpus. We design a corpus creation pipeline which\nconsists of crawling the web followed by filtering documents using language\ndetection, document classification, duplicate and near-duplication removal, and\ncontent extraction. We investigate the composition of the corpus and show\nresults from readability tests, document similarity, keyphrase extraction, and\nexplored the corpus through topic modeling.\n","authors":["Mukund Srinath","Shomir Wilson","C. Lee Giles"],"pdf_url":"https://arxiv.org/pdf/2004.11131v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17372v2","updated":"2024-03-30T04:43:20Z","published":"2024-03-26T04:16:57Z","title":"An Empirical Study of Training ID-Agnostic Multi-modal Sequential\n  Recommenders","summary":"  Sequential Recommendation (SR) aims to predict future user-item interactions\nbased on historical interactions. While many SR approaches concentrate on user\nIDs and item IDs, the human perception of the world through multi-modal\nsignals, like text and images, has inspired researchers to delve into\nconstructing SR from multi-modal information without using IDs. However, the\ncomplexity of multi-modal learning manifests in diverse feature extractors,\nfusion methods, and pre-trained models. Consequently, designing a simple and\nuniversal \\textbf{M}ulti-\\textbf{M}odal \\textbf{S}equential\n\\textbf{R}ecommendation (\\textbf{MMSR}) framework remains a formidable\nchallenge. We systematically summarize the existing multi-modal related SR\nmethods and distill the essence into four core components: visual encoder, text\nencoder, multimodal fusion module, and sequential architecture. Along these\ndimensions, we dissect the model designs, and answer the following\nsub-questions: First, we explore how to construct MMSR from scratch, ensuring\nits performance either on par with or exceeds existing SR methods without\ncomplex techniques. Second, we examine if MMSR can benefit from existing\nmulti-modal pre-training paradigms. Third, we assess MMSR's capability in\ntackling common challenges like cold start and domain transferring. Our\nexperiment results across four real-world recommendation scenarios demonstrate\nthe great potential ID-agnostic multi-modal sequential recommendation. Our\nframework can be found at: https://github.com/MMSR23/MMSR.\n","authors":["Youhua Li","Hanwen Du","Yongxin Ni","Yuanqi He","Junchen Fu","Xiangyan Liu","Qi Guo"],"pdf_url":"https://arxiv.org/pdf/2403.17372v2.pdf","comment":"An Empirical Study of Training ID-Agnostic Multi-modal Sequential\n  Recommenders"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2311.08118v3","updated":"2024-03-30T23:49:19Z","published":"2023-11-14T12:33:19Z","title":"Evaluating Neighbor Explainability for Graph Neural Networks","summary":"  Explainability in Graph Neural Networks (GNNs) is a new field growing in the\nlast few years. In this publication we address the problem of determining how\nimportant is each neighbor for the GNN when classifying a node and how to\nmeasure the performance for this specific task. To do this, various known\nexplainability methods are reformulated to get the neighbor importance and four\nnew metrics are presented. Our results show that there is almost no difference\nbetween the explanations provided by gradient-based techniques in the GNN\ndomain. In addition, many explainability techniques failed to identify\nimportant neighbors when GNNs without self-loops are used.\n","authors":["Oscar Llorente","Rana Fawzy","Jared Keown","Michal Horemuz","Péter Vaderna","Sándor Laki","Roland Kotroczó","Rita Csoma","János Márk Szalai-Gindl"],"pdf_url":"https://arxiv.org/pdf/2311.08118v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16108v2","updated":"2024-03-30T23:46:29Z","published":"2024-03-24T11:52:39Z","title":"A Transformer approach for Electricity Price Forecasting","summary":"  This paper presents a novel approach to electricity price forecasting (EPF)\nusing a pure Transformer model. As opposed to other alternatives, no other\nrecurrent network is used in combination to the attention mechanism. Hence,\nshowing that the attention layer is enough for capturing the temporal patterns.\nThe paper also provides fair comparison of the models using the open-source EPF\ntoolbox and provide the code to enhance reproducibility and transparency in EPF\nresearch. The results show that the Transformer model outperforms traditional\nmethods, offering a promising solution for reliable and sustainable power\nsystem operation.\n","authors":["Oscar Llorente","Jose Portela"],"pdf_url":"https://arxiv.org/pdf/2403.16108v2.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2303.09373v3","updated":"2024-03-30T22:10:36Z","published":"2023-03-16T15:01:50Z","title":"MAPSeg: Unified Unsupervised Domain Adaptation for Heterogeneous Medical\n  Image Segmentation Based on 3D Masked Autoencoding and Pseudo-Labeling","summary":"  Robust segmentation is critical for deriving quantitative measures from\nlarge-scale, multi-center, and longitudinal medical scans. Manually annotating\nmedical scans, however, is expensive and labor-intensive and may not always be\navailable in every domain. Unsupervised domain adaptation (UDA) is a\nwell-studied technique that alleviates this label-scarcity problem by\nleveraging available labels from another domain. In this study, we introduce\nMasked Autoencoding and Pseudo-Labeling Segmentation (MAPSeg), a\n$\\textbf{unified}$ UDA framework with great versatility and superior\nperformance for heterogeneous and volumetric medical image segmentation. To the\nbest of our knowledge, this is the first study that systematically reviews and\ndevelops a framework to tackle four different domain shifts in medical image\nsegmentation. More importantly, MAPSeg is the first framework that can be\napplied to $\\textbf{centralized}$, $\\textbf{federated}$, and\n$\\textbf{test-time}$ UDA while maintaining comparable performance. We compare\nMAPSeg with previous state-of-the-art methods on a private infant brain MRI\ndataset and a public cardiac CT-MRI dataset, and MAPSeg outperforms others by a\nlarge margin (10.5 Dice improvement on the private MRI dataset and 5.7 on the\npublic CT-MRI dataset). MAPSeg poses great practical value and can be applied\nto real-world problems. GitHub: https://github.com/XuzheZ/MAPSeg/.\n","authors":["Xuzhe Zhang","Yuhao Wu","Elsa Angelini","Ang Li","Jia Guo","Jerod M. Rasmussen","Thomas G. O'Connor","Pathik D. Wadhwa","Andrea Parolin Jackowski","Hai Li","Jonathan Posner","Andrew F. Laine","Yun Wang"],"pdf_url":"https://arxiv.org/pdf/2303.09373v3.pdf","comment":"CVPR 2024 camera-ready (8 pages, 3 figures) with the supplemental\n  materials (5 pages, 4 figures). Xuzhe Zhang and Yuhao Wu are co-first\n  authors. Andrew F. Laine and Yun Wang are co-senior supervising authors"},{"id":"http://arxiv.org/abs/2312.11517v3","updated":"2024-03-30T21:14:37Z","published":"2023-12-12T19:34:23Z","title":"A Natural Language Processing-Based Classification and Mode-Based\n  Ranking of Musculoskeletal Disorder Risk Factors","summary":"  This research delves into Musculoskeletal Disorder (MSD) risk factors, using\na blend of Natural Language Processing (NLP) and mode-based ranking. The aim is\nto refine understanding, classification, and prioritization for focused\nprevention and treatment. Eight NLP models are evaluated, combining pre-trained\ntransformers, cosine similarity, and distance metrics to categorize factors\ninto personal, biomechanical, workplace, psychological, and organizational\nclasses. BERT with cosine similarity achieves 28% accuracy; sentence\ntransformer with Euclidean, Bray-Curtis, and Minkowski distances scores 100%.\nWith 10-fold cross-validation, statistical tests ensure robust results. Survey\ndata and mode-based ranking determine severity hierarchy, aligning with the\nliterature. \"Working posture\" is the most severe, highlighting posture's role.\nSurvey insights emphasize \"Job insecurity,\" \"Effort reward imbalance,\" and\n\"Poor employee facility\" as significant contributors. Rankings offer actionable\ninsights for MSD prevention. The study suggests targeted interventions,\nworkplace improvements, and future research directions. This integrated NLP and\nranking approach enhances MSD comprehension and informs occupational health\nstrategies.\n","authors":["Md Abrar Jahin","Subrata Talapatra"],"pdf_url":"https://arxiv.org/pdf/2312.11517v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.00720v2","updated":"2024-03-30T21:13:14Z","published":"2023-03-01T18:26:14Z","title":"Cross-Modal Entity Matching for Visually Rich Documents","summary":"  Visually rich documents (e.g. leaflets, banners, magazine articles) are\nphysical or digital documents that utilize visual cues to augment their\nsemantics. Information contained in these documents are ad-hoc and often\nincomplete. Existing works that enable structured querying on these documents\ndo not take this into account. This makes it difficult to contextualize the\ninformation retrieved from querying these documents and gather actionable\ninsights from them. We propose Juno -- a cross-modal entity matching framework\nto address this limitation. It augments heterogeneous documents with\nsupplementary information by matching a text span in the document with\nsemantically similar tuples from an external database. Our main contribution in\nthis is a deep neural network with attention that goes beyond traditional\nkeyword-based matching and finds matching tuples by aligning text spans and\nrelational tuples on a multimodal encoding space without any prior knowledge\nabout the document type or the underlying schema. Exhaustive experiments on\nmultiple real-world datasets show that Juno generalizes to heterogeneous\ndocuments with diverse layouts and formats. It outperforms state-of-the-art\nbaselines by more than 6 F1 points with up to 60% less human-labeled samples.\nOur experiments further show that Juno is a computationally robust framework.\nWe can train it only once, and then adapt it dynamically for multiple\nresource-constrained environments without sacrificing its downstream\nperformance. This makes it suitable for on-device deployment in various\nedge-devices. To the best of our knowledge, ours is the first work that\ninvestigates the information incompleteness of visually rich documents and\nproposes a generalizable, performant and computationally robust framework to\naddress it in an end-to-end way.\n","authors":["Ritesh Sarkhel","Arnab Nandi"],"pdf_url":"https://arxiv.org/pdf/2303.00720v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07148v2","updated":"2024-03-30T20:18:54Z","published":"2024-02-11T10:23:34Z","title":"X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for\n  Large Language Models with Applications in Protein Mechanics and Molecular\n  Design","summary":"  We report a mixture of expert strategy to create fine-tuned large language\nmodels using a deep layer-wise token-level approach based on low-rank\nadaptation (LoRA). Starting with a set of pre-trained LoRA adapters, our gating\nstrategy uses the hidden states to dynamically mix adapted layers, allowing the\nresulting X-LoRA model to draw upon different capabilities and create\nnever-before-used deep layer-wise combinations to solve tasks. The design is\ninspired by the biological principles of universality and diversity, where\nneural network building blocks are reused in different hierarchical\nmanifestations. Hence, the X-LoRA model can be easily implemented for any\nexisting large language model (LLM) without a need for modifications of the\nunderlying structure. We develop a tailored X-LoRA model that offers scientific\ncapabilities including forward/inverse analysis tasks and enhanced reasoning\ncapability, focused on biomaterial analysis, protein mechanics and design. The\nimpact of this work include access to readily expandable and adaptable models\nwith strong domain knowledge and the capability to integrate across areas of\nknowledge. Featuring experts in biology, mathematics, reasoning, bio-inspired\nmaterials, mechanics and materials, chemistry, protein biophysics, mechanics\nand quantum-mechanics based molecular properties, we conduct a series of\nphysics-focused case studies. We examine knowledge recall, protein mechanics\nforward/inverse tasks, protein design, adversarial agentic modeling including\nontological knowledge graph construction, as well as molecular design. The\nmodel is capable not only of making quantitative predictions of nanomechanical\nproperties of proteins or quantum mechanical molecular properties, but also\nreasons over the results and correctly predicts likely mechanisms that explain\ndistinct molecular behaviors.\n","authors":["Eric L. Buehler","Markus J. Buehler"],"pdf_url":"https://arxiv.org/pdf/2402.07148v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12304v4","updated":"2024-03-30T19:16:29Z","published":"2023-11-21T02:46:14Z","title":"Discovering Effective Policies for Land-Use Planning with Neuroevolution","summary":"  How areas of land are allocated for different uses, such as forests, urban\nareas, and agriculture, has a large effect on the terrestrial carbon balance,\nand therefore climate change. Based on available historical data on land-use\nchanges and a simulation of the associated carbon emissions and removals, a\nsurrogate model can be learned that makes it possible to evaluate the different\noptions available to decision-makers efficiently. An evolutionary search\nprocess can then be used to discover effective land-use policies for specific\nlocations. Such a system was built on the Project Resilience platform and\nevaluated with the Land-Use Harmonization dataset LUH2 and the bookkeeping\nmodel BLUE. It generates Pareto fronts that trade off carbon impact and amount\nof land-use change customized to different locations, thus providing a\npotentially useful tool for land-use planning.\n","authors":["Risto Miikkulainen","Olivier Francon","Daniel Young","Elliot Meyerson","Clemens Schwingshackl","Jacob Bieker","Hugo Cunha","Babak Hodjat"],"pdf_url":"https://arxiv.org/pdf/2311.12304v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13300v2","updated":"2024-03-30T16:58:59Z","published":"2024-03-20T04:57:27Z","title":"Kernel Multigrid: Accelerate Back-fitting via Sparse Gaussian Process\n  Regression","summary":"  Additive Gaussian Processes (GPs) are popular approaches for nonparametric\nfeature selection. The common training method for these models is Bayesian\nBack-fitting. However, the convergence rate of Back-fitting in training\nadditive GPs is still an open problem. By utilizing a technique called Kernel\nPackets (KP), we prove that the convergence rate of Back-fitting is no faster\nthan $(1-\\mathcal{O}(\\frac{1}{n}))^t$, where $n$ and $t$ denote the data size\nand the iteration number, respectively. Consequently, Back-fitting requires a\nminimum of $\\mathcal{O}(n\\log n)$ iterations to achieve convergence. Based on\nKPs, we further propose an algorithm called Kernel Multigrid (KMG). This\nalgorithm enhances Back-fitting by incorporating a sparse Gaussian Process\nRegression (GPR) to process the residuals after each Back-fitting iteration. It\nis applicable to additive GPs with both structured and scattered data.\nTheoretically, we prove that KMG reduces the required iterations to\n$\\mathcal{O}(\\log n)$ while preserving the time and space complexities at\n$\\mathcal{O}(n\\log n)$ and $\\mathcal{O}(n)$ per iteration, respectively.\nNumerically, by employing a sparse GPR with merely 10 inducing points, KMG can\nproduce accurate approximations of high-dimensional targets within 5\niterations.\n","authors":["Lu Zou","Liang Ding"],"pdf_url":"https://arxiv.org/pdf/2403.13300v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.14295v2","updated":"2024-03-30T16:33:36Z","published":"2024-01-25T16:34:00Z","title":"Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of\n  Thoughts","summary":"  The field of natural language processing (NLP) has witnessed significant\nprogress in recent years, with a notable focus on improving large language\nmodels' (LLM) performance through innovative prompting techniques. Among these,\nprompt engineering coupled with structures has emerged as a promising paradigm,\nwith designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts,\nin which the overall LLM reasoning is guided by a structure such as a graph. As\nillustrated with numerous examples, this paradigm significantly enhances the\nLLM's capability to solve numerous tasks, ranging from logical or mathematical\nreasoning to planning or creative writing. To facilitate the understanding of\nthis growing field and pave the way for future developments, we devise a\ngeneral blueprint for effective and efficient LLM reasoning schemes. For this,\nwe conduct an in-depth analysis of the prompt execution pipeline, clarifying\nand clearly defining different concepts. We then build the first taxonomy of\nstructure-enhanced LLM reasoning schemes. We focus on identifying fundamental\nclasses of harnessed structures, and we analyze the representations of these\nstructures, algorithms executed with these structures, and many others. We\nrefer to these structures as reasoning topologies, because their representation\nbecomes to a degree spatial, as they are contained within the LLM context. Our\nstudy compares existing prompting schemes using the proposed taxonomy,\ndiscussing how certain design choices lead to different patterns in performance\nand cost. We also outline theoretical underpinnings, relationships between\nprompting and other parts of the LLM ecosystem such as knowledge bases, and the\nassociated research challenges. Our work will help to advance future prompt\nengineering techniques.\n","authors":["Maciej Besta","Florim Memedi","Zhenyu Zhang","Robert Gerstenberger","Guangyuan Piao","Nils Blach","Piotr Nyczyk","Marcin Copik","Grzegorz Kwaśniewski","Jürgen Müller","Lukas Gianinazzi","Ales Kubicek","Hubert Niewiadomski","Aidan O'Mahony","Onur Mutlu","Torsten Hoefler"],"pdf_url":"https://arxiv.org/pdf/2401.14295v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14671v2","updated":"2024-03-30T16:25:21Z","published":"2024-03-05T08:50:21Z","title":"Understanding the Transit Gap: A Comparative Study of On-Demand Bus\n  Services and Urban Climate Resilience in South End, Charlotte, NC and\n  Avondale, Chattanooga, TN","summary":"  Urban design significantly impacts sustainability, particularly in the\ncontext of public transit efficiency and carbon emissions reduction. This study\nexplores two neighborhoods with distinct urban designs: South End, Charlotte,\nNC, featuring a dynamic mixed-use urban design pattern, and Avondale,\nChattanooga, TN, with a residential suburban grid layout. Using the TRANSIT-GYM\ntool, we assess the impact of increased bus utilization in these different\nurban settings on traffic and CO2 emissions. Our results highlight the critical\nrole of urban design and planning in transit system efficiency. In South End,\nthe mixed-use design led to more substantial emission reductions, indicating\nthat urban layout can significantly influence public transit outcomes. Tailored\nstrategies that consider the unique urban design elements are essential for\nclimate resilience. Notably, doubling bus utilization decreased daily emissions\nby 10.18% in South End and 8.13% in Avondale, with a corresponding reduction in\noverall traffic. A target of 50% bus utilization saw emissions drop by 21.45%\nin South End and 14.50% in Avondale. At an idealistic goal of 70% bus\nutilization, South End and Avondale witnessed emission reductions of 37.22% and\n27.80%, respectively. These insights are crucial for urban designers and\npolicymakers in developing sustainable urban landscapes.\n","authors":["Sanaz Sadat Hosseini","Babak Rahimi Ardabili","Mona Azarbayjani","Srinivas Pulugurtha","Hamed Tabkhi"],"pdf_url":"https://arxiv.org/pdf/2403.14671v2.pdf","comment":"6 pages, 4 figures, Recently accepted by the PLEA 2024 Conference for\n  Sustainable Architecture and Urban Design (Re-Thinking Resilience), Wroclaw,\n  Poland"},{"id":"http://arxiv.org/abs/2310.06081v2","updated":"2024-03-30T16:21:39Z","published":"2023-10-09T18:38:56Z","title":"Ito Diffusion Approximation of Universal Ito Chains for Sampling,\n  Optimization and Boosting","summary":"  In this work, we consider rather general and broad class of Markov chains,\nIto chains, that look like Euler-Maryama discretization of some Stochastic\nDifferential Equation. The chain we study is a unified framework for\ntheoretical analysis. It comes with almost arbitrary isotropic and\nstate-dependent noise instead of normal and state-independent one as in most\nrelated papers. Moreover, in our chain the drift and diffusion coefficient can\nbe inexact in order to cover wide range of applications as Stochastic Gradient\nLangevin Dynamics, sampling, Stochastic Gradient Descent or Stochastic Gradient\nBoosting. We prove the bound in $W_{2}$-distance between the laws of our Ito\nchain and corresponding differential equation. These results improve or cover\nmost of the known estimates. And for some particular cases, our analysis is the\nfirst.\n","authors":["Aleksei Ustimenko","Aleksandr Beznosikov"],"pdf_url":"https://arxiv.org/pdf/2310.06081v2.pdf","comment":"Appears in: The Twelfth International Conference on Learning\n  Representations (ICLR 2024). 27 pages, 3 tables. Reference:\n  https://openreview.net/forum?id=fjpfCOV4ru"},{"id":"http://arxiv.org/abs/2310.18715v2","updated":"2024-03-30T16:16:56Z","published":"2023-10-28T14:24:26Z","title":"Robust Offline Reinforcement learning with Heavy-Tailed Rewards","summary":"  This paper endeavors to augment the robustness of offline reinforcement\nlearning (RL) in scenarios laden with heavy-tailed rewards, a prevalent\ncircumstance in real-world applications. We propose two algorithmic frameworks,\nROAM and ROOM, for robust off-policy evaluation and offline policy optimization\n(OPO), respectively. Central to our frameworks is the strategic incorporation\nof the median-of-means method with offline RL, enabling straightforward\nuncertainty estimation for the value function estimator. This not only adheres\nto the principle of pessimism in OPO but also adeptly manages heavy-tailed\nrewards. Theoretical results and extensive experiments demonstrate that our two\nframeworks outperform existing methods on the logged dataset exhibits\nheavy-tailed reward distributions. The implementation of the proposal is\navailable at https://github.com/Mamba413/ROOM.\n","authors":["Jin Zhu","Runzhe Wan","Zhengling Qi","Shikai Luo","Chengchun Shi"],"pdf_url":"https://arxiv.org/pdf/2310.18715v2.pdf","comment":"23 pages, 6 figures. Proceedings of the 27th International Conference\n  on Artificial Intelligence and Statistics (AISTATS) 2024"},{"id":"http://arxiv.org/abs/2402.10038v2","updated":"2024-03-30T16:10:47Z","published":"2024-02-15T16:00:58Z","title":"RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization\n  Method for Alignment of Large Language Models","summary":"  Reinforcement learning from human feedback (RLHF) has been extensively\nemployed to align large language models with user intent. However, proximal\npolicy optimization (PPO) based RLHF is occasionally unstable requiring\nsignificant hyperparameter finetuning, and computationally expensive to\nmaximize the estimated reward during alignment. Recently, direct preference\noptimization (DPO) is proposed to address those challenges. However, DPO relies\non contrastive responses generated from human annotator and alternative LLM,\ninstead of the policy model, limiting the effectiveness of the RLHF. In this\npaper, we addresses both challenges by systematically combining rejection\nsampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the\ndevelopment of a supervised fine-tuned policy model (SFT). A varied set of k\nresponses per prompt are sampled directly from the SFT model. RS-DPO identifies\npairs of contrastive samples based on their reward distribution. Finally, we\napply DPO with the contrastive samples to align the model to human preference.\nOur experiments indicate that our proposed method effectively fine-tunes LLMs\nwith limited resource environments, leading to improved alignment with user\nintent. Furthermore, it outperforms existing methods, including RS, PPO, and\nDPO.\n","authors":["Saeed Khaki","JinJin Li","Lan Ma","Liu Yang","Prathap Ramachandra"],"pdf_url":"https://arxiv.org/pdf/2402.10038v2.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.07676v2","updated":"2024-03-30T16:09:34Z","published":"2023-10-11T17:21:03Z","title":"Composite Backdoor Attacks Against Large Language Models","summary":"  Large language models (LLMs) have demonstrated superior performance compared\nto previous methods on various tasks, and often serve as the foundation models\nfor many researches and services. However, the untrustworthy third-party LLMs\nmay covertly introduce vulnerabilities for downstream tasks. In this paper, we\nexplore the vulnerability of LLMs through the lens of backdoor attacks.\nDifferent from existing backdoor attacks against LLMs, ours scatters multiple\ntrigger keys in different prompt components. Such a Composite Backdoor Attack\n(CBA) is shown to be stealthier than implanting the same multiple trigger keys\nin only a single component. CBA ensures that the backdoor is activated only\nwhen all trigger keys appear. Our experiments demonstrate that CBA is effective\nin both natural language processing (NLP) and multimodal tasks. For instance,\nwith $3\\%$ poisoning samples against the LLaMA-7B model on the Emotion dataset,\nour attack achieves a $100\\%$ Attack Success Rate (ASR) with a False Triggered\nRate (FTR) below $2.06\\%$ and negligible model accuracy degradation. Our work\nhighlights the necessity of increased security research on the trustworthiness\nof foundation LLMs.\n","authors":["Hai Huang","Zhengyu Zhao","Michael Backes","Yun Shen","Yang Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.07676v2.pdf","comment":"To Appear in Findings of the Association for Computational\n  Linguistics: NAACL 2024, June 2024"},{"id":"http://arxiv.org/abs/2312.09238v2","updated":"2024-03-30T15:35:16Z","published":"2023-12-14T18:58:12Z","title":"Auto MC-Reward: Automated Dense Reward Design with Large Language Models\n  for Minecraft","summary":"  Many reinforcement learning environments (e.g., Minecraft) provide only\nsparse rewards that indicate task completion or failure with binary values. The\nchallenge in exploration efficiency in such environments makes it difficult for\nreinforcement-learning-based agents to learn complex tasks. To address this,\nthis paper introduces an advanced learning system, named Auto MC-Reward, that\nleverages Large Language Models (LLMs) to automatically design dense reward\nfunctions, thereby enhancing the learning efficiency. Auto MC-Reward consists\nof three important components: Reward Designer, Reward Critic, and Trajectory\nAnalyzer. Given the environment information and task descriptions, the Reward\nDesigner first design the reward function by coding an executable Python\nfunction with predefined observation inputs. Then, our Reward Critic will be\nresponsible for verifying the code, checking whether the code is\nself-consistent and free of syntax and semantic errors. Further, the Trajectory\nAnalyzer summarizes possible failure causes and provides refinement suggestions\naccording to collected trajectories. In the next round, Reward Designer will\nfurther refine and iterate the dense reward function based on feedback.\nExperiments demonstrate a significant improvement in the success rate and\nlearning efficiency of our agents in complex tasks in Minecraft, such as\nobtaining diamond with the efficient ability to avoid lava, and efficiently\nexplore trees and animals that are sparse in the plains biome.\n","authors":["Hao Li","Xue Yang","Zhaokai Wang","Xizhou Zhu","Jie Zhou","Yu Qiao","Xiaogang Wang","Hongsheng Li","Lewei Lu","Jifeng Dai"],"pdf_url":"https://arxiv.org/pdf/2312.09238v2.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2205.11720v3","updated":"2024-03-30T15:27:22Z","published":"2022-05-24T02:02:05Z","title":"ENS-t-SNE: Embedding Neighborhoods Simultaneously t-SNE","summary":"  When visualizing a high-dimensional dataset, dimension reduction techniques\nare commonly employed which provide a single 2-dimensional view of the data. We\ndescribe ENS-t-SNE: an algorithm for Embedding Neighborhoods Simultaneously\nthat generalizes the t-Stochastic Neighborhood Embedding approach. By using\ndifferent viewpoints in ENS-t-SNE's 3D embedding, one can visualize different\ntypes of clusters within the same high-dimensional dataset. This enables the\nviewer to see and keep track of the different types of clusters, which is\nharder to do when providing multiple 2D embeddings, where corresponding points\ncannot be easily identified. We illustrate the utility of ENS-t-SNE with\nreal-world applications and provide an extensive quantitative evaluation with\ndatasets of different types and sizes.\n","authors":["Jacob Miller","Vahan Huroyan","Raymundo Navarrete","Md Iqbal Hossain","Stephen Kobourov"],"pdf_url":"https://arxiv.org/pdf/2205.11720v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.07615v2","updated":"2024-03-30T15:00:07Z","published":"2023-02-15T12:11:27Z","title":"Similarity, Compression and Local Steps: Three Pillars of Efficient\n  Communications for Distributed Variational Inequalities","summary":"  Variational inequalities are a broad and flexible class of problems that\nincludes minimization, saddle point, and fixed point problems as special cases.\nTherefore, variational inequalities are used in various applications ranging\nfrom equilibrium search to adversarial learning. With the increasing size of\ndata and models, today's instances demand parallel and distributed computing\nfor real-world machine learning problems, most of which can be represented as\nvariational inequalities. Meanwhile, most distributed approaches have a\nsignificant bottleneck - the cost of communications. The three main techniques\nto reduce the total number of communication rounds and the cost of one such\nround are the similarity of local functions, compression of transmitted\ninformation, and local updates. In this paper, we combine all these approaches.\nSuch a triple synergy did not exist before for variational inequalities and\nsaddle problems, nor even for minimization problems. The methods presented in\nthis paper have the best theoretical guarantees of communication complexity and\nare significantly ahead of other methods for distributed variational\ninequalities. The theoretical results are confirmed by adversarial learning\nexperiments on synthetic and real datasets.\n","authors":["Aleksandr Beznosikov","Martin Takáč","Alexander Gasnikov"],"pdf_url":"https://arxiv.org/pdf/2302.07615v2.pdf","comment":"Appears in: Advances in Neural Information Processing Systems 36\n  (NeurIPS 2023)\n  (https://proceedings.neurips.cc/paper_files/paper/2023/hash/5b4a459db23e6db9be2a128380953d96-Abstract-Conference.html).\n  36 pages, 3 algorithms, 1 figure, 1 table"},{"id":"http://arxiv.org/abs/2210.06554v3","updated":"2024-03-30T14:56:26Z","published":"2022-10-12T19:47:34Z","title":"Toward the application of XAI methods in EEG-based systems","summary":"  An interesting case of the well-known Dataset Shift Problem is the\nclassification of Electroencephalogram (EEG) signals in the context of\nBrain-Computer Interface (BCI). The non-stationarity of EEG signals can lead to\npoor generalisation performance in BCI classification systems used in different\nsessions, also from the same subject. In this paper, we start from the\nhypothesis that the Dataset Shift problem can be alleviated by exploiting\nsuitable eXplainable Artificial Intelligence (XAI) methods to locate and\ntransform the relevant characteristics of the input for the goal of\nclassification. In particular, we focus on an experimental analysis of\nexplanations produced by several XAI methods on an ML system trained on a\ntypical EEG dataset for emotion recognition. Results show that many relevant\ncomponents found by XAI methods are shared across the sessions and can be used\nto build a system able to generalise better. However, relevant components of\nthe input signal also appear to be highly dependent on the input itself.\n","authors":["Andrea Apicella","Francesco Isgrò","Andrea Pollastro","Roberto Prevete"],"pdf_url":"https://arxiv.org/pdf/2210.06554v3.pdf","comment":"Accepted to be presented at XAI.it 2022 - Italian Workshop on\n  Explainable Artificial Intelligence (https://ceur-ws.org/Vol-3277/paper1.pdf)"},{"id":"http://arxiv.org/abs/2305.15938v2","updated":"2024-03-30T13:50:06Z","published":"2023-05-25T11:11:31Z","title":"First Order Methods with Markovian Noise: from Acceleration to\n  Variational Inequalities","summary":"  This paper delves into stochastic optimization problems that involve\nMarkovian noise. We present a unified approach for the theoretical analysis of\nfirst-order gradient methods for stochastic optimization and variational\ninequalities. Our approach covers scenarios for both non-convex and strongly\nconvex minimization problems. To achieve an optimal (linear) dependence on the\nmixing time of the underlying noise sequence, we use the randomized batching\nscheme, which is based on the multilevel Monte Carlo method. Moreover, our\ntechnique allows us to eliminate the limiting assumptions of previous research\non Markov noise, such as the need for a bounded domain and uniformly bounded\nstochastic gradients. Our extension to variational inequalities under Markovian\nnoise is original. Additionally, we provide lower bounds that match the oracle\ncomplexity of our method in the case of strongly convex optimization problems.\n","authors":["Aleksandr Beznosikov","Sergey Samsonov","Marina Sheshukova","Alexander Gasnikov","Alexey Naumov","Eric Moulines"],"pdf_url":"https://arxiv.org/pdf/2305.15938v2.pdf","comment":"Appears in: Advances in Neural Information Processing Systems 36\n  (NeurIPS 2023). 41 pages, 3 algorithms, 2 tables"},{"id":"http://arxiv.org/abs/2205.14375v5","updated":"2024-03-30T13:49:58Z","published":"2022-05-28T09:08:50Z","title":"WaveMix: A Resource-efficient Neural Network for Image Analysis","summary":"  We propose a novel neural architecture for computer vision -- WaveMix -- that\nis resource-efficient and yet generalizable and scalable. While using fewer\ntrainable parameters, GPU RAM, and computations, WaveMix networks achieve\ncomparable or better accuracy than the state-of-the-art convolutional neural\nnetworks, vision transformers, and token mixers for several tasks. This\nefficiency can translate to savings in time, cost, and energy. To achieve these\ngains we used multi-level two-dimensional discrete wavelet transform (2D-DWT)\nin WaveMix blocks, which has the following advantages: (1) It reorganizes\nspatial information based on three strong image priors -- scale-invariance,\nshift-invariance, and sparseness of edges -- (2) in a lossless manner without\nadding parameters, (3) while also reducing the spatial sizes of feature maps,\nwhich reduces the memory and time required for forward and backward passes, and\n(4) expanding the receptive field faster than convolutions do. The whole\narchitecture is a stack of self-similar and resolution-preserving WaveMix\nblocks, which allows architectural flexibility for various tasks and levels of\nresource availability. WaveMix establishes new benchmarks for segmentation on\nCityscapes; and for classification on Galaxy 10 DECals, Places-365, five EMNIST\ndatasets, and iNAT-mini and performs competitively on other benchmarks. Our\ncode and trained models are publicly available.\n","authors":["Pranav Jeevan","Kavitha Viswanathan","Anandu A S","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2205.14375v5.pdf","comment":"20 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.18035v2","updated":"2024-03-30T13:28:54Z","published":"2024-03-26T18:40:36Z","title":"Bidirectional Consistency Models","summary":"  Diffusion models (DMs) are capable of generating remarkably high-quality\nsamples by iteratively denoising a random vector, a process that corresponds to\nmoving along the probability flow ordinary differential equation (PF ODE).\nInterestingly, DMs can also invert an input image to noise by moving backward\nalong the PF ODE, a key operation for downstream tasks such as interpolation\nand image editing. However, the iterative nature of this process restricts its\nspeed, hindering its broader application. Recently, Consistency Models (CMs)\nhave emerged to address this challenge by approximating the integral of the PF\nODE, largely reducing the number of iterations. Yet, the absence of an explicit\nODE solver complicates the inversion process. To resolve this, we introduce the\nBidirectional Consistency Model (BCM), which learns a single neural network\nthat enables both forward and backward traversal along the PF ODE, efficiently\nunifying generation and inversion tasks within one framework. Notably, our\nproposed method enables one-step generation and inversion while also allowing\nthe use of additional steps to enhance generation quality or reduce\nreconstruction error. Furthermore, by leveraging our model's bidirectional\nconsistency, we introduce a sampling strategy that can enhance FID while\npreserving the generated image content. We further showcase our model's\ncapabilities in several downstream tasks, such as interpolation and inpainting,\nand present demonstrations of potential applications, including blind\nrestoration of compressed images and defending black-box adversarial attacks.\n","authors":["Liangchen Li","Jiajun He"],"pdf_url":"https://arxiv.org/pdf/2403.18035v2.pdf","comment":"40 pages, 25 figures"},{"id":"http://arxiv.org/abs/2401.04071v2","updated":"2024-03-30T12:05:52Z","published":"2024-01-08T18:18:02Z","title":"Fun with Flags: Robust Principal Directions via Flag Manifolds","summary":"  Principal component analysis (PCA), along with its extensions to manifolds\nand outlier contaminated data, have been indispensable in computer vision and\nmachine learning. In this work, we present a unifying formalism for PCA and its\nvariants, and introduce a framework based on the flags of linear subspaces, ie\na hierarchy of nested linear subspaces of increasing dimension, which not only\nallows for a common implementation but also yields novel variants, not explored\npreviously. We begin by generalizing traditional PCA methods that either\nmaximize variance or minimize reconstruction error. We expand these\ninterpretations to develop a wide array of new dimensionality reduction\nalgorithms by accounting for outliers and the data manifold. To devise a common\ncomputational approach, we recast robust and dual forms of PCA as optimization\nproblems on flag manifolds. We then integrate tangent space approximations of\nprincipal geodesic analysis (tangent-PCA) into this flag-based framework,\ncreating novel robust and dual geodesic PCA variations. The remarkable\nflexibility offered by the 'flagification' introduced here enables even more\nalgorithmic variants identified by specific flag types. Last but not least, we\npropose an effective convergent solver for these flag-formulations employing\nthe Stiefel manifold. Our empirical results on both real-world and synthetic\nscenarios, demonstrate the superiority of our novel algorithms, especially in\nterms of robustness to outliers on manifolds.\n","authors":["Nathan Mankovich","Gustau Camps-Valls","Tolga Birdal"],"pdf_url":"https://arxiv.org/pdf/2401.04071v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09237v2","updated":"2024-03-30T11:55:26Z","published":"2022-07-19T12:49:00Z","title":"Semi-supervised Predictive Clustering Trees for (Hierarchical)\n  Multi-label Classification","summary":"  Semi-supervised learning (SSL) is a common approach to learning predictive\nmodels using not only labeled examples, but also unlabeled examples. While SSL\nfor the simple tasks of classification and regression has received a lot of\nattention from the research community, this is not properly investigated for\ncomplex prediction tasks with structurally dependent variables. This is the\ncase of multi-label classification and hierarchical multi-label classification\ntasks, which may require additional information, possibly coming from the\nunderlying distribution in the descriptive space provided by unlabeled\nexamples, to better face the challenging task of predicting simultaneously\nmultiple class labels.\n  In this paper, we investigate this aspect and propose a (hierarchical)\nmulti-label classification method based on semi-supervised learning of\npredictive clustering trees. We also extend the method towards ensemble\nlearning and propose a method based on the random forest approach. Extensive\nexperimental evaluation conducted on 23 datasets shows significant advantages\nof the proposed method and its extension with respect to their supervised\ncounterparts. Moreover, the method preserves interpretability and reduces the\ntime complexity of classical tree-based models.\n","authors":["Jurica Levatić","Michelangelo Ceci","Dragi Kocev","Sašo Džeroski"],"pdf_url":"https://arxiv.org/pdf/2207.09237v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17584v3","updated":"2024-03-30T10:43:19Z","published":"2023-10-26T17:07:43Z","title":"A minimax optimal control approach for robust neural ODEs","summary":"  In this paper, we address the adversarial training of neural ODEs from a\nrobust control perspective. This is an alternative to the classical training\nvia empirical risk minimization, and it is widely used to enforce reliable\noutcomes for input perturbations. Neural ODEs allow the interpretation of deep\nneural networks as discretizations of control systems, unlocking powerful tools\nfrom control theory for the development and the understanding of machine\nlearning. In this specific case, we formulate the adversarial training with\nperturbed data as a minimax optimal control problem, for which we derive first\norder optimality conditions in the form of Pontryagin's Maximum Principle. We\nprovide a novel interpretation of robust training leading to an alternative\nweighted technique, which we test on a low-dimensional classification task.\n","authors":["Cristina Cipriani","Alessandro Scagliotti","Tobias Wöhrer"],"pdf_url":"https://arxiv.org/pdf/2310.17584v3.pdf","comment":"7 pages, 2 figures and 1 table. Correction of typos and improvement\n  of Section 4 (Numerics)"},{"id":"http://arxiv.org/abs/2306.03506v2","updated":"2024-03-30T10:39:13Z","published":"2023-06-06T08:52:44Z","title":"Subgraph Networks Based Contrastive Learning","summary":"  Graph contrastive learning (GCL), as a self-supervised learning method, can\nsolve the problem of annotated data scarcity. It mines explicit features in\nunannotated graphs to generate favorable graph representations for downstream\ntasks. Most existing GCL methods focus on the design of graph augmentation\nstrategies and mutual information estimation operations. Graph augmentation\nproduces augmented views by graph perturbations. These views preserve a locally\nsimilar structure and exploit explicit features. However, these methods have\nnot considered the interaction existing in subgraphs. To explore the impact of\nsubstructure interactions on graph representations, we propose a novel\nframework called subgraph network-based contrastive learning (SGNCL). SGNCL\napplies a subgraph network generation strategy to produce augmented views. This\nstrategy converts the original graph into an Edge-to-Node mapping network with\nboth topological and attribute features. The single-shot augmented view is a\nfirst-order subgraph network that mines the interaction between nodes,\nnode-edge, and edges. In addition, we also investigate the impact of the\nsecond-order subgraph augmentation on mining graph structure interactions, and\nfurther, propose a contrastive objective that fuses the first-order and\nsecond-order subgraph information. We compare SGNCL with classical and\nstate-of-the-art graph contrastive learning methods on multiple benchmark\ndatasets of different domains. Extensive experiments show that SGNCL achieves\ncompetitive or better performance (top three) on all datasets in unsupervised\nlearning settings. Furthermore, SGNCL achieves the best average gain of 6.9\\%\nin transfer learning compared to the best method. Finally, experiments also\ndemonstrate that mining substructure interactions have positive implications\nfor graph contrastive learning.\n","authors":["Jinhuan Wang","Jiafei Shao","Zeyu Wang","Shanqing Yu","Qi Xuan","Xiaoniu Yang"],"pdf_url":"https://arxiv.org/pdf/2306.03506v2.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.18539v2","updated":"2024-03-30T10:07:21Z","published":"2024-03-27T13:14:29Z","title":"Safe and Robust Reinforcement Learning: Principles and Practice","summary":"  Reinforcement Learning (RL) has shown remarkable success in solving\nrelatively complex tasks, yet the deployment of RL systems in real-world\nscenarios poses significant challenges related to safety and robustness. This\npaper aims to identify and further understand those challenges thorough the\nexploration of the main dimensions of the safe and robust RL landscape,\nencompassing algorithmic, ethical, and practical considerations. We conduct a\ncomprehensive review of methodologies and open problems that summarizes the\nefforts in recent years to address the inherent risks associated with RL\napplications.\n  After discussing and proposing definitions for both safe and robust RL, the\npaper categorizes existing research works into different algorithmic approaches\nthat enhance the safety and robustness of RL agents. We examine techniques such\nas uncertainty estimation, optimisation methodologies, exploration-exploitation\ntrade-offs, and adversarial training. Environmental factors, including\nsim-to-real transfer and domain adaptation, are also scrutinized to understand\nhow RL systems can adapt to diverse and dynamic surroundings. Moreover, human\ninvolvement is an integral ingredient of the analysis, acknowledging the broad\nset of roles that humans can take in this context.\n  Importantly, to aid practitioners in navigating the complexities of safe and\nrobust RL implementation, this paper introduces a practical checklist derived\nfrom the synthesized literature. The checklist encompasses critical aspects of\nalgorithm design, training environment considerations, and ethical guidelines.\nIt will serve as a resource for developers and policymakers alike to ensure the\nresponsible deployment of RL systems in many application domains.\n","authors":["Taku Yamagata","Raul Santos-Rodriguez"],"pdf_url":"https://arxiv.org/pdf/2403.18539v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15796v2","updated":"2024-03-30T09:55:12Z","published":"2024-03-23T11:03:31Z","title":"Understanding Emergent Abilities of Language Models from the Loss\n  Perspective","summary":"  Recent studies have put into question the belief that emergent abilities in\nlanguage models are exclusive to large models. This skepticism arises from two\nobservations: 1) smaller models can also exhibit high performance on emergent\nabilities and 2) there is doubt on the discontinuous metrics used to measure\nthese abilities. In this paper, we propose to study emergent abilities in the\nlens of pre-training loss, instead of model size or training compute. We\ndemonstrate that the models with the same pre-training loss, but different\nmodel and data sizes, generate the same performance on various downstream\ntasks. We also discover that a model exhibits emergent abilities on certain\ntasks -- regardless of the continuity of metrics -- when its pre-training loss\nfalls below a specific threshold. Before reaching this threshold, its\nperformance remains at the level of random guessing. This inspires us to\nredefine emergent abilities as those that manifest in models with lower\npre-training losses, highlighting that these abilities cannot be predicted by\nmerely extrapolating the performance trends of models with higher pre-training\nlosses.\n","authors":["Zhengxiao Du","Aohan Zeng","Yuxiao Dong","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2403.15796v2.pdf","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2402.07365v2","updated":"2024-03-30T09:29:01Z","published":"2024-02-12T01:40:31Z","title":"A Deep Learning Method for Optimal Investment Under Relative Performance\n  Criteria Among Heterogeneous Agents","summary":"  Graphon games have been introduced to study games with many players who\ninteract through a weighted graph of interaction. By passing to the limit, a\ngame with a continuum of players is obtained, in which the interactions are\nthrough a graphon. In this paper, we focus on a graphon game for optimal\ninvestment under relative performance criteria, and we propose a deep learning\nmethod. The method builds upon two key ingredients: first, a characterization\nof Nash equilibria by forward-backward stochastic differential equations and,\nsecond, recent advances of machine learning algorithms for stochastic\ndifferential games. We provide numerical experiments on two different financial\nmodels. In each model, we compare the effect of several graphons, which\ncorrespond to different structures of interactions.\n","authors":["Mathieu Laurière","Ludovic Tangpi","Xuchen Zhou"],"pdf_url":"https://arxiv.org/pdf/2402.07365v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07710v2","updated":"2024-03-30T09:20:36Z","published":"2024-02-12T15:23:19Z","title":"Optimizing Sparse Convolution on GPUs with CUDA for 3D Point Cloud\n  Processing in Embedded Systems","summary":"  In recent years, there has been a significant increase in the utilization of\ndeep learning methods, particularly convolutional neural networks (CNNs), which\nhave emerged as the dominant approach in various domains that involve\nstructured grid data, such as picture analysis and processing. Nevertheless,\nthe exponential growth in the utilization of LiDAR and 3D sensors across many\ndomains has resulted in an increased need for the analysis of 3D point clouds.\nThe utilization of 3D point clouds is crucial in various applications,\nincluding object recognition and segmentation, as they offer a spatial\ndepiction of things within a three-dimensional environment. In contrast to\nphotos, point clouds exhibit sparsity and lack a regular grid, hence posing\ndistinct processing and computational issues.\n","authors":["Chester Luo","Kevin Lai"],"pdf_url":"https://arxiv.org/pdf/2402.07710v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2310.18743v2","updated":"2024-03-30T08:53:49Z","published":"2023-10-28T15:57:58Z","title":"Optimization of utility-based shortfall risk: A non-asymptotic viewpoint","summary":"  We consider the problems of estimation and optimization of utility-based\nshortfall risk (UBSR), which is a popular risk measure in finance. In the\ncontext of UBSR estimation, we derive a non-asymptotic bound on the\nmean-squared error of the classical sample average approximation (SAA) of UBSR.\nNext, in the context of UBSR optimization, we derive an expression for the UBSR\ngradient under a smooth parameterization. This expression is a ratio of\nexpectations, both of which involve the UBSR. We use SAA for the numerator as\nwell as denominator in the UBSR gradient expression to arrive at a biased\ngradient estimator. We derive non-asymptotic bounds on the estimation error,\nwhich show that our gradient estimator is asymptotically unbiased. We\nincorporate the aforementioned gradient estimator into a stochastic gradient\n(SG) algorithm for UBSR optimization. Finally, we derive non-asymptotic bounds\nthat quantify the rate of convergence of our SG algorithm for UBSR\noptimization.\n","authors":["Sumedh Gupte","Prashanth L. A.","Sanjay P. Bhat"],"pdf_url":"https://arxiv.org/pdf/2310.18743v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.10490v2","updated":"2024-03-30T08:52:40Z","published":"2022-05-21T02:38:16Z","title":"Aligning Logits Generatively for Principled Black-Box Knowledge\n  Distillation","summary":"  Black-Box Knowledge Distillation (B2KD) is a formulated problem for\ncloud-to-edge model compression with invisible data and models hosted on the\nserver. B2KD faces challenges such as limited Internet exchange and edge-cloud\ndisparity of data distributions. In this paper, we formalize a two-step\nworkflow consisting of deprivatization and distillation, and theoretically\nprovide a new optimization direction from logits to cell boundary different\nfrom direct logits alignment. With its guidance, we propose a new method\nMapping-Emulation KD (MEKD) that distills a black-box cumbersome model into a\nlightweight one. Our method does not differentiate between treating soft or\nhard responses, and consists of: 1) deprivatization: emulating the inverse\nmapping of the teacher function with a generator, and 2) distillation: aligning\nlow-dimensional logits of the teacher and student models by reducing the\ndistance of high-dimensional image points. For different teacher-student pairs,\nour method yields inspiring distillation performance on various benchmarks, and\noutperforms the previous state-of-the-art approaches.\n","authors":["Jing Ma","Xiang Xiang","Ke Wang","Yuchuan Wu","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2205.10490v2.pdf","comment":"To appear at CVPR 2024; significantly rewritten with extra\n  experiments since the preliminary report"},{"id":"http://arxiv.org/abs/2402.07868v3","updated":"2024-03-30T08:31:53Z","published":"2024-02-12T18:29:17Z","title":"Nesting Particle Filters for Experimental Design in Dynamical Systems","summary":"  In this paper, we propose a novel approach to Bayesian experimental design\nfor non-exchangeable data that formulates it as risk-sensitive policy\noptimization. We develop the Inside-Out SMC\\textsuperscript{2} algorithm, a\nnested sequential Monte Carlo technique to infer optimal designs, and embed it\ninto a particle Markov chain Monte Carlo framework to perform gradient-based\npolicy amortization. Our approach is distinct from other amortized experimental\ndesign techniques, as it does not rely on contrastive estimators. Numerical\nvalidation on a set of dynamical systems showcases the efficacy of our method\nin comparison to other state-of-the-art strategies.\n","authors":["Sahel Iqbal","Adrien Corenflos","Simo Särkkä","Hany Abdulsamad"],"pdf_url":"https://arxiv.org/pdf/2402.07868v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.14000v3","updated":"2024-03-30T08:18:15Z","published":"2022-07-28T10:44:46Z","title":"Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study\n  on Out-of-Distribution Generalisation","summary":"  Combining deep learning with symbolic logic reasoning aims to capitalize on\nthe success of both fields and is drawing increasing attention. Inspired by\nDeepLogic, an end-to-end model trained to perform inference on logic programs,\nwe introduce IMA-GloVe-GA, an iterative neural inference network for multi-step\nreasoning expressed in natural language. In our model, reasoning is performed\nusing an iterative memory neural network based on RNN with a gated attention\nmechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES\nV1 and CONCEPTRULES V2. Experimental results show DeepLogic with gated\nattention can achieve higher test accuracy than DeepLogic and other RNN\nbaseline models. Our model achieves better out-of-distribution generalisation\nthan RoBERTa-Large when the rules have been shuffled. Furthermore, to address\nthe issue of unbalanced distribution of reasoning depths in the current\nmulti-step reasoning datasets, we develop PARARULE-Plus, a large dataset with\nmore examples that require deeper reasoning steps. Experimental results show\nthat the addition of PARARULE-Plus can increase the model's performance on\nexamples requiring deeper reasoning depths. The source code and data are\navailable at\nhttps://github.com/Strong-AI-Lab/Multi-Step-Deductive-Reasoning-Over-Natural-Language.\n","authors":["Qiming Bao","Alex Yuxuan Peng","Tim Hartill","Neset Tan","Zhenyun Deng","Michael Witbrock","Jiamou Liu"],"pdf_url":"https://arxiv.org/pdf/2207.14000v3.pdf","comment":"10 pages, 3 figures, The 2nd International Joint Conference on\n  Learning & Reasoning and 16th International Workshop on Neural-Symbolic\n  Learning and Reasoning (IJCLR-NeSy 2022)"},{"id":"http://arxiv.org/abs/2307.12872v2","updated":"2024-03-30T06:59:35Z","published":"2023-07-24T15:10:22Z","title":"Latent Code Augmentation Based on Stable Diffusion for Data-free\n  Substitute Attacks","summary":"  Since the training data of the target model is not available in the black-box\nsubstitute attack, most recent schemes utilize GANs to generate data for\ntraining the substitute model. However, these GANs-based schemes suffer from\nlow training efficiency as the generator needs to be retrained for each target\nmodel during the substitute training process, as well as low generation\nquality. To overcome these limitations, we consider utilizing the diffusion\nmodel to generate data, and propose a novel data-free substitute attack scheme\nbased on the Stable Diffusion (SD) to improve the efficiency and accuracy of\nsubstitute training. Despite the data generated by the SD exhibiting high\nquality, it presents a different distribution of domains and a large variation\nof positive and negative samples for the target model. For this problem, we\npropose Latent Code Augmentation (LCA) to facilitate SD in generating data that\naligns with the data distribution of the target model. Specifically, we augment\nthe latent codes of the inferred member data with LCA and use them as guidance\nfor SD. With the guidance of LCA, the data generated by the SD not only meets\nthe discriminative criteria of the target model but also exhibits high\ndiversity. By utilizing this data, it is possible to train the substitute model\nthat closely resembles the target model more efficiently. Extensive experiments\ndemonstrate that our LCA achieves higher attack success rates and requires\nfewer query budgets compared to GANs-based schemes for different target models.\nOur codes are available at \\url{https://github.com/LzhMeng/LCA}.\n","authors":["Mingwen Shao","Lingzhuang Meng","Yuanjian Qiao","Lixu Zhang","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2307.12872v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2310.02279v3","updated":"2024-03-30T06:29:48Z","published":"2023-10-01T05:07:17Z","title":"Consistency Trajectory Models: Learning Probability Flow ODE Trajectory\n  of Diffusion","summary":"  Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion\nmodel sampling at the cost of sample quality but lack a natural way to\ntrade-off quality for speed. To address this limitation, we propose Consistency\nTrajectory Model (CTM), a generalization encompassing CM and score-based models\nas special cases. CTM trains a single neural network that can -- in a single\nforward pass -- output scores (i.e., gradients of log-density) and enables\nunrestricted traversal between any initial and final time along the Probability\nFlow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables\nthe efficient combination of adversarial training and denoising score matching\nloss to enhance performance and achieves new state-of-the-art FIDs for\nsingle-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at\n64x64 resolution (FID 1.92). CTM also enables a new family of sampling schemes,\nboth deterministic and stochastic, involving long jumps along the ODE solution\ntrajectories. It consistently improves sample quality as computational budgets\nincrease, avoiding the degradation seen in CM. Furthermore, unlike CM, CTM's\naccess to the score function can streamline the adoption of established\ncontrollable/conditional generation methods from the diffusion community. This\naccess also enables the computation of likelihood. The code is available at\nhttps://github.com/sony/ctm.\n","authors":["Dongjun Kim","Chieh-Hsin Lai","Wei-Hsiang Liao","Naoki Murata","Yuhta Takida","Toshimitsu Uesaka","Yutong He","Yuki Mitsufuji","Stefano Ermon"],"pdf_url":"https://arxiv.org/pdf/2310.02279v3.pdf","comment":"International Conference on Learning Representations"},{"id":"http://arxiv.org/abs/2308.09552v3","updated":"2024-03-30T03:48:00Z","published":"2023-08-18T13:33:02Z","title":"Attesting Distributional Properties of Training Data for Machine\n  Learning","summary":"  The success of machine learning (ML) has been accompanied by increased\nconcerns about its trustworthiness. Several jurisdictions are preparing ML\nregulatory frameworks. One such concern is ensuring that model training data\nhas desirable distributional properties for certain sensitive attributes. For\nexample, draft regulations indicate that model trainers are required to show\nthat training datasets have specific distributional properties, such as\nreflecting diversity of the population. We propose the notion of property\nattestation allowing a prover (e.g., model trainer) to demonstrate relevant\ndistributional properties of training data to a verifier (e.g., a customer)\nwithout revealing the data. We present an effective hybrid property attestation\ncombining property inference with cryptographic mechanisms.\n","authors":["Vasisht Duddu","Anudeep Das","Nora Khayata","Hossein Yalame","Thomas Schneider","N. Asokan"],"pdf_url":"https://arxiv.org/pdf/2308.09552v3.pdf","comment":"European Symposium on Research in Computer Security (ESORICS), 2024"},{"id":"http://arxiv.org/abs/2403.19561v2","updated":"2024-03-30T03:47:25Z","published":"2024-03-28T16:46:53Z","title":"Self-Improved Learning for Scalable Neural Combinatorial Optimization","summary":"  The end-to-end neural combinatorial optimization (NCO) method shows promising\nperformance in solving complex combinatorial optimization problems without the\nneed for expert design. However, existing methods struggle with large-scale\nproblems, hindering their practical applicability. To overcome this limitation,\nthis work proposes a novel Self-Improved Learning (SIL) method for better\nscalability of neural combinatorial optimization. Specifically, we develop an\nefficient self-improved mechanism that enables direct model training on\nlarge-scale problem instances without any labeled data. Powered by an\ninnovative local reconstruction approach, this method can iteratively generate\nbetter solutions by itself as pseudo-labels to guide efficient model training.\nIn addition, we design a linear complexity attention mechanism for the model to\nefficiently handle large-scale combinatorial problem instances with low\ncomputation overhead. Comprehensive experiments on the Travelling Salesman\nProblem (TSP) and the Capacitated Vehicle Routing Problem (CVRP) with up to\n100K nodes in both uniform and real-world distributions demonstrate the\nsuperior scalability of our method.\n","authors":["Fu Luo","Xi Lin","Zhenkun Wang","Xialiang Tong","Mingxuan Yuan","Qingfu Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.19561v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00276v2","updated":"2024-03-30T03:25:51Z","published":"2023-12-01T01:25:04Z","title":"Automating Continual Learning","summary":"  General-purpose learning systems should improve themselves in open-ended\nfashion in ever-changing environments. Conventional learning algorithms for\nneural networks, however, suffer from catastrophic forgetting (CF) --\npreviously acquired skills are forgotten when a new task is learned. Instead of\nhand-crafting new algorithms for avoiding CF, we propose Automated Continual\nLearning (ACL) to train self-referential neural networks to meta-learn their\nown in-context continual (meta-)learning algorithms. ACL encodes all desiderata\n-- good performance on both old and new tasks -- into its meta-learning\nobjectives. Our experiments demonstrate that ACL effectively solves \"in-context\ncatastrophic forgetting\"; our ACL-learned algorithms outperform hand-crafted\nones, e.g., on the Split-MNIST benchmark in the replay-free setting, and\nenables continual learning of diverse tasks consisting of multiple few-shot and\nstandard image classification datasets.\n","authors":["Kazuki Irie","Róbert Csordás","Jürgen Schmidhuber"],"pdf_url":"https://arxiv.org/pdf/2312.00276v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11511v2","updated":"2024-03-30T03:01:42Z","published":"2023-12-12T05:38:55Z","title":"ComplexityNet: Increasing LLM Inference Efficiency by Learning Task\n  Complexity","summary":"  We present ComplexityNet, a streamlined language model designed for assessing\ntask complexity. This model predicts the likelihood of accurate output by\nvarious language models, each with different capabilities. Our initial\napplication of ComplexityNet involves the Mostly Basic Python Problems (MBPP)\ndataset. We pioneered the creation of the first set of labels to define task\ncomplexity. ComplexityNet achieved a notable 79% accuracy in determining task\ncomplexity, a significant improvement over the 34% accuracy of the original,\nnon fine-tuned model. Furthermore, ComplexityNet effectively reduces\ncomputational resource usage by 90% compared to using the highest complexity\nmodel, while maintaining a high code generation accuracy of 86.7%. This study\ndemonstrates that fine-tuning smaller models to categorize tasks based on their\ncomplexity can lead to a more balanced trade-off between accuracy and\nefficiency in the use of Large Language Models. Our findings suggest a\npromising direction for optimizing LLM applications, especially in\nresource-constrained environments.\n","authors":["Henry Bae","Aghyad Deeb","Alex Fleury","Kehang Zhu"],"pdf_url":"https://arxiv.org/pdf/2312.11511v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.02649v2","updated":"2024-03-30T02:49:29Z","published":"2022-08-04T13:31:11Z","title":"Visually Evaluating Generative Adversarial Networks Using Itself under\n  Multivariate Time Series","summary":"  Visually evaluating the goodness of generated Multivariate Time Series (MTS)\nare difficult to implement, especially in the case that the generative model is\nGenerative Adversarial Networks (GANs). We present a general framework named\nGaussian GANs to visually evaluate GANs using itself under the MTS generation\ntask. Firstly, we attempt to find the transformation function in the\nmultivariate Kolmogorov Smirnov (MKS) test by explicitly reconstructing the\narchitecture of GANs. Secondly, we conduct the normality test of transformed\nMST where the Gaussian GANs serves as the transformation function in the MKS\ntest. In order to simplify the normality test, an efficient visualization is\nproposed using the chi square distribution. In the experiment, we use the\nUniMiB dataset and provide empirical evidence showing that the normality test\nusing Gaussian GANs and chi sqaure visualization is effective and credible.\n","authors":["Qilong Pan"],"pdf_url":"https://arxiv.org/pdf/2208.02649v2.pdf","comment":"This is just a manuscript draft where the experiment is not evident,\n  and need to be studied further"},{"id":"http://arxiv.org/abs/2312.09788v2","updated":"2024-03-30T01:21:42Z","published":"2023-12-15T13:43:24Z","title":"Collaborating Foundation Models for Domain Generalized Semantic\n  Segmentation","summary":"  Domain Generalized Semantic Segmentation (DGSS) deals with training a model\non a labeled source domain with the aim of generalizing to unseen domains\nduring inference. Existing DGSS methods typically effectuate robust features by\nmeans of Domain Randomization (DR). Such an approach is often limited as it can\nonly account for style diversification and not content. In this work, we take\nan orthogonal approach to DGSS and propose to use an assembly of CoLlaborative\nFOUndation models for Domain Generalized Semantic Segmentation (CLOUDS). In\ndetail, CLOUDS is a framework that integrates FMs of various kinds: (i) CLIP\nbackbone for its robust feature representation, (ii) generative models to\ndiversify the content, thereby covering various modes of the possible target\ndistribution, and (iii) Segment Anything Model (SAM) for iteratively refining\nthe predictions of the segmentation model. Extensive experiments show that our\nCLOUDS excels in adapting from synthetic to real DGSS benchmarks and under\nvarying weather conditions, notably outperforming prior methods by 5.6% and\n6.7% on averaged miou, respectively. The code is available at :\nhttps://github.com/yasserben/CLOUDS\n","authors":["Yasser Benigmim","Subhankar Roy","Slim Essid","Vicky Kalogeiton","Stéphane Lathuilière"],"pdf_url":"https://arxiv.org/pdf/2312.09788v2.pdf","comment":"https://github.com/yasserben/CLOUDS ; Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2312.05264v3","updated":"2024-03-30T00:11:28Z","published":"2023-12-05T19:15:51Z","title":"All Rivers Run to the Sea: Private Learning with Asymmetric Flows","summary":"  Data privacy is of great concern in cloud machine-learning service platforms,\nwhen sensitive data are exposed to service providers. While private computing\nenvironments (e.g., secure enclaves), and cryptographic approaches (e.g.,\nhomomorphic encryption) provide strong privacy protection, their computing\nperformance still falls short compared to cloud GPUs. To achieve privacy\nprotection with high computing performance, we propose Delta, a new private\ntraining and inference framework, with comparable model performance as\nnon-private centralized training. Delta features two asymmetric data flows: the\nmain information-sensitive flow and the residual flow. The main part flows into\na small model while the residuals are offloaded to a large model. Specifically,\nDelta embeds the information-sensitive representations into a low-dimensional\nspace while pushing the information-insensitive part into high-dimension\nresiduals. To ensure privacy protection, the low-dimensional\ninformation-sensitive part is secured and fed to a small model in a private\nenvironment. On the other hand, the residual part is sent to fast cloud GPUs,\nand processed by a large model. To further enhance privacy and reduce the\ncommunication cost, Delta applies a random binary quantization technique along\nwith a DP-based technique to the residuals before sharing them with the public\nplatform. We theoretically show that Delta guarantees differential privacy in\nthe public environment and greatly reduces the complexity in the private\nenvironment. We conduct empirical analyses on CIFAR-10, CIFAR-100 and ImageNet\ndatasets and ResNet-18 and ResNet-34, showing that Delta achieves strong\nprivacy protection, fast training, and inference without significantly\ncompromising the model utility.\n","authors":["Yue Niu","Ramy E. Ali","Saurav Prakash","Salman Avestimehr"],"pdf_url":"https://arxiv.org/pdf/2312.05264v3.pdf","comment":"Camera-ready for CVPR 2024"},{"id":"http://arxiv.org/abs/2309.10298v2","updated":"2024-03-30T00:11:04Z","published":"2023-09-19T04:03:42Z","title":"Learning Orbitally Stable Systems for Diagrammatically Teaching","summary":"  Diagrammatic Teaching is a paradigm for robots to acquire novel skills,\nwhereby the user provides 2D sketches over images of the scene to shape the\nrobot's motion. In this work, we tackle the problem of teaching a robot to\napproach a surface and then follow cyclic motion on it, where the cycle of the\nmotion can be arbitrarily specified by a single user-provided sketch over an\nimage from the robot's camera. Accordingly, we contribute the Stable\nDiffeomorphic Diagrammatic Teaching (SDDT) framework. SDDT models the robot's\nmotion as an Orbitally Asymptotically Stable (O.A.S.) dynamical system that\nlearns to stablize based on a single diagrammatic sketch provided by the user.\nThis is achieved by applying a \\emph{diffeomorphism}, i.e. a differentiable and\ninvertible function, to morph a known O.A.S. system. The parameterised\ndiffeomorphism is then optimised with respect to the Hausdorff distance between\nthe limit cycle of our modelled system and the sketch, to produce the desired\nrobot motion. We provide novel theoretical insight into the behaviour of the\noptimised system and also empirically evaluate SDDT, both in simulation and on\na quadruped with a mounted 6-DOF manipulator. Results show that we can\ndiagrammatically teach complex cyclic motion patterns with a high degree of\naccuracy.\n","authors":["Weiming Zhi","Tianyi Zhang","Matthew Johnson-Roberson"],"pdf_url":"https://arxiv.org/pdf/2309.10298v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2403.19002v2","updated":"2024-03-30T14:00:27Z","published":"2024-03-27T20:52:30Z","title":"Robust Active Speaker Detection in Noisy Environments","summary":"  This paper addresses the issue of active speaker detection (ASD) in noisy\nenvironments and formulates a robust active speaker detection (rASD) problem.\nExisting ASD approaches leverage both audio and visual modalities, but\nnon-speech sounds in the surrounding environment can negatively impact\nperformance. To overcome this, we propose a novel framework that utilizes\naudio-visual speech separation as guidance to learn noise-free audio features.\nThese features are then utilized in an ASD model, and both tasks are jointly\noptimized in an end-to-end framework. Our proposed framework mitigates residual\nnoise and audio quality reduction issues that can occur in a naive cascaded\ntwo-stage framework that directly uses separated speech for ASD, and enables\nthe two tasks to be optimized simultaneously. To further enhance the robustness\nof the audio features and handle inherent speech noises, we propose a dynamic\nweighted loss approach to train the speech separator. We also collected a\nreal-world noise audio dataset to facilitate investigations. Experiments\ndemonstrate that non-speech audio noises significantly impact ASD models, and\nour proposed approach improves ASD performance in noisy environments. The\nframework is general and can be applied to different ASD approaches to improve\ntheir robustness. Our code, models, and data will be released.\n","authors":["Siva Sai Nagender Vasireddy","Chenxu Zhang","Xiaohu Guo","Yapeng Tian"],"pdf_url":"https://arxiv.org/pdf/2403.19002v2.pdf","comment":"15 pages, 5 figures"},{"id":"http://arxiv.org/abs/2309.01327v2","updated":"2024-03-30T06:50:28Z","published":"2023-09-04T03:06:04Z","title":"Can I Trust Your Answer? Visually Grounded Video Question Answering","summary":"  We study visually grounded VideoQA in response to the emerging trends of\nutilizing pretraining techniques for video-language understanding.\nSpecifically, by forcing vision-language models (VLMs) to answer questions and\nsimultaneously provide visual evidence, we seek to ascertain the extent to\nwhich the predictions of such techniques are genuinely anchored in relevant\nvideo content, versus spurious correlations from language or irrelevant visual\ncontext. Towards this, we construct NExT-GQA -- an extension of NExT-QA with\n10.5$K$ temporal grounding (or location) labels tied to the original QA pairs.\nWith NExT-GQA, we scrutinize a series of state-of-the-art VLMs. Through\npost-hoc attention analysis, we find that these models are extremely weak in\nsubstantiating the answers despite their strong QA performance. This exposes\nthe limitation of current VLMs in making reliable predictions. As a remedy, we\nfurther explore and propose a grounded-QA method via Gaussian mask optimization\nand cross-modal learning. Experiments with different backbones demonstrate that\nthis grounding mechanism improves both grounding and QA. With these efforts, we\naim to push towards trustworthy VLMs in VQA systems. Our dataset and code are\navailable at https://github.com/doc-doc/NExT-GQA.\n","authors":["Junbin Xiao","Angela Yao","Yicong Li","Tat Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2309.01327v2.pdf","comment":"Accepted to CVPR'24. (Compared with preprint version, we mainly\n  improve the presentation, discuss more related works, and extend experiments\n  in Appendix.)"}]}}